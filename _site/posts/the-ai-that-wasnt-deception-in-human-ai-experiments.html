<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.25">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Jason Collins">
<meta name="dcterms.date" content="2025-11-14">

<title>The AI that wasn’t: deception in human-AI experiments – Jason Collins blog</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-7b89279ff1a6dce999919e0e67d4d9ec.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-9be722aa83fbcc428616ae682c30a50a.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script src="../site_libs/quarto-contrib/glightbox/glightbox.min.js"></script>
<link href="../site_libs/quarto-contrib/glightbox/glightbox.min.css" rel="stylesheet">
<link href="../site_libs/quarto-contrib/glightbox/lightbox.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-C50MEPDMZ9"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-C50MEPDMZ9', { 'anonymize_ip': true});
</script>
<meta name="quarto:status" content="draft">


<link rel="stylesheet" href="../styles.css">
<meta property="og:title" content="The AI that wasn’t: deception in human-AI experiments – Jason Collins blog">
<meta property="og:description" content="Behavioural economics, data science and artificial intelligence.">
<meta property="og:image" content="https://www.jasoncollins.blog/posts/img/the-ai-that-wasnt-deception-in-human-ai-experiments/bucinca-et-al-2021-fig-1.png">
<meta property="og:site_name" content="Jason Collins blog">
<meta property="og:image:height" content="384">
<meta property="og:image:width" content="620">
<meta name="twitter:title" content="The AI that wasn’t: deception in human-AI experiments – Jason Collins blog">
<meta name="twitter:description" content="Behavioural economics, data science and artificial intelligence.">
<meta name="twitter:image" content="https://www.jasoncollins.blog/posts/img/the-ai-that-wasnt-deception-in-human-ai-experiments/bucinca-et-al-2021-fig-1.png">
<meta name="twitter:image-height" content="384">
<meta name="twitter:image-width" content="620">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-fixed fullcontent quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top"><div id="quarto-draft-alert" class="alert alert-warning"><i class="bi bi-pencil-square"></i>Draft</div>
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../index.html" class="navbar-brand navbar-brand-logo">
    </a>
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">Jason Collins blog</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../research.html"> 
<span class="menu-text">Research</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../subscribe.html"> 
<span class="menu-text">Subscribe</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../teaching.html"> 
<span class="menu-text">Teaching</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="../index.xml"> <i class="bi bi-rss" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">The AI that wasn’t: deception in human-AI experiments</h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Jason Collins </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">November 14, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<p>Experimental economists have a strong norm against deception in experiments. Here’s Andreas <span class="citation" data-cites="ortmann2019">Ortmann (<a href="#ref-ortmann2019" role="doc-biblioref">2019</a>)</span>:</p>
<blockquote class="blockquote">
<p>I open with a statement of fact: deception in the form of acts of commission, while still widespread in (social) psychology and business sciences such as marketing, is factually banned in laboratory experimental economics. This ban is not legislated but, until recently, was a social norm that the community of experimental economists had been able to reasonably enforce.</p>
</blockquote>
<p>For some experimental economics journals, deception is grounds for desk rejection.</p>
<p>There are a few reasons for this norm. An important one is that if participants suspect they are being deceived or provided incorrect information, they will alter their behavioural responses and undermine the experimental results. An example of this is the famous <a href="https://en.wikipedia.org/wiki/Asch_conformity_experiments">Solomon Asch conformity experiments</a>, in which those who believed they were being deceived were less likely to conform with the group.</p>
<p>There is also an externality from deception to other experiments. After being deceived in one experiment - as they often discover when promises aren’t delivered or in experimental debriefs - participants will be less trusting in future experiments, again changing their behaviour. Today where many experiments are run with public subject pools of people who are completing many experiments and surveys, regular deception could shift their expectations.</p>
<p>There is no norm against deception in psychology. Deception is common and argued to be required for many experiments.</p>
<p>There is also no such norm in human-computer interaction research, as I have discovered the last couple of years. I am not talking about deception by omission: not telling people everything about the experiment. Rather, I mean acts of commission, such as telling people things that are not true.</p>
<p>One common form of deception is that there is no actual AI. Rather, the participants are shown hand-crafted responses. I have seen this called “synthetic AI”.</p>
<p>It also seems common for experimenters to tell the occasional lie to generate the desired response.</p>
<p>Below are examples of each and a few questions in response. I don’t have a firm view on some of this deception except for having a general queasiness and desire to reduce this, so the below is more observations than hardened views or argument.</p>
<section id="synthetic-ai" class="level2">
<h2 class="anchored" data-anchor-id="synthetic-ai">Synthetic-AI</h2>
<p>To run an experiment on human responses to an AI, you want an AI. But that’s a challenge. For one, you need to procure an AI, or develop an AI to train on the task or data. Depending on the nature of the AI, you might also design an interface. And apart from being a lot of work, there’s no guarantee that your AI will have the features you desire, such as a certain error rate or error boundary.</p>
<p>In economics, this challenge has typically led to the use of statistical prediction tasks. Given a dataset about, say, student achievement, you develop a statistical model (called an AI) to predict achievement based on other features of the student. Apart from the easy availability of datasets to create these tasks (such as those available on <a href="https://www.kaggle.com/datasets">Kaggle</a> or in repositories such as the <a href="https://archive.ics.uci.edu/">UC Irvine Machine Learning Repository</a>), developing statistical models is a bread and butter task for an economist. You can often train a model on these relatively well-structured datasets in a few lines of code.</p>
<p>An interesting feature of these models is that they typically outperform human judgment by a considerable margin. The optimal interaction is that the human hands off the decision to the AI. That makes these models great for algorithm aversion studies, but provides somewhat limited scope to examine collaborative decision making.</p>
<p>If you want a weaker model you might withhold data from the model (as one done in a recent paper <a href="../posts/is-following-ai-advice-anchoring-bias.html">I posted about</a> on anchoring). This can introduce asymmetric information by giving the human something the model doesn’t have. Ultimately, however, statistical prediction is only a subset of the space in which you might want to study human-AI interaction. That moves us into more complex domains where it’s not so easy to develop a model.</p>
<p>In experiments coming from the human-computer interaction field, you do see statistical models, but often the experiments require a richer environment. And that’s where they turn to deception via synthetic AI.</p>
<p>One example of synthetic AI is in work by <span class="citation" data-cites="buçinca2021">Buçinca et al. (<a href="#ref-buçinca2021" role="doc-biblioref">2021</a>)</span> on whether triggering slow thinking can increase engagement with explanations of AI recommendations. They created a task where participants were shown a picture of a meal and information on the main ingredients, and asked what ingredients to substitute to make it low-carb while maintaining the original flavour.</p>
<div id="fig-bucinca_et_al_2021_fig_1" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-fig" id="fig-bucinca_et_al_2021_fig_1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: <span class="citation" data-cites="buçinca2021">Buçinca et al. (<a href="#ref-buçinca2021" role="doc-biblioref">2021</a>)</span>, Figure 1
</figcaption>
<div aria-describedby="fig-bucinca_et_al_2021_fig_1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="img/the-ai-that-wasnt-deception-in-human-ai-experiments/bucinca-et-al-2021-fig-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="Figure&nbsp;1: @buçinca2021, Figure 1"><img src="img/the-ai-that-wasnt-deception-in-human-ai-experiments/bucinca-et-al-2021-fig-1.png" class="img-fluid figure-img"></a>
</div>
</figure>
</div>
<p>And how was this AI developed? They write:</p>
<blockquote class="blockquote">
<p>We designed a simulated AI for the experiment, which had 75% accuracy of correctly recognizing the ingredient with the highest carbohydrate impact in the image of the meal. Note that we did not train an actual machine learning model for this task because we wanted to have control over the type and prevalence of error the AI would make.</p>
</blockquote>
<p>This is one of the more direct statements that the AI was fabricated, but even where less explicitly stated, it is common practice.</p>
<p>The question is then: what do participants believe of this scenario? Do they believe that an AI is actually delivering these suggestions and associated explanations? To the extent they don’t, does this affect their likelihood of following the AI recommendation?</p>
<p>Some people in the human-computer interaction field do go the extra yard of developing a more complex model - and full praise to them. As an example, <span class="citation" data-cites="lai2022">Lai et al. (<a href="#ref-lai2022" role="doc-biblioref">2022</a>)</span> developed neural architecture to classify Wikipedia comments and Reddit hate speech. The particular architecture they used allowed identification of the rationale, communicated through highlights of the relevant words in the text. It looks like a lot of effort, but they’re not deceiving participants. It also looks like a more ecologically valid experimental setup. That would boost my confidence in the results.</p>
<div id="fig-lai_et_al_2022_table_1" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-fig" id="fig-lai_et_al_2022_table_1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: <span class="citation" data-cites="lai2022">Lai et al. (<a href="#ref-lai2022" role="doc-biblioref">2022</a>)</span>, Table 1
</figcaption>
<div aria-describedby="fig-lai_et_al_2022_table_1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="img/the-ai-that-wasnt-deception-in-human-ai-experiments/lai-et-al-2022-table-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-2" title="Figure&nbsp;2: @lai2022, Table 1"><img src="img/the-ai-that-wasnt-deception-in-human-ai-experiments/lai-et-al-2022-table-1.png" class="img-fluid figure-img"></a>
</div>
</figure>
</div>
<p>Sometimes to avoid deception, people come up with creative ways to describe what is being provided to participants. <span class="citation" data-cites="logg2019">Logg et al. (<a href="#ref-logg2019" role="doc-biblioref">2019</a>)</span> wanted to test whether algorithm aversion was due to aversion to algorithms themselves or an aversion to external advice. As a result, they wanted to deliver identical predictions to experimental participants (to maintain experimental control) while being able to describe it as coming from either an algorithm or a human.</p>
<p>In experiment 1, 202 participants were asked to estimate the weight of a person in a photo. Everyone received advice, described as coming from either an algorithm or a person, that the person was 163lb, based on estimates of 415 people in another experiment. (The person weighed 164lb.) The precise wording was either:</p>
<blockquote class="blockquote">
<p>An algorithm ran calculations based on estimates of participants from a past study. The output that the algorithm computed as an estimate was: 163 pounds.</p>
</blockquote>
<blockquote class="blockquote">
<p>The average estimate of participants from a past experiment was: 163 pounds.</p>
</blockquote>
<p>Both statements are technically true, though calling averaging an algorithm is generous.</p>
<p>The word “algorithm” is stretched even more in Experiment 1C, a task to estimate a person’s attractiveness from another person’s perspective based on a text description of that person. The wording in each condition was:</p>
<blockquote class="blockquote">
<p>An algorithm estimated this (wo)man’s attractiveness (humor/ enjoyableness) from Mike’s (Julia’s) perspective. The algorithm’s estimate was: X</p>
</blockquote>
<blockquote class="blockquote">
<p>In another study, 48 people estimated this (wo)man’s attractiveness (humor/enjoyableness) from Mike’s (Julia’s) perspective. Their estimate was: X</p>
</blockquote>
<p><span class="citation" data-cites="logg2019">Logg et al. (<a href="#ref-logg2019" role="doc-biblioref">2019</a>)</span> described their choice of procedure as allowing them to avoid deception. I am not convinced and I suspect experimental economists wouldn’t agree.</p>
<p>The word “algorithm” triggers expectations that don’t match reality. If someone told me an algorithm was estimating attractiveness, I would assume something more sophisticated than averaging estimates from humans. I’d likely assume a text analysis model.</p>
<p>One ameliorating factor is that this experiment ran pre-ChatGPT in 2019. Participants might not immediately imagine a text analysis algorithm. But in 2025 I’d assume exactly that and would respond based on that assumption. This raises the question: if participants couldn’t picture a text analysis algorithm in 2019, what were they assuming?</p>
<p>An interesting element of the most recent literature is that we are seeing much more actual use of AI, in the form of ChatGPT and other LLM models. It’s finally easy to get an “AI” for almost any task with low effort. The deception is fading away, but that is likely coming at the cost of control. I haven’t seen many ChatGPT experiments that tell me something useful about human-AI interaction or that isn’t already out of date by the time it is released. But that’s a topic for a different post.</p>
</section>
<section id="straight-out-porkie-pies" class="level2">
<h2 class="anchored" data-anchor-id="straight-out-porkie-pies">Straight-out porkie pies</h2>
<p>I sympathise with the synthetic AI approach, albeit not being completely comfortable. Coming up with experimental tasks where the AI really is an AI performing the task you say it is is tough. For many specific questions, it’s beyond the technical capability of the experimenters.</p>
<p>But the deception in the human-computer interaction literature tends not to be limited to synthetic AI. The AI is often completely misrepresented.</p>
<p>Here’s an example from Rastogi et al (2022), who examined whether they could de-anchor experimental participants from AI advice. I <a href="../posts/is-following-ai-advice-anchoring-bias.html">posted about this paper last month</a>.</p>
<p>The authors write:</p>
<blockquote class="blockquote">
<p>To induce anchoring bias, the participant was informed at the start of the training section that the AI model was 85% accurate (we carefully chose the training trials to ensure that the AI was indeed 85% accurate over these trials), while the model’s actual accuracy is 70.8% over the entire training set and 66.5% over the test set. Since our goal is to induce anchoring bias and the training time is short, we stated a high AI accuracy.</p>
</blockquote>
<p>The training set they reference is the data they trained the model on. Effectively, they told people the AI had an accuracy that it didn’t. The training trials would have affirmed this belief.</p>
<p>However, they go on to state:</p>
<blockquote class="blockquote">
<p>[T]his disparity between stated accuracy (85%) and true accuracy (70.8%) is realistic if there is a distribution shift between the training and the test set, which would imply that the humans’ trust in AI is misplaced. In addition to stating AI accuracy at the beginning, we informed the participants about the AI prediction for each training trial after they have attempted it so that they can learn about AI’s performance first-hand.</p>
</blockquote>
<p>Deception of this nature undermines the experiment. The “anchoring” here is justified given what they were told and observed. You can’t study anchoring bias when you’ve deliberately created the conditions for justified trust.</p>
<p>Here are a few more examples. <span class="citation" data-cites="yin2024">Yin et al. (<a href="#ref-yin2024" role="doc-biblioref">2024</a>)</span> told participants that responses came from either a human or AI, irrespective of the actual source, in their analysis of whether people “feel heard”. <span class="citation" data-cites="longoni2023">Longoni et al. (<a href="#ref-longoni2023" role="doc-biblioref">2023</a>)</span> used articles on AI failures to create news stories in which either an AI or a human caused the problem. <span class="citation" data-cites="khadpe2020">Khadpe et al. (<a href="#ref-khadpe2020" role="doc-biblioref">2020</a>)</span> used human workers to simulate chatbots, with participants told that these “AI” workers were modelled on one of a toddler, middle schooler, young student, recent graduate or trained professional.</p>
<p>In each case you’re relying on there being no AI or human “smell” to the responses that doesn’t match the purported communication. You’re relying on them believing your statement even when the evidence in front of them might point elsewhere (or worse, treating their trust in you as an error).</p>
<p>There is also little thought given to contamination of the pool. The participants in most of these studies are workers from panels such as Amazon Turk, who spend their day doing studies like these. How many times do they read an experimental debrief that the wool has been pulled over their eyes? What does that do to their future interactions?</p>
<p>I don’t know the answers to these questions, but these deceptive practices introduce another layer of doubt as to whether we should trust these results (over and above that I described in a previous post on the poor experimental practices in this space).</p>
</section>
<section id="what-to-do" class="level2">
<h2 class="anchored" data-anchor-id="what-to-do">What to do?</h2>
<p>The human-computer / human-AI literature is shaky. As I noted in a <a href="../posts/why-i-dont-trust-most-human-ai-interaction-experimental-research.html">previous post</a>, experimental practices look like those in psychology circa 2005: small sample sizes, no pre-registration, no data openness. All the hallmarks of a replication crisis.</p>
<p>At the same time, the field is flooded. I have some google alerts and AI-run literature searches that deliver to me dozens of papers in this space every week, and I’m not even catching all of them. Most are trivial, and even when they have an interesting or useful hypothesis, I typically don’t trust them.</p>
<p>As a result, people should slow down, invest in their experimental designs and AIs, and make them as robust as possible. Most of the deception could be eliminated with a bit of thought and work and we’d get a much more valuable literature.</p>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" data-line-spacing="2" role="list">
<div id="ref-buçinca2021" class="csl-entry" role="listitem">
Buçinca, Z., Malaya, M. B., and Gajos, K. Z. (2021). To trust or to think: Cognitive forcing functions can reduce overreliance on AI in AI-assisted decision-making. <em>Proceedings of the ACM on Human-Computer Interaction</em>, <em>5</em>(CSCW1), 188:1188:21. <a href="https://doi.org/10.1145/3449287">https://doi.org/10.1145/3449287</a>
</div>
<div id="ref-khadpe2020" class="csl-entry" role="listitem">
Khadpe, P., Krishna, R., Fei-Fei, L., Hancock, J. T., and Bernstein, M. S. (2020). Conceptual Metaphors Impact Perceptions of Human-AI Collaboration. <em>Proceedings of the ACM on Human-Computer Interaction</em>, <em>4</em>(CSCW2), 1–26. <a href="https://doi.org/10.1145/3415234">https://doi.org/10.1145/3415234</a>
</div>
<div id="ref-lai2022" class="csl-entry" role="listitem">
Lai, V., Carton, S., Bhatnagar, R., Liao, Q. V., Zhang, Y., …. (2022). Human-AI collaboration via conditional delegation: A case study of content moderation. <em>arXiv</em>. <a href="https://doi.org/10.48550/arXiv.2204.11788">https://doi.org/10.48550/arXiv.2204.11788</a>
</div>
<div id="ref-logg2019" class="csl-entry" role="listitem">
Logg, J. M., Minson, J. A., and Moore, D. A. (2019). Algorithm appreciation: People prefer algorithmic to human judgment. <em>Organizational Behavior and Human Decision Processes</em>, <em>151</em>, 90–103. <a href="https://doi.org/10.1016/j.obhdp.2018.12.005">https://doi.org/10.1016/j.obhdp.2018.12.005</a>
</div>
<div id="ref-longoni2023" class="csl-entry" role="listitem">
Longoni, C., Cian, L., and Kyung, E. J. (2023). Algorithmic Transference: People Overgeneralize Failures of AI in the Government. <em>Journal of Marketing Research</em>, <em>60</em>(1), 170–188. <a href="https://doi.org/10.1177/00222437221110139">https://doi.org/10.1177/00222437221110139</a>
</div>
<div id="ref-ortmann2019" class="csl-entry" role="listitem">
Ortmann, A. (2019). <em>Deception</em> (A. Schram and A. Ule, Eds.). Edward Elgar Publishing. <a href="https://china.elgaronline.com/view/edcoll/9781788110556/9781788110556.00009.xml">https://china.elgaronline.com/view/edcoll/9781788110556/9781788110556.00009.xml</a>
</div>
<div id="ref-yin2024" class="csl-entry" role="listitem">
Yin, Y., Jia, N., and Wakslak, C. J. (2024). AI can help people feel heard, but an AI label diminishes this impact. <em>Proceedings of the National Academy of Sciences</em>, <em>121</em>(14), e2319112121. <a href="https://doi.org/10.1073/pnas.2319112121">https://doi.org/10.1073/pnas.2319112121</a>
</div>
</div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/www\.jasoncollins\.blog");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
            // target, if specified
            link.setAttribute("target", "_blank");
            if (link.getAttribute("rel") === null) {
              link.setAttribute("rel", "noopener");
            }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<input type="hidden" id="giscus-base-theme" value="light">
<input type="hidden" id="giscus-alt-theme" value="light">
<script>
  function loadGiscus() {
    // Function to get the theme based on body class
    const getTheme = () => {
      let baseTheme = document.getElementById('giscus-base-theme').value;
      let altTheme = document.getElementById('giscus-alt-theme').value;
      return document.body.classList.contains('quarto-dark') ? altTheme : baseTheme;
    };
    const script = document.createElement("script");
    script.src = "https://giscus.app/client.js";
    script.async = true;
    script.dataset.repo = "jasonacollins/jasoncollinsblog";
    script.dataset.repoId = "R_kgDOIbjYgQ";
    script.dataset.category = "Announcements";
    script.dataset.categoryId = "DIC_kwDOIbjYgc4CSiuB";
    script.dataset.mapping = "pathname";
    script.dataset.reactionsEnabled = "0";
    script.dataset.emitMetadata = "0";
    script.dataset.inputPosition = "bottom";
    script.dataset.theme = getTheme();
    script.dataset.lang = "en";
    script.crossOrigin = "anonymous";
    // Append the script to the desired div instead of at the end of the body
    document.getElementById("quarto-content").appendChild(script);
  }
  loadGiscus();
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
      <ul class="footer-items list-unstyled">
    <li class="nav-item">
    <a class="nav-link" href="https://creativecommons.org/licenses/by/4.0/">
<p>Copyright: CC-BY</p>
</a>
  </li>  
</ul>
    </div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>
<script>var lightboxQuarto = GLightbox({"closeEffect":"zoom","descPosition":"bottom","loop":false,"openEffect":"zoom","selector":".lightbox"});
(function() {
  let previousOnload = window.onload;
  window.onload = () => {
    if (previousOnload) {
      previousOnload();
    }
    lightboxQuarto.on('slide_before_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      const href = trigger.getAttribute('href');
      if (href !== null) {
        const imgEl = window.document.querySelector(`a[href="${href}"] img`);
        if (imgEl !== null) {
          const srcAttr = imgEl.getAttribute("src");
          if (srcAttr && srcAttr.startsWith("data:")) {
            slideConfig.href = srcAttr;
          }
        }
      } 
    });
  
    lightboxQuarto.on('slide_after_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(slideNode);
      }
    });
  
  };
  
})();
          </script>




</body></html>