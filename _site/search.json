[
  {
    "objectID": "writing.html",
    "href": "writing.html",
    "title": "Writing",
    "section": "",
    "text": "Writing\nApart from this blog and my academic publications, I have written for various other venues. Below is a selection:\n\n2022\n“We don’t have a hundred biases, we have the wrong model”, Works in progress\n\n\n2019\n“Principles for the Application of Human Intelligence”, Behavioral Scientist. Discussed in The Guardian.\n\n\n2018\n“Simple heuristics that make algorithms smart”, Behavioral Scientist\n“When Everything Looks Like a Nail: Building Better “Behavioral Economics” Teams”, Behavioral Scientist\n\n\n2017\n“What to do when algorithms rule”, Behavioral Scientist\n“Rationalizing the “irrational”, Behavioral Scientist\n“Don’t touch the computer”, Behavioral Scientist\n\n\n2015\n“It’s time to end the demographic pessimism”, The Drum, ABC\n\n\n2013\n“Sex in the Economy” Australasian Science Vol. 34, Issue 6 (pdf)\nBE NOTIFIED OF NEW POSTS VIA EMAIL"
  },
  {
    "objectID": "subscribe.html",
    "href": "subscribe.html",
    "title": "Subscribe",
    "section": "",
    "text": "BE NOTIFIED OF NEW POSTS VIA EMAIL"
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research",
    "section": "",
    "text": "My research and writing tend to revolve around the following themes:\nBelow is my academic work (largely neglected between the completion of my PhD in 2015 and my return to academia in 2022). For my non-academic writing, see here."
  },
  {
    "objectID": "research.html#publications",
    "href": "research.html#publications",
    "title": "Research",
    "section": "Publications",
    "text": "Publications\n\n2025\nAltinger, Maher, Jones, Collins, Linder, Bell, Lin, Tracy, Boroumand and Traeger (2025). “Multiple Suggested Care Alternatives and Decision-Making of Primary Care Physicians: A Randomized Clinical Trial”, JAMA Network Open, 8(11), e2542949.\n\n\n2024\nCollins (2024) “Optimally Irrational: The Good Reasons we Behave the Way we Do, by Lionel Page (Cambridge University Press, Cambridge, 2022), pp. 322.”, Economic Record, 100(329), 271-274 (ungated pdf; preprint pdf)\n\n\n2019\nCollins and Page (2019) “The heritability of fertility makes world population stabilization unlikely in the foreseeable future” Evolution & Human Behavior 40(1), 105-111 (pdf) (post). Discussion at Forbes and Institute for Family Studies.\n\n\n2016\nBaer, Collins, Maalaps and den Boer (2016) “Sperm use economy of honeybee (Apis mellifera) queens” (2016) Ecology and Evolution 6(9), 2877-2885 (pdf) (post)\nCollins, Baer and Weber (2016) “Economics in Evolutionary Biology: A Review” (2016) Economic Record 92(297), 291-312 (pdf) (post)\n\n\n2015\nCollins, Baer and Weber (2015) “Sexual Selection, Conspicuous Consumption and Economic Growth” (2015) Journal of Bioeconomics 17(2), 189-206 (pdf) (post). Discussion in The Times, The Daily Mail, The Huffington Post, The Conversation, Stumbling and Mumbling, and in The Wall Street Journal (Matt Ridley). And Paul Frijters has prepared a critique of an earlier version, which you can find here.\n\n\n2014\nCollins, Baer and Weber (2014) “Economic Growth and Evolution: Parental Preferences for Quality and Quantity of Offspring” (2014) Macroeconomic Dynamics 18, 1773-1796 (pdf) (post)"
  },
  {
    "objectID": "research.html#working-papers-in-progress",
    "href": "research.html#working-papers-in-progress",
    "title": "Research",
    "section": "Working papers (in progress)",
    "text": "Working papers (in progress)\nBrodeur et al. (2024) Mass Reproducibility and Replicability: A New Hope, I4R Discussion Paper Series #107, Institute for Replication\nCollins, Baer and Weber, (2013) Population, Technological Progress and the Evolution of Innovative Potential (with Boris Baer and Juerg Weber) (post)"
  },
  {
    "objectID": "research.html#working-papers-inactive",
    "href": "research.html#working-papers-inactive",
    "title": "Research",
    "section": "Working papers (inactive)",
    "text": "Working papers (inactive)\nCollins, Denham, Du and Waller (2024) “A Comment on “Influence Motives in Social Signaling: Evidence from COVID-19 Vaccinations in Germany” I4R Discussion Paper Series #139) (pdf)\nAlbrecht, Collins, Gauriot and Wu (2023) “A Comment on Alesina, Miano and Stancheva (2023)”, I4R Discussion Paper Series #40 (pdf)\nCollins and Richards (2013) Evolution, Fertility and the Ageing Population UWA Discussion Papers in Economics 13.02 (post)"
  },
  {
    "objectID": "research.html#phd",
    "href": "research.html#phd",
    "title": "Research",
    "section": "PhD",
    "text": "PhD\nI completed a PhD at the University of Western Australia. My thesis can be downloaded here, although most of the chapters that make up the thesis were published and are listed above. You can access the examiner reports here.\nBE NOTIFIED OF NEW POSTS VIA EMAIL"
  },
  {
    "objectID": "posts/what-can-we-infer-about-someone-who-rejects-a-5050-bet-to-win-110-or-lose-100-the-rabin-paradox-explored.html",
    "href": "posts/what-can-we-infer-about-someone-who-rejects-a-5050-bet-to-win-110-or-lose-100-the-rabin-paradox-explored.html",
    "title": "What can we infer about someone who rejects a 50:50 bet to win $110 or lose $100? The Rabin paradox explored",
    "section": "",
    "text": "Consider the following claim:\nRisk aversion is the concept that we prefer certainty to a gamble with the same expected value. For example, a risk-averse person would prefer $100 for certain over a 50-50 gamble between $0 and $200, which has an expected value of $100. The higher their risk aversion, the less they would value the 50:50 bet. They would also be willing to reject some positive expected value bets.\nLoss aversion is the concept that losses loom larger than gains. If the loss is weighted more heavily than the gain - it is often said that losses hurt twice as much as gains bring us joy - then this could also explain the decision to reject a 50:50 bet of the type above. Loss aversion is distinct from risk aversion as its full force applies to the first dollar on either side of the reference point from which the person is assessing the change (and at which point risk aversion should be negligible).\nSo, do we need loss aversion to explain the rejection of this bet, or does risk aversion suffice?\nOne typical response to the above claim is loosely based on the Rabin Paradox, which comes from a paper published in 2000 by Matthew Rabin:\nFor the remainder of this post I am going to pull apart Rabin’s argument from his justifiably famous paper Risk Aversion and Expected-Utility Theory: A Calibration Theorem (pdf). A more readable version of this argument was also published in 2001 in an article by Rabin and Richard Thaler.\nTo understand Rabin’s point, I have worked through the math in his paper. You can see my mathematical workings in an Appendix at the bottom of this post. There were quite a few minor errors in the paper - and some major errors in the formulas - but I believe I’ve captured the crux of the argument. (I’d be grateful for some second opinions on this).\nI started working through these two articles with the impression that Rabin’s argument was a fatal blow to the idea that expected utility theory accurately describes the rejection of bets such as that above. I would have been comfortable making the above response. However, after playing with the numbers and developing a better understanding of the paper, I would say that the above response is not strictly true. Rabin’s paper makes an important point, but it is far from a fatal blow by itself. (That fatal blow does come, just not solely from here.)"
  },
  {
    "objectID": "posts/what-can-we-infer-about-someone-who-rejects-a-5050-bet-to-win-110-or-lose-100-the-rabin-paradox-explored.html#describing-rabins-argument",
    "href": "posts/what-can-we-infer-about-someone-who-rejects-a-5050-bet-to-win-110-or-lose-100-the-rabin-paradox-explored.html#describing-rabins-argument",
    "title": "What can we infer about someone who rejects a 50:50 bet to win $110 or lose $100? The Rabin paradox explored",
    "section": "Describing Rabin’s argument",
    "text": "Describing Rabin’s argument\nRabin’s argument starts with a simple bet: suppose you are offered a 50:50 bet to win $110 or lose $100, and you turn it down. Suppose further that you would reject this bet no matter what your wealth (this is an assumption we will turn to in more detail later). What can you infer about your response to other bets?\nThis depends on what decision-making model you are using.\nFor an expected utility maximiser - someone who maximises the probability-weighted subjective value of these bets - we can infer that they will turn down any 50:50 bet of losing $1,000 and gaining any amount of money. For example, they would reject a 50:50 bet to lose $1,000, win one billion dollars.\nOn face value, that is ridiculous, and that is the crux of Rabin’s argument. Rejection of the low-value bet to win $110 and lose $100 would lead to absurd responses to higher-value bets. This leads Rabin to argue that risk aversion or the diminishing value of money has nothing to do with the rejection of the low-value bets.\nThe intuition behind Rabin’s argument is relatively simple. Suppose we have someone that rejects a 50:50 bet for gain $11, lose $10. They are an expected utility maximiser with a weakly concave utility curve: that is, they are risk neutral or risk averse at all levels of wealth.\nFrom this, we can infer that they weight the average of each dollar between their current wealth (W) and their wealth if they win the bet (W+11) only 10/11 as much as they weight the average dollar of the last $10 of their current wealth (between W-10 and W). We can also say that they therefore weight their W+11th dollar at most 10/11 as much as their W-10th dollar (relying on the weak concavity here).\nSuppose their wealth is now W+21. We have assumed that they will reject the bet at all levels of wealth, so they will also reject at this wealth. Iterating the previous calculations, we can say that they will weight their W+32nd dollar only 10/11 as much as their W+11th dollar. This means they value their W+32nd dollar only (10/11)2 as much as their W-10th dollar.\nKeep iterating in this way and you end up with some ridiculous results. You value the 210th dollar above your current wealth only 40% as much as your last current dollar of your wealth [reducing by a constant factor of 10/11 every $21 - (10/11)10]. Or you value the 900th dollar above your current wealth at only 2% of your last current dollar [(10/11)40]. This is an absurd rate of discounting.\nThose numbers are from the 2001 Rabin and Thaler paper. In his 2000 paper, Rabin gives figures of 3/20 for the 220th and 1/2000 for the 880th dollar, effectively calculating (10/11)20 and (10/11)80, which is a reduction by a factor of 10/11 every 11 dollars. This degree of discounting could be justified and reflects the equations provided in the Appendix to his paper, but it requires a slightly different intuition than the one relating to the comparison between every 21st dollar. If instead you note that the $11 above a reference point is valued less than the $10 below, you only need to iterate up $11 to get another discount of 10/11, as the next $11 is valued at most as much as the previous $10.\nRegardless of whether you use the numbers from the 2000 or 2001 paper, taking this iteration to the extreme, it doesn’t take long for additional money to have effectively zero value. Hence the result, reject the 50:50 win $110, lose $100 and you’ll reject the win any amount, lose $1,000 bet."
  },
  {
    "objectID": "posts/what-can-we-infer-about-someone-who-rejects-a-5050-bet-to-win-110-or-lose-100-the-rabin-paradox-explored.html#what-is-the-utility-curve-of-this-person",
    "href": "posts/what-can-we-infer-about-someone-who-rejects-a-5050-bet-to-win-110-or-lose-100-the-rabin-paradox-explored.html#what-is-the-utility-curve-of-this-person",
    "title": "What can we infer about someone who rejects a 50:50 bet to win $110 or lose $100? The Rabin paradox explored",
    "section": "What is the utility curve of this person?",
    "text": "What is the utility curve of this person?\nThis argument sounds compelling, but we need to examine the assumption that you will reject the bet at all levels of wealth.\nIf someone rejects the bet at all levels of wealth, what is the least risk averse they could be? They would be close to indifferent to the bet at all levels of wealth. If that were the case across the whole utility curve, their absolute level of risk aversion is constant.\nThe equation used to represent utility with constant absolute risk aversion is exponential utility (with a&gt;0). A feature of the exponential utility function is that, for a risk averse person, utility caps out at a maximum. Beyond a certain level of wealth, they gain no additional utility - hence Rabin’s ability to define bets where they reject infinite gains.\nThe need for utility to cap out is also apparent from the fact that someone might reject a bet that involves the potential for infinite gain. The utility of infinite wealth cannot be infinite, as any bet involving the potential for infinite utility would be accepted, no matter how small the probability of that infinite gain.\nIn the 2000 paper, Rabin brings the constant absolute risk aversion function into his argument more explicitly when he examines what proportion of their portfolio a person with an exponential utility function would invest in stocks (under some particular return assumptions). There he shows a ridiculous level of risk aversion and states that “While it is widely believed that investors are too cautious in their investment behavior, no one believes they are this risk averse.”\nHowever, this effective (or explicit) assumption of constant absolute risk aversion is not particularly well grounded. Most empirical evidence is that people exhibit decreasing absolute risk aversion, not constant. Exponential utility functions are used more for mathematical tractability than for realistically reflecting the decision making processes that people use.\nYet, under Rabin’s assumption of rejecting the bet at all levels of wealth, constant absolute risk aversion and a utility function such as the exponential is the most accommodating assumption we can make. While Rabin states that “no one believes they are this risk averse”, it’s not clear that anyone believes Rabin’s underlying assumption either.\nThis ultimately means that the ridiculous implications for rejecting low-value bets is the result of Rabin’s unrealistic assumption of rejecting the bet no matter what their wealth."
  },
  {
    "objectID": "posts/what-can-we-infer-about-someone-who-rejects-a-5050-bet-to-win-110-or-lose-100-the-rabin-paradox-explored.html#relaxing-the-all-levels-of-wealth-assumption",
    "href": "posts/what-can-we-infer-about-someone-who-rejects-a-5050-bet-to-win-110-or-lose-100-the-rabin-paradox-explored.html#relaxing-the-all-levels-of-wealth-assumption",
    "title": "What can we infer about someone who rejects a 50:50 bet to win $110 or lose $100? The Rabin paradox explored",
    "section": "Relaxing the “all levels of wealth” assumption",
    "text": "Relaxing the “all levels of wealth” assumption\nRabin is, of course, aware that the assumption of rejecting the bet at all levels of wealth is a weakness, so he provides a further example that applies to someone who only rejects this bet for all levels of wealth below $300,000.\nThis generates less extreme but still clearly problematic bets that the bettor can be inferred to also reject.\nFor example, consider someone who rejects the 50:50 bet to win $110, lose $100 when they have $290,000 of wealth, and who would also reject that bet up to a wealth of $300,000. As for the previous example, each time you iterate up $110, each dollar in that $110 is valued at most 10/11 of the previous $110. It takes 90 iterations of $110 to cover that $10,000, meaning that a dollar around wealth $300,000 will be valued only (10/11)90 (0.02%) of a dollar at wealth $290,000. Each dollar above $300,000 is not discounted any further, but by then the damage has already been done, with that money of almost no utility.\nFor instance, this person will reject a bet of gain $718,190, lose $1,000. Again, this person would be out of their mind.\nYou might now ask whether a person with a wealth of $290,000 to $300,000 actually rejects bets of this nature? If not, isn’t this just another unjustifiable assumption designed to generate a ridiculous result?\nIt is possible to make this scenario more realistic. Rabin doesn’t mention this in his paper (nor do Rabin and Thaler), but we can generate the same result at much lower levels of wealth. All we need to find is someone who will reject that bet over a range of $10,000, and still have enough wealth to bear the loss - say someone who will reject that bet up to a wealth of $11,000. That person will also reject a win $718,190 lose $1,000 bet.\nRejection of the win $110, lose $100 bet over that range does not seem as unrealistic, and I could imagine a person with that preference existing. If we empirically tested this, we would also need to examine liquid wealth and cash flow, but the example does provide a sense that we could find some people whose rejection of low-value bets would generate absurd results under expected utility maximisation."
  },
  {
    "objectID": "posts/what-can-we-infer-about-someone-who-rejects-a-5050-bet-to-win-110-or-lose-100-the-rabin-paradox-explored.html#the-log-utility-function",
    "href": "posts/what-can-we-infer-about-someone-who-rejects-a-5050-bet-to-win-110-or-lose-100-the-rabin-paradox-explored.html#the-log-utility-function",
    "title": "What can we infer about someone who rejects a 50:50 bet to win $110 or lose $100? The Rabin paradox explored",
    "section": "The log utility function",
    "text": "The log utility function\nLet’s compare Rabin’s example utility function with a more commonly assumed utility function, that of log utility. Log utility has decreasing absolute risk aversion (and constant relative risk aversion), so is both more empirically defensible and does not generate utility that asymptotes to a maximum like the exponential utility function.\nA person with log utility would reject the 50:50 bet to win $110, lose $100 up to a wealth of $1,100. Beyond that, they would accept the bet. So, for log utility we should see most people accept this bet.\nA person with log utility will reject some quite unbalanced bets: such as a 50:50 bet to win $1 million, lose $90,900, but only up to a wealth of $100,000, beyond which they would accept. Rejection only occurs when a loss is near ruinous.\nThe result is that log utility does not generate the types of rejected bets that Rabin labels as ridiculous, but would also fail to provide much of an explanation for the rejection of low-value bets with positive expected value."
  },
  {
    "objectID": "posts/what-can-we-infer-about-someone-who-rejects-a-5050-bet-to-win-110-or-lose-100-the-rabin-paradox-explored.html#the-empirical-evidence",
    "href": "posts/what-can-we-infer-about-someone-who-rejects-a-5050-bet-to-win-110-or-lose-100-the-rabin-paradox-explored.html#the-empirical-evidence",
    "title": "What can we infer about someone who rejects a 50:50 bet to win $110 or lose $100? The Rabin paradox explored",
    "section": "The empirical evidence",
    "text": "The empirical evidence\nDo people actually turn down 50:50 bets of win $110, lose $100? Surprisingly, I couldn’t find an example of this bet (if someone knows a paper that directly tests this, let me know).\nMost examinations of loss aversion examine symmetric 50:50 bets where the potential gain and the loss are the same. They compare a bet centred around 0 (e.g. gain $100 or lose $100) and a similar bet in a gain frame (e.g. gain $100 or gain $300, or take $200 for certain). If more people reject the first bet than the latter, then this is evidence of loss aversion.\nIt makes sense that this is the experimental approach. If the bet is not symmetric, it becomes hard to tease out loss aversion from risk aversion.\nHowever, there is a pattern in the literature that people often reject risky bets with a positive expected value in the ranges explored by Rabin. We don’t know a lot about their wealth (or liquidity), but Rabin’s illustrative numbers for rejected bets don’t seem completely unrealistic. It’s the range of wealth over which the rejection occurs that is questionable.\nRather than me floundering around on this point, some papers explicitly ask whether we can observe a set of bets for a group of experimental subjects and map a curve to those choices that resembles expected utility.\nFor instance, Holt and Laury’s 2002 AER paper (pdf) examined a set of hypothetical and incentivised bets over a range of stakes (finding among other things that hypothetical predictions of their response to incentivised high-stakes bets were not very accurate). They found that if you are flexible about the form of the expected utility function that is used, rejection of small gambles does not result in absurd conclusions on large gambles. The pattern of bets could be made consistent with expected utility, assuming you correctly parameterise the equation. Over subsequent years there was some back and forth on whether this finding was robust [see here (pdf) and here (pdf)], but the basic result seemed to hold.\nThe utility curve that best matched Holt and Laury’s experimental findings had increasing relative risk aversion, and decreasing absolute risk aversion. By having decreasing absolute risk aversion, the absurd implications of Rabin’s paper are avoided.\nPapers such as this suggest that while Rabin’s paper makes an important point, its underlying assumptions are not consistent with empirical evidence. It is possible to have an expected utility maximiser reject low-value bets without generating ridiculous outcomes.\nSo what can you infer about our bettor who has rejected the win $110, lose $100 bet?\nFrom the argument above, I would say not much. We could craft a utility function to accommodate this bet without leading to ridiculous consequences. I feel this defence is laboured (that’s a subject for another day), but the bet is not in itself fatal to the argument that they are an expected utility maximiser.\nMy other posts on loss aversion can be found here:\n\nKahneman and Tversky’s debatable loss aversion assumption\nWhat can we infer about someone who rejects a 50:50 bet to win $110 or lose $100? The Rabin paradox explored (this post)\nThe case against loss aversion\nErgodicity economics - a primer\n\n–"
  },
  {
    "objectID": "posts/what-can-we-infer-about-someone-who-rejects-a-5050-bet-to-win-110-or-lose-100-the-rabin-paradox-explored.html#appendix",
    "href": "posts/what-can-we-infer-about-someone-who-rejects-a-5050-bet-to-win-110-or-lose-100-the-rabin-paradox-explored.html#appendix",
    "title": "What can we infer about someone who rejects a 50:50 bet to win $110 or lose $100? The Rabin paradox explored",
    "section": "Appendix",
    "text": "Appendix\n\nThe utility of a gain\nLet’s suppose someone will reject a 50:50 bet with gain g and loss l for any level of wealth. What utility will they get from a gain of x? Rabin defines an upper bound of the utility of gaining x to be:\n\nU(w+x)-U(w)\\leq\\sum_{i=0}^{k^{**}(x)}\\left(\\frac{l}{g}\\right)^ir(w)\n\n\nk^{**}(x)=int\\left(\\frac{x}{g}\\right)\n\n\nr(w)=U(w)-U(w-l)\n\nThis formula effectively breaks down x into g size components, successively discounting each additional g at l/g of the previous g.\nYou need k^{**}(x)+1 lots of g to cover x. For instance, if x was 32 and we had a 50:50 bet for win $11, lose $10, 32/11=2. You need 2+1 lots of 11 to fully cover 32. It actually covers a touch more than 32, hence the calculation being for an upper bound.\nIn the paper, Rabin defines k^{**}(x)=int((x/g)+1) This seems to better capture the required number of g to fully cover x, but the iterations in the above formula start at i=0. The calculations I run below with my version of the formula replicate Rabin’s, supporting the suggestion that the addition of 1 in the paper is an error.\nr(w) is shorthand for the amount of utility sacrificed from losing the gamble (i.e. losing l). We know that the utility of the gain g is less than this, as the bet is rejected. If we let r(w)=1, the equation can be thought of as giving you the maximum utility you could get from the gain of x relative to the utility of the loss of l.\nPutting this together, the upper bound of the utility of the possible gain x is therefore less than, first, the upper bound of the relative utility from the first $11, (10/11)^0r(w)=r(w), the upper bound of utility from the next $11, (10/11)^1r(w), and the upper bound of the utility from the remaining $10 - taking a conservative approach this is calculated as though it were a full $11: (10/11)^2r(w).\n\n\nThe utility of a loss\nRabin also gives us a lower bound of the utility of a loss of x for this person who will reject a 50:50 bet with gain g and loss l for any level of wealth:\n\nU(w)-U(w-x)\\geq{2}\\sum_{i=1}^{k^{*}(x)}\\left(\\frac{g}{l}\\right)^{i-1}{r(w)}\n\n\nk^{*}(x)=int\\left(\\frac{x}{2l}\\right)\n\nThe intuition behind k^{*}(x) comes from Rabin’s desire to provide a relatively uncomplicated proof for the proposition. Effectively, the utility scales down with each step of g by at least g/l. Since Rabin wants to express this in terms of losses, he defines 2l\\geq{g}\\geq{l}. He can thereby say that utility scales down by at least \\frac{g}{l} every 2 lots of l.\nOtherwise, the intuition for this loss formula is the same as that for the gain. The summation starts at i=1 as this formula is providing a lower bound, so does not require the final iteration to fully cover x. The formula is also multiplied by 2 as each iteration covers two lots of l, whereby r(w) is for a single span of l.\n\n\nRunning some numbers\nThe below code implements the above two formulas as a function, calculating the potential utility gain for a win of G or a loss of L for a person who rejects a 50:50 bet win g, lose l at all levels of wealth. It then states whether we know the person will reject a win G, lose L bet - we can’t state they will accept as we have upper and lower bounds of the utility change from the gain and loss.\n\n\nCode\nRabin_bet &lt;- function(g, l, G, L){\n\n  k_2star &lt;- as.integer(G/g)\n  k_star &lt;- as.integer(L/(2*l))\n\n  U_gain &lt;- 0\n  for (i in 0:k_2star) {\n    U_step &lt;- (l/g)^i\n    U_gain &lt;- U_gain + U_step\n  }\n\n  U_loss &lt;- 0\n  for (i in 1:k_star) {\n    U_step &lt;- 2*(g/l)^(i-1)\n    U_loss &lt;- U_loss + U_step\n  }\n\n  ifelse(U_gain &lt; U_loss,\n         print(\"REJECT\"),\n         print(\"CANNOT CONFIRM REJECT\")\n         )\n  print(paste0(\"Max U from gain =\", U_gain))\n  print(paste0(\"Min U from loss =\", U_loss))\n}\n\n\nTake a person who will reject a 50:50 bet to win $110, lose $100. Taking the table from the paper, they would reject a win $1,000,000,000, lose $1,000 bet.\n\n\nCode\nRabin_bet(110, 100, 1000000000, 1000)\n\n\n[1] \"REJECT\"\n[1] \"Max U from gain =11\"\n[1] \"Min U from loss =12.2102\""
  },
  {
    "objectID": "posts/what-can-we-infer-about-someone-who-rejects-a-5050-bet-to-win-110-or-lose-100-the-rabin-paradox-explored.html#relaxing-the-wealth-assumption",
    "href": "posts/what-can-we-infer-about-someone-who-rejects-a-5050-bet-to-win-110-or-lose-100-the-rabin-paradox-explored.html#relaxing-the-wealth-assumption",
    "title": "What can we infer about someone who rejects a 50:50 bet to win $110 or lose $100? The Rabin paradox explored",
    "section": "Relaxing the wealth assumption",
    "text": "Relaxing the wealth assumption\nIn the Appendix of his paper, Rabin defines his proof where the bet is rejected over a range of wealth w\\in(\\bar w, \\underline{w}). In that case, relative utility for each additional gain of size g is l/g of the previous g until \\bar w. Beyond that point, each additional gain of g gives constant utility until x is reached. The formula for the upper bound on the utility gain is:\n\nU(w+x)-U(w)\\leq\n\\left\\{\n\\begin{matrix*}[l]\n\\displaystyle \\sum_{i=0}^{k^{**}(x)} \\left(\\frac{l}{g}\\right)^ir(w) & \\text{if}&\\quad x\\leq{\\bar w}-w\\\\[18pt]\n\\displaystyle\\sum_{i=0}^{k^{**}(\\bar w)}\\left(\\frac{l}{g}\\right)^{i}r(w)+\\left[\\frac{x-(\\bar w-w)}{g}\\right]\\left(\\frac{l}{g}\\right)^{k^{**}(\\bar w)}r(w) & \\text{if}&\\quad x\\geq{\\bar w}-w\n\\end{matrix*}\n\\right.\n\nThe first term of the equation where x\\geq\\bar w-w involves iterated discounting as per the situation where the bet is rejected for all levels of wealth, but here the iteration is only up to wealth \\bar w. The second term of that equation captures the gain beyond \\bar w discounted at a constant rate.\nThere is an error in Rabin’s formula in the paper. Rather than the term (x-(\\bar w-w))/g in the second equation, Rabin has it as x-\\bar w. As for the previous equations, we need to know the number of iterations of the gain, not total dollars, and we need this between \\bar w and w+x.\nWhen Rabin provides the examples in Table II of the paper, from the numbers he provides I believe he uses a formula of the type int[(x-(w-\\underline w))/g+1], which reflects a desire to calculate the upper-bound utility across the stretch above \\bar w in a similar manner to below, although this is not strictly necessary given the discount is constant across this range. I have implemented as per my formula, which means that bets are still rejected g higher than for Rabin (which given their scale is not material).\nSimilarly, for the loss:\n\nU(w)-U(w-x)\\geq\n\\left\n\\{\\begin{matrix*}[l]\n2\\displaystyle\\sum_{i=1}^{k^{*}(x)}\\left(\\frac{g}{l}\\right)^{i-1}r(w)\\quad &\\text{if}\\quad {w-\\underline w+2l}\\geq{x}\\geq{2l}\\\\[18pt]\n2\\displaystyle\\sum_{i=1}^{k^{*}(w-\\underline w+2l)}\\left(\\frac{g}{l}\\right)^{i-1}{r(w)}+\\\\[12pt]\n\\displaystyle\\qquad\\left[\\frac{x-(w-\\underline w+l)}{2l}\\right]\\left(\\frac{g}{l}\\right)^{k^{*}(w-\\underline w+2l)}{r(w)} &\\text{if}\\quad x\\geq{w-\\underline w+2l}\n\\end{matrix*}\n\\right.\n\nThere is a similar error here, with Rabin using the term x-(w-\\underline w+l) rather than (x-(w-\\underline w+l))/2l. I can’t determine how this was implemented by Rabin as his examples do not examine behaviour below a lower bound \\underline w.\n\nRunning some more numbers\nThe below R code implements the above two formulas as a function, calculating the potential utility gain for a win of G or a loss of L for a person who rejects a 50:50 bet win g, lose l at wealth w\\in(\\bar w, \\underline{w}). It then states whether we know the person will reject a win G, lose L bet - as before, we can’t state they will accept as we have upper and lower bounds of the utility change from the gain and loss.\n\n\nCode\nRabin_bet_general &lt;- function(g, l, G, L, w, w_max, w_min){\n\n  ifelse(\n    G &lt;= (w_max-w),\n    k_2star &lt;- as.integer(G/g),\n    k_2star &lt;- as.integer((w_max-w)/g)\n  )\n\n   ifelse(\n    w-w_min+2*l &gt;= L,\n    k_star &lt;- as.integer(L/(2*l)),\n    k_star &lt;- as.integer((w-w_min+2*l)/(2*l))\n  )\n\n  U_gain &lt;- 0\n  for (i in 0:k_2star) {\n    U_step &lt;- (l/g)^i\n    U_gain &lt;- U_gain + U_step\n  }\n\n  ifelse(\n    G &lt;= (w_max-w),\n    U_gain &lt;- U_gain,\n    U_gain &lt;- U_gain + ((G-(w_max-w))/g)*(l/g)^k_2star\n  )\n\n  U_loss &lt;- 0\n  for (i in 1:k_star) {\n    U_step &lt;- 2*(g/l)^(i-1)\n    U_loss &lt;- U_loss + U_step\n  }\n\n  ifelse(\n    w-w_min+2*l &gt;= L,\n    U_loss &lt;- U_loss,\n    U_loss &lt;- U_loss + ((L-(w-w_min+l))/(2*l))*(g/l)^k_star\n  )\n\n  ifelse(U_gain &lt; U_loss,\n         print(\"REJECT\"),\n         print(\"CANNOT CONFIRM REJECT\")\n         )\n  print(paste0(\"Max U from gain =\", U_gain))\n  print(paste0(\"Min U from loss =\", U_loss))\n}\n\n\nImagine someone who turns down the win $110, lose $100 bet with a wealth of $290,000, but who would only reject this bet up to $300,000. They will reject a win $718,190, lose $1000 bet.\n\n\nCode\nRabin_bet_general(110, 100, 718190, 1000, 290000, 300000, 0)\n\n\n[1] \"REJECT\"\n[1] \"Max U from gain =12.2098745626936\"\n[1] \"Min U from loss =12.2102\"\n\n\nThe nature of Rabin’s calculation means that we can scale this calculation to anywhere on the wealth curve. We need only say that someone who rejects this bet over (roughly) a range of $10,000 plus the size of the potential loss will exhibit the same decisions. For example a person with $10,000 wealth who would reject the bet up to $20,000 wealth would also reject the win $718,190, lose $1000 bet.\n\n\nCode\nRabin_bet_general(110, 100, 718190, 1000, 10000, 20000, 0)\n\n\n[1] \"REJECT\"\n[1] \"Max U from gain =12.2098745626936\"\n[1] \"Min U from loss =12.2102\"\n\n\n\n\nComparison with log utility\nThe below is an example with log utility, which is U(W)=ln(W). This function determines whether someone of wealth w will reject or accept a 50:50 bet for gain g and loss l.\n\n\nCode\nlog_utility &lt;- function(g, l, w){\n  log_gain &lt;- log(w+g)\n  log_loss &lt;- log(w-l)\n  EU_bet &lt;- 0.5*log_gain + 0.5*log_loss\n  EU_certain &lt;- log(w)\n\n  if(EU_certain == EU_bet){\n    print(\"INDIFFERENT\")\n  } else if(EU_certain &gt; EU_bet){\n    print(\"REJECT\")\n  } else if(EU_certain &lt; EU_bet){\n    print(\"ACCEPT\")\n  }\n\n  print(paste0(\"Expected utility of bet = \", EU_bet))\n  print(paste0(\"Utility of current wealth = \", EU_certain))\n}\n\n\nTesting a few numbers, someone with log utility is indifferent about a 50:50 win $110, lose $100 bet at wealth $1100. They would accept for any level of wealth above that level.\n\n\nCode\nlog_utility(110, 100, 1100)\n\n\n[1] \"INDIFFERENT\"\n[1] \"Expected utility of bet = 7.00306545878646\"\n[1] \"Utility of current wealth = 7.00306545878646\"\n\n\nThat same person will always accept a 50:50 win $1100, lose $1000 bet above $11,000 in wealth.\n\n\nCode\nlog_utility(1100, 1000, 11000)\n\n\n[1] \"ACCEPT\"\n[1] \"Expected utility of bet = 9.30565055178051\"\n[1] \"Utility of current wealth = 9.30565055178051\"\n\n\nCan we generate any bets that don’t seem quite right? It’s quite hard unless you have a bet that will bring the person to ruin or near ruin. For instance, for a 50:50 bet with a chance to win $1 million, a person with log utility and $100,000 wealth would still accept the bet with a potential loss of $90,900, which brings them to less than 10% of their wealth.\n\n\nCode\nlog_utility(1000000, 90900, 100000)\n\n\n[1] \"ACCEPT\"\n[1] \"Expected utility of bet = 11.5134252151368\"\n[1] \"Utility of current wealth = 11.5129254649702\"\n\n\nThe problem with log utility is not the ability to generate ridiculous bets that would be rejected. Rather, it’s that someone with log utility would tend to accept most positive value bets (in fact, they would always take a non-zero share if they could). Only if the bet brings them near ruin (either through size or their lack of wealth) would they turn down the bet.\nThe isoelastic utility function - of which log utility is a special case - is a broader class of function that exhibits constant relative risk aversion:\n\nU(x)=\\frac{w^{1-\\rho}-1}{1-\\rho}\n\nIf \\rho=1, this simplifies to log utility (you need to use L’Hopital’s rule to get this as the fraction is undefined when \\rho=1.) The higher \\rho, the higher the level of risk aversion. We implement this function as follows:\n\n\nCode\nCRRA_utility &lt;- function(g, l, w, rho=2){\n\n  if(rho==1){\n    print(\"function undefined\")\n  }\n\n  log_gain &lt;- ((w+g)^(1-rho)-1)/(1-rho)\n  log_loss &lt;- ((w-l)^(1-rho)-1)/(1-rho)\n\n  EU_bet &lt;- 0.5*log_gain + 0.5*log_loss\n  EU_certain &lt;- (w^(1-rho)-1)/(1-rho)\n\n  if(EU_certain == EU_bet){\n    print(\"INDIFFERENT\")\n  } else if(EU_certain &gt; EU_bet){\n    print(\"REJECT\")\n  } else if(EU_certain &lt; EU_bet){\n    print(\"ACCEPT\")\n  }\n  \n  print(paste0(\"Expected utility of bet = \", EU_bet))\n  print(paste0(\"Utility of current wealth = \", EU_certain))\n}\n\n\nIf we increase \\rho, we can increase the proportion of low-value bets that are rejected.\nFor example, a person with \\rho=2 will reject the 50:50 win $110, lose $100 bet up to a wealth of $2200. The rejection point scales with \\rho.\n\n\nCode\nCRRA_utility(110, 100, 2200, 2)\n\n\n[1] \"INDIFFERENT\"\n[1] \"Expected utility of bet = 0.999545454545455\"\n[1] \"Utility of current wealth = 0.999545454545455\"\n\n\nFor a 50:50 chance to win $1 million at wealth $100,000, the person with \\rho=2 is willing to risk a far smaller loss, and rejects even when the loss is only $48,000, or less than half their wealth (which admittedly is still a fair chunk).\n\n\nCode\nCRRA_utility(1000000, 48000, 100000, 2)\n\n\n[1] \"REJECT\"\n[1] \"Expected utility of bet = 0.99998993006993\"\n[1] \"Utility of current wealth = 0.99999\"\n\n\nHigher values of \\rho start to become completely unrealistic as utility is almost flat beyond an initial level of wealth.\nIt is also possible to have values of \\rho between 0 (risk neutrality) and 1. These would result in even fewer rejected low-value bets than log utility, and fewer rejected bets with highly unbalanced potential gains and losses."
  },
  {
    "objectID": "posts/using-generative-AI-as-academic-july-2024-edition.html",
    "href": "posts/using-generative-AI-as-academic-july-2024-edition.html",
    "title": "Using generative AI as an academic - July 2024 edition",
    "section": "",
    "text": "I first wrote a version of this post in April 2023. A lot has changed since then in both the tools and how I use them.\nAs was the case then, if you want a sense of the frontier, others such as Ethan Mollick will give you a better flavour. But if you’re after some practical examples, you might find this useful."
  },
  {
    "objectID": "posts/using-generative-AI-as-academic-july-2024-edition.html#my-toolkit",
    "href": "posts/using-generative-AI-as-academic-july-2024-edition.html#my-toolkit",
    "title": "Using generative AI as an academic - July 2024 edition",
    "section": "My toolkit",
    "text": "My toolkit\nI use multiple tools, switching between them depending on the task and comparing the responses. I want to gain a sense of the frontier.\nAs an academic, I gain free access to Github Copilot. (Students can also get free access.) CoPilot provides code suggestions in coding environments. But while badged this way, CoPilot also provides text suggestions more broadly. To access CoPilot, I write markdown documents in Visual Studio Code or RStudio. (For those who want more specific guidance on how to set this up, I’ve added that at the bottom of this post.) If you use CoPilot in Visual Studio Code you also gain access to Copilot Chat.\nI also work with ChatGPT Plus and Claude Pro open in a browser. I subscribe to the Pro/Plus version of both: it is worth it for the superior tools. As you’ll see from the below, I use them all the time, although more recently I lean toward Claude.\nI run some other large language models locally on my computer using Ollama. I do this more to get a feel for them than for current utility. Most of my recent experimentation has been with Llama 3. I use these models via the command line or the CoPilot plugin in Obsidian.\nI also subscribe to ElevenLabs for its voice-generation capabilities. More on that below.\nI’ve experimented with other tools, but they haven’t yet formed a large part of my workflow. I don’t use Bing much, except for images if ChatGPT isn’t giving me what I want. I like that it gives four images for every prompt. I experimented with Google Gemini but haven’t found a reason for it to supplant the Claude plus ChatGPT combination. I revisit it every month or two to see whether I should add it to the mix."
  },
  {
    "objectID": "posts/using-generative-AI-as-academic-july-2024-edition.html#thinking",
    "href": "posts/using-generative-AI-as-academic-july-2024-edition.html#thinking",
    "title": "Using generative AI as an academic - July 2024 edition",
    "section": "Thinking",
    "text": "Thinking\n\nEfficiency gain: uncertain\n\nI am increasingly using Claude and ChatGPT as interactive rubber ducks. When thinking through something new, I’ll often state my thoughts and ask for comments. I’ll often ask them to explain a concept to me, then I will ask follow-up questions. Sometimes I’ll do this in ChatGPT with voice mode when I am walking. I know that I’m not getting 100% accurate information, but I wouldn’t talking to a friend either.\nSimilarly, after reading an article and forming my own views on its message, strengths and weaknesses, I’ll ask Claude or ChatGPT their views. I find this can be hit and miss, but on net worth doing. Sometimes the description is spot on, and the critiques hit points I haven’t thought of.\nFinally, if I have a technical question, I’ll ask before googling. It’s faster and, I would say, more accurate than the top Google hits."
  },
  {
    "objectID": "posts/using-generative-AI-as-academic-july-2024-edition.html#coding-and-data-analysis",
    "href": "posts/using-generative-AI-as-academic-july-2024-edition.html#coding-and-data-analysis",
    "title": "Using generative AI as an academic - July 2024 edition",
    "section": "Coding and data analysis",
    "text": "Coding and data analysis\n\nEfficiency gain: 10x\nQuality gain: 50%\n\nDespite spending a lot of time in R, I am a crap coder. As a result, my typical approach is to write a comment and let CoPilot do the first cut of the code. I will then tweak the code until it is in shape. That tweaking isn’t manual tweaking either. If there is an error or problem that I can’t resolve, I’ll use CoPilot Chat to get a solution. Chat will usually provide code solving the problem.\nCoPilot is also great in helping me understand someone else’s code. I’ll highlight sections and ask CoPilot Chat to explain what it does. Often, I’ll paste poorly documented code into Claude or ChatGPT and ask it to write comments. Further, If the code looks overly complicated, a request to simplify often yields good results. (I’ve been asking ChatGPT to review some of my old code recently. I’m a bit embarrassed at how much more efficiently I could have written it.)\nOne large benefit has come from cross-language translation. Last year I wanted to reproduce the results in Berkeley Dietvorst’s Algorithm aversion: People erroneously avoid algorithms after seeing them err. Data and the code for analysis were available on ResearchBox. The problem was that the code was in Stata, and I have no desire to learn Stata.\nI pasted the Stata code into ChatGPT and asked for a conversion to R. What I received was 90% of the way there. Initially, there was an error, but pasting the error into ChatGPT gave me the solution first shot. I then tweaked a couple of the variable manipulations and specified that some of the t-tests were paired t-tests. That was it. In less than 10 minutes I had code that reproduced exactly the results from Study 1. I’ve now done this translation dozens of times when examining and reproducing studies by others.\nI’m increasingly asking ChatGPT and Claude to provide me with working programs. For example, I am building a website with an underlying database and model for predicting AFL games. (I wanted to experiment with how far I could go with these tools.) I obtained a web interface and underlying code from Claude to access the database. I used ChatGPT for the first version of the predictive model. I’m now playing with the details, but the core is there. I wouldn’t even attempt it without these tools.\n\nData exploration\n\nEfficiency gain: 2x\n\nAny time I am about to work with new data, confidentiality permitting, I’ll give the data to ChatGPT and ask about it. I’ll start generally: “tell me about this file”. Then I’ll work down to the details and ask for basic visualisations and analysis. Finally, I’ll export the Python code (or ask for an R version of the code).\nEver since the Dan Ariely car insurance data fraud, which is obvious from a simple plot, I’ve made it a habit of looking at the data before jumping into analysis (which I didn’t do as often as I should have in the past). It’s surprising how often I find that something is not quite right or as expected. Having ChatGPT on hand makes this first step quick and easy.\n\n\nExtracting pdf data\n\nEfficiency gain: 10x\n\nThis is a small one in that I don’t need to do that often, but it’s a massive time saver. If there is a large table in a PDF, I ask ChatGPT to output it as a CSV. Recently, I had data spread across half a dozen PDFs in poorly formatted tables. I used ChatGPT to combine and arrange the data. I check the accuracy as I don’t fully trust ChatGPT, but the accuracy is typically great, saving me a lot of time."
  },
  {
    "objectID": "posts/using-generative-AI-as-academic-july-2024-edition.html#writing",
    "href": "posts/using-generative-AI-as-academic-july-2024-edition.html#writing",
    "title": "Using generative AI as an academic - July 2024 edition",
    "section": "Writing",
    "text": "Writing\n\nEfficiency gain: 20%\nQuality gain: 10%\n\nWhen I first started using GitHub CoPilot, I would have it on whenever I was writing. I would receive suggestions as I typed. Today, I generally turn it off when writing a first draft. The distraction in filtering suggestions was too costly, and I was worried about getting dragged in random directions.\nHowever, I use Claude and ChatGPT a lot to improve my writing. When I’m having trouble getting a sentence or paragraph right, I’ll often hand it over and ask for 10 possible rewrites. Similarly, I will ask for simpler, shorter or more conversational text. I’ll rarely take a rewrite as provided, but it often gives me a word or turn of phrase that does the job.\n\nLecture notes\nFor more mechanical lecture notes, there are some clear gains. The notes for my undergraduate behavioural economics subject (which you can access here) were produced with CoPilot assistance.\nHere are a few instances where CoPilot has been useful.\nOften the notes include a statement of a concept followed by a step-by-step example or illustration. The gain comes when you’re writing those steps.\nFor example, in a section about decision making under risk, I might write:\n\nBill is a risk-averse agent with utility function u(x)=\\text{ln}(x) and wealth of $100. He is offered a 50:50 bet to win $100, lose $50.\nWill Bill accept the bet?\n\nIf I write the first couple of words of the answer “First, we calculate…”, CoPilot is away.\n\nFirst, we calculate the expected utility of each option. The expected utility of the bet is:\n\n```{=tex}\n\\begin{aligned}\nEU(\\text{bet}) &= 0.5 \\times u(100) + 0.5 \\times u(-50) \\\\\n&= 0.5 \\times \\text{ln}(100) + 0.5 \\times \\text{ln}(50) \\\\\n&= 4.259\n\\end{aligned}\n```\n\nThe expected utility of not taking the bet is:\n\n\n```{=tex}\n\\begin{aligned}\nEU(\\text{no bet}) &= U(100) \\\\\n&= \\text{ln}(100) \\\\\n&= 4.605\n\\end{aligned}\n```\n\nAs the expected utility of the bet is less than the expected utility of not taking the bet, Bill will not accept the bet.\n\nNot bad for a few seconds of work. For those unfamiliar with the mathematical notation, this is \\LaTeX, which renders into nice equations like this.\n\\begin{aligned}\nEU(\\text{bet}) &= 0.5 \\times u(100) + 0.5 \\times u(-50) \\\\\n&= 0.5 \\times \\text{ln}(100) + 0.5 \\times \\text{ln}(50) \\\\\n&= 4.259\n\\end{aligned}\nI might beef this up with a better explanation, or paste the question into ChatGPT directly, where I tend to get more detailed answers. CoPilot has done OK with the math here - the logs are correct!\nCoPilot is also great when I’m doing repetitive tasks such as describing the elements of an equation or diagram. Start describing the first element and it might give you the rest. And if you write a point followed by “Conversely, …”, CoPilot is often on the money.\nI also use CoPilot with ggplot to produce graphs for the notes. When I wanted to generate a chart showing a probability weighting function from prospect theory, I created an R code block and typed a comment:\n\n#Plot of probability weighting function using ggplot2\n\nHere’s the result that ChatGPT produced with the diagram rendered below:\n\n#Plot of probability weighting function using ggplot2\nlibrary(ggplot2)\n\n#Define probability weighting function\nprob_weight &lt;- function(p, alpha){\n  exp(-(-log(p))^alpha)\n}\n\n#Create data frame of probabilities and weights\nprob &lt;- seq(0, 1, 0.001)\nprob_df &lt;- data.frame(prob = prob, weight = prob_weight(prob, 0.6))\n\n#Plot\nggplot(prob_df, aes(x = prob, y = weight)) +\n  geom_line() +\n\n  #Add labels\n  labs(x = \"Probability\", y = \"Weight\")\n\n\n\n\n\n\n\n\nExactly what I wanted. When I implemented this in the notes, I only tweaked the style and added a 45-degree line.\nCoPilot’s offering uses Prelec’s (1998) probability weighting function. Whether it picked that up from earlier text in my notes or just gave the most common function for probability weighting, I don’t know, but it’s what I would have used if doing it manually. However, in writing this post I wasn’t able to replicate what I did when writing the lecture notes. I got better results in the lecture notes themselves. A few paragraphs about probability weighting before I ask for the code generated better results than asking for the chart straight up.\nCoPilot didn’t offer this full chunk of code at once. Each comment was offered, then the piece of code after it, one after the other. But the only work I did was writing the first comment and pressing tab several times as each succeeding comment or chunk of code was suggested. (CoPilot has certainly increased the level of comments in my code too.)\n\n\nWriting organisational fluff\nWork in any decent-sized modern organisation and you will have to write some level of fluff to satisfy the higher-ups, clients, government requirements and the like. I am hopeless at those tasks. I have to invest heavily to make fluff sound decent.\nMy approach to these exercises depends on the degree of pointlessness.\nIf the task relates to a process that will have zero impact on what anyone will do, I simply give the task to ChatGPT, let it do the first cut and then tweak as required. One or two sentences of guidance often get you 80% of the way there.\nIf I think there is a positive benefit to the task, or it’s for public consumption, I’ll be more proactive first up. I will write a rough draft, not caring much about the writing but making sure the concepts I want to include are there. It might be in dot points. Then I’ll ask ChatGPT for a version that is “clearer”, “simpler” or “better written” or “for a ten-year-old”.\nRecently, I needed to describe how a program met a set of quality criteria. I uploaded some documents describing the program and a pdf that contained the criteria (in a table). I then asked Claude to provide me with short paragraphs describing how the program met the criteria. Some mild tweaking and I was there.\n\n\nGenerating quiz questions\n\nEfficiency gain: 3x\n\nOne of the best ways to learn is to be tested. As a result, I offer students in my undergraduate subject a series of practice quizzes that they can work through.\nIt’s hard work to generate questions in bulk, and I struggle to generate plausible-sounding but incorrect answers to multiple-choice questions. I now ask ChatGPT to generate them. I upload my subject notes and ask that they be used as the basis of the questions. In the prompts, I vary in the specificity of the questions. “Give me 20 multiple choice questions testing the concept of loss aversion.” “Give me 20 multiple choice questions testing prospect theory.” Out of each batch, only a few will be suitable. But by tweaking my instruction by, say, describing the level of the students (undergraduate) or more explicitly defining the concept, it doesn’t take long to get 10 or so good questions.\nI have also used ChatGPT to generate question ideas for assessable quizzes and exams. That exam is a closed-book AI-invigilated exam, so is less vulnerable to someone simply feeding the questions back to ChatGPT. One helpful approach was to upload some previous exams plus my subject notes and ask for new exam questions at the same level of difficulty or on the same concepts."
  },
  {
    "objectID": "posts/using-generative-AI-as-academic-july-2024-edition.html#ai-voiced-lectures",
    "href": "posts/using-generative-AI-as-academic-july-2024-edition.html#ai-voiced-lectures",
    "title": "Using generative AI as an academic - July 2024 edition",
    "section": "AI-voiced lectures",
    "text": "AI-voiced lectures\n\nEfficiency gain: 10%\nQuality gain: 50%\n\nLast year I decided to base my undergraduate behavioural economics subject around short pre-recorded videos, interactive online seminars and tutorials. To produce the videos I wrote scripts, which also form the subject notes. I read the notes accompanied by slides.\nThere are three pain points in this exercise. First, I speak too fast (even when I think I’m speaking slow) and I enunciate many words poorly. Second, recording takes a lot of time. I rarely get a two to five-minute video in a single take that I’m happy with. Third, if I want to tweak the recording, I need to either re-record solid chunks of text or fiddle around with video edits.\nSo, why not get an artificial voice to do the speaking? When I wrote my last post, AI-voiced lectures were the future. It’s now a core part of my subject delivery.\nI experimented with Murf.ai, Speechify, play.ht and ElevenLabs. Initially I used Murf.ai to make some videos, as it allowed me to pair slides and text easily. I could also integrate with Google Slides. However, the voices sound a bit robotic, so I currently use ElevenLabs, even though all I get is an audio file. ElevenLabs has fantastic voices. Even a hint of AI voicing gets a negative reaction from students, so voice quality needs to be my primary criterion. If the Murf.ai voices improve and Eleven doesn’t develop video integration, I may shift back.\nThe production process is easy. I paste the text into an ElevenLabs project chapter and render the voice. Typically, the pacing won’t be quite right, so I’ll add some commas and dashes to create some pauses. I then export the voice file and combine it with images in Final Cut Pro. I can normally create a five to 10 minute video from text and slides in less than a hour.\nThe fantastic part is that if I want to update a section or change a sentence, I update the script in ElevenLabs, re-render the relevant sentences, load a new voice file into the Final Cut Pro project and tweak the slide timings (if the voice edits require it). It’s fast, simple and gives me a consistently high-quality video.\nElevenLabs can “clone” your voice. I tried a quick clone on play.ht, but it gave me an American accent. On ElevenLabs, you can get a professional clone of a voice by uploading three or more hours of audio. I did that, and it sounds just like me, even with the Australian accent. Unfortunately, it comes with my faults, particularly speaking too fast. ElevenLabs doesn’t have an option to slow down the speaking outside of putting a mountain of dashes through the text. It also reflects the quality of my home recording setup and sounds a bit echoey. So, for now, I’m using one of their off-the-shelf Australian voices.\nHere are two samples of the above paragraph, an off-the-shelf Australian voice and my “cloned” voice.\n\n\n\n\nI’ve also started looking at options to create an AI avatar based on videos or photos of me for some parts of the videos. I’m leaving that one for the moment but can see myself revisiting it in the next year."
  },
  {
    "objectID": "posts/using-generative-AI-as-academic-july-2024-edition.html#accessing-github-copilot",
    "href": "posts/using-generative-AI-as-academic-july-2024-edition.html#accessing-github-copilot",
    "title": "Using generative AI as an academic - July 2024 edition",
    "section": "Accessing GitHub CoPilot",
    "text": "Accessing GitHub CoPilot\nThe below gives the basic steps to access GitHub CoPilot.\nIf you are an academic, you can sign up for (free) academic access to CoPilot at this link. If you’re not an academic or student, sign up for a free trial as described here. You’ll need a Github account to do this. The $10 a month is worth it.\nVisual Studio Code is available here. (I use the Mac version.) Download and install.\nInstall the Github CoPilot extension into Visual Studio Code. Instructions on installing the extension are here. On installation you’ll be prompted to login to GitHub to gain access to authorise CoPilot.\nOnce you have done that, using CoPilot is easy. You simply type. As you type, you will be given suggestions.\nThe setup in RStudio is even easier. Download RStudio from here. Under Tools/Global Options is a CoPilot menu item. Enable CoPilot there. You will be asked to enter your GitHub login details to get it running.\nI write in markdown (and at the moment, a flavour of markdown called Quarto). It allows me to include \\LaTeX math and R computations within any document (as I have in this post). If you’re an academic stuck in the \\LaTeX ecosystem, you can also write in Visual Studio Code wholly in \\LaTeX. You can also just get CoPilot in Word. I’m looking forward to my university moving beyond the experimental stage and giving us access. (I took a one-month free trial of Microsoft CoPilot on the home computer, but didn’t find the time to give it a proper go.)"
  },
  {
    "objectID": "posts/the-preregistration-halo.html",
    "href": "posts/the-preregistration-halo.html",
    "title": "The preregistration halo",
    "section": "",
    "text": "When we analyse experimental data, we have many choices. What observations do we exclude? What variables do we compare? What statistical tests do we use? And so on. These choices lead us into what is often called the garden of forking paths.\nThe problem is that some paths lead to a “significant” result. Hurrah! Publication on the way. Others lead nowhere. So when we read a published paper, you might ask: did they choose their path because it led to a significant result?\nOne solution to this problem is to preregister the analysis. Specify the analysis before seeing the data.\nIf preregistration is made public, we can see all research on a topic. We won’t be limited to just the studies authors get published. Preregistration constrains the analysis by making it harder for authors to p-hack or hypothesise their way to a significant result. Or, at least, it makes it transparent that they have done so.\nBut, preregistration is no guarantee of good science. It doesn’t solve many of the real problems in science (for example). Preregistration can’t rescue weak theory. Many preregistrations remain private. Other preregistrations are left to wither on the vine. Here’s a plot of the status of preregistrations submitted to the American Economic Association’s RCT registry. The data goes up to 1 January 2024.\n\n\nCode\nlibrary(readr)\nlibrary(ggplot2)\nlibrary(tidyverse)\n\n# load data file, which is a subset of the AEA registry to 1 January 2024, available from https://doi.org/10.7910/DVN/HPYPGH\n\n# code to create subset from the raw csv file as follows, although not executed as part of this document - raw file too large to maintain on website\n\n# trials &lt;- read_csv(\"trials.csv\")\n\n#remove columns not used in this analysis + RCT_ID\n# trials &lt;- trials %&gt;% \n#  select(RCT_ID, \"First registered on\", \"Intervention end date\", \"End date\", Status)\n\n#save as cvs named \"trials-processed-jan-2024.csv\"\n# write_csv(trials, \"trials-subset-jan-2024.csv\")\n\ntrials &lt;- read_csv(\"data/trials-subset-jan-2024.csv\")\n\n# set order for charts so completed is first, then abandoned, then other\ntrials &lt;- trials %&gt;%\n  rename(\n    intervention_end_date = \"Intervention end date\",\n    first_registered_on = \"First registered on\",\n    end_date = \"End date\"\n    ) %&gt;%\n  mutate(\n    first_registered_year = year(first_registered_on),\n    intervention_end_year = year(intervention_end_date),\n    end_year = year(end_date)\n    ) %&gt;%\n  mutate(Status = ifelse(Status == \"abandoned\" | Status == \"withdrawn\", \"abandoned_or_withdrawn\", Status)) %&gt;%\n  mutate(Status = factor(Status, levels = c(\"on_going\", \"abandoned_or_withdrawn\", \"in_development\", \"completed\")))\n\n# colourblind palette\ncbPalette &lt;- c(\"#999999\", \"#E69F00\", \"#56B4E9\", \"#009E73\", \"#F0E442\", \"#0072B2\", \"#D55E00\", \"#CC79A7\")\n\n# plot of status by year of registration\nggplot(trials, aes(fill=Status, x=first_registered_year)) + \n  geom_bar(position=\"fill\", stat=\"count\") + \n  scale_x_continuous(limits = c(2012.5, 2023.5), breaks = seq(2013, 2023)) +\n  scale_fill_manual(values=cbPalette) +\n  scale_y_continuous(labels = scales::percent_format()) +\n  labs(x = \"Year of registration\", y = \"Percentage of trials\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nCode\n# plot of status by reported end year\nggplot(trials, aes(fill=Status, x=end_year)) + \n  geom_bar(position=\"fill\", stat=\"count\") + \n  scale_x_continuous(limits = c(2012.5, 2025.5), breaks = seq(2013, 2025)) + \n  scale_fill_manual(values=cbPalette) +\n  scale_y_continuous(labels = scales::percent_format()) +\n  labs(x = \"End date\", y = \"Percentage of trials\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nThe proposed “end date” entered by registrants is likely the best benchmark for when we should see a result. Around half of the trials proposed to end between 2014 and 2019 have been marked as completed. The completion rate declines in more recent years. The registry gives a glimpse of the file drawer, but we see a non-random selection of what’s in it.\nEliot Abrams, Jonathan Libgoer and John List (2020) also catalogued some of the issues with the AEA Registry. Over 90% of RCTs don’t register. Only 50% of those who register do so before the intervention begins. And most preregistrations lack detail.\nNow to the recent story, via Jessica Hullman\nJohn Protzko and friends (2023) published a paper in Nature Human Behaviour, High replicability of newly discovered social-behavioural findings is achievable. Part of the abstract reads:\n\nThis paper reports an investigation by four coordinated laboratories of the prospective replicability of 16 novel experimental findings using rigour-enhancing practices: confirmatory tests, large sample sizes, preregistration and methodological transparency. In contrast to past systematic replication efforts that reported replication rates averaging 50%, replication attempts here produced the expected effects with significance testing (P &lt; 0.05) in 86% of attempts, slightly exceeding the maximum expected replicability based on observed effect sizes and sample sizes.\n\nIn the text, they note that they preregistered their analyses. We have a preregistered analysis of the benefits of preregisteration (or so it seems):\n\nAll confirmatory tests, replications and analyses were preregistered both in the individual studies (Supplementary Information section 3 and Supplementary Table 2) and for this meta-project.\n\nHowever, Joseph Bak-Coleman and Berna Devezer (2023) noted there are many forking paths in the analysis. This is particularly the case around the choice of metrics. Those chosen do not match past practice. Bak-Coleman and Devezer note that “preregistration could provide a justification for preferring one metric over another”. However, a problem arises when we look at the preregistration:\n\nAlthough the authors claim “All confirmatory tests, replications and analyses were preregistered”, the analysis on which titular claim depends was not preregistered. There is no mention of examining the relationship between replicability and rigor-improving methods, nor even how replicability would be operationalized despite extensive descriptions of the calculations of other quantities. With nothing indicating this comparison or metric it rests on were planned a priori, it is hard to distinguish the core claim in this paper from selective reporting and hypothesizing after the results are known.\n\nIt seems the reviewers didn’t bother to check that the analysis matched the pre-registration.\nProtzko and friends are now searching for a preregistration that might match their analysis. An editor’s note now appended to the paper reads:\n\nEditor’s Note: Readers are alerted that this paper is subject to criticisms that are being considered by the editors. The criticisms relate to the preregistration and corresponding reporting of hypotheses, predictions, and analyses. We are investigating the concerns raised and an editorial response will follow the resolution of these issues.\n\nFunnily enough, Protzko and friends provide evidence in support of preregistration. We have an analysis with many forking paths. A preregistration might have provided some constraint. Unfortunately, this case for preregistration is not the one that they intended.\n\n\n\n\nReferences\n\nAbrams, E., Libgober, J., and List, J. A. (2020). Research registries: Facts, myths, and possible improvements. NBER Working Paper. https://doi.org/10.3386/w27250\n\n\nBak-Coleman, J., and Devezer, B. (2023). Causal claims about scientific rigor require rigorous causal evidence. https://doi.org/10.31234/osf.io/5u3kj\n\n\nProtzko, J., Krosnick, J., Nelson, L., Nosek, B. A., Axt, J., … Schooler, J. W. (2023). High replicability of newly discovered social-behavioural findings is achievable. Nature Human Behaviour, 1–9. https://doi.org/10.1038/s41562-023-01749-9"
  },
  {
    "objectID": "posts/subject-notes-on-behavioural-economics.html",
    "href": "posts/subject-notes-on-behavioural-economics.html",
    "title": "Subject notes on behavioural economics",
    "section": "",
    "text": "Each year I teach an undergraduate subject in behavioural economics.\nI have pulled together the notes for the subject into a website, which you can find here. The notes include the subject content, plus “exercises” that form the basis for the tutorials. If you work through the content, you’re effectively getting the same content as my students, minus the interactive seminars with me, tutorials with the teaching assistants, and the assessments.\nI have also developed a set of videos to accompany the notes, which can be found here. The videos are effectively voice versions of the notes (or you can think of the notes as transcripts for the videos), so whether you watch or read, you will get the same content. The videos are largely voiced by me, although the more recent videos tend to have an AI voice, as I discuss here.\nI take a traditional approach in these notes, starting with the typical economic approaches to decision making (expected utility theory, exponential discounting, game theory) and then adding the behavioural twists (prospect theory, hyperbolic discounting, bounded rationality). Partly, I took this approach because I iterated toward this content over a couple of years from the version of the subject taught by previous lecturers. As a result, I’ve retained much of the past structure (although the content is new). But, there’s also some benefit in building a solid foundation in how behavioural economists traditionally approach the subject before expanding into new territory.\nI’m developing a new set of notes that take a more “Jason” approach, applying a critical lens to the research, pulling in interdisciplinary content, and framing humans as good decision makers given our computational constraints. I’ll post those notes when they are ready.\nPlease feel free to take these notes and use them for your own purposes; the notes are CC-BY. The site is generated using quarto, so you can also clone the repository and make your own version of the notes, or simply take the text files underlying the website.\nFinally, I have several other sets of subject notes, including material on financial and corporate decision making. This more applied content is linked here."
  },
  {
    "objectID": "posts/replicating-scarcity.html",
    "href": "posts/replicating-scarcity.html",
    "title": "Replicating scarcity",
    "section": "",
    "text": "In Scarcity: Why having too little means so much, Sendhil Mullainathan and Eldar Shafir tell the following (now famous) story:\n\nTo see the effect of scarcity on fluid intelligence, we ran some studies with our graduate student, Jiaying Zhao, in which we gave people in a New Jersey mall the Raven’s Progressive Matrices test. First, half the subjects were presented with simple hypothetical scenarios, such as this one:\nImagine that your car has some trouble, which requires a $300 service. Your auto insurance will cover half the cost. You need to decide whether to go ahead and get the car fixed, or take a chance and hope that it lasts for a while longer. How would you go about making such a decision? Financially, would it be an easy or a difficult decision for you to make?\nWe then followed this question with a series of Raven’s Matrices problems. Using self-reported household income, we divided subjects, by median split, into rich and poor. In this setup we found no statistically significant difference between the rich and poor mallgoers. …\nFor the remaining subjects, we ran the same study but with a slight twist. They were given this question instead (with the change shown in bold):\nImagine that your car has some trouble, which requires an expensive $3,000 service. Your auto insurance will cover half the cost. You need to decide whether to go ahead and get the car fixed, or take a chance and hope that it lasts for a while longer. How would you go about making such a decision? Financially, would it be an easy or a difficult decision for you to make?\nAll we have done here is replace the $300 with $3,000. Remarkably, this change affected the two groups differently. …\n… The well-off subjects … did just as well here as if they had seen the easy scenario. The poorer subjects, on the other hand, did significantly worse. A small tickle of scarcity and all of a sudden they looked significantly less intelligent. …\nWe have run these studies numerous times, always with the same results. …\nIn all the replications, the effects were equally big. …\n… our effects correspond to between 13 and 14 IQ points. By most commonly used descriptive classifications of IQ, 13 points can move you from the category of “average” to one labeled “superior” intelligence. Or, if you move in the other direction, losing 13 points can take you from “average” to a category labeled “borderline deficient.”\n\nI have long considered this finding, drawn from a paper in Science by Mani and friends (2013) (pdf), to be one of the most unreliable findings in the behavioural sciences.\nIt fails the “effect is too large” heuristic. This can’t be anything but a massively biased effect size.\nIt relies on a prime. We have all seen how the priming literature has fared.\nAnd despite the claim “We have run these studies numerous times, always with the same results”, for many years I’ve heard of failed replications and extensions stuffed into file drawers.\nAs a result, I’ve been keeping a keen eye on the topic and was excited to see a story about Leif Nelson and Don Moore training PhD students by doing a mass pre-registered replication of the scarcity literature. A great idea.\nThe result of that exercise has now been published in PNAS. The replication materials are available on OSF.\nThe punchline was that, of the 20 studies, only four generated a significant result. (That raw score of 20% is underperforming the replicability of social psychology.) This was despite the replications using 2.5 times the sample size of the original, giving them more power. Six studies had effects in the opposite direction to the original. Effect sizes were also markedly smaller, suggesting the initial experiments were massively underpowered. The following figure illustrates the results.\n\nStudy 4 from Mani and friends was one of the iterations of the New Jersey experiment and, as for most of them, there was no significant effect.\nSummarising their findings, the authors write:\n\nScarcity is a real and enduring societal problem, yet our results suggest that behavioral scientists have not fully identified the underlying psychology. Although this project has neither the goal nor the capacity to “accept the null” hypothesis for any of these tests, the replications of these 20 studies indicate that within this set, scarcity primes have a minimal influence on cognitive ability, product attitudes, or well being. Nevertheless, feeling poor may influence financial and consumer decisions.\n\nIt’s not time to write off the concept of scarcity. A few of the results replicated, at least one them surprisingly so for me (economic insecurity causing physical pain). These were also lab experiments, so shouldn’t necessarily be used to ignore scarcity research from the field.\nBut it is time to take a more nuanced view of scarcity. The term captures what I consider to be several different phenomena. Which of these concepts might hold? And where? And it is definitely time to retire the story of a hypothetical car repair dropping a poor person’s IQ by 13 to 14 points.\nOn that more nuanced view, I have a rather long post (or possible review article) on scarcity in the works. This replication exercise is another great input. Hopefully my post will see the light of day this year."
  },
  {
    "objectID": "posts/john-lists-the-voltage-effect-a-review.html",
    "href": "posts/john-lists-the-voltage-effect-a-review.html",
    "title": "John List’s The Voltage Effect: A review",
    "section": "",
    "text": "Over a decade ago when I started reading the behavioural economics literature, John List quickly became one of my favourite academics. Whenever I read an interview with List he always seemed to ask great, critical questions. He was rarely happy taking others’ assumptions as given. I saw him as someone who, on hearing “in my experience….”, would be the first to say “Should we run an experiment?”.\nMy impression of List has somewhat changed in the last year. His Twitter posts seem more focused on promotion than shooting down weak ideas. There goes my image of List as a cynic! But I also suppose that’s the angle you need to take when you are pushing a new book.\nThat book is The Voltage Effect: How to Make Good Ideas Great and Great Ideas Scale.\nThe term Voltage Effect comes from the literature on implementation science. The “electric” potential of what appears to be a promising intervention will often dissipate when rolled out into the real world. In List’s case, this “voltage drop” occurs between the initial pilot of an economic intervention and the full-scale rollout of the intervention. Why are those rollouts so often disappointing despite the initial promise?\nList seems the right person to answer that question. List pioneered the use of field experiments in economics. He was the Chief Economist for Uber, then Lyft and now Walmart. (I believe the Walmart role post-dates this book.) He has a rare catalogue of field trials and implementation experience.\nBut rather than being the author of this book, I would rather see List in another role. I’d love to read a review of this book by ….. John List.\nWhy?\nIn the book’s first half, List describes five problems that can cause voltage drops. In the second half, he gives four “secrets” to high-voltage scaling. I agreed with most of his high-level points in the first half and saw some potential in those in the second.\nBut the problem is that these points aren’t drawn from experiments themselves. They come from List’s experience.\nJohn List’s forte is that he runs experiments to see if things are true. Throughout the book, List has stories where someone makes a claim, and List responds, “Let’s run an experiment”. But when he tries to talk about what works at the level above the experiment - the book’s theme - we have to listen to John List say, “Well, in my experience …”. I respect List’s experience but kept thinking, “Maybe that’s true, but I’m not convinced … could we run an experiment?”."
  },
  {
    "objectID": "posts/john-lists-the-voltage-effect-a-review.html#the-five-problems-that-cause-voltage-drops",
    "href": "posts/john-lists-the-voltage-effect-a-review.html#the-five-problems-that-cause-voltage-drops",
    "title": "John List’s The Voltage Effect: A review",
    "section": "The five problems that cause voltage drops",
    "text": "The five problems that cause voltage drops\nList’s five barriers to scaling all make sense. Watch out for false positives. Know your audience. Make sure the ingredients to your implementation can scale. Consider spillovers. And understand what scaling will cost.\nOn the first barrier, List notes that many ideas might seem promising during trials. But the statistical frameworks that we use can lead to false positives. Don’t be afraid to use a second trial or staged implementation to check that the first trial wasn’t a fluke. You should seek independent replication of your own work. And consider the incentives of those who are pushing any particular idea.\nI agree with this advice, although perhaps with a different emphasis. List doesn’t give much ink to the replication crisis or mention that the behavioural science literature is littered with false positives. Some of the literature you’re using as inspiration is likely rubbish. List does recognise scientific misconduct - telling the story of Brian Wansink - but doesn’t hint at how untrustworthy much of the literature is, even in the absence of fraud.\nI am also sceptical of List’s reasons why false positives can cause such problems. One reason he gives is confirmation bias, which prevents us from seeing evidence that might challenge our assumptions. He even gives us an evolutionary spin:\n\nThis tendency might appear counter to our own interests, but in the context of our species’s long-ago Darwinian history, confirmation bias makes perfect sense. Our brain evolved to reduce uncertainty and streamline our responses. For our ancestors, a shadow might have meant a predator, so if they assumed it was one and started running, this assumption could have saved their lives. If they stopped to gather more information and really think about it, they might have ended up as dinner.\n\nI’m not sure what is being described in that paragraph is confirmation bias. But more notably, we seem to have entered “drop a bias” territory: see a behaviour and tell a post-fact story about what bias is supposedly causing this problem. List tells us about the bandwagon effect, where we jump on board with others, effectively leaving the selection of ideas to a few people rather than the collective group. We hear about sunk cost bias. List even shoehorns the winner’s curse into the story, a phenomenon in common-value auctions (an auction where the auctioned good has the same value to everyone) whereby the winner tends to overpay. The winner will tend to have the most optimistic estimate of the value of the good - and if you’ve got a higher estimate than everyone else, there’s a decent chance you’ve overestimated. How does this apply to scaling false positives? After reading the section several times, I’m not sure.1\nAnd where’s the experimental evidence that these biases are driving the problem? At this point, List’s stories feel loose.\nList’s second barrier to scaling relates to” knowing your audience”. This is essentially a story of heterogeneity in your sample and the population in which the intervention will be rolled out. How broadly will your intervention work when you move from your trial sample to full-scale implementation? This includes a perspective across time: your current and future audience might differ. Sound advice.\nThe third barrier is whether all the elements of the intervention can scale. Is the pilot representative of the circumstances at scale? For example, if your educational intervention pilot involves all of the good teachers, it likely won’t work when it scales and the weaker teachers become involved. If you are monitoring compliance in the pilot - that is, the experimenters are watching - failure to monitor during the rollout may result in a substantial voltage drop. I have seen this myself, where the deck was stacked in the pilot.\nTo illustrate this point, List tells the story of the restaurant chain Jamie’s Italian. Initially, the key to the restaurants was the simple ingredients and recipes, easily replicable at a larger scale. Brand and fame can also scale. But later on the unscalable part of the operation - Jamie Oliver himself - became stretched, and the whole enterprise came down.\nI understand why people include stories like this in pop science books. I use examples like this myself. But this involves going out on a limb, ascribing causes to events that aren’t backed by a robust analysis of causation. We’re telling stories.\nList’s story involves the new managing director of the enterprise being described as “woefully unqualified”. But what’s the metric for “unqualified” beyond the post-fact assessment based on the chain’s failure? Is this just the (negative) halo of the outcome being extended to those involved? (There’s me dropping a bias…) I’ll return to this point of storytelling in a bit.\nThe fourth barrier is the presence of spillovers. List has a nice story from Uber to illustrate. An initial trial of giving $5 coupons to a group of Uber riders was a success: the riders used Uber more, with the increased earnings more than offsetting the cost of the coupons. However, when rolled out at scale, the earnings increase did not materialise. The spike in demand caused by the coupons caused an increase in fares and wait times, which reduced demand.\nThe final barrier concerns the ultimate cost of scaling. Sometimes expected economies of scale don’t materialise. There simply might not be enough of what you need at a reasonable cost: for example, what is the cost of hiring enough “good” teachers to implement at scale?"
  },
  {
    "objectID": "posts/john-lists-the-voltage-effect-a-review.html#the-four-secrets-to-scaling",
    "href": "posts/john-lists-the-voltage-effect-a-review.html#the-four-secrets-to-scaling",
    "title": "John List’s The Voltage Effect: A review",
    "section": "The four secrets to scaling",
    "text": "The four secrets to scaling\nFrom the barriers List then moves to the secrets to success. I was more ambivalent toward the four secrets, but they still have some wisdom.\nThe first is having incentives that scale. This isn’t just about the size of the incentives, but considering how they can be made more powerful (e.g. by using loss aversion) and whether non-monetary incentives such as social norms can be leveraged. Most of this is relatively uncontroversial material. It is, after all, the reason why there has been some success in applied behavioural economics. But the following claim stood out:\n\n[T]he fact that human psychology doesn’t vary much across groups (most people experience loss aversion to a similar degree, and nearly everyone cares about their social image) makes this type of incentive strategy highly scalable. In contrast, the level of financial rewards necessary to incentivize one’s employees will vary widely from person to person, and for some might be unduly high.\n\nI couldn’t locate in the book the reference supporting this claim. And it doesn’t align with any of the studies I might look to on this point. On variation, Joe Henrich and friends (2001) showed substantial variation across societies in trust and reciprocity in economic games. Harrison and Rutström (2009) showed evidence for a mix of “economic” and “behavioural” decision makers in a group of experimental subjects. The whole concern about experiments on WEIRD people is due to the potential for variation across cultures.\nTo be honest, I believe that the worry about the generalisability of experiments using WEIRD subjects is overblown (particularly compared to the other dimensions we should be worrying about). The Many Labs 2 project (Klein et al., 2018) found little heterogeneity across cultures as to which effects were replicable. But I would expect an equivalent experiment using incentives would similarly replicate across cultures.\nOn the power of psychological interventions versus incentives, ideally, we’d want a suite of studies comparing the two directly. List has a couple of studies where the psychological intervention outperforms the incentive. But it’s easy enough to find studies going the other way. Here’s one from DellaVigna and Pope (2018). In their particular (artificial) context, incentives were the more powerful motivator and effort scaled with the incentives. Amusingly, the “experts” predicting the results also underestimated the power of the incentives.\n\nPutting this together, I’m sure there are cases where a behavioural strategy will outperform a simple monetary incentive. But I don’t buy the general claim about variability or scalability.\nList’s second secret seems a no-brainer, to an economist at least. When calculating costs and benefits, look to the margins. Ask what benefit the last dollar spent gets you. Don’t simply compare total costs and benefits as, even if the total benefits are greater, you could be spending less for greater net benefit.\nThe third secret is optimal quitting. Don’t sink more time into the wrong idea. We get stories about PayPal and Twitter emerging from other failing strategies, and List’s own decision to give up on a professional golf career. We get another dose of biases here, including sunk cost bias and ambiguity aversion, but it’s sound advice all the same. The challenge is, as List highlights, determining the optimal point to quit. It’s hard.\nThe chapter on the fourth and final secret, the need to scale culture, opens with the story of two Brazilian villages, Cabuçu and Santo Estêvão.\nCabuçu is a small fishing community in which the men fish in groups of three to eight. This teamwork is needed to manage the boats on the choppy Atlantic waters, set the nets and pull in the large fish. A single fisherman would fail.\nSanto Estêvão sits inland of Cabuçu on the Paraguaçu rover. The villagers from Santo Estêvão catch small fish alone on the calm waters.\nList and colleagues ran games in the two villages to test whether their style of fishing, shaped by the environment each found themselves in, was reflected in their level of trust and cooperation. They found that the villagers from Cabuçu proposed more equal offers in the Ultimatum game, contributed more in public goods games and invested more in the trust game (and sent back more). They also donated more to charity. The habit of cooperation in their work flowed over into other domains.\nList leverages this story into one of scaling up workplace cultures. How the organisation works will affect the values underlying that organisation.\nThis leads to the case of Uber and Travis Kalanick. The story, in a nutshell, was a series of scandals in early 2017 - claims of sexism and sexual harassment, a lawsuit by Waymo alleging the theft of trade secrets, a video of Kalanick berating an Uber driver who challenged him on driver compensation, and most damaging, the revelation of software designed to evade law enforcement and regulators - ultimately led to Kalanick resigning as CEO.\nList pins this series of scandals on Uber’s culture. Kalanick believed in a “meritocracy” where the best ideas should win. For those ideas to emerge, Uber required a combative culture. However, as Uber scaled that combative culture no longer did. Rather than surfacing the best ideas, ideas succeeded based on who was the loudest or the best internal politician. When the best ideas and people ceased to rise to the top, staff lost trust in leadership and the behaviour of staff shifted.\nUltimately, when Kalanick exited, he conceded these flaws. Kalanick wrote:\n\nI favored logic over empathy, when sometimes it’s more important to show you care than to prove you’re right, … I focused on getting the right individuals to build Uber, without doing enough to ensure we’re building the right kind of teams.\n\nand\n\nUltimately, we lost track of what our purpose is all about—people. We forgot to put people first and as we grew, we left behind too many of the inspiring employees we work with and too many of the amazing partners who serve our cities …. Growth is something to celebrate, but without the appropriate checks and balances can lead to serious mistakes. At scale, our mistakes have a much greater impact—on our teams, customers and the communities we serve. That’s why small company approaches must change when you scale. I succeeded by acting small, but failed in being bigger.\n\nThis might all be right, but I find it amusing hearing a story about how the founder of a company, personally worth over $4b, stuffed up in creating their (as of the day of writing this paragraph) $87b market cap company by having the wrong culture. I am sure Kalanick could have done better, but I’m also sure he could have done worse. (Lyft, the company to which List moved after Uber, has a market cap of $3.8b.) Would a more toned-down Kalanick have been able to build Uber? Could someone with the personality to drive the smaller company simply transition as required? And is it even optimal to do so?\nThere’s a lot of “halo effect” storytelling going on here - problems emerge, must have been the culture, a disaster waiting to happen. And yet by the measurable outcome, arguably the number one measure for a company, Uber is a massive success for Kalanick. It reminds me of those criticizing the range of choice on Amazon as paralysing consumers with excessive choice. And what would happen if you tried to get the fisherman of Santo Estêvão to work in teams? Maybe they’d cooperate more in economic games, but would they do better at feeding their families?"
  },
  {
    "objectID": "posts/john-lists-the-voltage-effect-a-review.html#concluding-note",
    "href": "posts/john-lists-the-voltage-effect-a-review.html#concluding-note",
    "title": "John List’s The Voltage Effect: A review",
    "section": "Concluding note",
    "text": "Concluding note\nAs is typically the case for posts on this blog, most of the above is picking at the seams. But there is a lot to like in the book and some good advice. Thinking about each of the barriers and secrets should improve the experiments you run and how they scale. I’m just not sure all of the stories List tells are true."
  },
  {
    "objectID": "posts/john-lists-the-voltage-effect-a-review.html#other-thoughts-lying-around",
    "href": "posts/john-lists-the-voltage-effect-a-review.html#other-thoughts-lying-around",
    "title": "John List’s The Voltage Effect: A review",
    "section": "Other thoughts lying around",
    "text": "Other thoughts lying around\n\nIn his discussion of confirmation bias as a reason why false positives can cause such problems, List states “Because we have limited brainpower to process all of this, we use mental shortcuts to make quick, often gut-level decisions. One such mental shortcut is to essentially filter out or ignore the information that is inconsistent with our expectations or assumptions.” That second sentence is almost the opposite of perceptual control theory / predictive processing, which is based on the idea that our brain responds to differences from our expectations.\nList states: “If that suggestion sounds outrageous, consider an explosive study conducted by researchers at Johns Hopkins Medicine in 2016, which estimated that more than 250,000 Americans die each year from medical errors, making such errors the third-leading cause of death behind heart disease and cancer!” This stat is rubbish and there are a pile of academic and other papers pulling it apart. Here’s one\nList writes: “Kahneman and Tversky demonstrated that as a result of this human tendency, we make all sorts of harebrained decisions. For instance, when housing prices fall after a boom, sellers attempt to avoid the inevitable loss by setting a higher asking price—often higher than their home’s current value—and in turn the property sits on the market for a longer period of time than it otherwise might.” Genesove and Mayer (2001), who I assume is the source of List’s claim, also found that those people receive a higher sale price. Sounds sensible rather than harebrained to me, particularly when other evidence suggests people sell too fast (such as Levitt and Syverson (2008)).\nList jumps on the diversity bandwagon: “I mean diversity in all senses: race, sex, age, ethnicity, religion, class background, sexual orientation, gender identity, neurotype, and other characteristics. Diversity in people’s backgrounds equates to cognitive diversity when they are together, which produces not just greater innovation but also greater resilience.” List wants all these types of diversity to achieve “cognitive diversity”, but doesn’t mention cognitive diversity itself. We could measure it! And as usual, you seldom see “political views” on the list. Among the many disciplines and groups I have worked with (law, economics, data science, regulators, policy advisers, environmental campaigners), the behavioural scientists and behavioural economists would be the most cognitively uniform group."
  },
  {
    "objectID": "posts/john-lists-the-voltage-effect-a-review.html#footnotes",
    "href": "posts/john-lists-the-voltage-effect-a-review.html#footnotes",
    "title": "John List’s The Voltage Effect: A review",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nA related point might be that if you are running many comparisons, the most promising will likely have an exaggerated effect size, but there are simple statistical methods (that I expect people like List would be using) to correct for this.↩︎"
  },
  {
    "objectID": "posts/is-following-ai-advice-anchoring-bias.html",
    "href": "posts/is-following-ai-advice-anchoring-bias.html",
    "title": "Is following AI advice “anchoring bias”?",
    "section": "",
    "text": "Want to demonstrate human “irrationality”? Ask half of your audience the following question:\nAsk the other half:\nThen ask both groups:\nThose asked if Elvis was older or younger than age 45 tend to estimate a lower age of death than those asked about age 75.\nThis is an illustration of the “anchoring and adjustment” heuristic. When people estimate a quantity, they often start from an initial value (the anchor) and adjust from that anchor to get a final answer. The heuristic can be biased where we use weak anchors or insufficiently adjust from the anchor.\nOne famous experiment on anchors was by Tversky and Kahneman (1974). They spun a wheel with numbers between 0 and 100 on it, but it was rigged to stop at either 10 or 65. They then asked experimental participants who had seen the spin whether the proportion of African countries who were members of the United Nations was above or below that number (this experiment was in the 1970s). The participants were also asked for a numerical estimate. Those who saw 10 on the wheel estimated 25 percent. Those who saw 65 estimated 45 percent. The (assumed) randomness of the anchor was a nice experimental feature. In my Elvis example above, the respondent might assume I chose the number (45 or 75) for a reason. There might be information in that number. There is no such information in the spinning of the wheel.\nInsufficient adjustment from the anchor is the flipside of using an irrelevant anchor. If there is any trace of an irrelevant anchor in your answer, you haven’t adjusted enough. However, many anchors we use are sensible starting points - we just don’t adjust enough for our particular case.\nAlthough anchoring is often called a “bias”, it is better to think of anchoring as a heuristic that can backfire in some task environments. If you’re considering making an offer for a four-bedroom house, it’s not a bad strategy to start with the sale price of the neighbouring three-bedroom house. Adjust for the extra bedroom and any other differences between the two. Griffiths et al. (2015) argued that anchoring and adjustment can be “resource-rational”, as accounting for the computational cost makes anchoring and adjustment an optimal estimation strategy.\nWhich brings me to the question of anchoring to artificial intelligence (AI) advice.\nA challenge in human-AI interaction is calibrating the user’s trust in the AI system. When should a user trust the AI? When should they deviate from the AI recommendation? Most experimental evidence suggests we don’t calibrate well. Achieving complementary performance - where the human-AI team outperforms either alone - is difficult. Typically, AI assistance improves human performance but the combination still underperforms the AI working alone.\nRastogi et al. (2022) proposed that one obstacle to complementary performance is “anchoring bias”. People do not explore the alternative hypotheses once the AI decision has provided an anchor.\nTo examine this question, Rastogi et al. (2022) conducted two experiments, which I walk through below."
  },
  {
    "objectID": "posts/is-following-ai-advice-anchoring-bias.html#experiment-1",
    "href": "posts/is-following-ai-advice-anchoring-bias.html#experiment-1",
    "title": "Is following AI advice “anchoring bias”?",
    "section": "Experiment 1",
    "text": "Experiment 1\nThe authors asked a group of Amazon Mechanical Turk workers to use student data to predict whether a student would pass or fail a class. The data, drawn from a student performance dataset, included student characteristics, past performance and demographics. The workers were also provided with an AI model recommendation (logistic regression) based on the 10 most important student features (e.g. mother’s and father’s education, hours spent studying weekly).\nParticipants completed 15 training examples, during which the authors deliberately misled them about AI performance:\n\nTo induce anchoring bias, the participant was informed at the start of the training section that the AI model was 85% accurate (we carefully chose the training trials to ensure that the AI was indeed 85% accurate over these trials), while the model’s actual accuracy is 70.8% over the entire training set and 66.5% over the test set. Since our goal is to induce anchoring bias and the training time is short, we stated a high AI accuracy.\n\nAfter each training estimate, the workers were shown the correct answer and the estimate made by the AI.\nEffectively, participants are told the AI is 85% accurate and then see performance through the training set that aligns with that accuracy. The exact wording isn’t provided, but the intent is clear. I’m reluctant to call behaviour “biased” when it’s based on information designed to deceive. (Deception is common in human-AI experiments. I have another post on this coming.)\nThe test section had 36 trials. The AI accuracy was even lower than 66% because the experimenters deliberately “flipped” eight AI predictions, reducing task accuracy to 58%. Workers who followed these flipped predictions (called “probe trials” by the authors) would perform worse than those receiving unmanipulated AI advice.\nThe primary hypothesis the authors tested in Experiment 1 is whether anchoring is reduced if the workers were given more time to make their prediction. Workers were given 10, 15, 20 or 25 seconds to make their prediction. If they were less likely to follow the erroneous prediction in those flipped trials, that would be evidence of reduced anchoring.\nThe results at first glance, as presented in the following figure, support the hypothesis. Workers with 25 seconds were more likely to deviate from the incorrect recommendation. This might be thought of as being in line with Griffiths et al. (2015), in that more resources enable more calculation and adjustment.\n\n(I also suspect there is an error in this chart. The average disagreement for the non-probe trials is unbelievably low.)\nOnly one statistical test is reported for the pattern in the figure. No p-value is given, but the confidence intervals suggest it barely reaches significance. The authors also don’t report broader accuracy data.\nThe lack of pre-registration is a concern. There are many ways to test the effect of time on anchoring. The authors used a linear regression on 5,000 bootstrapped resamples - a choice that isn’t explained. Other approaches might yield different results.\nFurther, like much human-computer interaction literature, the paper provides limited results without public data, making it impossible to test robustness or explore alternative analyses.\nFinally, “adjustment” itself is hard to measure here. The only observable signal is disagreement with the AI. That leaves open whether participants are truly adjusting from an anchor or simply deferring to the AI unless they see clear reasons not to. These are two distinct processes."
  },
  {
    "objectID": "posts/is-following-ai-advice-anchoring-bias.html#experiment-2",
    "href": "posts/is-following-ai-advice-anchoring-bias.html#experiment-2",
    "title": "Is following AI advice “anchoring bias”?",
    "section": "Experiment 2",
    "text": "Experiment 2\nIn experiment 2, the authors test a broader set of interventions to reduce “anchoring bias”. They also report richer data (in the form of a more detailed chart), so we’re able to see a little more of the dynamic.\nExperiment 2 used the same task but the experimenters degraded the AI by training it without three important features: study hours, social hours, and educational support enrolment. Workers could see these features, creating an information asymmetry where human input could potentially improve AI predictions.\nThe authors don’t state the degraded AI’s accuracy. Workers were again told it had 85% accuracy during training, but its actual accuracy on the main task was around 60%.\nWorkers were then placed in the following conditions.\n\nHuman only: Workers provide their prediction without an AI prediction. They are given 25 seconds for each prediction\nConstant time: Workers provide their prediction with the help of the AI prediction, with 18 second per prediction.\n\nRandom time: AI assistance with the time allocation randomly set to either 10 seconds or 25 seconds each trial.\n\nConfidence-based time: AI assistance with 10 or 25 seconds depending of AI confidence (low confidence = more time).\n\nConfidence-based time with explanation: As for confidence-based time but the AI confidence is explicitly provided (“low” or “high”).\n\nEach of these conditions aligns with a hypothesis that they might reduce anchoring. The one I will focus on is H2: Anchoring bias has a negative effect on human-AI collaborative decision-making accuracy when AI is incorrect.\nThe results are summarised in the following figures.\n\nThe human only group has higher accuracy than all of the AI groups on instances where the AI is incorrect. The authors take this as evidence in support of H2.\nThis raises the question of whether “anchoring bias” is the right frame. To avoid lower accuracy when the AI errs, workers would need to ignore the AI completely on those trials, removing the anchor entirely. But that’s unreasonable. The workers were told the AI has 85% accuracy, and their training experience confirmed it.\n“Anchoring” is the wrong label here and “anchoring bias” even more so. If using information degrades a decision, that doesn’t make the use of that information a bias.\nTo determine whether behaviour is biased, we need a model that accounts for the information the experimenters provided. The authors call the AI estimate an anchor. It’s better understood as a prior, shaped by the experimenters’ claim of 85% accuracy. In Bayesian terms, participants start with a belief about the AI’s accuracy and update it as they see evidence.\nIn experimental economics, where deception is forbidden, participants would rationally place high confidence in such a claim. The prior is strong. Even in this human-computer interaction experiment, treating the stated accuracy as credible information (or at least having some signal) is reasonable behaviour, not bias.\nIf one of my kids tells me they’ve got a normal coin and flip two heads, I update only marginally from my strong prior that the coin is fair. What is the level of trust of experimental participants in experimenters? What is the appropriate level of trust? (One reason deception is frowned upon in economics is because it degrades trust in experimenters, leading to an additional variable you need to account for in your analysis.)\nTurning back to the figure, the results also point to a complementary hypothesis not identified by the authors:\n\nAnchoring bias has a positive effect on human-AI collaborative decision-making accuracy when AI is correct.\n\nAgain, for this not to be true, we would need the person to completely ignore the AI. And unsurprisingly, eyeballing the figure, my hypothesis is true. The AI conditions all markedly outperform the human-only group on estimates where the AI recommendation is correct.\nThere’s an interesting pattern across all the conditions. Whenever you decrease the probability of following an incorrect AI recommendation, you also decrease the probability of following a correct recommendation. The result is that performance across all conditions is largely the same. Because the AI in this experiment is degraded to a quality similar to that of humans alone, reliance on the AI is largely a wash.\nFinally, the authors provide another interesting interpretation when they state that they achieve complementary knowledge. Due to the three excluded variables, the human has more information than the AI. There are a lot of real-world application where the human will have information the AI doesn’t - an AI-financial adviser for example - so finding ways to enhance the use of unshared information is vital to achieving complementary performance. In this case, we get complementary knowledge in that the humans do as well as the AI alone despite the fact they don’t always follow the AI. Unfortunately, we don’t get complementary performance - that is, outperformance by the human-AI team of both the human and AI alone. But examining how to get people to use unshared information seems a prospective path."
  },
  {
    "objectID": "posts/is-following-ai-advice-anchoring-bias.html#the-problem-is-more-often-insufficient-anchoring",
    "href": "posts/is-following-ai-advice-anchoring-bias.html#the-problem-is-more-often-insufficient-anchoring",
    "title": "Is following AI advice “anchoring bias”?",
    "section": "The problem is more often insufficient anchoring",
    "text": "The problem is more often insufficient anchoring\nThis experiment involved an AI with capability on par with unassisted humans. But in most statistical tasks such as the one in this experiment, the AI outperforms the human. Further, human-AI teams tend to underperform the AI alone.\nIn other words, experiments typically find that people adjust too much! Here’s some data from Dietvorst et al. (2018). People were more likely to use an algorithm if they could adjust it. That led to higher performance, largely because of this greater likelihood of selecting the model.\n\n\nDietvorst et al. (2018) Figure 2b\n\n\n\nBut how does the performance of those who selected the model compare to the model itself. Here is a chart I have generated from the experimental data provided in the supplementary materials. (Again noting that business schools more often share data.) In the “Adjust by 10” group, the participant’s adjustments were a wash, with their performance on par with the unadjusted model. But for those in the “Change 10” group, their changes increased the error. The AI alone was a stronger performer.\n\n\nCode\nlibrary(tidyverse)\nlibrary(scales)\n\n# Read the data - specify na values to handle periods as missing\ndata &lt;- read_csv(\"data/Overcoming_Algorithm_Aversion_Data-study-1-data.csv\",\n                 na = c(\"\", \"NA\", \".\"))\n\n# Convert necessary columns to numeric (many are stored as strings due to periods)\ndata &lt;- data %&gt;%\n  mutate(across(c(ModelBonus, ModelAAEEstimate, HumanAAEEstimate,\n                  ModelConfidence, HumanConfidence, Age, Gender, Education,\n                  AAE, ModelAAE, HumanAAE, Bonus, BonusFromModel, BonusFromHuman,\n                  CorrelationWithModel, AvDiffFromModel, AvAdjustmentSize,\n                  AdjustmentDividedByPotential, HumModelCorrelation, HumModelAvDiff,\n                  AvgLargest10Changes, AnyBonus), \n                as.numeric))\n\n# Define condition labels\ncondition_labels &lt;- c(\"1\" = \"Can't change\", \n                     \"2\" = \"Adjust by 10\", \n                     \"3\" = \"Change 10\", \n                     \"4\" = \"Use freely\")\n\n# Filter for participants who chose the model (ModelBonus == 1)\nmodel_choosers &lt;- data %&gt;%\n  filter(ModelBonus == 1)\n\n# Calculate summary statistics by condition\nsummary_stats &lt;- model_choosers %&gt;%\n  group_by(Condition) %&gt;%\n  summarise(\n    n = n(),\n    ModelAAE_mean = mean(ModelAAE, na.rm = TRUE),\n    AAE_mean = mean(AAE, na.rm = TRUE),\n    ModelAAE_sd = sd(ModelAAE, na.rm = TRUE),\n    AAE_sd = sd(AAE, na.rm = TRUE),\n    Difference = AAE_mean - ModelAAE_mean,\n    PercentChange = (Difference / ModelAAE_mean) * 100,\n    AvgAdjustmentSize = mean(AvAdjustmentSize, na.rm = TRUE)\n  ) %&gt;%\n  mutate(ConditionLabel = condition_labels[as.character(Condition)])\n\n# Add info about total participants per condition\ntotal_by_condition &lt;- data %&gt;%\n  group_by(Condition) %&gt;%\n  summarise(total_n = n())\n\nsummary_stats &lt;- summary_stats %&gt;%\n  left_join(total_by_condition, by = \"Condition\") %&gt;%\n  mutate(PropChoseModel = n / total_n)\n\n# Prepare data for plotting\nplot_data &lt;- summary_stats %&gt;%\n  filter(!is.na(AAE_mean) & Condition != 1) %&gt;%  # Remove can't change condition and any conditions with no data\n  select(Condition, ConditionLabel, ModelAAE_mean, AAE_mean) %&gt;%\n  pivot_longer(cols = c(ModelAAE_mean, AAE_mean),\n               names_to = \"ErrorType\",\n               values_to = \"AAE\") %&gt;%\n  mutate(\n    ErrorType = case_when(\n      ErrorType == \"ModelAAE_mean\" ~ \"Model (no adjustment)\",\n      ErrorType == \"AAE_mean\" ~ \"Actual (with adjustment)\"\n    ),\n    ErrorType = factor(ErrorType, levels = c(\"Model (no adjustment)\", \"Actual (with adjustment)\"))\n  )\n\n# Create the plot\np &lt;- ggplot(plot_data, aes(x = ConditionLabel, y = AAE, fill = ErrorType)) +\n  geom_bar(stat = \"identity\", position = \"dodge\", width = 0.7) +\n  scale_fill_manual(values = c(\"Model (no adjustment)\" = \"#3498db\", \n                              \"Actual (with adjustment)\" = \"#e74c3c\")) +\n  labs(\n    title = \"Error Rates for Model Choosers by Adjustment Condition\",\n    subtitle = \"Comparing model performance with and without adjustments\",\n    x = \"Condition\",\n    y = \"Average Absolute Error (AAE)\",\n    fill = \"Error Type\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 16, face = \"bold\", hjust = 0.5),\n    plot.subtitle = element_text(size = 12, hjust = 0.5),\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    legend.position = \"top\",\n    legend.title = element_text(face = \"bold\"),\n    panel.grid.minor = element_blank()\n  ) +\n  scale_y_continuous(limits = c(0, max(plot_data$AAE, na.rm = TRUE) * 1.1), \n                     breaks = pretty_breaks(n = 5)) +\n  # Add percentage change labels on top of bars\n  geom_text(data = summary_stats %&gt;% filter(!is.na(AAE_mean) & Condition != 1),\n            aes(x = ConditionLabel, y = AAE_mean + 0.5, \n                label = paste0(ifelse(Difference &gt;= 0, \"+\", \"\"), \n                               round(PercentChange, 1), \"%\")),\n            inherit.aes = FALSE,\n            size = 3.5,\n            fontface = \"bold\")\n\n# Display the plot\nprint(p)\n\n\n\n\n\n\n\n\n\nA meta-analysis by Vaccaro et al. (2024) examined 370 effect sizes from 106 studies. Human-AI combinations usually underperformed the better of the two working alone, with the exception of creative tasks where teams more often outperformed individuals.\n\nBringing this back to the anchoring experiment, if the AI actually had accuracy of 85% as the experimental participants were informed, I suspect we would be talking about a different problem: insufficient anchoring. Anchoring and overadjustment. That’s a very different problem to solve.\nThe challenge, therefore, is not reducing “anchoring bias” but recognising when the anchor contains useful information and calibrating how much to adjust. It’s typically a problem of anchors being too weak, not too strong."
  },
  {
    "objectID": "posts/human-ai-collaboration-is-it-better-when-the-human-is-asleep-at-the-wheel.html",
    "href": "posts/human-ai-collaboration-is-it-better-when-the-human-is-asleep-at-the-wheel.html",
    "title": "Human-AI collaboration: is it better when the human is asleep at the wheel?",
    "section": "",
    "text": "In his book Co-Intelligence: Living and Working with AI, Ethan Mollick describes an experiment by Fabrizio Dell’Acqua:\nWhat are the costs of falling asleep at the wheel? We can get a better sense from the as yet unpublished working paper. (I expect it will be published somewhere reasonably prestigious before too long.)\nThe “bad AI” had an accuracy of 75%. The “good AI” had an accuracy of 85%. What was the accuracy of the recruiters? My reading of Table 4 of the paper is that, without any AI support, the recruiters had 72.3% accuracy. With the bad AI that was 75.4%, and with good AI was 74.4%. So on the raw outcome, about the same across all three treatments. (I’ve based these calculations on the assumption that the results come from a linear regression for columns (1) and (2). The author could have used a logistic regression given the binary dependent variable, but the magnitude of the coefficients suggests that isn’t the case.)\nBut think about what we could achieve if we just eliminated the recruiters. The recruiters with the bad AI add nothing. Their deviations from the AI recommendations are a wash. The recruiters with the good AI intervene enough to degrade the performance from 85% to 74%. If anything, the recruiters with the bad AI aren’t asleep enough! If they didn’t do a thing, they’d get 85% accuracy. Instead of thinking about getting recruiters to pay attention, we should remove them from simple prediction tasks like this.\nThe underperformance of human-AI combinations is a common theme across the literature on statistical versus human prediction. Combine a human with a good algorithm and you will improve the human’s performance. However, their performance will still be below the level of the algorithm alone. (I’ve written in Behavioural Scientist on this previously.)\nBefore moving on, I should point out that the measure of decision quality highlighted in the preregistration was not the decision to interview the candidate, but rather a measure of confidence. (I can’t check this as the pre-registration is embargoed.) For each decision, recruiters were asked to rate their confidence on a 1 to 5 scale. Using this confidence measure, the recruiters with the bad AI performed significantly better than those with the good AI (scraping under the 0.05 threshold once some additional controls are added). I can’t confirm from the paper whether the good AI model would outperform on this measure - or even whether it generates measures of confidence - but I’d be very surprised if a model wouldn’t outperform."
  },
  {
    "objectID": "posts/human-ai-collaboration-is-it-better-when-the-human-is-asleep-at-the-wheel.html#understanding-ai-quality",
    "href": "posts/human-ai-collaboration-is-it-better-when-the-human-is-asleep-at-the-wheel.html#understanding-ai-quality",
    "title": "Human-AI collaboration: is it better when the human is asleep at the wheel?",
    "section": "Understanding AI quality",
    "text": "Understanding AI quality\nHaving said the above, it’s unclear what is driving the headline result and whether it would replicate. The recruiters were given one of the following descriptions. For the good AI:\n\nThe AI tool that will support you has been performing very well in prior analysis and we have been very pleased with the candidates selected. However, it made a few mistakes for candidates that were close calls.\nWe reviewed the algorithm’s recommendations using performance data, and we found that the vast majority of AI’s recommendations about whether to interview a candidate or not were correct (about 85% of cases).\n\nThose who were given the bad AI read:\n\nThe AI tool that will support you has been performing well in prior analysis and we have been pleased with the candidates selected. However, it made some mistakes for candidates that were close calls.\nWe reviewed the algorithm’s recommendations using performance data, and we found that the large majority of AI’s recommendations about whether to interview a candidate or not were correct (about 75% of cases).\n\nThe differences are performed “very well” versus “well”, “very pleased” versus “pleased”, making “a few” versus “some” mistakes, “vast majority” versus “large majority”, and “85%” versus “75%”.\nWhich change or changes are driving the effect? We can’t tell. I lean toward the accuracy numbers having no effect. People tend not to react differently to numbers in those ranges. The words might be doing something, but which ones? The framing of the mistakes? How pleased the developers are?\nFurther, is the problem identified in the experiment giving people a good AI or if it is describing it poorly? We could easily have used the bad AI description for the good AI (except the number, which I predict has no effect). Would we see an effect if we gave the bad AI description (except for the number) for the good AI? If not, we’ve solved the asleep at the wheel problem!\nI’d love to see a replication of this experiment, mixing the combinations of words to understand better what people are hooking into. My hunch is that the differences in accuracy wouldn’t replicate. This paper has some of the classic signs: a weak treatment and a p-value scraping under 0.05. (I’m more optimistic about time and effort.)"
  },
  {
    "objectID": "posts/explaining-base-rate-neglect.html",
    "href": "posts/explaining-base-rate-neglect.html",
    "title": "Explaining base rate neglect",
    "section": "",
    "text": "In a seminar for a team from an investment manager I described how base rates are often neglected when people are grappling with conditional probabilities. My description was somewhat confusing, so the below is a short write-up for the participants.\n–\nConsider the following question scenario.\nWhen students, doctors and other experimental subjects answer variants of this question, a common response is around 80% to 90%.\nYet, the answer is approximately 9%. There are far more false positives than true positives. For those that are mathematically inclined, this is a fairly simple application of Bayes’ rule (shown in the Appendix below).\nThe correct answer becomes more apparent if I show an alternative representation, such as the following:\nThis use of natural frequencies was first proposed by Cosmides and Tooby (1996) (pdf). Natural frequencies are derived from observing cases that have been representatively sampled from a population.\nIn a paper by Gigerenzer and Ulrich Hoffrage, they report that this change in representation increased the proportion of correct answers among physicians from 10% to 46%.\nThere is some evidence (e.g.) that you can get further gains through a frequency tree representation. Here’s one such tree from Gigerenzer (2011), which also has the benefit of further highlighting the benefits of the natural frequency approach.\nThe numbers at the bottom of the conditional probability tree do not contain the base rate information. You can’t simply compare across them in calculating conditional probabilities. You also need to refer back to the middle layer. Conversely, the natural frequency tree contains all you need in the bottom row.\nNote that we could convert the numbers in the bottom of the conditional probability tree into frequencies: 900 in 1000, 10 in 1000, 90 in 1000 and 910 in 1000. Gigerenzer calls these simple frequencies.\nThere are many scenarios where simple frequencies can make a problem more tractable, but they do not solve the computational complexity here. Simple frequencies are just a restatement of the probabilities. In contrast, natural frequencies are joint frequencies, such as the number of people who test positive and who have COVID-19.\nSo why do people make errors of the type described above in the first place?\nOne explanation is that people confuse the conditional probability of having COVID-19 given a positive test with the conditional probability of receiving a positive result given they have COVID. That is, they confuse P(COVID-19 | positive) with P(positive | COVID-19).\nThis mistake leads to the largest errors when the base rate of the event you are attempting to predict - in this case having COVID-19 - is low. Confusing the two conditional probabilities effectively results in the base rate being neglected.\nAnother explanation is that there are two types of information here: general information in the form of a base rate and specific information in the form of a positive test. The positive test is representative of a person who has COVID-19, so is given greater weight than the more general information.\nLet’s now put this classic problem into a domain relevant to you.\nIt’s not 90%. We are not asking the probability of outperformance if they are skilled.\nFor this example we have a higher base rate than for the COVID-19 example: 10% of teams are skilled. But the evidence on performance has a weaker link to skill than a positive test does to COVID-19. 40% of unskilled teams outperform. There are many false positives.\nThe result: 20% of outperforming teams are skilled. There is some signal in the noise, but it is weak signal. You cannot neglect that low base rate of skilled teams."
  },
  {
    "objectID": "posts/explaining-base-rate-neglect.html#mathematical-appendix",
    "href": "posts/explaining-base-rate-neglect.html#mathematical-appendix",
    "title": "Explaining base rate neglect",
    "section": "Mathematical appendix",
    "text": "Mathematical appendix\nApplying Bayes’ rule to the COVID diagnosis:\n\\begin{align*}\nP(\\text{COVID | positive}) &=\\dfrac{P(\\text{positive | COVID})P(\\text{COVID})}{P(\\text{positive})} \\\\[12pt]\n&=\\dfrac{P(\\text{positive | COVID})P(\\text{COVID})}{\\Bigg(\\begin{align*}\nP&(\\text{positive | COVID})P(\\text{COVID}) \\\\ &+P(\\text{positive | not COVID})P(\\text{not COVID})\n\\end{align*}\\Bigg)} \\\\[30pt]\n&=\\dfrac{0.9\\times0.01}{0.01\\times0.9+0.99\\times0.09} \\\\[12pt]\n&=0.092\n\\end{align*}\nApplying Bayes’ rule to the assessment of skill:\n\\begin{align*}\nP(\\text{skill | outperform}) &=\\dfrac{P(\\text{outperform | skill})P(\\text{skill})}{P(\\text{outperform})} \\\\[12pt]\n&=\\dfrac{P(\\text{outperform | skill})P(\\text{skill})}{\\Bigg(\\begin{align*}\nP(&\\text{outperform | skill})P(\\text{skill}) \\\\\n&+P(\\text{outperform | no skill})P(\\text{no skill})\n\\end{align*}\\Bigg)} \\\\[30pt]\n&=\\dfrac{0.9\\times0.1}{0.9\\times0.1+0.4\\times0.9} \\\\[12pt]\n&=0.2\n\\end{align*}"
  },
  {
    "objectID": "posts/do-students-learn-less-from-experts.html",
    "href": "posts/do-students-learn-less-from-experts.html",
    "title": "Do students learn less from experts?",
    "section": "",
    "text": "I firmly believe in going straight to the source before sharing a story I’ve heard elsewhere. Here is another example of why.\nIn a recent article in Behavioral Scientist, Adam Grant writes:\n\nIn a clever study, economists wanted to find out whether students really learn more from experts. They collected data on every freshman at Northwestern University from 2001 to 2008. They investigated whether freshmen did better in their second course in a subject if their introductory class was taught by more qualified instructors.\nYou might assume that students would be better off learning the basics from an expert (a tenure‑track or tenured professor) than a nonexpert (a lecturer with less specialized knowledge). But the data showed the opposite: students who took their initial class with an expert ended up with poorer grades in the next class.\nThe pattern was robust across fields: students learned less from introductory classes taught by experts in every subject. It held across years—with over 15,000 students—and in courses with tougher as well as easier grading. And the experts were especially bad at teaching students who were less academically prepared.\nIt turns out that if you’re taking a new road, the best experts are often the worst guides.\n\nI’ve been compiling some thoughts on how universities don’t take teaching seriously, so was interested in finding the source of this story.\nThere is no link to the paper in the Behavioral Scientist article, but the paper appears to be one by David Figlio, Morton Schapiro and Kevin Soter (2015) from The Review of Economics and Statistics, titled “Are Tenure Track Professors Better Teachers?”. The basic facts match”: 15,662 freshman students between fall 2011 and fall 2008.\nFiglio and friends describe the result in the abstract as follows:\n\nWe find consistent evidence that students learn relatively more from contingent faculty in their first-term courses.\n\nUnder Adam Grant’s interpretation of this paper, the tenure track and tenured faculty are the experts. (I’ll call them “tenured” for brevity.) The contingent faculty are the non-experts.\nBut who are these contingent faculty? Given this is Northwestern University, a selective and highly-ranked university, it can attract decent contingent faculty. Further, “a substantial majority of contingent faculty at Northwestern are full-time faculty members with long-term contracts and benefits and therefore may have a stronger commitment to the institution than some of their contingent counterparts at other institutions.” Many of those who are part-time have long-term relationships with the university, teaching in addition to their professional careers. They are not duds and it is questionable to label them as non-experts.\nThen we dig into the details of what is driving the difference. The abstract continues:\n\nThis result is driven by the fact that the bottom quarter of tenure track/tenured faculty (as indicated by our measure of teaching effectiveness) has lower “value added” than their contingent counterparts.\n\nThe value add distribution is virtually identical for the top three quarters of tenured and contingent faculty.\nWhat does this mean? We’ve got a subset of tenured faculty who teach poorly. As per Grant’s story, is this a cohort suffering from the curse of expertise? I suppose it’s possible, but this makes the curse somewhat less than general, with most experts able to overcome it. But the authors suggest another explanation:\n\nIn some ways, this is exactly what we might have expected: contingent faculty members who are hired to teach and who perform relatively poorly are less likely to be renewed than are those who perform well, while tenure track faculty who are relatively poor teachers may be promoted and retained for reasons other than their teaching ability.\n\nI would have hypothesised the same. They are not duds because they are experts. They are simply poor teachers or don’t care. They aren’t fired as they perform in other dimensions that the university cares about. They face an incentive structure different to the contingent staff.\nUnfortunately, there were no administrative records to confirm this selection effect, although Figlio and friends did find that the relative gap between contingent and tenured staff emerges among teachers who have been teaching for six or more years. That’s consistent with selection. It could also be consistent with Grant’s story of expertise if you assumed that the tenured faculty gained expertise of a different kind from experienced contingent faculty. Still, there’s no direct evidence of this, and even if there were, it only affects the bottom quarter of the faculty.\nPutting it together, there is little evidence in that paper that experts are worse teachers. There may be some underlying curse of knowledge, but this paper doesn’t have the right tests to pick it out.\nI also poked around in some of the cited and citing literature and didn’t see anything there to support the claim. It might exist - I didn’t search thoroughly - but if something decent existed, I would have expected Grant to use that instead. Here are two of the interesting ones:\n\nCarrell and West (2010) find that “that less experienced and less qualified professors produce students who perform significantly better in the contemporaneous course being taught, whereas more experienced and highly qualified professors produce students who perform better in the follow-on related curriculum.” Somewhat ambiguous, but I’d put more weight on long-term performance and suggest a tick for expertise.\nFeld et al. (2019) found that “students are almost as effective as senior instructors”, although “exclusively using them will likely negatively affect student outcomes.” Not a big tick to the benefits of expertise, but also not in line with a story that experts are worse teachers.\n\n\n\n\n\nReferences\n\nCarrell, Scott E., and West, James E. (2010). Does professor quality matter? Evidence from random assignment of students to professors. Journal of Political Economy, 118(3), 409–432. https://doi.org/10.1086/653808\n\n\nFeld, J., Salamanca, N., and Zölitz, U. (2019). Students are almost as effective as professors in university teaching. Economics of Education Review, 73, 101912. https://doi.org/10.1016/j.econedurev.2019.101912\n\n\nFiglio, D. N., Schapiro, M. O., and Soter, K. B. (2015). Are Tenure Track Professors Better Teachers? Review of Economics and Statistics, 97(4), 715–724. https://doi.org/10.1162/REST_a_00529"
  },
  {
    "objectID": "posts/chimps-1-humans-0.html",
    "href": "posts/chimps-1-humans-0.html",
    "title": "Chimps 1, Humans 0",
    "section": "",
    "text": "See an update here.\nAt the recent The Biological Basis of Preferences and Behaviour conference, Colin Camerer presented the results of a paper about work he and his co-authors had done on chimpanzees at the Primate Research Institute at Kyoto University.\nAt the beginning of the presentation, Camerer showed a couple of videos of experiments dealing with the working memories of chimps. The videos show subjects undergoing a test in which they see five numbers briefly flash on a screen before the numbers are covered with white boxes. The subject must then press the boxes in the order of the numerals. Of the three videos below, the first is a human subject, the second and third a chimpanzee. The chimpanzee (Ayumu) is receiving pieces of apple for each correct answer, which he is collecting from the lower right of the screen.\nWhile the contrast between the first two videos is striking, the third video shows the power of the snapshot that Ayumu has in his mind.\nMore on this work can be found in Current Biology.\n\nHuman\n\n\n\nChimp\n\n\n\nDistracted chimp\n\n*Jeff Ely beat me to putting up these videos over at Cheap Talk, but since I already had the post put together, it’s still worth a share."
  },
  {
    "objectID": "posts/behavioral-science-policy-recommendations-early-in-the-pandemic-were-largely-correct-if-you-ignore-those-that-were-not.html",
    "href": "posts/behavioral-science-policy-recommendations-early-in-the-pandemic-were-largely-correct-if-you-ignore-those-that-were-not.html",
    "title": "Behavioral science policy recommendations early in the pandemic were LARGELY CORRECT, if you ignore those that were not",
    "section": "",
    "text": "In late April 2020, a group of behavioural scientists (Van Bavel et al., 2020) published a paper in Nature Human Behaviour, “Using social and behavioural science to support COVID-19 pandemic response”. They provided a range of suggestions for policy makers.\nThe paper sparked some debates about the readiness of behavioural science to inform the pandemic response. One of these critiques was in an article by IJzerman et al. (2020), which advised caution when applying behavioural science to policy.\nA new article in Nature (Ruggeri et al., 2023) has reviewed the policy recommendations in that April 2020 paper. Part of the abstract reads:\n\nIn April 2020, an influential paper proposed 19 policy recommendations (‘claims’) detailing how evidence from behavioural science could contribute to efforts to reduce impacts and end the COVID-19 pandemic. Here we assess 747 pandemic-related research articles that empirically investigated those claims. We report the scale of evidence and whether evidence supports them to indicate applicability for policymaking. Two independent teams, involving 72 reviewers, found evidence for 18 of 19 claims, with both teams finding evidence supporting 16 (89%) of those 18 claims.\n\nIt’s a tick for that 2020 paper.\nBut, I wondered how long it would take for someone to extend this finding about a specific set of claims in ONE paper to a claim that “behavioural science was right!”.\nIt didn’t take long. Here’s one statement from the lead author of the Nature paper:\n\nBehavioral science policy recommendations early in the pandemic were LARGELY CORRECT. Our global collaboration in @Nature covers 747 studies with an average sample size over 16,000! Evidence supports 16 of 19 claims, with lessons for science & policy. https://t.co/OzS7njsrzv pic.twitter.com/3xoGQHBJFB\n— Kai Ruggeri (@kairuggeri) December 13, 2023\n\nA quick peruse of twitter can find others of a similar vein.\nIt takes a relatively short memory to forget that, whatever the merits of this single paper, the behavioural science community was the source of plenty of rubbish.\nHere’s one classic from a co-author of both the 2020 paper and the Nature review, Cass Sunstein, published on 29 February 2020:\n\nAt this stage, no one can specify the magnitude of the threat from the coronavirus. But one thing is clear: A lot of people are more scared than they have any reason to be. They have an exaggerated sense of their own personal risk.\n\nBased on an experiment involving 156 undergraduate students receiving electric shocks, Sunstein suggested people exhibited “probability neglect” in worrying about the coming pandemic.\nFour weeks later he was writing: “This Time the Numbers Show We Can’t Be Too Careful”. No acknowledgment that the earlier prognostication might not have been on the mark. He just moved on as if nothing had happened.\nI posted more about these claims here and here.\nThe Van Bavel et al. article was a good target for review. Praise its sobriety.\nBut a review of a single paper is not a foundation for a general claim that the behavioural science community covered itself in glory. That would require a much broader study of what came out of it. That includes thought bubbles from Cass Sunstein and other behavioural scientists on public platforms. It includes outputs of the various behavioural insights teams, many of which were central to government pandemic responses. It includes the broader body of published behavioural science recommendations.\nSuch a broad review is a considerable effort. (Reviewing the single paper in such depth was impressive). But it wouldn’t take long to see that a general claim of correctness is overreach. Just look at some of the claims by those authors themselves.\n\n\n\n\nReferences\n\nIJzerman, H., Lewis, N. A., Przybylski, A. K., Weinstein, N., DeBruine, L., … Anvari, F. (2020). Use caution when applying behavioural science to policy. Nature Human Behaviour, 4(11), 1092–1094. https://doi.org/10.1038/s41562-020-00990-w\n\n\nRuggeri, K., Stock, F., Haslam, S. A., Capraro, V., Boggio, P., … Willer, R. (2023). A synthesis of evidence for policy from behavioural science during COVID-19. Nature, 1–14. https://doi.org/10.1038/s41586-023-06840-9\n\n\nVan Bavel, J. J., Baicker, K., Boggio, P. S., Capraro, V., Cichocka, A., … Willer, R. (2020). Using social and behavioural science to support COVID-19 pandemic response. Nature Human Behaviour, 4(5), 460–471. https://doi.org/10.1038/s41562-020-0884-z"
  },
  {
    "objectID": "posts/a-week-of-links-53.html",
    "href": "posts/a-week-of-links-53.html",
    "title": "A week of links",
    "section": "",
    "text": "Links this week:\n\nIt’s from late last year, but this piece on the biological origins of morality is worth reading.\nA new journal, Economic Anthropology, with the debut issue on greed and excess (and sorry, gated for those without academic access).\nDiane Coyle points to some older work on wealth and inheritance. She also pointed me to this good interview with E.O Wilson.\nAnother good bash of p-values.\nWho is buying the cigarettes? HT: Bryan Caplan\nBaby names. HT: Eric Crampton\n\nAnd Kelly Slater’s winning “Wave of the Winter”:"
  },
  {
    "objectID": "media.html",
    "href": "media.html",
    "title": "Media",
    "section": "",
    "text": "Media\nBelow is a list of selected media about my writing or work.\nThe list includes several podcast episodes. I am definitely more a writer than a speaker.\n\n2022\nNudge podcast - Beware of Behaviour Science BS\nBrain for Business - Is it time to re-think behavioural economics\n\n\n2021\n42courses - Behavioural Science & Evolutionary Biology\n\n\n2020\nBloomberg on ergodicity economics\nTodd Nief - Loss Aversion and Ergodicity Economics\nA Bunch of BS - Knowing Our Limits, Expanding Our Knowledge\n\n\n2018\nRationally Speaking with Julia Galef - A skeptical take on behavioral economics (post)\nBetween Worlds with Mike Walsh - Why humans should stop making making so many decisions\nChallenging “Nudge Economics”: Reframing the market as a community of advantage: A conversation with Prof Robert Sugden.\nDiscussion of my fertility paper with Lionel Page at Forbes\n\n\n2015\nDiscussion on my paper on conspicuous consumption and sexual selection in The Times, The Daily Mail, The Huffington Post and the The Wall Street Journal."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Behavioural and data science. Economics. PhD economics and evolutionary biology.\n\nMy research\nMy non-academic writing\nMedia\nMy work profile\n\nFor direct contact, you can find me through the above links. I don’t host guest posts or advertising.\nBE NOTIFIED OF NEW POSTS VIA EMAIL"
  },
  {
    "objectID": "posts/zuks-paleofantasy.html",
    "href": "posts/zuks-paleofantasy.html",
    "title": "Zuk’s Paleofantasy",
    "section": "",
    "text": "For some time, the “Paleo” lifestyle has been due for a decent critique from the perspective of growing evidence about the rapid rate of human evolution. Humans have evolved markedly since the dawn of agriculture, with adaptations ranging from disease resistance to the improved ability to digest starch. So when I heard of Marlene Zuk’s Paleofantasy: What Evolution Really Tells Us about Sex, Diet, and How We Live, I was looking forward to that critique being provided.\nAnd Zuk certainly provides a critique. She relentlessly pulls apart the ideas that we are well adapted to the Paleolithic environment, or that rapid environmental change means that there is mismatch between the environment and our genes. As Zuk points out, evolution does not result in stasis and there is no point at which we are ever completely adapted to our environment. We will always be a collection of bootstrapped responses to changing conditions.\nHowever, Zuk’s critique was not the one I was hoping for. As was the case for the promotion pieces and reviews that I read before reaching the main course, Zuk parades a series of straw men rather than searching for the more sophisticated arguments of Paleo advocates. Many chapters begin with misspelled comments that Zuk found under blog posts. While Zuk shoots the fish in the barrel, the more interesting targets are not addressed.\nIn a Nature review, John Hawks suggests that Zuk’s use of the term ‘fantasy’ is a reference to hypothesis forming, where we play with hypotheses and try to falsify them. I tried to read the book in that light, but it is hard to do. Zuk appears so keen to throw Paleo-enthusiasts under the bus that she does not give many of the hypotheses the thought that they deserve.  At times Zuk seemed almost desperate to find someone to be on the other side of her debate, such as when she chose Ryan and Jetha’s Sex at Dawn as her punching bag for her discussion of sex differences, even though Ryan and Jetha are almost on their own with their thesis. Where she did write qualifiers about not everyone believing in the straw man that she was about to dispatch (which was often), she would proceed to torch the straw man as though the just acknowledged view did not exist.\nTake the chapter on diet. Zuk points out that many humans have evolved lactose tolerance. Those with a longer history of agricultural diets have genetic adaptations to allow them to digest starch more efficiently. Even the foods we eat have evolved, with potatoes formerly bitter and lumpy, and corn having a shape and size more like a stalk of rice. But these are points most Paleo-advocates would happily concede. They are not seeking a historical re-enactment. Their argument is not that we have not changed at all, but that the changes have not been enough to make a diet full of grains and sugar superior to a diet of meat, nuts, fruit and vegetables. Whether that is the best diet could be the subject of an interesting debate (i.e. testing the hypothesis), but this is not the argument that Zuk engages with.\nMoving to a more substantial point, a review in Evolutionary Psychology nicely summarises Zuk’s mismatch argument as being at three levels: there are no mismatches as all species are adapted to past environments, not current ones; even if mismatches occur, we are not in a position to understand them; and the mismatch perspective has not proven beneficial.\nOn the first of these, Zuk pushes too far by highlighting cases of rapid evolution while ignoring cases where mismatch does occur. Her description of the evolutionary bootstrapping that must occur when organisms encounter new environments points to the potential of mismatch. Zuk’s arguments about the difficulty in understanding mismatches leans towards saying it is all too hard, and not engaging in the sort of hypothesis testing that Hawks refers to. And on the point that the mismatch hypothesis has not proven useful, there are times where Zuk shows just how useful a mismatch perspective might be. She acknowledges that the high sugar content diet of the last 50 years is not something we are matched to, and provides some interesting points on barefoot running and the evidence of the cost of cushioned shoes. The mismatch between current and past physical activity (accompanied by the suggestion that people should simply “get off the couch”) is almost too obvious to mention.\nHaving said all the above, I still enjoyed the book. There is a lot of punchy writing, many important research results and underneath her advocacy style, some interesting questions about diet, exercise and other “Paleo” lifestyle features. However, my instinct is that many people won’t see these points due to Zuk’s approach, which will lead to a less interesting debate than we could have had. I suppose this post is further evidence of that.\n*To avoid getting completely derailed by style instead of substance, next week I’ll write a post on what I consider to be the most interesting questions about recent evolution and the Paleo lifestyle (read it here)."
  },
  {
    "objectID": "posts/wrong-predictions.html",
    "href": "posts/wrong-predictions.html",
    "title": "Wrong predictions",
    "section": "",
    "text": "As I’ve sat on trains and planes over the last week, I sorted through my article archives. Among them I found one by Michael Lewis from January 2007 lamenting the doom and gloom in Davos. In criticising the pessimism, Lewis writes:\n\nBut the most striking thing about the growing derivatives markets is the stability that has come with them. More than eight years ago, after Long-Term Capital Management blew up and lost a few billion dollars, the Federal Reserve had to be wheeled in to save capitalism as we know it.\nLast year Amaranth Advisors blew up, lost more than LTCM, and the financial markets hardly batted an eyelash. “The financial markets in 2007,’’ some member of the global economic elite might have said but didn’t,”are astonishingly robust. They seem to be working out how to absorb and distribute risk more intelligently than any member of the global economic elite could on his own.’’\n\nApart from how wrong this statement was shown to be, what scares me about it is how willingly I would have signed my name to a similar article. I am lucky I was not a financial blogger at the time. However, how many of my blog posts will I look at in five years and wonder what I was thinking? I am relatively confident that I won’t have anything as striking as the example above, as I tend to make less dramatic predictions and they are likely to be borne out over longer periods. However, that is no certainty.\nThe other interesting element of the article is Lewis’s closing remarks:\n\nAnd if they really believe the markets mispriced risk, or were about to adjust, they must also believe they could make vast sums of money if they quit their day jobs and opened a hedge fund to take the other side of stupid trades. But they don’t really believe that, or at least some of them would be off doing it, rather than spilling the beans to Bloomberg News.\n\nDespite being wrong, I still believe that Lewis was fair to call the doom-sayers’ bluff. Most of the doom-sayers did not make their billions (despite some notable examples such as those catalogued by Lewis in The Big Short: Inside the Doomsday Machine). How many of them believed they were right to the extent that they were willing to put their money on the line rather than taking the relatively safe bet that the media will forget about them if they are wrong? It is another advantage to betting on opinions. After the fact, you can separate those who believed what they were saying from those who may have been seeking to get in front of the cameras and happened to luck into the correct side of the debate."
  },
  {
    "objectID": "posts/world-population-500bc.html",
    "href": "posts/world-population-500bc.html",
    "title": "World population 500BC",
    "section": "",
    "text": "Spurred by this chart, a number of bloggers (such as Robin Hanson and Razib Khan) have asked what was happening around 500 BC to cause the jump in population. Was this an almost industrial revolution (with population in the Malthusian state the primary indicator of the level of technology)?\nThe data in the chart is based on the low estimate by the United States Census Bureau, which in turn comes from a range of sources. Looking at the way the Bureau put the low estimate together, I am not sure that there was a major event around 500 BC as the chart suggests. The main source of data between 5,000 BC and 700 AD is the Atlas of World Population History by Jones and McEvedy. They provide all the data points between those two dates, except they have no measure for 400 BC, which is then sourced from Biraben’s 1980 paper, An Essay Concerning Mankind’s Evolution. This combination of the two data sources is what causes the appearance of a rapid acceleration of population growth into 400 BC. Splitting out these two data sources, the sudden surge into 400 AD becomes a gentle rise:\n\nThe growth in population is not beyond that which might be expected from a scale effect in the development of technology (more people, more ideas - such as that proposed by Kremer). In such a case, you would expect the population to increase at greater than an exponential rate (an exponential increase would be a straight line with the logarithmic scale on the y-axis).\nOne point of interest which does remain is why population growth slowed after this point, and looking at the Biraben data, there was a marked decrease in population. At this time, Europe entered the Dark Ages. Was the technological stagnation and economic decay through this period responsible for the population decline?"
  },
  {
    "objectID": "posts/wisdom-from-tolstoy.html",
    "href": "posts/wisdom-from-tolstoy.html",
    "title": "Wisdom from Tolstoy",
    "section": "",
    "text": "I have just finished Leo Tolstoy’s War and Peace, and along the way marked a couple of passages.\nThe first two fit with the story that much behavioural science is formalisation of common sense. First, hindsight bias:\n\nIn historical works on the year 1812 French writers are very fond of saying that Napoleon felt the danger of extending his line, that he sought a battle and that his marshals advised him to stop at Smolensk, and of making similar statements to show that the danger of the campaign was even then understood. Russian authors are still fonder of telling us that from the commencement of the campaign a Scythian war plan was adopted to lure Napoleon into the depths of Russia, and this plan some of them attribute to Pfuel, others to a certain Frenchman, others to Toll, and others again to Alexander himself—pointing to notes, projects, and letters which contain hints of such a line of action. But all these hints at what happened, both from the French side and the Russian, are advanced only because they fit in with the event. Had that event not occurred these hints would have been forgotten, as we have forgotten the thousands and millions of hints and expectations to the contrary which were current then but have now been forgotten because the event falsified them. There are always so many conjectures as to the issue of any event that however it may end there will always be people to say: “I said then that it would be so,” quite forgetting that amid their innumerable conjectures many were to quite the contrary effect.\n\nNext, confirmation bias:\n\nBut these were only suppositions, which seemed important to the younger men but not to Kutuzov. With his sixty years’ experience he knew what value to attach to rumors, knew how apt people who desire anything are to group all news so that it appears to confirm what they desire, and he knew how readily in such cases they omit all that makes for the contrary.\n\nTolstoy also knew something of statistics and selection bias:\n\nOne would have thought that under the almost incredibly wretched conditions the Russian soldiers were in at that time—lacking warm boots and sheepskin coats, without a roof over their heads, in the snow with eighteen degrees of frost, and without even full rations (the commissariat did not always keep up with the troops)—they would have presented a very sad and depressing spectacle.\nOn the contrary, the army had never under the best material conditions presented a more cheerful and animated aspect. This was because all who began to grow depressed or who lost strength were sifted out of the army day by day. All the physically or morally weak had long since been left behind and only the flower of the army—physically and mentally—remained.\n\nAnd the value of medicine in those days:\n\nHe had what the doctors termed “bilious fever.” But despite the fact that the doctors treated him, bled him, and gave him medicines to drink, he recovered."
  },
  {
    "objectID": "posts/wilson-and-pinker-on-evolutionary-psychology.html",
    "href": "posts/wilson-and-pinker-on-evolutionary-psychology.html",
    "title": "Wilson and Pinker on evolutionary psychology",
    "section": "",
    "text": "David Sloan Wilson has just posted a five-part series on the importance of the evolutionary toolkit in the social sciences. I’ve found the series hard work, but in the fifth post Wilson has pointed to an interesting exchange in Edge between his cousin Timothy Wilson and Steven Pinker. Timothy Wilson starts with an examination of the state of social psychology, and then turns to the role that evolutionary psychology can play:\n\nThere are some striking parallels between psychoanalytic theory and evolutionary theory. Both theories, at some general level are true. Evolutionary theory, of course, shows how the forces of natural selection operated on human beings. Psychoanalytic theory argues that our childhood experiences mold us in certain ways and give us outlooks on the world. … But both theories led to a lot of absurd conclusions, and both are very hard to test rigorously. …  Evolutionary theory … can explain virtually anything. It can be a useful heuristic, as I mentioned. But at the same time, I think it is way too broad.\n\nTo make his point, Wilson creates an adaptive explanation of why blood is red. Pinker swats it away like the fly it is - drawing on the chemistry and physics, the non-adaptive explanations for red blood are known. Pinker also suggests a range of empirical tests of Wilson’s faux claim.\nIn further defence of evolutionary psychology, Pinker argues that evolutionary psychology has been successful and, in particular, uses empirical evidence:\n\nIn a 2003 Psychological Bulletin article, David Buss listed fifty novel predictions about social behavior derived from evolutionary theory, most of which had been supported at the time by empirical tests. Entire fields of social-psychological research—on violence, love, beauty, motherhood, religion, sexual desire, parent-offspring conflict, dominance, status, self-conscious emotions, and yes, sex differences (which everyone in the world but Wilson thinks is an important phenomenon)—have been driven by tests of evolutionary hypotheses. Many other evolutionary hypotheses—the nepotism theory of homosexuality, for example, and the Trivers-Willard hypothesis applied to female infanticide—have been empirically falsified as well, leaving the phenomena in question unexplained. It’s simply not true that evolutionary hypotheses that make correct empirical predictions can “explain anything.”\n\nHaving defended his turf, Pinker then lines up social psychology:\n\nWhy doesn’t social psychology get more respect? I readily agree that social psychology, not least Wilson’s own research, has made profound discoveries, which deserve a greater place in policy and personal recommendations. But the field has been self-handicapped with a relentless insistence on theoretical shallowness: on endless demonstrations that People are Really Bad at X, which are then “explained” by an ever-lengthening list of Biases, Fallacies, Illusions, Neglects, Blindnesses, and Fundamental Errors, each of which restates the finding that people are really bad at X.\n\nWhen you make broad statements such as this to experts in the field, you are almost assured of getting counter-examples - and Wilson and some other participants in the discussion give just that. However, Pinker keeps asking why, why, why. There is a limit to the generality of an explanation if it ignores, in the case of social psychology, the biological underpinnings. Pinker describes this issue nicely:\n\nA satisfying explanation invokes principles that are fewer in number, more general, earlier in the causal chain, and closer to irreducible physical and mathematical laws than the ones that immediately fit the data in question. And that will almost always take one outside the boundaries of one’s academic specialty. In the case of social psychology, any explanation must ultimately invoke a conception of what our social emotions and reasoning processes are for.\n\nReading this, I kept thinking how similar Pinker’s description of social psychology is to my perception of behavioural and experimental economics. Behavioural economists have generated a mass of biases, illusions and heuristics but lack a framework to put them together. It is reflected in wikipedia pages like this. Claiming that there is no framework at all is overreach, in the same way that Pinker’s generalisation may have missed some specific examples, but as a general critique it holds. Each time a new paper comes out that finds a bias or heuristic, I generally don’t read beyond the abstract. There are so many of them that it is hard to know if it matters. Further, if there was a framework, we would probably discover that many of the biases are versions of the same feature. We’d end up with a much smaller list.\nPinker closes by making an obvious point:\n\nTim asks who would be best equipped to solving a social problem, a social psychologist, an evolutionary psychologist, or an economist, but this strikes me as the wrong question. The right question is, who is better equipped, a social psychologist who uses relevant ideas from evolutionary biology and economics (and other fields), or a social psychologist who doesn’t?\n\nTo me, the point is so obvious that it is trite, but it seems to need plenty of repeating. In his commentary, David Sloan Wilson noted the lack of cross-referencing between evolutionary and social psychology - and that lack applies both ways. In his words, they inhabit “parallel universes”.\nAnd again, this point applies equally to economics. Who is better equipped: an economist who uses relevant ideas from evolutionary biology, or an economist who doesn’t? Over the next decade, the evidence is going to strongly favour the economist trained in evolutionary biology."
  },
  {
    "objectID": "posts/why-isnt-economics-evolutionary.html",
    "href": "posts/why-isnt-economics-evolutionary.html",
    "title": "Why isn’t economics evolutionary?",
    "section": "",
    "text": "Despite the massive influence of Richard Nelson and Sidney Winter’s An Evolutionary Theory of Economic Change within evolutionary economic circles, the book and the body of work it inspired has had a limited effect through mainstream economics. I believe there are a few reasons for this, but I’ve always thought that this 1996 speech by Paul Krugman to a bunch of evolutionary economists captures one of them:\n\nTo read the real thing in evolution - to read, say, John Maynard Smith’s Evolution and the Theory of Games, or William Hamilton’s new book of collected papers, Narrow Roads in Gene Land, is a startling experience to someone whose previous idea of evolution comes from magazine articles and popular books. The field does not look at all like the stories. What it does look like, to a remarkable degree, is - dare I say it? - neoclassical economics. And it offers very little comfort to those who want a refuge from the harsh discipline of maximization and equilibrium. … Evolutionary theorists, even though they have a framework that fundamentally tells them that you cannot safely assume maximization-and-equilibrium, make use of maximization and equilibrium as modelling devices - as useful fictions about the world that allow them to cut through the complexities. And evolutionists have found these fictions so useful that they dominate analysis in evolution almost as completely as the same fictions dominate economic theory.\n\nKrugman illustrates his point with an example:\n\nWilliam Hamilton’s wonderfully named paper “Geometry for the Selfish Herd” imagines a group of frogs sitting at the edge of a circular pond, from which a snake may emerge - and he supposes that the snake will grab and eat the nearest frog. Where will the frogs sit? To compress his argument, Hamilton points out that if there are two groups of frogs around the pool, each group has an equal chance of being targeted, and so does each frog within each group - which means that the chance of being eaten is less if you are a frog in the larger group. Thus if you are a frog trying to maximize your choice of survival, you will want to be part of the larger group; and the equilibrium must involve clumping of all the frogs as close together as possible. Notice what is missing from this analysis. Hamilton does not talk about the evolutionary dynamics by which frogs might acquire a sit-with-the-other-frogs instinct; he does not take us through the intermediate steps along the evolutionary path in which frogs had not yet completely “realized” that they should stay with the herd. Why not? Because to do so would involve him in enormous complications that are basically irrelevant to his point, whereas - ahem - leapfrogging straight over these difficulties to look at the equilibrium in which all frogs maximize their chances given what the other frogs do is a very parsimonious, sharp-edged way of gaining insight.\n\nIt was an interesting paper to select for the example. While Hamilton did assume the frogs had a fixed instinct for wanting to minimise their chances of being eaten, Hamilton ran a simulation to show his point rather than solving a set of equations for an equilibrium. Krugman continues:\n\nNow some people would say that this kind of creation of useful fictions is a thing of the past, because now we can study complex dynamics using computer simulations. But anyone who has tried that sort of thing - and I have, at great length - eventually comes to realize just what a wonderful tool paper-and-pencil analysis based on maximization and equilibrium really is. By all means let us use simulation to push out the boundaries of our understanding; but just running a lot of simulations and seeing what happens is a frustrating and finally unproductive exercise unless you can somehow create a “model of the model” that lets you understand what is going on.\n\nI take Krugman’s argument to be a call for heterogeneity of approach. And in evolutionary biology, multiple approaches are often used. If we stick with William Hamilton, his famous 1981 Science paper with Robert Axelrod on the evolution of cooperation uses a dynamic out-of-equilibrium approach to examine how cooperation could initially emerge in a population of defectors. For that purpose, a maximisation and equilibrium approach is insufficient. However, this work was not done in a vacuum, and there was a lot of work using a maximisation and equilibrium approach that informed it.\nToday, simulation and other methods of examining dynamic processes are becoming more prevalent in evolutionary research. Computers have come some way since Krugman’s speech in 1996. Much of the theoretical research on the evolution of handicaps or Fisherian runaway selection is now simulation based. However, although they are important and their use is growing, I’m tempted to agree with Krugman’s assessment that “evolutionary” approaches are not the dominant approach in evolutionary biology.\nGiven this, it is a challenge to argue that economics should be “evolutionary” when an evolutionary approach is not the dominant paradigm in the field that bears its name. One of the stronger signs of the lack of the “evolutionary” approach in evolutionary biology is that most of Nelson and Winter’s evolutionary models are not sourced from evolutionary biologists. There was not a body of work that could easily be transferred across.\nI will, however, stop well short of saying that the use of evolutionary approaches is currently at the right level in either economics or evolutionary biology. Krugman goes on to criticise those who see maximisation and equilibrium as truths and not as useful fictions to discard when the time is right. There is certainly space for an evolutionary approach in economics.\nFor me, the strongest case for a more evolutionary approach in economics comes from the timeframes with which economics is engaged. In evolutionary biology, maximisation and equilibrium approaches typically assume plenty of time for the traits of interest to have evolved. Economics is more interested in short-term dynamics, possibly more in the style of ecological models where dynamic modelling is the norm. In that case, there may be a stronger case for evolutionary economics than for “evolutionary” evolutionary biology.\n*I’ve posted about other parts of this Krugman article before - here and here."
  },
  {
    "objectID": "posts/why-do-we-work-less.html",
    "href": "posts/why-do-we-work-less.html",
    "title": "Why do we work less?",
    "section": "",
    "text": "I am sympathetic to the argument by Robert Frank and others that competition for positional goods is a major factor driving our behaviour. The natural outcome of this is that we should want to work more, or at least more than anyone else. However, recent trends in working hours do not neatly fit with this story.\nWhen Jared Diamond proposed that agriculture was the worst mistake in the history of the human race, he was referring primarily to work hours. Diamond writes:\n\n[T]he average time devoted each week to obtaining food is only 12 to 19 hours for one group of Bushmen, 14 hours or less for the Hadza nomads of Tanzania. One Bushman, when asked why he hadn’t emulated neighboring tribes by adopting agriculture, replied, “Why should we, when there are so many mongongo nuts in the world?”\n\nThere are plenty of studies which support this view. Hans-Joachim Voth summarises the results of a series of studies in his book Time and Work in England 1750-1830 and notes that worUking hours increased from an average 4.9 hours per day in hunter-gatherer communities to 7.4 hours per day in mixed societies through to 10.9 hours per day in advanced sedentary agricultural societies.\nVoth also describes how work hours increased further around the time of the Industrial Revolution, with the average hours worked per year by a London resident increasing from 2,288 hours per year in the 1750s to 3,366 hours per year in 1800-1803 and in 1830. This equates to around 64 hours per week.\nBut work hours peaked around that time. Today, workers in the United States work, on average, around 35 hours per week. With the exception of South Korea, almost no developed country has an average work week of more than 40 hours. Work hours have generally been declining for over 150 years.\nWhy, with competition for positional goods such as entry to good schools and neighbourhoods, are we working less? The story about competition for positional goods needs to accommodate the fact that while we still work more than hunter-gatherers, recent trends are towards working less."
  },
  {
    "objectID": "posts/why-do-married-men-earn-more.html",
    "href": "posts/why-do-married-men-earn-more.html",
    "title": "Why do married men earn more?",
    "section": "",
    "text": "Even after controlling for observable traits such as IQ, married men earn more. Bryan Caplan suggests there are three economic explanations for this male marriage premium:\n\nAbility bias: Qualities that make a man attractive to a woman are also attractive to employers.\nHuman capital: Marriage makes men more productive.\nSignalling: Marriage signals to employers that a man has desirable traits.\n\nCaplan notes a study that argues that ability bias accounts for less than 10 per cent of the premium. However, Caplan suggests that ability bias accounts for 50 per cent of the premium, while signalling accounts for less than 10 per cent. That leaves human capital to fill the gap.\nI would make a different split, as there are two other candidate explanations. Each derives from the evolutionary biology insight that females seek high-quality genes, which they cannot directly observe, and they desire resources for their offspring.\nThe first explanation is also a signalling explanation, but in this case relates to signalling by men to women. When women are seeking a mate and trying to assess male quality, a man’s income (often displayed in the form of conspicuous consumption) is one of the most obvious signals. Income and conspicuous consumption are not perfect signals of quality, and there is likely to be noise in the signal through sources such as luck and men lying, but it is a useful proxy. Given that income is not a perfect signal of quality, female use of income as a proxy for quality would result in a higher marriage premium than would be expected on the basis of quality alone.\nIn relation to resources, women care about income in itself as they can direct those resources to raising offspring. Even if the female knows that he is an idiot who received his wealth through inheritance, that inheritance still has some value.\nAs a result, I would suggest that human capital accounts for a smaller proportion of the marriage premium than Caplan suggests, with mate signalling and resource acquisition filling the gap. I would also put a slightly different bent on the ability bias explanation by noting that women seek in men many qualities that employers favour, not only because are these are desirable attributes in a marriage partner, but because employers favour them and they can increase income.\nHaving made these claims, I should address the study by Ginther and Zavodny that suggests the ability bias and resource acquisition explanations have little power in explaining the male marriage premium. They use the interesting approach of comparing normal marriages with “shotgun” marriages, where a child was born within seven months of the date of marriage. If the likelihood of shotgun marriage occurring is not related to income or ability due to their unforeseen nature, we would expect no marriage premium to exist in those cases. However, the authors show that shotgun marriages also have an income premium and the premium is not significantly different from that in normal marriages. As a result, the premium must not be due to underlying quality.\nThe authors’ finding rests on this clever shotgun wedding mechanism, but ultimately it is flawed. One obvious problem noted by the authors is that many shotgun weddings were planned before the conception or would have occurred regardless of the pregnancy. In those cases, the pregnancy is not the primary reason for the marriage or only affected its timing.\nThe authors also address another problem, which is the issue of whether the occurrence of a shotgun wedding is correlated with the man’s income. First, they note studies that show that a man’s income does not affect the chance of a couple legitimating a birth. They also ask whether men who marry following a premarital conception are more desirable than men who do not, and note that there is no substantive difference in quality or income.\nThis explanation misses an important element. The decisions to mate with a man in the first place, to take less caution with contraception, and to keep the child are all likely to be contingent on the quality of the male and their income. If women prefer to mate with high quality, high-income men before marriage and to keep their children, shotgun weddings will not provide an independent comparison.\nTo disentangle these effects, the first place I would look is income of men before and after marriage. My rough understanding of the literature is that they are highly correlated, which is not what you would expect in a pure signalling to the employer model.\nAnother interesting area we could look at is the quality of the wives. We need to remember that these are two-sided markets. If women are attracted by income, and men buy the best they can afford, we would see a strong correlation between husband income and wife quality. We would expect to see this correlation under the signalling to females, resource acquisition and ability bias explanations. Under the signalling to the employer theory, we would see no link between male income and wife quality, whereas the human capital explanation would be only dependent on productive qualities of the wife, such as conscientiousness. We would also find that the male marriage premium is smaller at the low-quality female end of the scale.\n*Caplan has also written a post on the marriage penalty for women."
  },
  {
    "objectID": "posts/whitfield-on-the-darwin-economy.html",
    "href": "posts/whitfield-on-the-darwin-economy.html",
    "title": "Whitfield on the Darwin Economy",
    "section": "",
    "text": "There have been a few reviews of Robert Frank’s The Darwin Economy: Liberty, Competition, and the Common Good recently, but John Whitfield’s in Slate is one of the more interesting.\nFirst, Whitfield picks on Frank’s choice of evolutionary metaphor:\n\nAs a biological analogy, Frank suggests the difference between running speed and antler size. A faster gazelle is better equipped to outrun a cheetah, and so, he writes, “being faster conferred advantages for both the individual and the species.” Antlers, on the other hand, are used for fighting with other males. The pressure to have bigger ones than your rivals leads to an arms race that consumes resources that could have been used more efficiently for other things, such as fighting off disease. As a result, every male ends up with a cumbersome and expensive pair of antlers, says Frank, and “life is more miserable for bull elk as a group.”\n… But evolutionarily speaking, the distinction is bogus.\nNatural selection sees no difference between running speed and antler size: All evolution is positional. When one gazelle got faster, the slower ones got eaten (a point Frank relegates to a footnote). And when gazelles got fast, so did cheetahs. Cheetahs and gazelles would all be better off if they’d stayed slow, because running fast uses energy you might “better” invest in offspring, and legs that are built for speed are more prone to fracture. The lissome cheetah, meanwhile, is bullied and often killed by bigger carnivores such as lions.\n\nWhitfield’s argument picks up on a broader point in evolution. When a new gene spreads through a species, it is not really for the good of the species. All of the members of the species without that gene die out. The “species” that now has this gene is solely composed of descendants of the lucky individual that had the gene. A fitness enhancing mutation is a highly positional good.\nThe second thread of Whitfield’s review is that Robert Frank is running a group-selectionist argument. I don’t see the group selection analogy, but Whitfield does have a more pertinent point:\n\nFrom Frank’s book, you might conclude that what stands in the way of his reforms is not differing interests, but irrationality—an imperfect understanding of how competition works. … But what evolutionary biology teaches us is that it’s not enough to assume, as Frank does, that everyone just wants to create the biggest economic pie. That’s like saying a gazelle cares more about the average speed of its herd than whether it can outrun a hungry cheetah.\nIn fact, those leading and funding opposition to progressive taxation are rational enough—they’re the one who do best if society becomes an arms race won by those with the biggest antlers and the priciest suitcases, with the lions getting anyone who can’t keep up. What those opposing them need to show is not just how the common good can be maximized, but how it can be reconciled with the self-interest of enough people to vote it into being.\n\nOn one hand, this misrepresents Frank’s case, as he does try to show that self-interest and group-interest can be aligned. But it is fair to ask why some people fight proposals such as Frank’s. As Frank states through his book, his arguments don’t need irrationality at the micro-level, but the fact they are not accepted implies that either some people are irrational in their understanding of policy, or that there is another piece to the puzzle.\nOne possible reconciliation is that Frank’s ideas come as a package which have only a few attractive elements. Frank’s argument is not simply that we should constrain positional competition, but he also wants more funding for government and to give it a greater role in some areas.\nBut the deeper side to the opposition is that some people benefit from positional competition. In an evolutionary scenario, while the rank matters, the size of the win also matters. For example, if  one person has vastly more resources than another and there is an environmental shock, the difference in probability of survival is likely to be significant. However, if someone has only marginally more resources than the other, the difference in probability of survival may be small, and luck may result in the lower ranked individual coming out on top."
  },
  {
    "objectID": "posts/when-your-neighbour-wins-the-lottery.html",
    "href": "posts/when-your-neighbour-wins-the-lottery.html",
    "title": "When your neighbour wins the lottery",
    "section": "",
    "text": "I’m not sure if the format of the Dutch postcode lottery is common, but it certainly creates some interesting incentives. In this lottery, a random postcode is drawn from the 430,000 postcodes in the Netherlands, with each postcode having, on average, 19 households. Each person in that postcode who has purchased a ticket in the lottery receives €12,500 for each ticket that they hold (people can buy more than one ticket), and one ticket in the postcode is awarded a BMW.\nIf your postcode is drawn but you do not own a ticket, you know with certainty that you would have won if you had purchased one. You will also know that there were other winners in your postcode, and given the typically small size of a postcode, it is likely that you know some of those winners. With 30% of the Dutch participating in the lottery, there is a good chance that one of those winners is your neighbour.\nIt was with information from this lottery that Peter Kuhn and colleagues decided (ungated working paper here) to look at two of the more interesting questions in economics. First, how do people react to a wealth shock? Do they smooth consumption over their lifespan, or do they blow it all at once? Second, how do other people react to this change in relative wealth? If my neighbour wins the lottery, does it change my behaviour even though it does not affect my absolute well-being?\nKuhn and colleagues obtained their data by surveying people covering four groups - winning lottery participants, people who lived in a postcode that won but who had not purchased a lottery ticket, lottery participants in postcodes who had not won, and those who do not play the lottery from those unsuccessful postcodes. This combination allowed the researchers to compare behaviour of those with and without lottery tickets in winning postcodes, while controlling for differences in characteristics between those who play the lottery or not by examining differences in unsuccessful postcodes. The researchers asked questions about a range of factors, including household composition, demographic variables, labor supply, happiness, car ownership, income and lottery participation. Questions were asked about both current behaviour and behaviour a year earlier, which was intended to capture behaviour six months before and after the lottery result.\nOn the question of how the income shock affects the behaviour of the lottery winner, the results supported the idea that people smooth consumption over their lifecycle as winning the lottery had no effect on most household expenditure. What they did find, however, was an increase in car and durable purchasers by the winners, which supports the idea that shifting the timing of purchases of durables is one way in which people smooth consumption. It is also consistent with the idea that people have self-imposed borrowing constraints.\nThe result for which this study is more famous is the effect on the neighbours. Having a neighbour win the lottery increases the unsuccessful person’s probability of purchasing a car in the next six months by around 7 per cent, which the authors consider large relative to the size of the effects on the lottery winners themselves. This also reduces the average age of their car (no surprise given they are more likely to have purchased a new one). Interestingly, there was no effect on happiness for either the lottery winners or their neighbours, which is problematic for a relative income based model of happiness.\nIt’s not easy to interpret this result. That the increase in consumption by neighbours occurred only in relation to a visible item of consumption - a car that the neighbour will surely see - is suggestive of what the authors call “keeping up with the van den Bergs”, but the transmission path is unclear. Are they copying the purchase of a car, or responding to the known change in relative income? I sense that the dynamic between a neighbour who knows they missed out on a certain lottery win if they had bought a ticket and their triumphant neighbour would be much different to the case where that income shock was from another source.\nI’d like to see two separate experiments, one involving knowledge that a neighbour won the lottery (but without the car or durable expenditure by the winner) and a second with only the signs of expenditure. My (speculative) suspicion is that the lottery knowledge would dominate. An experiment tracking neighbour responses to new car purchases might also be interesting, as if visible consumables are what the neighbour responds to, the windfall may not be strictly required.\nAs an end note, I’d heard about this paper before I got around to reading it, and I was surprised at the gap between what I had heard and the strength of the author’s claims. I’d seen this result waved around a lot as a sign that people respond to relative income changes, but there are some complicating factors that make it difficult to interpret the causative pathway. The happiness result also does not help the “relative income matters” case (nor possibly my speculative suspicion)."
  },
  {
    "objectID": "posts/what-is-the-objective.html",
    "href": "posts/what-is-the-objective.html",
    "title": "What is the objective?",
    "section": "",
    "text": "An economist typically bases their economic models on an assumption that the economy is composed of agents who gain utility from consumption. From the beginning of the model, they take consumption to be the objective and all decisions by the agents aim to maximise their level of consumption within the budget constraint that they face.\nWhile I recently posted on how most economists’ fixation on consumption might be biologically justified, I would like to approach the issue from another angle. To do that, it is worth going back a few years to a 1979 article by Paul Rubin and Chris Paul II on risk preferences.\nRubin and Paul’s starting point is the obvious step (from a biological perspective, not so obvious from an economic perspective) of defining utility as fitness. In their model, utility depends on the number of mates that each man gets.\nThey then asked what level of income would be required to obtain (or support) one mate. If the man’s income is not enough to attract a single mate, there is no utility from that income. All we have is an angry young man. Once the young man obtains one mate, it would take a very large increase in income to attract a second mate. However, losing a small amount of income and dropping below the threshold could cost them the mate they have.\nIf obtaining a mate, rather than consumption, is the objective, people would have preferences towards wealth that contrast with the way economists typically assume people react. Changes in wealth below the single mate threshold deliver no utility. An increase in wealth from below to above the threshold delivers a large jump in utility. Further wealth then delivers further incremental increases in utility. Contrast this with taking consumption as the objective, where each increase in wealth would deliver smooth increases in utility.\nThis difference in objectives can result in significant differences in the decisions taken. Rubin and Paul theorised that where a young male did not have a mate, that individual may be faced with a choice of either undertaking risky behaviour (from the perspective of expected wealth) and possibly accumulating enough wealth to acquire a mate, or undertaking wealth maximising (risk neutral) behaviour and having no mate with certainty. The young male’s action may seem irrational, but it is only through taking the risk that he can possibly reach the required wealth threshold. The safe, steady, low-paying job might normally deliver the highest expected wealth, but it might never be enough.\nIf this were the case, risk seeking young men would acquire more mates and leave more offspring, leading to the spread of that trait. Once they had attracted a mate, however, the incentives change. They would then seek to minimise risks as there would be little upside. They might lose the wealth necessary to keep their mate. Hence, older people are risk averse.\nThis risk profile is different to what would be expected from assuming a nice, stable relationship between utility and consumption. By thinking about what their objective might actually be, Rubin and Paul have developed a model which may have more predictive power and better reflect empirical evidence. It delivers a nice illustration that it is worth putting more thought into the assumptions underpinning each model, and at the most basic level, asking what the objective of the agents might actually be."
  },
  {
    "objectID": "posts/what-economics-misses.html",
    "href": "posts/what-economics-misses.html",
    "title": "What economics misses",
    "section": "",
    "text": "Over at the Evolutionary Psychology Blog, Robert Kurzban has posted a fairly harsh take-down of a paper by Roy Baumeister and Kathleen Vohs.\nThe paper proposes that as sex is a scarce resource offered by women, it can be subject to economic analysis. From this, they come to some predictions such as men offering women other resources for sex.\nKurzban notes that the paper is a rehashing of work that dates back to Robert Trivers in 1972 and Don Symons in 1979. While I tend to agree with Kurzban’s assessment of the novelty of the paper (although I need to give the paper some more time), he then makes a more interesting point. Kurzban notes that the paper is novel as it cannot explain the coarsest patterns of human sexuality:\n\nUnlike Parental Investment Theory and Sexual Strategies Theory, the “new” “theory” can’t explain the origin of the preferences nor any of their texture. For instance, Baumeister and Vohs discuss the value of female virginity, but one needs Parental Investment Theory to explain this male preference. They assert that having sex for the first time “signifies the commencement of adult sexual activity and therefore may be an especially important step and choice,” a non-explanation for the value that men place on female virginity. The issue of paternity certainty, of course, makes this crystal clear.\nThey also claim that their theory can explain the data on infidelity because “Female sexual infidelity involves giving away a precious resource that the husband wants for himself, whereas male sexuality has no inherent value.” (p. 348). However, their theory doesn’t say anything about sexual exclusivity; nothing in the model says that female sexuality is something men want to monopolize, only something that men want to consume. Again, one requires Parental Investment Theory to explain the proximate psychology.\nTheir theory can’t explain the most basic aspects of mate preferences, such as why sex with women of a particular age is favored over sex with the elderly, or any of the other preferences that have been well documented. In short, the theory is unable to accommodate existing findings, and “predicts” – really, post-dicts –  empirical patterns well explained by other theories.\n\nWithout the evolutionary biology underpinning the analysis, the economic model is at best predictive. In contrast, an evolutionary analysis can be more successfully predictive and incorporate an understanding of what is occurring. This is not just an issue in this paper, but applies to many areas in economics - right down to the basic question of what a person is seeking to achieve. As evolutionary biology seeps into the practice of economics, we will fill some of those gaps."
  },
  {
    "objectID": "posts/week-links.html",
    "href": "posts/week-links.html",
    "title": "A week of links",
    "section": "",
    "text": "Links this week:\n\nDavid Sloan Wilson and Jonathan Haidt have kicked off an evolution and business blog at Forbes. It will be worth a read, and unsurprisingly the first post reflects Wilson and Haidt’s group selection leanings. It should give plenty of fodder for interesting posts. I’ve written about Haidt’s group selection views before, plus plenty of posts on Wilson’s (here and here for starters).\nAlso on Jonathan Haidt, he is the editor of a new business section at Evolution: This View of Life.\nA book that looks worth a read: The Rational Animal. Doug Kenrick posts on it.\nTim Harford’s article on Lin Ostrom’s work is good. Ostrom gets quoted a lot for her suggestion that a single international agreement to deal with climate change would be a mistake, less so for her suggestion of polycentric action at all levels - just the sort of thing that would have the typical economist decrying as horribly inefficient."
  },
  {
    "objectID": "posts/wealth-and-genes.html",
    "href": "posts/wealth-and-genes.html",
    "title": "Wealth and genes",
    "section": "",
    "text": "Go back ten years, and most published attempts to link specific genetic variants to a trait were false. These candidate-gene studies were your classic, yet typically rubbish, “gene for X” paper.\nThe proliferation of poor papers was in part because the studies were too small to discover the effects they were looking for (see here for some good videos describing the problems). As has become increasingly evident, most human traits are affected by thousands of genes, each with tiny effects. With a small sample - many of the early candidate-gene studies involved hundreds of people - all you can discover is noise.\nBut there was some optimism that robust links would eventually be drawn. Get genetic samples from a large enough population (say, hundreds of thousands), and you can detect these weak genetic effects. You can also replicate the findings across multiple samples to ensure the results are robust\nIn recent years that promise has started to be realised through genome-wide association studies (GWAS). Although more than 99% of the human genome is common across people, there are certain locations at which the DNA base pair can differ. These locations are known as single-nucleotide polymorphisms (SNPs). A GWAS involves looking across all of the sampled SNPs (typically one million or so SNPs for each person) and estimating the effect of each SNP against an outcome of interest. Those SNPs that meet certain statistical thresholds are treated as positive findings.\nA steady flow of GWAS papers are now being published, linking SNPs with traits such as cognitive function and outcomes such as educational attainment. A typical study title is “Study of 300,486 individuals identifies 148 independent genetic loci influencing general cognitive function”.\nOne innovation from this work is the use of “polygenic scores”. The effect of all measured SNPs from a GWAS is used to produce a single score for a person. That score is used to predict their trait or outcome. Polygenic scores are used regularly in animal breeding, and are now starting to be used to look at human outcomes, including those of interest to economists.\nThe latest example of this is an examination of the link between wealth and a polygenic score for education. An extract from the abstract of the NBER working paper by Daniel Barth, Nicholas Papageorge and Kevin Thom states:\n\nWe show that genetic endowments linked to educational attainment strongly and robustly predict wealth at retirement. The estimated relationship is not fully explained by flexibly controlling for education and labor income. … The associations we report provide preliminary evidence that genetic endowments related to human capital accumulation are associated with wealth not only through educational attainment and labor income, but also through a facility with complex financial decision-making.\n\n(If you can’t access the NBER paper, here is an ungated pdf of a slightly earlier working paper)\nIn more detail:\n\nWe first establish a robust relationship between household wealth in retirement and the average household polygenic score for educational attainment. A one-standard-deviation increase in the score is associated with a 33.1 percent increase in household wealth (approximately $144,000 in 2010 dollars). … Measures of educational attainment, including years of education and completed degrees, explain over half of this relationship. Using detailed income data from the Social Security Administration (SSA) as well as self-reported labor earnings from the HRS, we find that labor income can explain only a small part of the gene-wealth gradient that remains after controlling for education. These results indicate that while education and labor market earnings are important sources of variation in house-hold wealth, they explain only a portion of the relationship between genetic endowments and wealth.\n\nThe finding that the genes that affect education also affect other outcomes - in this case wealth - is no surprise. Whether these genes relate to, say, cognitive ability or conscientiousness, it is easy to imagine that they affect all of education, workplace performance, savings behaviour and a host of other factors that would in turn influence wealth.\nTo tease this out, I would be interested in seeing studies that examine the predictive power of polygenic scores for more fundamental characteristics, such as IQ and the big five personality traits. These would likely capture a good deal of the variation in outcomes being attributed to education. You might also look at some fundamental economic traits, such as risk or time preferences (to the extent these are not just reflections of IQ and the big five). If you know these more fundamental traits, most other behaviours are simply combinations of that.\nThis was a lesson learnt from research on heritability, where you could find studies calculating the heritability of everything from opinions on gun control to leisure interests. Although this had some value in that it led to the first law of behavioural genetics, namely that all human behavioural traits are heritable, a lot of these studies were simply capturing manifestations of differences in IQ and the big five. (It also benefited academics with padded CVs).\nMoving on, what does analysis using polygenic scores add to other work?\n\nOur work contributes to an existing literature on endowments, economic traits, and household wealth. One strand of this work examines how various measures of “ability,” such as IQ or cognitive test scores, predict household wealth and similar outcomes … However, parental investments and other environmental factors can directly affect test performance, making it difficult to separate the effects of endowed traits from endogenous human capital investments. A second strand of this literature focuses on genetic endowments, and seeks to estimate their collective importance using twin studies. Twin studies have shown that genetics play a non-trivial role in explaining financial behavior such as savings and portfolio choices … However, while twin studies can decompose the variance of an outcome into genetic and non-genetic contributions, they do not identify which particular markers influence economic outcomes. Moreover, it is typically impossible to apply twin methods to large and nationally representative longitudinal studies, such as the HRS, which offer some of the richest data on household wealth and related behavioral traits.\n\nTwin studies are fantastic at teasing out the role of genetics, but if you want to take genetic samples from a new population and use the genetic markers as controls in your analysis or to predict outcomes, you need something of the nature of these polygenic scores.\n\nWe note two important differences between the EA score and a measure like IQ that make it valuable to study polygenic scores. First, a polygenic score like the EA score can overcome some interpretational challenges related to IQ and other cognitive test scores. Environmental factors have been found to influence intelligence test results and to moderate genetic influences on IQ (Tucker-Drob and Bates, 2015). It is true that differences in the EA score may reflect differences in environments or investments because parents with high EA scores may also be more likely to invest in their children. However, the EA score is fixed at conception, which means that post-birth investments cannot causally change the value of the score. A measure like IQ suffers from both of these interpretational challenges.\n\nThe interpretational challenge with IQ doesn’t need to be viewed in isolation. Between twin and adoption studies and these studies, you can start to tease out how much a measure like IQ is practically (as opposed to theoretically) hampered by those challenges. An even better option might be an IQ polygenic score.\nThe paper ends with a warning that we know should have been attached to many papers for decades now, but this time with an increasingly tangible solution.\n\nEconomic research using information on genetic endowments is useful for understanding what has heretofore been a form of unobserved heterogeneity that persists across generations, since parents provide genetic material for their children. Studies that ignore this type of heterogeneity when studying the intergenerational persistence of economic outcomes, such as income or wealth, could place too much weight on other mechanisms such as attained education or direct monetary transfers between parents and children. The use of observed genetic information helps economists to develop a more accurate and complete understanding of inequality across generations.\n\nExamining intergenerational outcomes while ignoring genetic effects is generally a waste of time."
  },
  {
    "objectID": "posts/we-have-no-idea.html",
    "href": "posts/we-have-no-idea.html",
    "title": "We have no idea",
    "section": "",
    "text": "I have been listening to a podcast of an excellent talk by David Spiegelhalter on “Thinking and Feeling About Risk”. The video of the lecture is below.\nThe lecture covers a lot of interesting material - from the misrepresentation of cancer screening statistics to bicycle helmets - and I recommend listening to or watching the whole thing.\nOne interesting point was about the presentation of estimates of GDP growth. The Bank of England produces quarterly forecasts of GDP growth, but when they present them graphically, they don’t include their central estimate. The May 2015 graphic is below.\n\nThe story Spiegelhalter tells is that providing a central estimate leads to everyone focusing on that, rather than the considerable range of uncertainty. He shows a similar example where removing the central line for prediction of hurricane movements results in people who sit within the “cone of uncertainty” taking the risk to them more seriously.\nI see another benefit of this GDP growth forecast chart. It effectively communicates that The Bank of England has little idea what the level of growth will be. In fact, there is a large range for what they believe the growth rate was. If people are going to insist on publishing forecasts such as this (whatever their merits), the more people who come to understand that we have no idea, the better."
  },
  {
    "objectID": "posts/warfare-and-the-transition-to-agriculture.html",
    "href": "posts/warfare-and-the-transition-to-agriculture.html",
    "title": "Warfare and the transition to agriculture",
    "section": "",
    "text": "As I pointed out in my last post, Jared Diamond called the transition to agriculture the worst mistake in the history of the human race. Yet despite evidence that the early adopters of agriculture were shorter and had worse nutrition and health than the hunter-gatherers that preceded them, the agricultural way of life came to dominate human society. Health may have rebounded as humans adapted to their new lifestyles, and the long-term benefits are now clear, but why would the early adopters of agriculture shift to what would appear to be a much a poorer way of life?\nOne answer to that question is offered by Paul Seabright and Robert Rowthorn in a 2010 working paper. Seabright and Rowthorn’s idea is that early farming communities needed to develop defences to protect their stationary stores of food. With increased defensive ability comes increased offensive potential and ability to steal from neighbours. As a result, it is optimal for an individual group to adopt agriculture where the benefit from theft exceeds the cost of moving to agriculture.\nAs neighbouring groups face the same incentives (and hunter-gatherers next to agriculturalists are now experiencing increased theft), all may adopt agriculture. However, once all groups adopt agriculture and the associated defences, there is no longer opportunity to steal to make up for the costs of agriculture. This is effectively a prisoner’s dilemma where it is optimal for each party to adopt agriculture regardless of what the other party does but when all parties adopt, they are all worse off than they would be if no-one adopted.\nSeabright and Rowthorn develop a model that demonstrates this point and they considered that the result held for a variety of parameter values (including potential benefits from theft, costs of defences and the difference in productivity between agriculture and the hunter-gatherer lifestyle). They noted that the model could also be applied to other scenarios involving investment in defence which may also have offensive capability.\nI have some sympathy for this argument. Archaeological evidence from some of the earliest agricultural settlements indicate the development of defences. Agriculture allows for specialisation, including specialisation in developing the tools of theft and war.\nHowever, I am still to be convinced that this argument is necessary to explain the adoption of agriculture. If, even with their lower health, agricultural communities have higher numbers of children, they will come to form a larger part of the population. Given that many hunter-gatherers spaced births due to their nomadic lifestyle, an agricultural existence may allow for more births, outweighing the health costs of the agricultural existence. Hunter-gatherer populations could then be squeezed out.\nOne way of framing this argument could be to consider if the agricultural adopters had different preferences for quality or quantity of children to those who remained hunter-gatherers. If the farmers had preferences that gave more weight to number of offspring than health or leisure time, it may be a rational lifestyle to adopt. The higher number of offspring then leads it to become the dominant lifestyle in the total population."
  },
  {
    "objectID": "posts/victorian-naturalists.html",
    "href": "posts/victorian-naturalists.html",
    "title": "Victorian naturalists",
    "section": "",
    "text": "Being a naturalist in the Victorian era was a different exercise to today. From Darwin’s The Descent of Man:\n\nMany kinds of monkeys have a strong taste for tea, coffee, and spiritous liquors: they will also, as I have myself seen, smoke tobacco with pleasure. (6. The same tastes are common to some animals much lower in the scale. Mr. A. Nichols informs me that he kept in Queensland, in Australia, three individuals of the Phaseolarctus cinereus [koalas]; and that, without having been taught in any way, they acquired a strong taste for rum, and for smoking tobacco.) Brehm asserts that the natives of north-eastern Africa catch the wild baboons by exposing vessels with strong beer, by which they are made drunk. He has seen some of these animals, which he kept in confinement, in this state; and he gives a laughable account of their behaviour and strange grimaces. On the following morning they were very cross and dismal; they held their aching heads with both hands, and wore a most pitiable expression: when beer or wine was offered them, they turned away with disgust, but relished the juice of lemons. An American monkey, an Ateles, after getting drunk on brandy, would never touch it again, and thus was wiser than many men. These trifling facts prove how similar the nerves of taste must be in monkeys and man, and how similarly their whole nervous system is affected."
  },
  {
    "objectID": "posts/veblens-the-theory-of-the-leisure-class-part-iii.html",
    "href": "posts/veblens-the-theory-of-the-leisure-class-part-iii.html",
    "title": "Veblen’s The Theory of the Leisure Class, Part III",
    "section": "",
    "text": "Thorstein Veblen has been ranked seventh in a poll of economists on their favourite, dead, 20th century economist. He ranked behind Keynes, Friedman, Samuelson, Hayek, Schumpeter and Galbraith. His supporters were among the least liberal (in the classical sense of the word) of the survey participants. Given his approach to consumerism and the leisure class, as detailed in The Theory of the Leisure Class, this is no surprise. Following from my earlier posts on the book (here and here), I have finished reading it, with the rest of the book largely  applying Veblen’s framework to sport, religion and education.\nTo Veblen, sports reflected the predatory skills of the leisure class and delinquents. He disagreed with the common view that sports build temperament, and instead they involve chicanery, falsehood and browbeating. That is why we need umpires. For the industrial classes, Veblen stated that sport is more a diversion than a habit, although he might want to reassess the role of sport for the industrial class today.\nVeblen considered that the temperament that inclines one to sport inclines one to religion (and vice versa). Religion, and the conspicuous leisure and consumption associated with it, change the patterns of consumption in the community and lowers its vitality. As an example, Veblen referred to the religious Southern United States. He considered that their industry was more handicraft than industrial. Their range of other habits, such as duels, cock-fighting and male sexual incontinence (shown by the presence of mulattoes) were evidence of barbarian traits.\nOn education, Veblen saw the alignment of education institutions with sport and religion as evidence of education’s status as a leisure class activity. Higher education has many rituals and ceremonies and encourages proper speech and spelling (conspicuous leisure), while lower schools tend to more practical. The teaching of the classics and dead languages were, in particular, conspicuous consumption.\nOne interesting sideline is Veblen’s view on how industrialisation has affected the status of women. Veblen considers that industrialisation allows women to revert to a more primitive type (Veblen’s primitive type being peaceful and industrial). The leisure class, however, needs to keep women in their place to show vicarious conspicuous leisure. The highest places of education were reluctant to admit women, with the more industrialised countries and institutions seeing this occur first. When the institutions did admit them, women were primarily enrolled in courses with a quasi-artistic quality, which help women in performing vicarious leisure.\nVeblen also had a great shot at the link between religion and higher education:\n\nTheir putative familiarity with scientific methods and the scientific point of view should presumably exempt the faculties of these schools from animistic habits of thought; but there is still a considerable proportion of them who profess an attachment to the anthropomorphic beliefs and observances of an earlier culture.\n\nHaving finished the book, I enjoyed the interesting and still relevant discussion of signalling, conspicuous consumption and leisure. We should not ignore his assessment that people have motivations beyond maximising utility or consumption in the simplest sense. However, I was disappointed with Veblen’s use of evolutionary theory (discussed more in my second post), which was a strange mix of group selection and broad statements on inherent traits, without detailed consideration of the selection process that might have occurred. Veblen simply wanted to critique the leisure class and would use whatever tools were at his disposal.\nThe link to a full review is here."
  },
  {
    "objectID": "posts/variation-in-reproductive-success.html",
    "href": "posts/variation-in-reproductive-success.html",
    "title": "Variation in reproductive success",
    "section": "",
    "text": "Flipping through Ronald Fisher’s The Genetical Theory of Natural Selection this morning, I was reminded of this quite stunning factoid from the 1912 Australian Census:\n\nThe extraordinary variation in fertility in  Man has been noticed in a somewhat different manner by Dr. D. Heron, using material provided by the deaths (30,285 males and 21,892 females) recorded in the Commonwealth of Australia for 1912. Heron finds that half of the total number of children come from families of 8 or more, which are supplied by only one-ninth of the men or one-seventh of the women of the previous generation. It would be an overstatement to suggest that the whole of this differential reproduction is selective; a substantial portion of it is certainly due to chance, but on no theory does it seem possible to deny that an equally substantial portion is due to a genuine differential fertility, natural or artificial, among the various types which compose the human population.\n\nAs Fisher suggests, some of this variation must be due to luck, but how different might a population with this level of variation in reproductive success look several generations later?"
  },
  {
    "objectID": "posts/using-neuroeconomics-in-economics.html",
    "href": "posts/using-neuroeconomics-in-economics.html",
    "title": "Using neuroeconomics in economics",
    "section": "",
    "text": "An article by Josh Fischman in the Chronicle suggests that economists have been slow to take up the insights of neuroeconomics.\n\nPaul W. Glimcher, director of the Center for Neuroeconomics at New York University and author of the standard textbook in the field, wrote in a 2004 paper published in Science that “economics, psychology, and neuroscience are converging today into a single, unified discipline.” Today he is more measured. “We are a very young science,” he says, “and we’ve taken more from economics than we’ve given. I hope in the coming years you’ll start to see us give more back.”\nAnd economics does need some help, according to a few practitioners like the eminent Yale University economist Robert J. Shiller, who has argued that the discipline isn’t doing just fine. Most economic models didn’t predict the 2008 housing crash, he pointed out in a speech at last year’s Society of Neuroscience meeting. Adding some understanding of how the brain reacts to particular kinds of uncertainties or ambiguities in supply and demand, he said, might avoid this and other costly misfires.\n\nThe slow take-up of neuroeconomics in the short-term by the broader economics profession is not a bad thing. Neuroeconomics is often seeking to build on and test findings from behavioural economics, but the findings from neuroeconomics are not yet fundamentally changing the understanding of human behaviour that behavioural economics gives us. Take the first example used Fischman:\n\nOne recent study, published this summer, searched for brain regions associated with altruism and selfishness. Ernst Fehr, a professor of economics at the University of Zurich, and one of the few economists working extensively with neuroscientists, asked a group of 30 men and women to split a sum of money with another person or keep more for themselves. While each person was making the decision, Fehr’s team took images of his or her brain in a functional-magnetic-resonance-imaging machine. The fMRI scanner reveals fine details of brain anatomy and, crucially, measures how active brain regions are. It has become a standard tool in this field.\nThose people who were willing to split more money had more neurons in a region called the right temporo-parietal junction, an area toward the back of the brain that has been linked to empathy. Selfish people had a smaller junction. Moreover, the junction became more active as unselfish people decided to give more money away, Fehr and his colleagues found. It is almost as if the region worked hardest when people were trying to overcome what might be a natural—and rational—impulse toward selfishness.\n\nThe finding is interesting, but how would an economist incorporate this into their understanding of human action beyond that already provided by behavioural economics? The concept of competing brain structures in decision-making is common in psychology and evolutionary biology - or even economics. Pinning down a location in the brain (and trying to give it an interpretation) is not substantively changing this.\nIn some ways, neuroeconomics is in a similar state of development as genoeconomics, the use of molecular biology in economics (also referred to in Fischman’s article). Genoeconomics is seeking to build on the understanding of human behaviour that we can get from evolutionary biology. But genoeconomics is at such an early stage that it is not fundamentally changing this understanding. An economist seeking to incorporate evolutionary biology into their economic thinking can use evolutionary theory, twin studies and anthropological studies, after which they will gain limited additional understanding from genoeconomics.\nHowever, this points to the real problem. The areas that neuroeconomics and genoeconomics are building on, behavioural economics and evolutionary biology, could and should be used to a greater extent in economics. Economists don’t need to wait for more information on which region of the brain or which gene underlies a behaviour before incorporating it into a model. Behavioural economics is a mature field (albeit one lacking a framework - that evolutionary biology will one day offer) and our knowledge of human evolution and the heritability of traits gives great scope for evolutionary biology to be used.\nHaving said this, we should not ignore neuroeconomics and genoeconomics. The findings being developed today have value. I wish more research was being conducted in these areas and that economists were more involved. Colin Camerer notes the lack of economists involved in this research:\n\n“I would say that neuroeconomics is about 90 percent neuroscience and 10 percent economists,” says Colin F. Camerer, a professor of behavioral finance and economics at the California Institute of Technology and one of the prime movers in the new field. “We’ve taken a lot of mathematical models from economics to help describe what we see happening in the brain. But economists have been a lot slower to use any of our ideas.” …\nCamerer, who was trained in economics—he got an M.B.A. and a Ph.D. from the University of Chicago and “didn’t know anything about neuroscience until 2000”—says that assuming that economics can’t be improved by knowing how the brain computes value might be the most unsound prediction of all. “That’s really kind of a crazy bet,” he says.\n\nIt is just this research is not at a point where it can revolutionise economic theory in the way its more developed relations can. Of course, this will change over the next decade as neuroeconomics and genoeconomics mature and researchers develop causative explanations that add to our knowledge of human behaviour. At that point, the complaints about the failure to adopt neuroeconomics and genoeconomics will have substance. Until then, the lack of evolutionary biology and behavioural economics in the practice of most economics is the bigger issue."
  },
  {
    "objectID": "posts/updating-maddison.html",
    "href": "posts/updating-maddison.html",
    "title": "Updating Maddison",
    "section": "",
    "text": "Angus Maddison’s estimates of per capita GDP - from 1 AD through to the 2000s - are one of the most commonly used data sets in the examination of long-term economic growth. While Maddison passed away in 2010, a group of his colleagues created the Maddison Project, with the goal of continuing Maddison’s work.\nThe project has just produced one of its first major outputs, an update of the original Maddison dataset, including estimates of economic development across the world from 1 AD to 2010. A working paper describes the results, and you can download the new dataset (xlsx).\nMaddison’s work has not been without its critics, and the working paper contains judgment on some of these points. In particular, Maddison’s estimates of the gap between Asian and European incomes before industrialisation are supported:\n\nOne of the central questions in this literature was whether the level of economic development (in terms of GDP per capita) in China (and India and Japan) before industrialization was comparable to Western Europe (Pomeranz 2000). Maddison’s estimates for that period have been criticized because they show an already substantial gap in real incomes between the different parts of EurAsia; in Western Europe the average GDP per capita was about 1200 dollars, whereas China and India were estimated at between 500 and 600 dollars. …\nSumming up, a substantial amount of new work has been published in the past ten years which is generally consistent with the picture Maddison put forward in his 2001/2003 framework. The most severe criticisms at his estimates by Pomeranz (2000) and other specialists on Asian economic history, that he systematically underestimated real incomes in large parts of Asia in the 18th and early 19th century, has generally been proven wrong: detailed research by scholars working on India, Indonesia, Japan and China has shown that the magnitude of the real income gap as estimated by Maddison was about right. Another important result is that Maddison might have overestimated growth in Europe between 1300 and 1800, and that levels of real income were already quite high during the late Middle Ages.\n\nAs someone who uses Maddison’s datasets a bit, I’m pleased to see this work going on."
  },
  {
    "objectID": "posts/underestimating-heritability.html",
    "href": "posts/underestimating-heritability.html",
    "title": "Underestimating heritability",
    "section": "",
    "text": "It’s not normally a good sign when an attempt to skewer measurement of heritability opens with a link between genetics and eugenics via Francis Galton, and Brian Palmer’s critique of twin studies is no exception (HT Razib at Gene Expression).\nTwin studies are one of the primary methods to estimate the heritability of a trait. As monozygotic (identical) twins are more genetically similar than dizygotic (fraternal) twins, the similarity between identical twins and fraternal twins can be used to infer heritability. Palmer argues that this technique has a flaw:\n\nThat identical twins do not, in fact, have identical DNA has been known for some time. The most well-studied difference between monozygotic twins derives from a genetic phenomenon known as copy number variations. Certain, lengthy strands of nucleotides appear more than once in the genome, and the frequency of these repetitions can vary from one twin to another. By some estimates, copy number variations compose nearly 30 percent of a person’s genetic code.\nThese repeats matter. More than 40 percent of the known copy number variations involve genes that affect human development, and there are strong indications they explain observed differences between monozygotic twins. For example, it’s often the case that one identical twin will end up victimized by a genetically based disease like Parkinson’s while the other does not. This is probably the result of variations in the number of copies of a certain piece of DNA. Copy number variations are also thought to play a role in autism spectrum disorder, schizophrenia, and ADHD, all of which can appear in only one member of a monozygotic twin pair (PDF).\n\nThis point is starting to be raised more regularly in attacks on twin studies, but  it is not particularly damaging. In fact, to the extent that identical twins are less genetically similar than assumed, it is possible that heritability is being underestimated. If the similarity in traits occurs despite less than identical genetic similarity, this suggests that we should attribute more of the similarity to genes. Of course, if fraternal twins are also less similar than assumed due to these same effects, we are back where we started. However, we are no worse off than our starting point. I’ll start to worry when results from other techniques start to contradict twin studies, but so far, estimates of heritability from genomics are pointing in the right direction."
  },
  {
    "objectID": "posts/uncertainty-and-understanding-behaviour.html",
    "href": "posts/uncertainty-and-understanding-behaviour.html",
    "title": "Uncertainty and understanding behaviour",
    "section": "",
    "text": "From Cameron Murray on the trolley problem:\n\nIn Scenario A a trolley is barreling down the tacks toward five people who will be killed unless the trolley is stopped. Luckily, there is a fork in the tracks, and by simply pulling a lever, the trolley can be diverted onto a second set of tracks. Unfortunately there is a single person in the path of the tolled on this track who will be killed if you pull the lever.\nThe dilemma is whether you should pull the lever and save five people by sacrificing one? In surveys most people say they would.\nIn Scenario B you find yourself on a bridge next to a fat man where below the same dilemma is playing out, with a trolley hurtling down the tracks towards five people. The question here is whether it is permissible to pushing the person next to you onto the tracks if you knew it would stop the trolley and save the five people.\nMost people in this scenario would not push the man off the bridge, even though the same welfare gains in terms of lives saved would be the same as Scenario A (so you know, 68.2% of philosophers would push the man to save the five). …\nFundamentally the incompatibility of these two outcomes arises because we are presented with a dilemma in terms of risk, or knowable probabilities. …\nLet us now look at the question in terms of uncertainty. For a start, how do we know the trolley is out of control? Is it possible to delay the decision to get more information?\n… [I]n Scenario A, switching the tracks leads to a new situation that opens up the set of possible choices … while eliminating others. Switching the trolley onto the side track buys time and keeps options open without killing anyone.\nIn Scenario B, most people choose not to push the fat man. Here what the are doing is buying time before anyone gets killed. Even after the decision is made not to push the man, there will be time available for many other as-yet-unknowable situations to arise. …\nThe whole rationale of making decisions in a world of uncertainty revolves around keeping options for desirable outcomes open, and often this involves buying time by not making a decision at all.\n\nA couple of practical examples:\n\nIn criminal behaviour, Becker’s expected utility framework has been called into question due to the radical difference between human behaviour in a world of uncertainty versus a world of risk. Increasing chances of being caught and increasing punishment if caught are substitute methods for changing probability distributions of expected outcomes in a world of risk, but in a world of uncertainty they will have far different effect on criminal decisions.\nThe same logic of uncertainty can be applied in social psychology to understand the bystander effect. The bystander effect is the label given to the occasionally observed inverse relationship between the number of people witnessing a victim in need, and the number of people offering help. Various reasons for this empirical phenomena have emerged, with the idea of a diffusion of responsibility dominating explanations.\nBut when we dig a little deeper we can see the logic of uncertainty at play. Repeated experiments on the bystander effect show that the degree of ambiguity is a crucial determinant of the willingness to assist, with reaction times being much slower in the presence of more ambiguous situations.\n\nRead his full post."
  },
  {
    "objectID": "posts/two-perspectives-on-sex-differences.html",
    "href": "posts/two-perspectives-on-sex-differences.html",
    "title": "Two perspectives on sex differences",
    "section": "",
    "text": "First, from Rob Brooks:\n\nLead author Moshe Hoffman and his collaborators compared two tribes living in north-east India. …\nKarbi women may not own land, and property is passed from father to the oldest surviving son.\nBut the Khasi ban men from owning land, and men are expected to hand their earnings over to their wives. The youngest daughter in a Khasi family traditionally inherits the land from her mother. …\nThey gave villagers a very simple four-piece puzzle and timed how quickly each person solved it.\nIt turns out that, in the patrilineal Karbi, men took an average of 42 seconds but women took around 57 seconds to solve the puzzle. But Khasi girls and boys did not differ significantly (35 and 32 seconds respectively) from one another. …\nSuch a simple experiment shows a persistent and common sex difference can entirely disappear in a culture where girls and their education are considered every bit as valuable as boys and theirs.\n\nMeanwhile, Robert Kurzban notes that some sex differences are more robust when he takes on a new paper on six potential sex differences:\n\nThey discuss six areas of potential differences, and even though it’s a bit like starting the meal with dessert, I can’t resist opening with the second of the questions that they pose (p. 297). After discussing the issue of whether men or women want to have more sexual partners, they ask:\n\n\nBut what about when _actual _number of sexual partners are assessed? Are men actually having sex with large numbers of women whereas women are more selective?\n\nNow, because of the way that averages work – and the fact that sex is a two player game –  it just has to be true that, on average, men and women are having sex with the same number of partners. So, this is a silly question to ask, like, How many times have you committed suicide? or, Is anyone at Current Directions editing manuscripts?\n\n\nKurzban concludes:\n\nAnyway, of the six differences they discuss, the authors conclude that four of them are genuine (though they add “but’s” and offer some (proximate) explanations for the differences). In one of the remaining two, they find that women and men have the same number of actual partners, on average, a fact that had to be true. Finally, they conclude that women and men have the same preferences “in real world contexts,” based on a narrow set of data in the “real world context” of speed dating events set up by academics for college students.\n\nI recommend subscribing to Kurzban’s feed."
  },
  {
    "objectID": "posts/twin-studies-stand-up-to-the-critique-again.html",
    "href": "posts/twin-studies-stand-up-to-the-critique-again.html",
    "title": "Twin studies stand up to the critique, again",
    "section": "",
    "text": "The history of twin studies is littered with attempts to discredit them - such as this bit of rubbish. Yet every challenge has been met, with a couple of newish studies knocking off another.\nThe basic idea of twin studies is that by comparing the similarity of fraternal twins to the similarity of identical twins, you can tease out the influence of their genes. Twin studies tend to find that most behaviours have heritability of at least 0.2 (that is, 20 per cent of the variation is due to variation in genotype), IQ a heritability of over 0.5 and height around 0.8. However, twin studies require an assumption that identical and fraternal twins have equally similar environments, and this is where the critiques begin. If identical twins have a more similar environment, the estimates of heritability may be too high.\nThe responses, however, are plenty. There are studies of twins reared apart. Adoption studies find similar results. For those who believe that identical twins are treated differently to fraternal twins, there are studies of misidentified twins - where everyone thought they were identical or fraternal, but they were the other. Peter Visscher and friends took advantage of the differences in relatedness between siblings to generate estimates of heritability consistent with twin studies (You are 50% related to your siblings on average, which means you can test how similarity varies with variation in relatedness . For me, that study should have been the final nail in the coffin of any arguments that twin studies hadn’t told us anything).\nOne critique still floating around is that people who look more similar are treated similarly (although the misidentified twin studies deal with this to a degree). And the New York Times has reported two studies that take on that argument. In the first, Nancy Segal assessed the similarity in personality of 23 pairs of unrelated lookalikes. The similarity - effectively zero. Then in a replication, Segal got a skeptic, Ulrich Ettinger, involved in the project. They found the same result - no resemblance - unlike Ettinger’s expectation that people who looked alike would have similar personalities as people would treat them the same.\nThese studies involves a small sample. However, they are yet another piece of evidence pointing in the same direction as all the rest."
  },
  {
    "objectID": "posts/triverss-the-folly-of-fools.html",
    "href": "posts/triverss-the-folly-of-fools.html",
    "title": "Trivers’s The Folly of Fools",
    "section": "",
    "text": "Robert Trivers is one of the giants of biology. His work in altruism, parental investment and parent-offspring conflict is seminal. For this, he has been justly rewarded.\nTrivers’s later work on deception and self-deception is also important. His basic argument is that self-deception is not irrationality in the way we might normally categorise it. Rather self-deception plays an important role in convincing others of the “truth”. Believing in something prevents one from giving signs of deception, while possibly reducing cognitive load.\nAgainst that backdrop, I can only describe The Folly of Fools: The Logic of Deceit and Self-Deception in Human Life as a mixed bag. I learnt many things from the book. His discussion of the biological basis of deception in other species is interesting, such as his description of the evolutionary battle between birds that lay their eggs in other birds’ nests and the subjects of their deception. The immune effects of deception were new to me. His discussion of deception as one of the weapons in parent-offspring conflict links some of his most important work.\nThe book is also full of small insights that pop up in random places - Trivers’s writing style has a certain “flow of consciousness” feel about it. This has a cost, however, as many of those insights are sporadically placed through passages that are little more than political rants. When Trivers applied the framework developed in earlier chapters, it did not feel that there was any in-depth analysis. The chapters on Israel and United States imperialism, while possibly containing some fair points, consist of little but an assertion that the facts are obvious and that supporters of Israel or the United States are engaging in self-deception in denying them. I would not have minded that Trivers wore his heart on his sleeve if he was making a more substantive point on deception. Rather, the point was political, with little new insight in that direction.\nMuch of the book is reflective of a growing trend for people to accompany their work with assertions that biases and deception have led others to come to different conclusions or ignore their brilliant work. I’d prefer that they stick with arguing their point and acknowledge that bias and deception is a two-way street.\nOverall, I would still recommend reading the book, and I have a hunch that many of the ideas in it will come in useful. Just be ready to wade through a mix of substance and speeches from the soapbox to find them.\n(Or even better, as recommended by Razib at Gene Expression, get hold of a copy of Natural Selection and Social Theory: Selected Papers of Robert Trivers (Evolution and Cognition) - there is some true gold in there.)"
  },
  {
    "objectID": "posts/trivers-on-biology-in-economics.html",
    "href": "posts/trivers-on-biology-in-economics.html",
    "title": "Trivers on biology in economics",
    "section": "",
    "text": "In The Folly of Fools: The Logic of Deceit and Self-Deception in Human Life, Robert Trivers asks “Is economics a science?”  He answers:\n\nThe short answer is no. Economics acts like a science and quacks like one - it has developed an impressive mathematical apparatus and awards itself a Nobel Prize each year - but it is not yet a science. It fails to ground itself in underlying knowledge (in this case, biology).\n\nTrivers notes the cost of this:\n\n[T]he first piece of reality they should pay attention to - and this has been obvious for some thirty years now - is biology, in particular evolutionary theory. If only thirty years ago economists had built a theory of economic utility on a theory of biological self-interest - forget the beautiful math and pay attention to the relevant math - we might have been spared some of the extravagances of economic thought regarding, for example, built-in anti-deception mechanisms kicking in to protect us from the harmful effects of unrestrained economic egotism by those already at the top.\n\nTake the use of utility in economics. Economists assume that economic agents maximise utility. But what is utility? Trivers makes the point that biology has a theory, which is over one hundred years old, of what utility is. The concepts of reproductive success, or more particularly, inclusive fitness provide the answer. Rankings between goods by an economic agent could be assessed against this fitness objective. Trivers notes this might not always give the answer, but it is pointless to miss the obvious linkages.\nTrivers also has a short shot at the invisible hand metaphor. He notes that biology has hundreds of examples of where the pursuit of self-interest can have dramatic negative effects on group wellbeing. This reflects the recent arguments of Robert Frank.\nTrivers saves some sharper criticisms for behavioural economics, where he makes a point I have made before on this blog:\n\nOne recent effort by economics to link up with allied disciplines is called behavioral economics, a link with psychology that is most welcome. But as usual, economists resolutely refuse to make the final link to evolutionary theory, even when going through the motions. That is, even those economists who propose evolutionary explanations of economic behavior often do so with unusual, counterlogical assumptions. For example, a common recent mistake (published in all the best journals) is to assume that our behavior evolved specifically to fit artificial economic games.\n\nThis point is fair, as many interpretations of experimental games ignore the environment in which the relevant traits may have evolved. For much of our evolutionary history, humans lived in small bands where one-shot games with anonymous strangers would have been rare. For example, we might interpret punishment in the ultimatum game to indicate that people having an innate sense of fairness for which they are willing to bear a cost. However, this could equally be interpreted as a strategy that would maximise personal fitness in a small band through the repeated encounters the two people are likely to have. It may not be a sense of fairness driving their action, but rather pure self-interest.\nWe should be careful, however, not to take this critique too far. As a reading of Daniel Kahneman’s Thinking, Fast and Slow demonstrates, the findings of experiments are often shown to apply though many real-life situations. A methodological limitation does not imply that we cannot learn anything. We did not evolve to play economic games, but it is an evolved human that is playing them.\nThe growing use of experimental games by evolutionary biologists reflects this, which was my main takeaway from the Social Decision Making: Bridging Economics and Biology conference last year. While it seems that evolutionary biologists are a few years behind economists in obtaining some results, their (generally) superior methodologies and use of evolutionary biology as the starting framework for the experiments gives me some confidence that they will draw the required links.\nAs a final note, Trivers also writes chapters in which he makes a similar point about the lack of biology in anthropology, psychology and psychoanalysis. One observation by a biologically inclined anthropologist friend of Trivers describes the situation. “[T]hey think we’re Nazis and we think they are idiots”. That is a fair summary of where we are at."
  },
  {
    "objectID": "posts/trade-and-natural-selection.html",
    "href": "posts/trade-and-natural-selection.html",
    "title": "Trade and natural selection",
    "section": "",
    "text": "Economic theory tells us that trade makes the parties involved better off. Through trade, a person can specialise in the activity in which they have a comparative advantage. A person is better off even if they are trading with someone who is better than them at all activities. This is because the less productive person will still have a comparative advantage in some activities. By specialising, an individual can use income from the activity in which they have a comparative advantage to buy other goods and services.\nWhether trade is beneficial is not as clear from the perspective of the genotype. To examine this, Gilles Saint-Paul authored a paper in which he examined how trade may affect the evolution of humankind.\nSaint-Paul’s model involves a haploid population that has two productive activities - fight and defence. Haploid means that each loci, the specific location of the gene, has only one gene. Each person in the population has two unlinked loci, with the gene at the first locus determining fighting productivity and the gene at locus two determining productivity in defence. At each loci there can be one of two alleles, the higher productivity H or the lower productivity L.\nThis situation leads to four possible genotypes: HH, HL, LH and LL. HH has maximum productivity in both activities. HL is productive at fighting but not defence and so on.\nFirst, Saint-Paul considers the situation where there is no trade and each person must do everything for themselves. In this case, the unproductive L alleles are eliminated and eventually the population consists only of HH genotypes. The LL types are eliminated as they are unproductive at both activities. The HL and LH genotypes are also eliminated as, while they are productive in one activity, they are unproductive in the other and must still provide for themselves in that unproductive activity.\nUnder trade, not everyone needs to be perfect. Each person only needs one H allele and they can then specialise in that activity and trade for the other service in which they have lower productivity. HL genotypes can specialise in fight and trade their fight for defence provided by someone else. Similarly, LH genotypes can specialise. However, both HL and LH genotypes cannot both exist in the population in the equilibrium as when they mate, they will produce some LL genotype children. As a result, HL and LH genotypes will have lower fitness than the HH genotypes until the L allele is completely eliminated from one locus. Once that occurs, all children will have H in one locus, ensuring they are a productive in one activity that they can specialise in. The population will end up a mix of HH and HL or, HH and LH.\nHaving looked at these two scenarios, the natural question is what would happen if a trading and a non-trading society were part of a larger population. Would one grow faster than the other? In equilibrium, the answer is no - both societies in equilibrium have the same fitness as all production is done by those with the productive H allele at the relevant loci. If both populations started from a mixed group consisting of all four genotypes, the group which trades would reach equilibrium first, so it would grow faster during transition. However, once the populations reach equilibrium, both grow at the same rate, so neither population is eliminated.\nIf environmental shocks are thrown into the mix, with these shocks changing whether the H or L allele is more productive, then the trading population might grow faster. This is because it maintains some diversity in the form of the L allele in at least one loci. When the productive advantage shifts, the non-trading society has only the newly unproductive alleles for each task. The non-trading society would then have lower fitness, although not directly from not trading, but rather from their non-trading-induced lack of genetic diversity.\nOne observation on this model is that it differs from the typical comparative advantage story told in economics. Economics shows that trade can make someone better off even if they are absolutely less productive at all possible activities. The genetic story tells us that even when there is trade, only those who have maximum productivity in at least one activity will be present in the equilibrium population. Trade may make a completely unproductive person better off in the short-term, but over the long-term, their unproductive alleles will be eliminated - totally in the case of no trade and from at least one loci in the case of trade.\nI like the concept behind this model, but struggle to apply it to any examples. In societies with a history of exchange, do we see more genetic diversity? Conversely, are people in societies with little history of exchange productive across all areas which were relevant to their fitness? I am not sure there is evidence for either. More likely (or so I believe), have humans been trading in one form or another for so long that I am wasting my time looking for modern examples and that we should simply take Saint-Paul’s concept as one factor behind the diversity that we see today."
  },
  {
    "objectID": "posts/tom-griffiths-on-gigerenzer-versus-kahneman-and-tversky-plus-a-neat-explanation-on-why-the-availability-heuristic-can-be-optimal.html",
    "href": "posts/tom-griffiths-on-gigerenzer-versus-kahneman-and-tversky-plus-a-neat-explanation-on-why-the-availability-heuristic-can-be-optimal.html",
    "title": "Tom Griffiths on Gigerenzer versus Kahneman and Tversky. Plus a neat explanation on why the availability heuristic can be optimal",
    "section": "",
    "text": "From an interview of Tom Griffiths by Julia Galef on the generally excellent Rationally Speaking podcast (transcript here):\n\nJulia: There’s this ongoing debate in the heuristics and biases field and related fields. I’ll simplify here, but between, on the one hand, the traditional Kahneman and Tversky model of biases as the ways that human reasoning deviates from ideal reasoning, systematic mistakes that we make, and then on the other side of the debate are people, like for example Gigerenzer, who argue, “No, no, no, the human brain isn’t really biased. We’re not really irrational. These are actually optimal solutions to the problems that the brain evolved to face and to problems that we have limited time and processing power to deal with, so it’s not really appropriate to call the brain irrational, it’s just optimized for particular problems and under particular constraints.”\nIt sounds like your research is pointing towards the second of those positions, but I guess it’s not clear to me what the tension actually is with Kahneman and Tversky in what you’ve said so far.\nTom: Importantly, I think, we were using pieces of both of those ideas. I don’t think there’s necessarily a significant tension with the Kahneman and Tversky perspective.\nHere’s one way of characterizing this. Gigerenzer’s argument has focused on one particular idea which comes from statistics, which is called the bias‐variance trade off. The basic idea of this principle is that you don’t necessarily want to use the most complex model when you’re trying to solve a problem. You don’t necessarily want to use the most complex algorithm.\nIf you’re trying to build a predictive model, including more predictors into the model can be something which makes the model actually worse, provided you are doing something like trying to minimize the errors that you’re making in accounting for the data that you’ve seen so far. The problem is that, as your model gets more complicated, it can overfit the data. It can end up producing predictions which are driven by noise that appears in the data that you’re seeing, because it’s got such a greater expressive capacity.\nThe idea is, by having a simpler model, you’re not going to get into that problem of ending up doing a good job of modeling the noise, and as a consequence you’re going to end up making better predictions and potentially doing a better job of solving those problems.\nGigerenzer’s argument is that some of these heuristics, which you can think about as strategies that end up being perhaps simpler than other kinds of cognitive strategies you can engage in, they’re going to work better than a more complex strategy ‐‐ precisely because of the bias‐variance trade off, precisely because they take us in that direction of minimizing the amount that we’re going to be overfitting the data.\nThe reason why it’s called the bias‐variance trade off is that, as you go in that direction, you add bias to your model. You’re going to be able to do a less good job of fitting data sets in general, but you’re reducing variance ‐‐ you’re reducing the amount which the answers you’re going to get are going to vary around depending on the particular data that you see. Those two things are things that are both bad for making predictions, and so the idea is you want to find the point which is the right trade off between those two kinds of errors.\n…\nWhat’s interesting about that is that you basically get this one explanatory dimension where it says making things simpler is going to be good, but it doesn’t necessarily explain why you get all the way to the very, very simple kinds of strategies that Gigerenzer tends to advocate. Because basically what the bias‐ variance trade off tells you is that you don’t want to use the most complex thing, but you probably also don’t want to use the simplest thing. You actually want to use something which is somewhere in between, and that might end up being more complex than perhaps the simpler sorts of strategies that Gigerenzer has identified, things that, say, rely on just using a single predictor when you’re trying to make a decision.\nKahneman and Tversky, on the other hand, emphasized heuristics as basically a means of dealing with cognitive effort, or the way that I think about it is computational effort. Doing probabilistic reasoning is something which, as a computational problem, is really hard. It’s Bayesian inference… It falls into the categories of problems which are things that we don’t have efficient algorithms to get computers to do, so it’s no surprise that they’d be things that would be challenging for people as well. The idea is, maybe people can follow some simpler strategies that are reducing the cognitive effort they need to use to solve problems.\nGigerenzer argued against that. He argued against people being, I think the way he characterized it was being “lazy,” and said instead, “No, we’re doing a good job with solving these problems.”\nI think the position that I have is that I think both of those perspectives are important and they’re going to be important for explaining different aspects of the heuristics that we end up using. If you add in this third factor of cognitive effort, that’s something which does maybe push you a little bit further in terms of going in the direction of simplicity, but it’s also something that we can use to explain other kinds of heuristics.\n\nGriffiths later provides a great explanation of why the availability heuristic can be a good decision-making tool:\n\nTom: The basic idea behind availability is that if I ask you to judge the probability of something, to make a decision which depends on probabilities of outcomes, and then you do that by basically using those outcomes which come to mind most easily.\nAn example of this is, say, if you’re going to make a decision as to whether you should go snorkeling on holiday. You might end up thinking not just about the colorful fish you’re going to see, but also about the possibility of shark attacks. Or, if you’re going to go on a plane flight, you’ll probably end up thinking about terrorists more than you should. These are things which are very salient to us and jump out at us, and so as a consequence we end up overestimating their probabilities when we’re trying to make decisions.\nWhat Falk did was look at this question from the perspective of trying to think about a computational solution to the problem of calculating an expected utility. If you’re acting rationally, what you should be doing when you’re trying to make a decision as to whether you want to do something or not, is to work out what’s the probabilities of all of the different outcomes that could happen? What’s the utility that you assign to those outcomes? And then average together those utilities weighted by their probabilities. Then that gives you the value of that particular option.\nThat’s obviously a really computationally demanding thing, particularly for the kinds of problems that we face as human beings where there could be many possible outcomes, and so on and so on.\nA reasonable way that you could try and solve that problem instead is by sampling, by generating some sample of outcomes and then evaluating utilities of those outcomes and then adding those up.\nThen you have this question, which is, well, what distribution should you be sampling those outcomes from? I think the immediate intuitive response is to say, “Well, you should just generate those outcomes with the probability that they occur in the world. You should just generate an unbiased sample.” Indeed, if you do that, you’ll get an unbiased estimate of the expected utility.\nThe problem with that is that if you are in a situation where there are some outcomes that are extreme outcomes ‐‐ that, say, occur with relatively lower probability, which is I think the sort of context that we often face in the sorts of decisions that we make as humans ‐‐ then that strategy is going to not work very well. Because there’s a chance that you don’t generate those extreme outcomes, because you’re sampling from this distribution, and those things might have relatively low chance of happening.\n…\nThe answer is, in order to deal with that problem, you probably want to generate from a different distribution. And we can ask, what’s the best distribution to generate from, from the perspective of minimizing the variance in the estimates? Because in this case it’s the variance which really kills you, it’s the variability across those different samples. The answer is: Add a little bit of bias. It’s the bias‐variance trade off again. You generate from a biased distribution, that results in a biased estimate.\nThe optimal distribution to generate from, from the perspective of minimizing variance, is the distribution where the probability of generating an outcome is proportional to the probability of that outcome occurring in the world, multiplied by the absolute value of its utility.\nBasically, the idea is that you want to generate from a distribution where those extreme events that are either extremely good or extremely bad are given greater weight ‐‐ and that’s exactly what we end up doing when we’re answering questions using those available examples. Because the things that we tend to focus on, and the things that we tend to store in our memory, are those things which really have extreme utilities.\n\nCan we make the availability heuristic work better for us?\n\nI think the other idea is that, to the extent that we’ve already adopted these algorithms and these end up being strategies that we end up using, you can also ask the question of how we might structure our environments in ways that we end up doing a better job of solving the problems we want to solve, because we’ve changed the nature of the inputs to those algorithms. If intervening on the algorithms themselves is difficult, intervening on our environments might be easier, and might be the kind of thing that makes us able to do a better job of making these sorts of inferences.\nTo return to your example of shark attacks and so on, I think you could expect that there’s even more bias than the optimal amount of bias in availability‐based decisions because what’s available to us has changed. One of the things that’s happened is you can hear about shark attacks on the news, and you can see plane crashes and you can see all of these different kinds of things. The statistics of the environment that we operate in are also just completely messed up with respect to what’s relevant for making our own decisions.\nSo a basic recommendation that would come out of that is, if this is the way that your mind tends to work, try and put yourself in an environment where you get exposed to the right kind of statistics. I think the way you were characterizing that was in terms of you find out what the facts are on shark attacks and so on.\n\nListen to the fullepisode - or in fact, much of the Rationally Speaking back catalogue. I’m still only partway through, but recommend the interviews with Daniel Lakens on p-hacking, Paul Bloom on empathy, Bryan Caplan on parenting, Phil Tetlock on forecasting, Tom Griffiths’s reappearance with his Algorithms to Live By co-author, Brian Christian, and Don Moore on overconfidence. Julia Galef is a great interviewer - I like the sceptical manner in which she probes her guests and digs into the points she wants to understand."
  },
  {
    "objectID": "posts/three-algorithmic-views-of-human-judgment-and-the-need-to-consider-more-than-algorithms.html",
    "href": "posts/three-algorithmic-views-of-human-judgment-and-the-need-to-consider-more-than-algorithms.html",
    "title": "Three algorithmic views of human judgment, and the need to consider more than algorithms",
    "section": "",
    "text": "From Gerd Gigerenzer’s The bounded rationality of probabilistic mental models (PDF) (one of the papers mentioned in my recent post on the Kahneman and Tversky and Gigerenzer debate):\n\nDefenders and detractors of human rationality alike have tended to focus on the issue of algorithms. Only their answers differ. Here are some prototypical arguments in the current debate.\n\nStatistical algorithms\n\nCohen assumes that statistical algorithms … are in the mind, but distinguishes between not having a statistical rule and not applying such as rule, that is, between competence and performance. Cohen’s interpretation of cognitive illusions parallels J.J. Gibson’s interpretation of visual illusions: illusions are attributed to non-realistic experimenters acting as conjurors, and to other factors that mask the subjects’ competence: ‘unless their judgment is clouded at the time by wishful thinking, forgetfulness, inattentiveness, low intelligence, immaturity, senility, or some other competence-inhibiting factor, all subjects reason correctly about probability: none are programmed to commit fallacies or indulge in illusions’ … Cohen does not claim, I think, that people carry around the collected works of Kolmogoroff, Fisher, and Neyman in their heads, and merely need to have their memories jogged, like the slave in Plato’s Meno. But his claim implies that people do have at least those statistical algorithms in their competence that are sufficient to solve all reasoning problems studied in the heuristics and biases literature, including the Linda problem\n\n\nNon-statistical algorithms: heuristics\n\nProponents of the heuristics-and-biases programme seem to assume that the mind is not built to work by the rules of probability:\n\nIn making predictions and judgments under uncertainty, people do not appear to follow the calculus of chance or the statistical theory of prediction. Instead they rely on a limited number of heuristics which sometimes yield reasonable judgments and sometimes lead to severe and systematic errors.\n(Kahneman and Tversky, 1973:237)\n\n…\nCognitive illusions are explained by non-statistical algorithms, known as cognitive heuristics.\n\nStatistical and non-statistical heuristics\n\nProponents of a third position do not want to be forced to choose between statistical and non-statistical algorithms, but want to have them both. Fong and Nisbett … argue that people possess both rudimentary but abstract intuitive versions for statistical principles such as the law of large numbers, and non-statistical heuristics such as representativeness. The basis for these conclusions are the results of training studies. For instance, the experimenters first teach the subject the law of large numbers or some other statistical principle, and subsequently also explain how to apply this principle to a real-world domain such as sports problems. Subjects are then tested on similar problems front he same or other domains. The typical result is that more subjects reasons statistically, but transfer to domains not trained in is often low.\n\nHowever, Gigerenzer argues that we need to consider more than just the mental algorithms.\n\nInformation needs representation. In order to communicate information, it has to be represented in some symbols system. Take numerical information. This information can be represented by the Arabic numeral system, by the binary system, by Roman numbers, or other systems. These different representations can be mapped in a one-to-one way, and are in this sense equivalent representations. But they are not necessarily equivalent for an algorithm. Pocket calculators, for instance, generally work on the Arabic base-10 system, whereas general purpose computers work on the base-2 system. The numerals 10000 and 32 are representations of the number thirty-two in the binary and Arabic system, respectively. The algorithms of my pocket calculator will perform badly with the first kind of representation but work well on the latter.\nThe human mind finds itself in an analogous situation. The algorithms most Western people have stored in their minds - such as how to add, subtract and multiply - work well on Arabic numerals. But contemplate for a moment division in Roman numerals, without transforming them first into Arabic numerals.\nThere is more to the distinction between an algorithm and a representation of information. Not only are algorithms tuned to particular representations, but different representations make explicit different features of the same information. For instance, one can quickly see whether a number is a power of 10 in an Arabic numeral representation, whereas to see whether that number is a power of 2 is more difficult. The converse holds with binary numbers. Finally, algorithms are tailored to given representations. Some representations allow for simpler and faster algorithms than others. Binary representation, for instance, is better suited to electronic techniques than Arabic representation. Arabic numerals, on the other hand, are better suited to multiplication and elaborate mathematical algorithms than Roman numerals …"
  },
  {
    "objectID": "posts/thiels-zero-to-one.html",
    "href": "posts/thiels-zero-to-one.html",
    "title": "Thiel’s Zero to One",
    "section": "",
    "text": "I am sympathetic to many of Peter Thiel’s arguments in Zero to One: Notes on Startups, or How to Build the Future, but this is not a book where the arguments are buttressed with evidence to convince you they are true. Below are some random observations.\nThiel argues that competition is something a company should avoid. Competition erodes profits, so you want to be a monopoly. In some ways Thiel’s argument isn’t about avoiding competition, but a recommendation to compete in different spheres - competing to find the next monopoly, wealth, status, etc. It’s a recommendation to dream big - look for 10x improvements.\nThiel puts Apple and Google in this camp - they do what they do so much better than their competitors that they are effectively monopolies. In contrast, restaurants tend not be monopolies, with profits rapidly competed away. But this brings the exceptions to mind - the restaurant chains that explode in popularity - think Starbucks. Many businesses have done very well in competitive, commodotised markets. Were they 10x improvements? Is this an ex post justification where all extraordinarily successful businesses can be explained as 10x improvements?\nA thread with which I have much sympathy is Thiel’s critique of education conformity (in fact, conformity in general). Through high school the ambitious build their CV to get into a top university. They then aspire to go into law, management consulting or finance. Are they happy competing on this treadmill?\nI’m close to convinced that it would be a good thing if less bright people went to law, management consulting and finance (on the last, I’m reading John Kay’s Other People’s Money). But from the perspective of the student, I’m not sure the treadmill is a bad choice. It’s a pretty good payoff. And how many people diverge from this path, dream big and fail big? If society has a dysfunctioning structure - whether created by regulation or something else - why not take advantage of it?\nThis argument against conformity and the recommendation to compete in new ways is also a central thread in Malcolm Gladwell’s David and Goliath.\nThere are some sections of the book that didn’t work for me. In a chapter “You are not a lottery ticket”, Thiel disagrees with Malcolm Gladwell’s argument that luck is central to success (found in Outliers). Thiel quotes Warren Buffet’s statement that he is a “member of the lucky sperm club”, or Bill Gates’s suggestion that he “was lucky to be born with certain skills”, which at a level must be right. As Gladwell argues, if Gates was born anywhere but the small part of the US where he was born (with certain genes etc), it wouldn’t have panned out the same for him.\nThe only piece of evidence Thiel throws into the debate is the existence of serial entrepreneurs who have started multiple multimillion or billion dollar businesses. “If success were mostly a matter of luck, these kinds of serial entrepreneurs probably wouldn’t exist.” Thiel effectively ignores Gladwell’s central argument.\nFollowing this, Thiel moves to classify people’s (and whole countries’ and continents’) perspectives on the future along two spectrums - optimism/pessimism and definite/indefinite. For example, a definite optimist (in Thiel’s mind, the US before the 1980s) believes the future will be better and that you can plan how you will get there. An indefinite optimist also believes the future will be better, but doesn’t know how it will be better so they don’t plan. Rather, they try to keep their options open (e.g. the resume building Ivy League generalist).\nThiel argues for definite optimism. Those who plan and don’t believe it is all down to luck will be the ones who create the future - who go from zero to one. Is Thiel arguing for the “growth mindset” - even if much of success is innate talent through genes etc, those who pretend success is wholly due to effort and plan accordingly will do better?\nThe argument is interesting, but I’m not sure it’s anything more than storytelling. Are Americans today really less “definite” about the future than people in the 1960s (e.g. is there any better evidence than stories about moon landings etc.)? The argument also made me think of this paragraph from Hayek:\n\nThis is not a dispute about whether planning is to be done or not. It is a dispute as to whether planning is to be done centrally, by one authority for the whole economic system, or is to be divided among many individuals.\n\nIs it a problem if a nation is “indefinite” and doesn’t plan if its people are “definite” planners?"
  },
  {
    "objectID": "posts/there-is-but-one-social-science.html",
    "href": "posts/there-is-but-one-social-science.html",
    "title": "There is but one social science",
    "section": "",
    "text": "“There is only one social science and we are its practitioners” - George Stigler, economist\nI am not sure of the source of the above quote (it has been mentioned several times on Econtalk), but I consider that Stigler was thinking of the wrong field. He should have thought of biology.\nAn articlein the Economist this week noted the increasing use of biology in the study of business and management. Biology has been slowly entering field after field as people click that humans are biological organisms that have evolved over billions of years into our present state. We are not pure rationalising machines, nor solely the product of our environment or culture. We have natural predispositions and whatever drives our “utility”, the one measure that will always be present is reproductive success.\nThat does not mean that everything is biologically pre-determined or that there is no point is studying culture or environmental factors, but we need to recognise the biological basis. It would be fantastic to see the teaching of biology start to creep into business schools, economics faculties and the rest of the social sciences. With a sound biological foundation, I believe that many “puzzles” in these fields will suddenly seem a lot more tractable than they did before."
  },
  {
    "objectID": "posts/the-winner-effect-in-humans.html",
    "href": "posts/the-winner-effect-in-humans.html",
    "title": "The winner effect in humans",
    "section": "",
    "text": "I am using some material from John Coates’s excellent The Hour Between Dog and Wolf for a presentation I am giving next week, and decided it was worth sharing here:\n\nDuring moments of risk-taking, competition and triumph, of exuberance, there is one steroid in particular that makes its presence felt and guides our actions – testosterone. At Rockefeller University I came across a model of testosterone-fuelled behaviour that offered a tantalising explanation of trader behaviour during market bubbles, a model taken from animal behaviour called ‘the winner effect’.\nIn this model, two males enter a fight for turf or a contest for a mate and, in anticipation of the competition, experience a surge in testosterone, a chemical bracer that increases their blood’s capacity to carry oxygen and, in time, their lean-muscle mass. Testosterone also affects the brain, where it increases the animal’s confidence and appetite for risk. After the battle has been decided the winner emerges with even higher levels of testosterone, the loser with lower levels. The winner, if he proceeds to a next round of competition, does so with already elevated testosterone, and this androgenic priming gives him an edge, helping him win yet again. Scientists have replicated these experiments with athletes, and believe the testosterone feedback loop may explain winning and losing streaks in sports. However, at some point in this winning streak the elevated steroids begin to have the opposite effect on success and survival. Animals experiencing this upward spiral of testosterone and victory have been found after a while to start more fights and to spend more time out in the open, and as a result they suffer an increased mortality. As testosterone levels rise, confidence and risk-taking segue into overconfidence and reckless behaviour.\nCould this upward surge of testosterone, cockiness and risky behaviour also occur in the financial markets? This model seemed to describe perfectly how traders behaved as the bull market of the nineties morphed into the tech bubble. When traders, most of whom are young males, make money, their testosterone levels rise, increasing their confidence and appetite for risk, until the extended winning streak of a bull market causes them to become every bit as delusional, overconfident and risk-seeking as those animals venturing into the open, oblivious to all danger. The winner effect seemed to me a plausible explanation for the chemical hit traders receive, one that exaggerates a bull market and turns it into a bubble. The role of testosterone could also explain why women seemed relatively unaffected by the bubble, for they have about 10 to 20 per cent of the testosterone levels of men.\n\nCoates tested this on a London trading floor:\n\nI set up an experiment on the trading floor of a mid-sized firm in the City of London. The floor employed 250 traders, all but three of whom were men. They were all engaged in high-frequency trading, … meaning they bought and sold securities, sometimes in sizes ranging up to $1 or $2 billion, but held their bets only for a matter of hours or minutes, sometimes mere seconds. They therefore occupied the same market niche as the black boxes.\nThese traders were therefore up against some of the world’s most sophisticated and well-capitalised competitors. They lacked the large capital base and informational advantages of the flow traders at the big banks, and the deep pools of capital and inhuman processing speeds of the black boxes. Yet they were astonishingly successful: David against Goliath, John Connor against the Terminator. In fact they were some of the best traders I have ever seen: highly disciplined, consistent, and profitable.\nI sampled testosterone from these traders and recorded P&L over a two-week period. What we found was that their testosterone levels were significantly higher on days when they made an above-average profit. More intriguing, though, was what we found when we looked at testosterone levels in the morning, because these predicted how much money the traders would make in the afternoon. When the traders’ morning testosterone levels were high, they went on to make a lot more money in the afternoon than they did on days when their morning testosterone levels were low. Moreover, the difference in P&L between high- and low-testosterone days was large, amounting in statistical terms to one full standard deviation, a difference that if annualised could amount for some of the traders to over £500,000 in pay."
  },
  {
    "objectID": "posts/the-use-of-heritability-in-policy-development.html",
    "href": "posts/the-use-of-heritability-in-policy-development.html",
    "title": "The use of heritability in policy development",
    "section": "",
    "text": "The  heritability straw man has copped another bashing, this time in the Journal of Economic Perspectives. In it, Charles Manski picks up an old line of argument by Goldberger from 1979 and argues that heritability research is uninformative for the analysis of policy.\nManski starts by arguing that heritability estimates are based on the assumption that there is no gene-environment correlation. Manski writes:\n\nThe assumption that g and e are uncorrelated is at odds with the reasonable conjecture that persons who inherit relatively strong genetic endowments tend to grow up in families with more favorable environments for child development.\n\nAny review of discussions of heritability, whether in the peer-reviewed literature or the blogosphere, will show that his claim is generally false. The proviso that the heritability estimate is only relevant to the existing environment is usually threaded through any discussion of heritability.\nIt is true that gene-environment covariance can affect estimates of heritability. Yet this does not mean that existing estimates have no value, nor that there are not methods that seek to account for the covariance. For example, the use of comparisons between misdiagnosed identical twins and actual identical twins allows for bounded estimates of heritability to be developed (pdf).\nManski’s broader claim, adopted directly from Goldberger, is that even if you knew the heritability of a trait, it tells you nothing about social policy. Manski uses Goldberger’s eyeglasses example as an illustration:\n\nConsider Goldberger’s use of distribution of eyeglasses as the intervention. For simplicity, suppose that nearsightedness derives entirely from the presence of a particular allele of a specific gene. Suppose that this gene is observable, taking the value g = 0 if a person has the allele for nearsightedness and g = 1 if he has the one that yields normal sight.\nLet the outcome of interest be effective quality of sight, where “effective” means sight when augmented by eyeglasses, should they be available. A person has effective normal sight either if he has the allele for normal sight or if eyeglasses are available. A person is effectively nearsighted if that person has the allele for nearsightedness and eyeglasses are unavailable.\nNow suppose that the entire population lacks eyeglasses. Then the heritability of effective quality of sight is one. What does this imply about the usefulness of distributing eyeglasses as a treatment for nearsightedness? Nothing, of course. The policy question of interest concerns effective quality of sight in a conjectured environment where eyeglasses are available. However, the available data only reveal what happens when eyeglasses are unavailable.\n\nManski and Goldberger may be correct that the heritability estimate is uninformative as to the efficacy of distributing eyeglasses, but it is useful in assessing other policy responses to the problem and the trade-offs between them. Is it possible to prevent the eyesight loss in the first place? Is that policy cheaper and more effective than eyeglasses? If the heritability estimate was zero, you would look to the environmental causes and ask whether the eyesight problem is more appropriately dealt with by addressing the cause rather than by distribution of eyeglasses.\nThere is no shortage of other areas where heritability estimates might add value. Heritability estimates can inform whether it is an effective use of resources to make sure that everyone has a university degree or is over six-foot tall. Is everyone putty in the hands of the policy maker, or are there some constraints? On a personal level, Bryan Caplan’s use of heritability in Selfish Reasons to Have More Kids is a useful input to his parenting strategy.\nFor me, the most salient example of the usefulness of heritability research comes from examination of the heritability of IQ among children. Among high socioeconomic status families, the heritability tends to be high. Among low socioeconomic status families, it is significantly lower. This suggests that there is significant room to improve the outcomes of the children at the bottom of the socioeconomic ladder in the early years of their life (assuming those changes have effects that persist into adulthood). Increasing heritability of IQ might be evidence that environmental disadvantages are being ameliorated and opportunity equalised.\nThe latter part of Manski’s paper turns to the use of genes as covariates in statistical regressions. Regression identifies statistical association and not causation, which appears to be an important point in attracting Manski to this use. Noting the wealth of data being created and the possibility of observing changes in the effect of genes as the environment changes, Manski considers that these regression exercises may assist in examining how genes and environment interact.\nI don’t disagree with Manski, but at present, genome association studies have plenty of issues. First, there is the missing heritability problem. To date, the magnitude of the identified effect of genes on most traits accounts for a miniscule proportion of the trait’s heritability. This points to the important role played by heritability research to provide direction to research on genes as covariates. It also indicates that until these genes are found, heritability estimates will be more informative for social policy.\nA second issue is that with 30,000 odd genes and the ability to test so many of them for correlation with traits, many are found to have a statistically significant relationship through chance. As blogged about recently by Razib, this is shown when people seek to replicate earlier results - such as when it was found that most reported genetic associations with general intelligence are probably false positives (pdf).\nFinally, genome based research is now feeding back into estimates of heritability. From a recent paper:\n\nWe conducted a genome-wide analysis of 3511 unrelated adults with data on 549 692 single nucleotide polymorphisms (SNPs) and detailed phenotypes on cognitive traits. We estimate that 40% of the variation in crystallized-type intelligence and 51% of the variation in fluid-type intelligence between individuals is accounted for by linkage disequilibrium between genotyped common SNP markers and unknown causal variants. These estimates provide lower bounds for the narrow-sense heritability of the traits.\n\nDespite all the critiques about methodology, most new studies confirm that the old “methodologically poor” heritability estimates were in the right ballpark. The problem is not that the estimates are not useful, but rather that they are not used."
  },
  {
    "objectID": "posts/the-three-stages-of-evolutionary-economics.html",
    "href": "posts/the-three-stages-of-evolutionary-economics.html",
    "title": "The three stages of evolutionary economics",
    "section": "",
    "text": "Many of the suggested additions to my reading list in evolution and economics came from the fields of evolutionary economics and complexity theory.\nWhile my area of interest is sometimes described as “evolutionary economics”, evolutionary economics is a label generally applied to the study of the interactions of firms, institutions and agents in the economy using an evolutionary methodology. Businesses search the landscape for technology and other sources of competitive advantage, and those business modules (technologies, plans) with higher fitness are replicated and spread. Much research in evolutionary economics also has strong links to complexity theory. However, evolutionary economics it is not directly concerned with evolutionary biology.\nI tend to argue for the intersection of evolution and economics at a deeper level than that generally examined in evolutionary economics. I suggest there are three stages of evolutionary economics.\n\nThe metaphor: Economies are like evolutionary systems. The metaphor is the domain of popular books, magazine articles and conversation at the pub.\nEconomies and biological systems are both complex adaptive systems. This is where the field of evolutionary economics tends to operate. Economic activity occurs in an evolutionary system, as opposed to being just like one in Stage 1. Selection in the system is often at the level of businesses and the modules of their business plans.\nEconomies and biological systems are the same system. Economic activity is undertaken by evolved (and evolving) people, with traits and preferences reflecting their evolutionary past. As an extension to Stage 2, businesses are made up of parties with biological interests (shareholders, creditors, employees) and interact in social orders that emerged from interactions between those parties.\n\nThere is much useful research being undertaken at Stage 2, and in many cases it is the most useful level of analysis, but economics will only be an evolutionary science when it incorporates Stage 3.\nFollowing the feedback, I have added a couple of evolutionary economics and complexity theory books and articles to the reading list."
  },
  {
    "objectID": "posts/the-theoretical-ambition-of-behavioural-science.html",
    "href": "posts/the-theoretical-ambition-of-behavioural-science.html",
    "title": "The theoretical ambition of behavioural science",
    "section": "",
    "text": "From Richard Posner (HT: Ryan Murphy):\n\n[Behavioral economics] is undertheorized because of its residual, and in consequence purely empirical, character. Behavioral economics is defined by its subject rather than by its method and its subject is merely the set of phenomena that the most elementary, stripped down rational-choice models do not explain. It would not be surprising if many of these phenomena turned out to be unrelated to each other, just as the set of things that are not edible by man include stones, toadstools, thunder-claps, and the Pythagorean theorem. Describing, specifying, and classifying the empirical failures of a theory is a valid and important scholarly activity. But it is not an alternative theory. …\nThe behavioralists’ lack of interest in, and indeed hostility to, evolutionary theory is an example of their lack of theoretical ambition. But it is more. Most though by no means all behavioralists are political liberals. The use of evolutionary theory to explain human social rather than merely physical traits, the use that goes by the name “sociobiology” (recently renamed by its proponents “evolutionary ecology” because of the negative connotations that “sociobiology” had acquired among the politically correct) is anathema to liberals - as, indeed, is economics; and much of “behavioral economics” is really anti-economics. Political bias is especially conspicuous in the neglect by the behavioralists of vengeance, though it is the best attested example of the “fairness” instinct. Liberals do not like vengeance and prefer to think that our instinct for fairness is dominated by altruistic concerns that might provide a foundation for organizing society along socialist or collectivist rather than free-market lines.\n\nThe rest of the article is worth a read, although Posner’s defence of rationality is a touch too much."
  },
  {
    "objectID": "posts/the-stigler-diet.html",
    "href": "posts/the-stigler-diet.html",
    "title": "The Stigler diet",
    "section": "",
    "text": "In a 1945 paper, George Stigler, the 1982 winner of the Nobel Memorial Prize in Economic Sciences, examined what would be the cheapest way in which a 154 pound man could meet his National Research Council recommended dietary requirements of 3000 calories a day, including 70 grams of protein and a range of other vitamins and minerals.\nUsing 1939 prices, Stigler found that an annual diet consisting of 370 pounds of wheat flour, 57 cans of evaporated milk, 111 pounds of cabbage, 23 pounds of spinach and 285 pounds of navy beans would meet the requirements. A nice vegetarian diet. Using 1945 prices, the evaporated milk and beans are substituted out for pancake flour and pork liver (leading to over 700 pounds of wheat and pancake flour to be consumed annually). Each diet costs roughly $600 a year in 2012 dollar terms.\nOne interesting element of the diet, and the NRC recommendation, is the low protein content. The protein leverage hypothesis (which I have blogged about previously), suggests that we have evolved a stronger propensity to regulate protein content than non-protein calories. As a result, around 14 per cent of the diet should be protein to allow one to feel full and prevent overeating. Given that less than 10 per cent of the Stigler diet is protein, the Stigler diet will leave someone permanently hungry - unless they want to splash out on some extra wheat and pancake flour.\nA few minutes of googling did not turn up someone’s “year on the Stigler diet”."
  },
  {
    "objectID": "posts/the-speed-of-cities-part-ii.html",
    "href": "posts/the-speed-of-cities-part-ii.html",
    "title": "The speed of cities, part II",
    "section": "",
    "text": "As I described in my last post, there is a strong relationship between the size of cities and the residents’ speed of walking. The larger the city, the quicker its residents scamper from A to B. A number of studies have confirmed this relationship and have broadened the relationship to the speed of other activities (such as betel nuts changing hands quicker in Port Moresby than in rural centres in Papua New Guinea).\nAs the relationship between city size and speed has been shown to be surprisingly robust, it took some time before this research was extended to other factors that may influence the speed of walking. Do people walk faster in richer cities? In colder cities? Where these other factors were examined (such as by Levine et al), the samples had tended to be limited to cities within the same country or region, limiting the variation of the explanatory factors in the sample.\nThis gap was addressed by Robert Levine (a guest on the Radiolab podcast that triggered this series of posts) and Ara Norenzayan in a 1999 paper in which they examined the “pace of life” in 31 countries. The pace of life measure was composed of three elements: average walking speed, the speed with which postal clerks completed a simple request (a stamp purchase) and the accuracy of public clocks. The sample included cities in North and South America, Asia and Europe, plus one African city. Generally, they took measurements in the largest city in the country.\nAt the top of the pace of life score rankings were Japan (4th) and the Western European countries. In fact, the nine Western European countries all placed among the top 11 for pace, split only by Japan and Hong Kong (10th). In the middle of the rankings were the Eastern European countries, the United States (New York), Canada and newly industrialised Asian countries (such as Guangzhou, China). The slowest were those from the Middle East, Latin America and Asia.\nThe apparently faster pace of life in the Western European countries compared to the United States and Canada is surprising and seems to go against New York stereotypes. I would suggest that one reason for the apparently faster pace of the Western European countries is that two of the three measures, postal speed and the accuracy of public clocks, may be more weakly linked with speed of life than walking speed and are heavily influenced by the nature of the public service. When we look at only walking speed, the rankings are changed. The United States jumps to 6th (from 16th overall) and Kenya is 9th (compared to 22nd overall). Conversely, Austrians have the 23rd fastest walking pace, versus their overall place of 8th. The Irish are the fastest walkers and have the second highest pace of life score.\nGiven the lack of earlier analysis into factors besides population size, Levine and Norenzayan sought to test a range of hypothesis about what might affect the pace of life. These were:\n\nThe more economic vitality a city has, the faster its pace of life. They used the country’s GDP, purchasing power parity (PPP – a measure of economic well-being) and average calorific intake as measures of the vitality.\nHotter places are slower (using the average annual maximum).\nIndividualistic cultures are slower.\nBigger cities are faster. As 23 of the 28 cities for which population size was available had more than 1 million residents, the sample did not allow them to fully test this hypothesis.\n\nAn examination of the correlation between the pace of life and the characteristics used to test the hypotheses showed that the first three hypotheses could be supported. For the overall pace of life and walking speed, there was a strong correlation with GDP (0.74 and 0.61) and PPP (0.72 and 0.59). Climate (-0.58 and -0.47) and collectivism (-0.59 and -0.60) were strongly negatively correlated. Calorific intake had a weaker but still positive correlation (0.51 and 0.39).\nThe correlation between walking speed and the community characteristics was generally stronger than that between the postal service or clock accuracy measures and those characteristics. Walking speed had a stronger relationship with GDP, PPP and collectivism than the other pace-of-life measures, while it had a similar relationship to the rest. This could be considered another sign that the postal and clock measures of pace-of-life have more variation due to idiosyncratic characteristics of the country than the straight measure of walking speed.\nThe big question that comes out of these findings is the question of causation. We have a strong correlation, but which is causing which? Does the higher value of time in rich cities result in the residents walking faster or do cities with more active residents become richer? Is there a selection effect, whereby dynamic, rich cities attract dynamic, fast walking residents? A question might also be asked about the permanence of these traits. Do residents speed up as the city gets richer? Do residents of a hot city walk faster on a cold day (or when they are in another, colder city)?\nMy instinct is that it is a mix of both, but that the selection effect is a significant player. However, there would be expected to be clear incentive effects of higher valued time (although which wins out - the substitution or income effect as the city gets richer?).\nUnlike earlier studies, the authors found no significant link between walking speed and population size. This could be representative of the lack of variation in the sample. The authors also suggested that this could be evidence of a threshold effect in that once a city exceeds a certain size, additional population growth does not affect the pace of life. This makes some sense, in that there is a limit to the speed with which someone can realistically walk. Further, once there is a certain threshold of environmental stimulus (if we adopt the hypothesis of Milbrand and Bornstein and Bornstein discussed in my last post) or alternative forms of entertainment, it is unlikely to spur faster walking.\nOne interesting suggestion by the authors was the potential for reinforcement. If a large city has more economic opportunity, it may attract migrants, further increasing its size. They noted the potential for future study into which factors are mutually reinforcing. This ties back to the issue of selection effects. A city that is slightly richer may become significantly so if it can attract a certain type of resident.\nAs a last note,the authors also sought to look at some consequences of the pace-of-life and hypothesised that faster cities have higher rates of death from heart disease, higher smoking rates and higher subjective well-being. These were all found to be supported in the predicted directions (although the relationship was less strong than that between the other community characteristics and the pace of life). I won’t go into these findings here, but the heart disease and smoking elements make sense. The subjective well-being finding also seems reasonable if you assume that higher income and other city benefits deliver well-being. It also suggests that if there is any “over-stimulation” that residents are trying to avoid through a fast-walking speed, the cost of this is outweighed by the benefits of city life."
  },
  {
    "objectID": "posts/the-simon-ehrlich-bet.html",
    "href": "posts/the-simon-ehrlich-bet.html",
    "title": "The Simon-Ehrlich bet",
    "section": "",
    "text": "I’m back on the population bandwagon today, and I wanted to define a point where economists and ecologists often appear to be talking across each other (and where I disagree with both). The best way to delineate this is by revisiting the Julian Simon-Paul Ehrlich bet.\nSimon and Ehrlich entered into a wager in 1980 as to whether five metals would increase or decrease in price over the next ten years. Simon, who believed that the ultimate resource is the human mind, bet that prices would decrease as substitutes and innovation made the goods effectively less scarce. The increased population would provide solutions that more than counteract their increased demand. Ehrlich, the author of The Population Bomb, considered that population growth was outstripping growth in resources and took the other side of the bet.\nAs history shows, Simon won. I’ve always considered that Ehrlich was poorly advised in taking this bet (and he did seek advice). While fluctuations or temporary pressures may increase prices, the long-term price trend for most resources is down. This may not always be the case - Simon lost a bet on the price of timber - but on average, it’s right. What will oil prices be like over the next five years? I’m not sure, but I guess high. What will the oil price be in 2030? I’m reasonably confident that it will be lower in real terms than it is today.\nWhile Ehrlich should not have entered that bet, I have more sympathy for some of Ehrlich’s other points. For example, in 1995, the San Francisco Chronicle quoted Simon as having said that:\n\nEvery measure of material and environmental welfare in the United States and in the world has improved rather than deteriorated. All long-run trends point in exactly the opposite direction from the projections of the doomsayers.\n\n“Every measure in the world”? Following Simon’s statement, Ehrlich and Stephen Schneider sought to make a second bet on 15 environmental indicators, such as carbon dioxide levels, temperature, arable land and sulfur dioxide emissions. Simon’s response was as follows (from Miele, Frank. “Living without limits: an interview with Julian Simon.” Skeptic, vol. 5, no. 1, 1997, p.57):\n\nLet me characterize their offer as follows. I predict, and this is for real, that the average performances in the next Olympics will be better than those in the last Olympics. On average, the performances have gotten better, Olympics to Olympics, for a variety of reasons. What Ehrlich and others says is that they don’t want to bet on athletic performances, they want to bet on the conditions of the track, or the weather, or the officials, or any other such indirect measure.\n\nWhile Simon’s statement is a step back from the Chronicle quote, it highlights a difference in focus between many economists and ecologists. Simon was primarily concerned with human living conditions. Will human well-being continue to increase despite what is happening to the environment? Simon would point out that the answer is always yes.\nBut is this to say that the track conditions do not matter? While the performances at the next Olympics will almost certainly be better than the last, good track conditions could make them even better. I don’t expect that climate change, coral reef loss, extinctions or biodiversity destruction will affect the average developed country so much that the average person is poorer in 2050 than they are today (note my use of “average”), but they will be poorer than they could be. Ecosystems provide trillions of dollars in free services. Many people care about and get intrinsic value from these ecosystems. Improving the condition of the track could improve the outcomes that Simon cares about. Do we want to maximise these outcomes, or are we simply happy if they are better?"
  },
  {
    "objectID": "posts/the-rhetoric-of-irrationality.html",
    "href": "posts/the-rhetoric-of-irrationality.html",
    "title": "The Rhetoric of Irrationality",
    "section": "",
    "text": "From the opening of Lola Lopes’s 1991 article The Rhetoric of Irrationality (pdf) on the heuristics and biases literature:\n\nNot long ago, Newsweek ran a feature article describing how researchers at a major midwestern business school are exploring the process of choice in hopes of helping business executives and business students improve their ‘often rudimentary decision-making skills’\n…\n[T]he researchers have, in the author’s words, ‘sadly’ concluded that ‘most people’ are ‘woefully muddled information processors who stumble along ill-chosen shortcuts to reach bad conclusions’. Poor ‘saps’ and ‘suckers’ that we are, a list of our typical decision flaws would be so lengthy as to ‘demoralize’ Solomon.\n…\nThis is a powerful message, sweeping in its generality and heavy in its social and political implications. It is also a strange message, for it concerns something that we might suppose could not be meaningfully studied in the laboratory, that being the fundamental adequacy or inadequacy of people’s capacity to choose and plan wisely in everyday life. Nonetheless, the message did originate in the laboratory, in studies that have no greater claim to relevance than hundreds of others that are published yearly in scholarly journals. My goal of this article is to trace how this message of irrationality has been selected out of the literature and how it has been changed and amplified in passing through the logical and expository layers that exist between experimental conception and popularization.\n\nBelow are some of the more interesting passages. First:\n\nPrior to 1970 or so, most researchers in judgment and decision-making believed that people are pretty good decision-makers. In fact, the most frequently cited summary paper of that era was titled ‘Man as an intuitive statistician’ (Peterson & Beach, 1967). Since then, however, opinion has taken a decided turn for the worse, though the decline was not in any sense demanded by experimental results. Subjects did not suddenly become any less adept at experimental tasks nor did experimentalists begin to grade their performance against a tougher standard. Instead, researchers began selectively to emphasize some results at the expense of others.\n…\nThe Science article [Kahneman and Tversky’s 1974 article (pdf)] is the primary conduit through which the laboratory results made their way our of psychology and into other branches of the social sciences. … About 20 percent of the citations were in sources outside psychology. Of these, all used the citation to support the unqualified claim that people are poor decision-makers.\nAcceptance of this sort is not the norm for psychological research. Scholars from other fields in the social sciences such as sociology, political science, law, economics, business and anthropology look with suspicion on the tightly controlled experimental tasks that psychologists study in the laboratories, particularly when the studies are carried out using student volunteers. In the case of the biases and heuristics literature, however, the issue of generalizability is seldom raised and it is rarely so much as mentioned that the cited conclusions are based on laboratory research. Human incompetence is presented as a fact, like gravity.\nIf you think of it, this is a great trick, for the studies in question have managed to shed their experimental details without sacrificing scientific authority. Somehow the message of irrationality has been sprung free of its factual supports, allowing it to be seen entire, unobstructed by the hopeful assumptions and tedious methodologies that brace up all laboratory research.\n\nOne interesting thread concerns the purpose of the experiments and the contrasting conclusions drawn from them. For this discussion, Lopes looks at six of the experiments in four of Kahneman and Tversky papers published between 1971 and 1973, plus a summary article in Science from 1974. One example involved this question:\n\nConsider the letter R. Is R more likely to appear in the first position of a word or the third position of a word?\n\nThis problem involves the availability heuristic, the tendency to estimate the probability of an event by the ease with which instances of the event can be remembered or constructed in the imagination. Under the availability hypothesis, people will see how many words they can generate with R in the first or third position. It is easier to think of words with R in the first position than the third, leading them to conclude - in error - that R is more common in the first.\nLopes writes:\n\n[T]he question is posed so that there are only two possible results. One of these will occur if the subject reasons in accord with probability theory, and the other, if the subject reasons heuristically. …\nBy this logic, the implications of Figure 1 [a summary of the results] are clear: subjects reason heuristically and not according to probability theory. That is the result, signed, sealed and delivered, courtesy of strong inference. But the main contribution of the research is not this result since few would have supposed that naive people know much about combinations or variances of binomial proportions or how often R appears in the third position of words. Instead, the research commands attention and respect because the various problems function as thought experiments, strengthening our grasp of the task domain by revealing critical psychological variables that do not show up in the normative analysis. …\nThere is, however, another way to construe this set of studies and that is by considering the predictions of the two processing modes at a higher level of abstraction. If we think about performance in terms of correctness, we see that in every case the probability mode predicts correct answers and the heuristic mode predicts errors. … [T]he sheer weight of all the wrong answers tend to deform the basic conclusion, bending it away from an evaluatively neutral description of the process and toward something more like ‘people use heuristics to judge probabilities and they are wrong’, or even ‘people make mistakes when they judge probabilities because they use heuristics’.\nHappily, conclusions like these do not hold up. This is because the tuning that is necessary for constructing problems that allow strong inference on processing questions is systematically misleading when it comes to asking evaluative questions. For example, consider the letter R problem. Why was R chosen for study and not, say, B? … Of the 20 possible consonants, 12 are more common in the first position and 8 are more common in the third position. All of the consonants that Kahneman and Tversky studied were taken from the third-position group even though there are more consonants in the first-position group.\nThe selection of consonants was not malicious. Their use is dictated by the strong inference logic since only they yield unambiguous answers to the processing question. In other words, when a subject says that R occurs more frequently in the first position, we know that he or she must be basing the judgment on availability, since the actual frequency information would lead to the opposite conclusion. Had we used B, instead, and had the subject also judged it to occur more often in the first position, we would not be able to tell whether the judgment reflect availability or factual knowledge since B is, in fact, more likely to occur in the first position.\n…\nWe see, then, that the experimental logic constrains the interpretation of the data. We can conclude that people use heuristics instead of probability theory but we cannot conclude that their judgments are generally poor. All the same, it is the latter, unwarranted conclusion that is most often conveyed by this literature, particularly in settings outside psychology.\n\nLopes then turns her attention onto Kahneman and Tversky’s famous Science article.\n\nIn the original experimental reports, there is plenty of language to suggest that human judgments are often wrong, but the exposition focuses mostly on the delineation of process. In the Science article, however, Tversky and Kahneman (1974) shift their attention from heuristic processing to biased processing. In the introduction they tell us: ‘This article shows that people rely on a limited number of heuristic principles which reduce the complex tasks of assessing probabilities and predicting values to simpler judgmental operations’ (p. 1124). By the time we get to the discussion, however, the emphasis has changed. Now they say: ‘This article has been concerned with cognitive biases that stem from the reliance on judgmental heuristics’ (p. 1130).\nExamination of the body of the paper shows that the retrospective account is the correct one: the paper is more concerned with biases than with heuristics even though the experiments bear more on heuristics than on biases.\n\nThere is plenty more of interest in Lopes’s article. I recommend reading the full article (pdf)."
  },
  {
    "objectID": "posts/the-recent-evolution-of-musical-talent.html",
    "href": "posts/the-recent-evolution-of-musical-talent.html",
    "title": "The recent evolution of musical talent",
    "section": "",
    "text": "From a debate between Gary Marcus and Geoffrey Miller on the biological basis for musical talent:\n\nMiller: Music’s got some key features of an evolved adaptation: It’s universal across cultures, it’s ancient in prehistory, and kids learn it early and spontaneously. …\nMarcus: “Ancient” seems like a bit of stretch to me. The oldest known musical artifacts are some bone flutes that are only 35,000 years old, a blink in an evolutionary time. …\nMiller: The bone flutes are at least 35,000 years old, but vocal music might be a lot older, given the fossil evidence on humans and Neanderthal vocal tracts. Thirty-five-thousand years sounds short in evolutionary terms, but it’s still more than a thousand human generations, which is plenty of time for selection to shape a hard-to-learn cultural skill into a talent for music in some people, even if music did originate as a purely cultural invention. Maybe that’s not enough time to make music into a finely tuned mental ability like language, but nobody knows yet how long these things take.\n\nThe remainder of the debate is worth a read.\nHT: Rob Brooks"
  },
  {
    "objectID": "posts/the-predictive-power-of-marshmallows.html",
    "href": "posts/the-predictive-power-of-marshmallows.html",
    "title": "The predictive power of marshmallows",
    "section": "",
    "text": "I have gone through the back catalogue of podcasts for WNYC’s Radiolab for a couple of months now. I got into it after ABC radio substituted it for the Science Show for two weeks in late June. It is sensational – great content and entertaining.\nI just listened to the Radiolab podcast on Walter Mischel’s marshmallow experiment . The basic idea of the experiment concerned testing the ability of four-year olds to delay gratification. The experimenters left the children in a featureless room with a marshmallow (or Oreo cookie in later experiments) with a promise of more if they waited. The interesting outcome from the experiment has been how the ability to delay is a strong indicator of future success. For example, the average difference between those who waited 10 seconds and those who waited 15 minutes was around 210 points on the SAT. 210 points is roughly the difference between the 60th and 85th percentile.\nI was familiar with this experiment from earlier readings, but was not aware on how this had become such a strong indicator of future success. Mischel and his colleagues are still following a number of the subjects, so there should be more to come.\nSo what we can take from this experiment? One (optimistic) possibility is that by training children to delay gratification (those who delayed used a variety of tricks) there may be lifelong benefits. Another possibility, and the one to which I lean, is that the experiment is an indicator of a broader personality trait, and that teaching a technique to delay gratification in such a case will not have much long-term impact. I consider that it is more like testing children for IQ – you can do something but it will take some effort.\nAn interesting question is whether patience is a cause of the delayer’s future success or a symptom of the underlying cause. For example, is this test simply a proxy for intelligence, with more intelligent children able to foresee the consequences of their actions and devise methods to help them delay?"
  },
  {
    "objectID": "posts/the-political-implications-of-group-selection.html",
    "href": "posts/the-political-implications-of-group-selection.html",
    "title": "The political implications of group selection",
    "section": "",
    "text": "Group selection advocates often describe how human cooperation could only have evolved through competition between groups. I have wondered how these advocates view modern day group competition, particularly in the form of tribalism and patriotism. Should we continue to engage in group competition to allow cooperation to flourish?\nThis article by David Sloan Wilson gives one perspective. Some of the more interesting quotes:\n\nEven small groups can become dysfunctional (i.e., fail to “constitute themselves”) if the analogs of patriotism and civic duty are absent. If members of a small group do not perceive themselves as a group with a common purpose with obligations enforceable by punishment, then they will fall apart as surely as a nation. …\n[S]mall groups are essential building blocks of large-scale society. We are genetically adapted to function in small groups of individuals who know, like, and trust each other and who hold each other accountable for their actions. Multicellular organisms can’t be healthy unless their cells are healthy, and large-scale human society requires healthy “cells” of small groups responsible for managing their own affairs. …\nIf we want America to function as an adaptive social entity, we need to adopt it as our group identity, carry out its obligations, and make sure that other Americans do also. In addition, the iron law of multilevel selection states unequivocally that if America pursues its self-interest too narrowly, then its actions will undermine adaptation at larger scales. The truest American patriot works to make America a solid citizen of the global village; nothing less will do.\n\nDespite my ambition to see evolutionary biology incorporated into economic and policy thinking, the last paragraph is a good reminder that, even from the same framework, it is possible to come to vastly different answers."
  },
  {
    "objectID": "posts/the-patience-of-economists.html",
    "href": "posts/the-patience-of-economists.html",
    "title": "The patience of economists",
    "section": "",
    "text": "Over four years since release of the working paper (and two and half years since I posted about it), Henrik Cronqvist and Stephan Siegel’s paper The Origin of Savings Behaviorhas been published in the Journal of Political Economy (follow the working paper link for an ungated copy). The abstract is as follows:\n\n Analyzing the savings behavior of a large sample of identical and fraternal twins, we find that genetic differences explain about 33 percent of the variation in savings propensities across individuals. Individuals are born with a persistent genetic predisposition to a specific savings behavior. Parenting contributes to the variation in savings rates among younger individuals, but its effect decays over time. The environment when growing up (e.g., parents’ wealth) moderates genetic effects. Finally, savings behavior is genetically correlated with income growth, smoking, and obesity, suggesting that the genetic component of savings behavior reflects genetic variation in time preferences or self-control.\n\nAs I posted last time, the finding is unsurprising and matches findings from behavioural genetics about other traits. Genes matter, heritability increases with age, family environment has little influence and there is a large non-shared environmental effect.\nThat it takes so long for economics papers to be published makes me thankful for the practice of releasing working papers. We’ve known of and have been able to talk about this result for several years now. It’s always interesting to see the multiple waves of press and attention as different audiences first become aware of a paper at different stages of its life.\nBut this does have some perverse effects, particularly across disciplinary boundaries. I recently received a referee report suggesting we had neglected some recent literature. Yet the main omission, a paper published in the Proceedings of the Royal Society B, cites the working paper on which our submission was based. Our working paper has been out long enough for people in fields with a faster track to publication to cite it and be published themselves.\nSo, if we reference that paper, we create a circular set of citations. I’ve spent plenty of time over the last few years following citation chains that do not ultimately establish the point claimed, but I haven’t ended up back where I started too many times yet."
  },
  {
    "objectID": "posts/the-outsider-to-the-narrow-minded-profession.html",
    "href": "posts/the-outsider-to-the-narrow-minded-profession.html",
    "title": "The outsider to the narrow-minded profession",
    "section": "",
    "text": "I have always been a sucker for stories about an outsider tearing down what everyone believes to be true. With that, it’s no surprise that I have fond memories from my first read of M. Mitchell Waldrop’s book Complexity: The Emerging Science at the Edge of Order and Chaos (which must been around 20-years ago).\nThe book is framed around the coalescence of a group of researchers into the Santa Fe Institute. These researchers were misfits, not quite fitting into their respective fields, but coming together in an interdisciplinary effort to develop a new science of complexity.\nSince reading Waldrop’s book, I have devoured an enormous number of papers and publications from the Institute, with titles such as The Economy as an Evolving Complex System II. The Institute is a place where I would love to spend some time.\nThe book opens with the story of Brian Arthur. He is a major proponent of the idea of increasing returns in economics, with the book telling the story about how the economics profession resisted. Take the following excerpt as an example:\n\nTom Rothenberg, one of his former professors, had asked the inevitable question: “So, Brian, what are you working on these days?” Arthur had given him the two-word answer just to get started: “Increasing returns.” And the economics department chairman, Al Fishlow, had stared at him with a kind of deadpan look. “But—we know increasing returns don’t exist.” “Besides,” jumped in Rothenberg with a grin, “if they did, we’d have to outlaw them!”\nAnd then they’d laughed. Not unkindly. It was just an insider’s joke. Arthur knew it was a joke. It was trivial. Yet that one sound had somehow shattered his whole bubble of anticipation. He’d sat there, struck speechless. Here were two of the economists he respected most, and they just—couldn’t listen. Suddenly Arthur had felt naive. Stupid. Like someone who didn’t know enough not to believe in increasing returns. Somehow, it had been the last straw.\n\nI recently picked the book up again for some light reading before bed. And with the benefit of having now studied some economics, the first chapter jarred. Increasing returns is a topic with some history. Paul Krugman earned his Nobel Memorial Prize for his work in the area. Increasing returns wasn’t exactly, as the book seemed to implied, a concept that no-one in economics believed.\nA google search to test my fresh instinct quickly struck gold, in the form of Paul Krugman himself and a 1998 article in Slate titled The Legend of Arthur: A tale of gullibility at The New Yorker. Krugman’s article was triggered by a (paywalled) article by John Cassidy in The New Yorker, but presented an opportunity for Krugman to get his thoughts about Waldrop’s book off his chest too. Here are some snippets:\n\nIncreasing returns wasn’t a new idea, it wasn’t obstinately opposed–and if increasing returns play a larger role in mainstream economic theory now than they did 20 years ago, Arthur didn’t have much to do with that change. Indeed, the spread of the Arthurian legend is a better story than the legend itself: an object lesson in journalistic gullibility.\n\nOn a conversation with Victor Norman:\n\nHere’s how it all began, according to Waldrop. On Nov. 5, 1979, Brian Arthur wrote in his notebook a manifesto describing his project to develop a New Economics based on increasing returns. In a park in Vienna, he tried to explain it to a “distinguished international trade theorist” from Norway, who was baffled. So were other establishment economists. Thus began Arthur’s years in the wilderness. In 1983, he completed his seminal paper, but not until 1989, after 14 rewrites, was he able to publish it. “Gradually,” writes Cassidy, “a number of economists”–such as Georgetown University’s Steve Salop–“began to take Arthur’s conclusions seriously.”\nGreat story. Now let’s do a reality check, starting with that walk in the park. It is, indeed, truly astonishing that the Norwegian, Victor Norman, did not understand what Arthur was driving at. After all, there is a long tradition of increasing returns in international trade theory. If nothing else, Norman should have been familiar with his own co-authored book, Theory of International Trade, which was in galleys at the time. It contained a whole chapter devoted to increasing returns, based largely on a paper Norman himself had written three years before. Is it possible that Arthur misinterpreted Norman’s bafflement – that what Norman really couldn’t understand was why Arthur thought he was saying anything new?\n\nOn people paying attention to Arthur’s work:\n\nE conLit, the database of professional literature since 1970, reveals that by 1987 – the moment Waldrop’s book claims that Arthur’s theories about increasing returns began to be accepted – mainstream journals had published about 140 papers on the subject. Salop, the Georgetown professor Cassidy presents as an early convert to Arthur’s ideas, was early indeed. He wrote one of his own best-known papers on increasing returns in 1978 – a year before Arthur, by his own account, even began to think about the subject.\n\nAll rather funny, but it doesn’t end here. The debate continued in Slate, including letters from Cassidy, Waldrop, Kenneth Arrow! and a response from Krugman. Most of the letters defend Brian Arthur and his contribution. They point out that Arthur was always willing to give credit where it was due and to place his work in historical context. Fair enough.\nBut the impression that lingers from Waldrop and Cassidy is that economics is a narrow-minded profession unwilling to consider ideas that don’t fit with its cherished tenets. I have some sympathy for that argument. But this story of increasing returns provides little support for it.\nAs a final note, it amuses me who gets painted as (or paints themselves as) the outsider. Here we have Brian Arthur, the youngest endowed chair holder at Stanford when he gained his position at 37 (admittedly not for his work on increasing returns). He was introduced into the Santa Fe Institute by Kenneth Arrow. Tough ride.\nIt’s similar to Steven Levitt’s adoption of the “rogue economist” label in Freakonomics, if you can truly be rogue when you’re a Distinguished Service Professor of Economics at the University of Chicago. Or Richard Thaler’s complaints about being ignored by the profession, despite also being a Distinguished Service Professor at the University of Chicago and elevated to president of the American Economic Association.\nTo top if off, all three of these outsiders have multiple papers published in the top economics journals. That is a privilege never experienced by most members of the profession. If these guys are outsiders, what do you call Ole Peters?"
  },
  {
    "objectID": "posts/the-other-gender-gap.html",
    "href": "posts/the-other-gender-gap.html",
    "title": "The other gender gap",
    "section": "",
    "text": "The Economist discusses a new OECD report on a growing gender gap in schools:\n\nIt is a problem that would have been unimaginable a few decades ago. Until the 1960s boys spent longer and went further in school than girls, and were more likely to graduate from university. Now, across the rich world and in a growing number of poor countries, the balance has tilted the other way. …\nThe reversal is laid out in a report published on March 5th by the OECD, a Paris-based rich-country think-tank. Boys’ dominance just about endures in maths: at age 15 they are, on average, the equivalent of three months’ schooling ahead of girls. In science the results are fairly even. But in reading, where girls have been ahead for some time, a gulf has appeared. In all 64 countries and economies in the study, girls outperform boys. The average gap is equivalent to an extra year of schooling.\n\nThe gap is particularly stark at the bottom, with teenage boys “50% more likely than girls to fail to achieve basic proficiency in any of maths, reading and science.” While Larry Summers got crucified for referring to male-female differences in variation in different human traits, the higher rates of low proficiency among boys reflects the often ignored consequence of Summers’s statement - more males at the bottom.\nThe article notes other explanations for the gap such as time doing homework, attitude to school and reading at home, but one of the more interesting explanations might be discrimination:\n\nPerhaps because they can be so insufferable, teenage boys are often marked down. The OECD found that boys did much better in its anonymised tests than in teacher assessments. The gap with girls in reading was a third smaller, and the gap in maths—where boys were already ahead—opened up further. In another finding that suggests a lack of even-handedness among teachers, boys are more likely than girls to be forced to repeat a year, even when they are of equal ability.\n\nThere are likely ways to improve test outcomes for some of the lowest performing students. Anonymised tests could be one.\nBut will the net result of interventions to increase male or female school performance open or close the gap? Making the environment the same for everyone will exacerbate innate differences. Different teaching environments for boys and girls may maximise performance, but again, it’s not clear what size the gap would be under that arrangement (Or teaching could be adjusted based on traits such as IQ and big five personality traits - females tend to score higher on agreeableness and conscientiousness - removing direct discrimination, but effectively treating boys and girls differently on average)\nThe advantage of females over males continues through to tertiary education:\n\nIn the OECD women now make up 56% of students enrolled, up from 46% in 1985. By 2025 that may rise to 58%.\nEven in the handful of OECD countries where women are in the minority on campus, their numbers are creeping up. Meanwhile several, including America, Britain and parts of Scandinavia, have 50% more women than men on campus. …\nAccording to the OECD, the return on investment in a degree is higher for women than for men in many countries, though not all.\n\nSo why does the income gap persist in the workforce? The Economist article closes with research by Claudia Goldin:\n\nIn a recent paper in the American Economic Review Ms Goldin found that the difference between the hourly earnings of highly qualified men and their female peers grows hugely in the first 10-15 years of working life, largely because of a big premium in some highly paid jobs on putting in long days and being constantly on call. On the whole men find it easier than women to work in this way.\n\nIt is not easy to develop a policy response to this, nor for a business to try to close the gap without trade-offs. One possibility is gender quotas, although this may largely operate to the advantage of women who do not take time off for family reasons.\nAnother option is to require men to take paternity leave when they have children. But this penalises both men and women who have children. I am all for people bearing the costs of their children, but a policy to exacerbate those costs in the workplace seems misplaced.\nAnother question around this option is why time out of the workplace relating to children is specifically targeted. What of those who want to travel, study or volunteer for charity, each of which surely come with costs?\nA less prescriptive response is reflected in the closing part of Goldin’s paper, where she writes:\n\nThe last chapter must be concerned with how worker time is allocated, used and remunerated and it must involve a reduction in the dependence of remuneration on particular segments of time. It must involve greater independence and autonomy for certain types of workers and the ability of workers to substitute seamlessly for each other. Flexibility at work has become a prized benefit but flexibility is of less value if it comes at a high price in terms of earnings. The various types of temporal flexibility require changes in the structure of work so that their cost is reduced.\n\nGoldin mentions that some jobs necessarily involve long hours and availability at all times. These professions will be resistant to a closing of the gap. However, for jobs that do not require long hours, how easy it is to replace time-based measures? Outputs in many of those jobs are credence goods (think of a lot of output from consulting or even law). Hours worked may be the only tangible measure the employers and clients have."
  },
  {
    "objectID": "posts/the-origin-of-the-phrase-sneaky-fcker.html",
    "href": "posts/the-origin-of-the-phrase-sneaky-fcker.html",
    "title": "The origin of the phrase “sneaky f**cker”",
    "section": "",
    "text": "When low-status males have no chance of accessing females via traditional routes such as fighting or signalling their prowess, they may attempt more deceptive means of getting a mate.\nAs an example, some male fish take advantage of the external fertilisation of eggs by hiding on the periphery of the courting male’s territory, including in some cases disguising themselves as females. When the eggs are released, they rush in and attempt to fertilise them. This strategy can be dangerous, but for males that simply aren’t in the game, it may be their only option.\nThe technical term for this behaviour is kleptogamy, but it is better known as the “sneaky fucker” strategy. I’ve often heard and read in the blogosphere that John Maynard Smith coined the phrase, but I haven’t been able to turn up anything authoritative.\nThe earliest use of the term I have been able to find is by Richard Dawkins and John Krebs in their chapter Animal Signals: Information or Manipulation in the 1978 first edition of Krebs and Nicholas Davies Behavioural Ecology: An Evolutionary Approach (HT: Rob Brooks). The relevant passage is as follows:\n\nIt is especially interesting that the movements of the eventual loser of a contest closely parallel those of the winner until a few moments before giving up, just as one would expect if the contest involves both bluff and assessment. A similar effect was observed in red deer stags (Cervus elephus) by Clutton-Brock (in prep.); the stags compete for hinds to add to their harems, and contests consist of prolonged roaring duels. Escalated contests are rare, and they are costly because of the high risk of injury and because subordinate males, known as sneaky fuckers, may steal matings during a prolonged fight. Contests are settled by roaring: the two males roar at each other with a gradually increasing tempo until one suddenly gives up …\n\nInterestingly, the mention of sneaky fuckers is an aside and not the point of the paragraph. Also, the phrase “known as sneaky fuckers” suggests the term was already established.\nSo, does anyone know of an earlier reference to the sneaky fucker strategy?\nUPDATE: In the comments, Alex Sutherland notes an earlier reference from New Scientist in 1977 (Vol 75, p673). In an article The games animals play, Jeremy Cherfas writes (also on the subject of Clutton-Brock’s work on stags):\n\nA stag can gain females by successfully challenging other stags. Fights start with a session of roaring, which may be followed by parallel walking and perhaps a tussle with locked antlers. If the challenger had a harem, the winner takes control of both sets of hinds. The benefits of a larger harem in terms of increased reproductive potential, are clear, but what of the costs? Clutton-Brock estimates that during the average reproductive life of four years, 20 per cent of the stags are permanently injured in fights. The costs are considerable. For a bachelor without a harem the benefits outweigh the costs, and one finds that harem owners initiate fights less often than bachelors. The costs are also increased when both stags hold harems, largely because young stags take advantage of the fight to steal hinds from the harems. (This is known as kleptogyny, but Clutton-Brock and most others prefer “the sneaky fucker strategy”.) Fights, and injuries, are more frequent in mid rut, when most conceptions occur.\n\nThis passage confirms the phrase was already established by 1977. And again, the mention is an aside and not a discussion of the sneaky fucker strategy itself."
  },
  {
    "objectID": "posts/the-mating-reservation-wage.html",
    "href": "posts/the-mating-reservation-wage.html",
    "title": "The mating reservation wage",
    "section": "",
    "text": "Bryan Caplan makes an excellent point:\n\nFemale income has greatly increased, and men with low status jobs are “inferior goods” in the mating market.  As a result, the demand to date and marry such men has sharply declined.  The average guy with a low-status job is only modestly more dateable in women’s eyes than the average guy with no job at all.  Men respond by either working _much _harder to become “superior goods,” or saying “Why bother?” and giving up.  On this account, working class men are acting less industriously even though their _preferences _are no less industrious than they used to be.\n\nPeople often forget that money is not the primary purpose of working. If men work and earn to attract a mate, they are interested in how much they should earn to succeed in that goal. If they have no chance of reaching that objective, they will not work. Their mating reservation wage is not reached.\nI agree with Caplan. Preferences have not changed. The mating reservation wage is going up.\nThis has some consequences for minimum wage policy. If the mating reservation wage is going up, the minimum wage will have less effect on working decisions by men.\nHowever, one thing about this situation perplexes me. Why are low-skilled men accepting this so quietly? Crime is still going down. There is no evidence of the social disruption that is often predicted to go with the presence of many unpaired males. Does modern society provide enough opiates for men to quietly check out?"
  },
  {
    "objectID": "posts/the-magic-of-commerce.html",
    "href": "posts/the-magic-of-commerce.html",
    "title": "The magic of commerce",
    "section": "",
    "text": "A re-read of The Malay Archipelago reminded me of Alfred Russel Wallace’s occasional bleeding-heart libertarian leanings. From his time in remote Dobo in the Aru Islands of Eastern Indonesia:\n\nI daresay there are now near five hundred people in Dobbo of various races, all met in this remote corner of the East, as they express it, “to look for their fortune;” to get money any way they can. They are most of them people who have the very worst reputation for honesty as well as every other form of morality,—Chinese, Bugis, Ceramese, and half-caste Javanese, with a sprinkling of half-wild Papuans from Timor, Babber, and other islands, yet all goes on as yet very quietly. This motley, ignorant, bloodthirsty, thievish population live here without the shadow of a government, with no police, no courts, and no lawyers; yet they do not cut each other’s throats, do not plunder each other day and night, do not fall into the anarchy such a state of things might be supposed to lead to. It is very extraordinary! It puts strange thoughts into one’s head about the mountain-load of government under which people exist in Europe, and suggests the idea that we may be over-governed. Think of the hundred Acts of Parliament annually enacted to prevent us, the people of England, from cutting each other’s throats, or from doing to our neighbour as we would not be done by. Think of the thousands of lawyers and barristers whose whole lives are spent in telling us what the hundred Acts of Parliament mean, and one would be led to infer that if Dobbo has too little law England has too much.\nHere we may behold in its simplest form the genius of Commerce at the work of Civilization. Trade is the magic that keeps all at peace, and unites these discordant elements into a well-behaved community. All are traders, and know that peace and order are essential to successful trade, and thus a public opinion is created which puts down all lawlessness."
  },
  {
    "objectID": "posts/the-love-principle.html",
    "href": "posts/the-love-principle.html",
    "title": "The love principle",
    "section": "",
    "text": "In my recent post reviewing Paul Frijters and Gigi Foster’s An Economic Theory of Greed, Love, Groups, and Networks, I flagged that they considered their new theoretical contribution to be “the love principle”. In this post, I want to pull the idea apart. I’m not going to offer a perfect alternative to the love principle, but I hope that by giving it a good going over I can possibly understand it better.\nFrijters and Foster state the love principle as follows:\n\nLove derives from the attempt of the unconscious mind to bargain with something that is believed to be capable of fulfilling desires and that is perceived to be too powerful to be possessed by direct means.\n\nThe starting point for how the love principle works is a person who believes that an entity can deliver to them something that they want. This entity could be a family member, a stranger or an abstraction such as a god. The person then considers whether they can take what they want from the entity (i.e. they can dominate it) and if so, they take it (greed).\nIf they cannot take what they want through dominating the entity, their unconscious mind seeks to bargain with this entity (now the love object) in the hope that emotional investment will lead to them getting what they want. Frijters and Foster liken this response to (among other things) submission, although it is a more complete form of submission as there is an internal change in self-image. The person becomes partially “one” with their love object. They internalise the wellbeing of the love object into their own wellbeing and are willing to make transfers toward this love object.\nGiven that the love-greed response depends on whether a person can dominate an entity, the trigger for love is the nature of the power relationship. People will tend to love entities more powerful than themselves. This might appear to create difficulties where two people love each other, but Frijters and Foster try to make it work. As an example, they suggest that a young child wields power over their parents before birth through the costs that the mother must bear (hence, fathers are snared by children to a lesser degree). Conversely, children love their parents as they are initially helpless, and do not realise that they will have power later on.\nAs I foreshadowed in the last post, I am not a huge fan of the love principle for a few different reasons. The first is that their description of the agent becoming “one” with the love object is, in one particular case, more accurate than they suggest. Consequently, this groups together what I would argue are several different phenomena. The model that Frijters and Foster present at the end of the book is a useful tool to illustrate this point.\nIn this model, an agent lives for two periods and seeks to maximise his utility in both periods of his life. This completely naive individual (incorrectly) believes that an emotional investment in a love object will yield a more consumption in the first period. What the agent does not realise is that in the second period the agent’s utility function changes to incorporate the love object. Frijters and Foster do this by making the agent positively weight the love object’s utility in the agent’s own utility function, with the level of incorporation of the love object’s utility an increasing function of the agent’s initial level of emotional investment.\nAs consumption by the love object in the second period increases the agent’s utility, the agent transfers consumption goods to the love object. The net result of this action is that the agent consumes the same amount in the first period irrespective of their level of emotional investment, as the emotional investment does not actually increase their consumption. The agent consumes less in the second period due to their transfer to the love object. Their utility is higher in the second period, however, as they have now gained utility from the love object’s utility.\nFrom an evolutionary standpoint, I’m not sure how this system could evolve as an agent that does not love would consume more. There would need to be some other advantage to love not incorporated into the model. Frijters and Foster hint at a few possibilities, such as confidence and signalling, but for some of the examples they consider, there is an important option - kin selection. It is easy to incorporate kin selection into the model, as the weighting given to the love object by the agent would be the degree of relatedness. This turns the utility equation in the second period into a modified form of Hamilton’s rule, with the asymmetry of transfers between parent and child based on the lower reproductive value of the parents.\nBy making the love object kin, the love object would be part of the agent from the perspective of the relevant unit of selection, the gene. To the extent there is any internalisation going on due to the triggers or primes that lead to children and parents loving each other, evolution shapes it so that people internalise kin according to their relatedness.\nThis approach creates a simpler explanation for some of the love relationships Frijters and Foster describe. Children and parents love each other as they share genes. Mothers love more than fathers as the mother is more sure that the child is hers, and a father can produce many more love objects in a lifetime to which he can give his love. As Frijters and Foster note, this love of kin is not automatic and involves priming before and after birth. They point to the power relationship as triggering this love, and suggest that children do not love machines because machines have no power. But there are alternative explanations to this, such as having a type of object (e.g. a human face) that they are susceptible to be primed with.\nOf course, this approach does not extend to the cases where two non-kin love each other, which relates to the problem that I raised above. Love of kin has different evolutionary forces shaping it to love of partners, groups or gods. As a result, I’m not sure they should be grouped together, even if there are some common neurological or biological factors.\nFor these non-kin cases I take a different reading of the process leading to “love”. I like the build up to the love principle - decide what you want, see if you can take it, take it if you can. But I lose them at the point where the person decides they cannot dominate the love object. I would state the process as being that after someone assesses that they cannot get what they want by domination, they come up with a different strategy. Those strategies are complex and varied, but many of them are not love in the sense felt between kin, or even loyalty as it is usually defined (Frijters and Foster group love and loyalty together). Accordingly, many of the so-called battles between love and greed described in the book are simply assessments of the costs and benefits of different actions under varying constraints.\nConsider this in the context of another example they give, a reasonably well matched couple. If each has what the other wants but they can’t simply take it, love will flow both ways. Social conditions may affect that power balance, such as the social stigma associated with one of the couple receiving services from elsewhere. There may be similar neurological and biochemical processes associated with this form of love to those that occur with children. But the relationship is different - reciprocal rather than becoming one. An indication of the difference is the likelihood that love between a couple will fade if conditions change, such as the male gaining more wealth and resources, which is markedly different to the usual case with children.\nI’ve never come across a completely convincing evolutionary explanation for love outside of kin relationships, but lean towards love being a bootstrapped evolutionary solution to providing a commitment device. As argued by Robert Frank, love results in behaviour that honestly signals commitment to the other party. Or, my generally preferred explanation, it simply focuses the loving parties on the actions that will maximise their reproductive success.\nAlthough the evolutionary biology approach doesn’t offer a perfect answer, it does work for the out-of-sample predictions. Power is an aphrodisiac, as a woman wants a powerful man’s genes and resources. As a person becomes more powerful, love fades as they simply have more opportunity or resources. Parental investment theory provides a basis for sex differences in the response to power.\nWhen we move to love in groups (which is probably closer to loyalty as traditionally defined), Frijters and Foster describe the love that the submissive members of the group feel. In their examination of the evolutionary benefits of the love program, they suggest that love prevents stress as it allows internalisation of the success of the dominant group members (although the Whitehall and other experiments suggest low ranked people still experience stress - and if stress was costly, why do people feel stress?). They also point to survival of a long period of subordination through avoiding possibly harmful conflict. But I am not sure that these strategies are loyalty or love. The submissive member may be implementing kleptogamy, where lower status males within the group try alternative ways of accessing females (or as John Maynard Smith termed it, the “sneaky f*&%er” strategy - and if someone knows an authoritative source giving John Maynard Smith credit for this phrase, please let me know). They may form smaller coalitions, such as banding with other beta males, with the resulting reverse-dominance hierarchy keeping the highest status male from taking too many spoils for himself, lest he be stabbed in the night or overthrown. These are strategic relations, with greed at their heart. We don’t need a love principle to explain the fact that a weak male chooses to avoid conflict in the short-term and bide his time.\nThe one element of the love principle that seems strongest to me (and for which this evolutionarily informed approach is weakest) is for love of abstractions and symbolic expenses, such as sacrifices to gods. There are potential explanations such as signalling, maladaptation, spandrel or strengthened bonds in a group through the shared abstraction, but they are not completely satisfying. The love concept is neat in that it directly addresses this phenomena, but I am not convinced.\nAs I flagged in the last post, when Frijters and Foster turn to the applications of their theories, I become more strongly of the view that what they are calling love is simply constrained greed. For example, they note the victory of love in the lack of trade barriers, but under the process described in the book, it is the greed of politicians and economists within their own groups that brought trade barriers down. They are responding to the internal incentives of the group, and I am not sure the economists are feeling much love. Rather, it seems that they are labelling positive results for groups as love, even if no-one specifically feels love.\nThis points comes out further in their view on the balance between love and greed over time. In the short-term, they suggest we decide with our heart. However, evolutionary factors mean that we have to effectively be greedy in the medium term to avoid being eliminated, so in the medium term greed wins. In the longer-term, they suggest that love is winning out, as if we look at the level of nations, history tends towards institutions that recognise greed but constrain it for the common good. Is this a subtle shift in what they mean by love? I’m happy to call this love, but it may not directly relate to the love principle, as it does not require that people have become one with any love objects, and could simply be the result of constrained greed (which is what their evolutionary analysis of the medium term would suggest).\nSo, to pull a rather long post together, I’m not convinced about the mechanism behind the love principle, that the full scope of love and loyalty covered by the principle should be grouped together, and whether many of the actions labelled as love are anything but constrained greed. I’m open to being convinced on many of these points, but I’m struggling to accept the whole package. Still, its a pretty neat idea to pull apart some interesting issues."
  },
  {
    "objectID": "posts/the-limits-of-behavioural-science-coronavirus-edition.html",
    "href": "posts/the-limits-of-behavioural-science-coronavirus-edition.html",
    "title": "The limits of behavioural science: coronavirus edition",
    "section": "",
    "text": "Most articles on how behavioural science (or “behavioural economics”) can explain “X” are rubbish. “How behavioural economics explains Donald Trump’s election” or the equivalent would have been “How behavioural economics doomed Donald Trump” if he had failed to be elected. It’s after-the-fact storytelling of no scientific substance.\nThrough the last six weeks I have been collecting examples in the media of behavioural science applied to the coronavirus pandemic. There’s plenty of the usual junk.\nAs it turns out, Stuart Ritchie has also been on the case and written an article at UnHerd, Don’t trust the psychologists on coronavirus, saving me the trouble of writing my own. I would have chosen a different title, but follow the link and read the whole article. Below are some highlights.\nFirst, Ritchie on Cass Sunstein:\n\nFurther psychological insights were provided by Cass Sunstein, co-author of the best-selling book Nudge, which used lessons from behavioural economics (essentially psychology by another name) that could inform attempts to change people’s behaviour. In an article for Bloomberg Opinion on 28 February (by which point there were over 83,000 confirmed coronavirus cases worldwide), Sunstein wrote that anxiety regarding the coronavirus pandemic was mainly due to something called “probability neglect”.\nBecause the disease is both novel and potentially fatal, Sunstein reasoned, we suffer from “excessive fear” and neglect the fact that our probability of getting it is low. “Unless the disease is contained in the near future,” he continued, “it will induce much more fear, and much more in the way of economic and social dislocation, than is warranted by the actual risk”.\n\nThe opening paragraph of Sunstein’s article was somewhat bizarre:\n\nAt this stage, no one can specify the magnitude of the threat from the coronavirus. But one thing is clear: A lot of people are more scared than they have any reason to be. They have an exaggerated sense of their own personal risk.\n\nI know this shooting fish in the barrel, but how can you claim that people have an exaggerated sense of their own personal risk when no one can specify the magnitude of the threat?\n(As an aside, turns out I have joined the masses of people blocked by Cass Sunstein on Twitter. Given my tweets are almost solely broadcasts of my blog posts, and my criticism of Sunstein within those posts is rather mild - and unlikely read by Sunstein - my working hypothesis is that he has blocked everyone Nassim Taleb follows. Recalling the first sentence of his book #Republic: “In a well-functioning democracy, people do not live in echo chamber or information cocoons”. I’ll leave that for now…)\nRitchie also points out that Gerd Gigerenzer makes the same error as Sunstein:\n\nOn 12 March, the day after Italy had announced its 827th death from the virus, the eminent psychologist Gerd Gigerenzer published a piece in Project Syndicate entitled “Why What Does Not Kill Us Makes Us Panic”. It was, to say the least, confused: it opened with an acknowledgement that we don’t know how bad this epidemic could be, but immediately went on to make the case that we’d likely overreact, and failed to consider any opposing arguments.\n\nGigerenzer normally does an admirable job of defending human intuition against critiques from the outside, but he has always questioned our “risk literacy”.\nStories based on behavioural science aren’t just landing in the media. They are forming part of advice to government:\n\nThe Behavioural Insights Team, a consulting company nicknamed the “Nudge Unit”, has been brought in to assist the UK’s response. At first it seemed their focus was on how to encourage handwashing, but there appears to have been mission creep.\nFor example, the team’s head, David Halpern, was interviewed as a “government coronavirus science advisor” about broad policies of “cocooning” older people. He was also quoted in support of the idea — which might yet seem grievously misguided in hindsight — that social-distancing measures should only be brought in gradually, to avoid people becoming fatigued. After he was reportedly “bollocked” by No.10 a fortnight ago for introducing the unfortunate phrase “herd immunity” into the national conversation, Halpern hasn’t (to my knowledge) been heard from again in public.\n\nIn defence of Halpern, this story is light on details, and there is a lot of interesting research that could inform this argument - as Vaughan Bell points out. That said, backing herd immunity on an assumption of fatigue is quite a leap of faith.\nWhy are behavioural scientists struggling? One is generalisability of the scientific literature:\n\nTo back up his points about “probability neglect”, Sunstein had referred to a 2001 paper in the journal Psychological Science. It reported three experiments; Sunstein focused on the third one, which included 156 participants, all of whom were undergraduate students reasoning about how much they’d pay to avoid an imaginary electric shock. It’s not a criticism of the scientists to say that this experiment is only tenuously relevant to a global pandemic.\nIndeed, alongside talk of the “replication crisis” there’s been discussion of a “generalisability crisis”, with renewed realisation that results from lab experiments don’t necessary generalise to other contexts. A global pandemic of a completely novel virus is, by its very definition, a context never encountered before. So how can we be sure that the results of behavioural science experiments — even those that are based on bigger or more representative samples than 156 undergrads — are relevant to our current situation?\nThe answer is that we can’t. Exploring the human capacity for bias and irrationality can make for quirky, thought-provoking articles and books that make readers feel smarter (and can build towards a tentative scientific understanding of how the mind works). But when a truly dangerous disease comes along, relying on small-scale lab experiments and behavioural-economic studies results in dreadful misfires like the articles we encountered above.\nAlthough there are many behavioural phenomena that certainly seem relevant to today’s news — bias, sunk costs, the tragedy of the commons — it’s not at all clear how these concepts would be practically applied to do what needs to be done right now: slowing the spread of the disease.\n\nOn Ritchie’s closing, I agree:\n\nAs intriguing as many psychological studies are, the vast majority of the insights we’ve gained from our research are simply not ready for primetime — especially in the case of a worldwide emergency where millions of lives are at stake. Much of the useful advice behavioural scientists can give isn’t really based on “science” to any important degree, and is intuitive and obvious.\nWhere they try to be counter-intuitive — for instance, arguing that people are wrong to find a global pandemic frightening — they simply end up embarrassing themselves, or worse, endangering people by having them make fewer pandemic preparations. This isn’t to say that psychology isn’t useful when it stays in its own lane: it’ll be important to ensure that as many people as possible have access to psychotherapy for the mental-health effects of the pandemic, for instance. But that’s a secondary effect of the virus: my argument here is that psychology can give little reliable counsel about our immediate reaction to the pandemic.\nPsychologists should know their limits, and avoid over-stretching results from their small-scale studies to new, dissimilar situations. Decision-makers should, before using psychology research as the basis for policy, know just how weak and contentious so much of it is. And everyone else should stay at home, wash their hands — and beware psychologists bearing advice.\n\nSo could behavioural scientists be useful in this pandemic? They could help develop and test hypotheses to increase handwashing. They could help design communications about the need to stay at home. They have insights relevant to productivity while remote working. But they should be wary of telling stories about the accuracy of risk perception or how people will behave in the long term. Most of those claims are little more than storytelling."
  },
  {
    "objectID": "posts/the-iq-taboo.html",
    "href": "posts/the-iq-taboo.html",
    "title": "The IQ taboo",
    "section": "",
    "text": "While IQ research seems to be be emerging from its taboo phase, Anneli Rufus has written an article in Alternet which asks why the study of human intelligence was off the agenda for so long.\nThe analysis points to the usual suspects - the shadow of eugenics and racial research - but the article does have a couple of interesting quotes from Stephen Murdoch and Dennis Garlick. First, in much of the literature on IQ, the focus is on finding ‘g’, a single measure of general intelligence. Murdoch is agnostic as to whether ‘g’ exists:\n\n“The science underlying IQ tests isn’t like experiments in the life or hard sciences,” Murdoch insists. “IQ proponents believe in something they call general intelligence. That is, they believe there is one singular, measurable, inheritable kind of intelligence that we can all be ranked on. I have no idea if this is correct or not. Nor do I care.”\n\nWhile many of the debates about how well IQ captures ‘g’, if it exists, don’t seem to bear much fruit, IQ is clearly an important variable and has significant predictive power. I find it hard to be agnostic about a measure with such important life implications. And alternatives to ‘g’ do not seem to have more promise:\n\n“There is still little evidence to support many of the claims made by proponents of alternative intelligences,” Garlick says, “yet it is advocated that life-changing decisions should be made based upon them. One is tempted to say that the alternative intelligence industry is a reminder that snake-oil salesmen are alive and well in this day and age.”\n\nGarlick points to the need for IQ research:\n\n““I find it ironical that so much research is devoted to disorders like autism that only affect less than 1 percent of the population, but little research is devoted to understanding differences in IQ. … If the deficits of autism can be improved through research, why not IQ?”\n\nAs low IQ can have such significant costs, IQ research is an important basis for any policy discussion. Even if differences in IQ are intractable (which to an extent they certainly are), we can’t be any worse off for that knowledge."
  },
  {
    "objectID": "posts/the-interplay-of-genetic-and-cultural-evolution.html",
    "href": "posts/the-interplay-of-genetic-and-cultural-evolution.html",
    "title": "The interplay of genetic and cultural evolution",
    "section": "",
    "text": "In my last post, I discussed the framework for cultural evolution laid out by Claire El Mouden and colleagues in a new article in the Journal of Evolutionary Biology (ungated pdf and supporting information). By setting out clear definitions for the analysis of cultural evolution, such as cultural relatedness and fitness, a workable framework using evolutionary biology’s Price equation can be developed.\nAs I noted in that post, it is when the biological and genetic frameworks are laid on top of each other, as is the focus of the dual-inheritance literature, that things get interesting. With their framework, El Mouden and friends tackle a couple of the prominent gene-culture evolution questions.\nThe first question is to what extent cultural evolution increases or decreases genetic fitness. The authors note that a theme of the gene-culture evolution literature is that cultural evolution made the scale and distribution of today’s human population possible. How does this work?\nFor there to be a positive correlation between cultural and genetic fitness, those who have the most cultural influence must also leave the most offspring. It is easy to see circumstances where this holds, with those of high social status tending to have both cultural influence and more offspring (look at the number of partners of rock stars).\nAn example to the contrary is low fertility in developed countries. El Mouden and colleagues reference work by Richerson and Boyd, who suggest that high status is given to professions with high investment in education, which would allow the behaviour to spread despite the negative relationship between education and fertility.\nHowever, El Mouden and friends show that this mismatch between cultural and genetic fitness is evolutionarily unstable as genetic natural selection acts to align genetic and cultural fitness. They suggest two reasons for this, one involving transmission and the other selection.\nTheir transmission explanation relies on the already evolved propensity for people to avoid behaviours that are harmful to their genetic interests. For example, the nausea produced when consuming toxins would fight against any cultural pressure to eat toxic food. However, it is an interesting question as to how general this transmission effect would be, as many cultural forces are novel and without precedent in our evolutionary history. Transmission may be unlikely to constrain our desire for high status professions that require large investments in education any time soon.\nThe genetic selection explanation would seem to have broader power. In the supplementary information, the authors ask us to consider a population where cultural and genetic fitness were not aligned. Now, imagine a mutant in that population that causes people to pay attention to a cultural trait that is more highly correlated with genetic fitness. As these mutants have higher genetic fitness, they increase in proportion of the population, and cultural and genetic fitness are now more correlated. Cultural fitness now promotes genetic fitness. In the long-run, the two will be perfectly correlated (the exception being where cultural traits are neutral to genetic fitness).\nThe catch in that last sentence is the “long-run”. As cultural evolution can be so much faster than genetic evolution, systems can be far from genetic equilibrium until the genetic response evolves. Fertility in developed countries would be an example of this. There may also be some constraints that prevent perfect alignment, such as the presence of appropriate learning mechanisms.\nThis interaction of genetic and cultural evolution gets most interesting is when we turn to the evolution of altruism. In examining this question we must remember that genetic and cultural fitness are distinct. Cultural altruism reduces the altruist’s cultural fitness; that is, their influence. As a result, the claim that cultural evolution increases genetic altruism (the more common claim in the gene-culture evolution literature) needs to be made carefully.\nAs an illustration, consider this interesting example from the paper. A stranger is being attacked, so a good Samaritan steps in to defend them and dies as a result. Whether this is culturally altruistic would depend on whether the Samaritan’s deed was copied. If so, then the Samaritan’s act would actually have been increase cultural fitness as it would have increased their influence in respect of that cultural trait.\nConversely, their death is genetically altruistic. As a result, genetic selection would tend to act against it. Those who ignore this cultural trait will have higher genetic fitness, grow in proportion of the population and eventually bring cultural and genetic fitness into alignment.\nSo what of behaviours that are both culturally and genetically altruistic? Whether the behaviour spreads will depend on the degree of cultural and genetic relatedness.\nEvidence suggests that cultural relatedness within ethnic groups is higher than genetic relatedness (although it is still not high in absolute terms, with more within group than between group variation). This means that there are a wider range of circumstances for which cultural altruism can emerge than for genetic altruism. However, that domain in which cultural but not genetic altruism is likely to emerge will be subject to the forces described above to align cultural and genetic fitness.\nAnother important point is that each cultural trait should be considered separately. Even though a group may have the same language, giving them high relatedness for this cultural trait, this does not mean that they have the same views on giving their lives for strangers, for which they may have low cultural relatedness. Consideration of the conditions for altruism need to consider the specific cultural trait.\nThere are many other interesting points in the article - I recommend reading the whole thing - but I will close with a point on the practicality of modelling cultural evolution in this way. El Mouden and friends note that there is a host of complications not present in the genetic case. Cultural relatedness can vary wildly across cultural traits, whereas the nature of genetic transmission means that relatedness is similar across most of the genome. Recognising the pattern of inheritance is also a challenge, as ancestor numbers can vary in number and be of vastly different biological ages. In that context, there is no such thing as a standard length of generation.\nSo although this paper presents a nice approach to cultural evolution, it does not present an approach that is easily applied to empirical observation. However, given the lack of clarity across much of the gene-culture evolution literature, particularly when examined across authors and papers, it is nice to see an attempt to achieve some conceptual coherence."
  },
  {
    "objectID": "posts/the-intelligent-inheriting-the-earth.html",
    "href": "posts/the-intelligent-inheriting-the-earth.html",
    "title": "The intelligent inheriting the earth",
    "section": "",
    "text": "From a working paper by Garett Jones released earlier this year:\n\nSocial science research has shown that intelligence is positively correlated with patience and frugality, while growth theory predicts that more patient countries will save more. This implies that if nations differ in national average IQ, countries with higher average cognitive skills will tend to hold a greater share of the world’s tradable assets. I provide empirical evidence that in today’s world, countries whose residents currently have the highest average IQs have higher savings rates, higher ratios of net foreign assets to GDP, and higher ratios of U.S. Treasuries to GDP. These nations tend to be in East Asia and its offshoots. The relationship between national average IQ and net foreign assets has strengthened since the end of Bretton Woods.\n\nJones notes that as capital flows become increasingly free, the opportunity for high IQ people to increase their holdings of assets will also increase. East Asian populations, with the world’s highest average IQs, will come to hold a greater proportion of the world’s financial assets than they now do.\nJones argues that policies that lift cognitive skills should be implemented through nutrition, education or immigration policies. This will in turn increase the level of financial assets of the population as they increase their average saving rate. Plus, as Jones has noted in other papers, increased IQ has benefits beyond savings rates.\nOne interesting element of the paper is the title - Will the intelligent inherit the earth? IQ and time preference in the global economy - which I have paraphrased for the title of this post. While intelligence and reproductive success have an ambiguous relationship in modern settings, lower fertility for the high savers would result in them holding an increasing proportion of the world’s assets, while simultaneously forming a smaller proportion of the population. The intelligent may own the earth, but they might be vastly outnumbered."
  },
  {
    "objectID": "posts/the-human-factor-in-accidents.html",
    "href": "posts/the-human-factor-in-accidents.html",
    "title": "The human factor in accidents",
    "section": "",
    "text": "The below passage is from a neat article on how mistakes can save lives.\n\nCRM [crew resource management] as born of a realisation that in the late 20th century the most frequent cause of crashes wasn’t technical failure, but human error. Its roots go back to the Second World War, when the US army assigned a psychologist called Alphonse Chapanis to investigate a curious phenomenon. B-17 bombers kept crashing on to the runway on landing, even though there were no apparent mechanical problem with the planes. Rather than blaming the pilots, Chapanis pointed to the instrument panel. The lever to control the landing gear and the lever that operated the flaps were next to each other. Pilots, weary after long flights, were confusing the two, retracting the wheels and causing the crash. Chapanis suggested attaching a wheel to the handle of the landing lever and a triangle to the flaps lever, making each easily distinguishable by touch alone. Problem solved.\nChapanis had recognised that human beings’ propensity to make mistakes when they are tired is much harder to fix than the design of levers. His deeper insight was that people have limits, and many of their mistakes are predictable effects of those limits. That is why the architects of CRM defined its aim as the reduction of human error, rather than pilot error. Rather than trying to hire or train perfect pilots, it is better to design systems that minimise or mitigate inevitable human mistakes.\nIn the 1990s, a cognitive psychologist called James Reason turned this principle into a theory of how accidents happen in large organisations. When a space shuttle crashes or an oil tanker leaks, our instinct is to look for a single, “root” cause. This often leads us to the operator: the person who triggered the disaster by pulling the wrong lever or entering the wrong line of code. But the operator is at the end of a long chain of decisions, some of them taken that day, some taken long in the past, all contributing to the accident; like achievements, accidents are a team effort. Reason proposed a “Swiss cheese” model: accidents happen when a concatenation of factors occurs in unpredictable ways, like the holes in a block of cheese lining up.\nJames Reason’s underlying message was that because human beings are fallible and will always make operational mistakes, it is the responsibility of managers to ensure that those mistakes are anticipated, planned for and learned from. Without seeking to do away altogether with the notion of culpability, he shifted the emphasis from the flaws of individuals to flaws in organisation, from the person to the environment, and from blame to learning.\nThe science of “human factors” now permeates the aviation industry. It includes a sophisticated understanding of the kinds of mistakes that even experts make under stress.\n\nI recommend reading the full article. Among other things, it has a lot of interesting material about mistakes in medical settings."
  },
  {
    "objectID": "posts/the-heritability-debate-again.html",
    "href": "posts/the-heritability-debate-again.html",
    "title": "The heritability debate, again",
    "section": "",
    "text": "Like the level of selection debate, the debate about what heritability means has a life of its own. The latest shot comes from Scott Barry Kaufman who argues (among other things) that:\n\nThe heritability of a trait can vary from 0.00 to 1.00, depending on the environments from which research participants are sampled. Because we know that genes play some role in the development of any trait, the precise heritability estimate doesn’t matter in a practical sense.\nHeritability depends on the amount of variability in the environmental factors that contribute to a trait. The problem is that our understanding of the factors that contribute to the development of human traits in general – and to IQ in particular – is currently so deficient that we typically do not know if the environmental factors important in the development of a particular trait are stable across testing situations, vary somewhat across those situations, or vary wildly across those situations.\n\nIn his conclusion he states:\n\nAt the very least, heritability tells us how much of the variation in IQ can be accounted for by variation in genetic factors when development occurs in an exquisitely specific range of environments. However, David S. Moore has argued that even this is not significant when we realize that the magnitude of any heritability statistic reflects the extent of variation in unidentified non-genetic factors that contribute to the development of the trait in question.\n\n(HT: Bryan Caplan)\nThrough his post, Kaufman constructs a series of paper tigers, tears them down and implies that because the extreme case does not hold, we should be wary of heritability estimates. I did not find much to disagree with in his examples, but the I differed on the conclusions we should draw.\nSo, where I do not agree - first, the heritability estimate does matter. While I don’t think it is hugely important whether the heritability of IQ in a specific sample is 0.5 or 0.6, it is important whether the measured heritability is 0 or 0.6. As Caplan notes in his post:\n\nMy money says, for example, that the average adult IQ heritability estimate published in 2020 will exceed .5.\n\nI think that Caplan is right (although I might have stated some conditions about the relevant sample), and Kaufman’s argument overstates how finely tuned the environment needs to be to get a meaningful heritability estimate. Heritability estimates of a sample of children growing up in extreme poverty might be much lower (or zero) but as is found again and again, once the basic requirements of a child are met, heritability estimates for IQ are consistently above 0.4. We can construct arguments that in each study there are different gene-environment interactions and so on, but if genes weren’t important in variation in IQ and the gene-environment interactions weren’t consistent to some degree, why would such consistent heritability results (and correlation between parent and child IQ) be found?\nFurther, these results matter. They suggest that poverty is affecting the IQ of some children, and policies could be tailored to cut this disadvantage. For children not subject to deficient environments, the high heritability of IQ should influence policies such as those for education. Children are different and the education system should take this into account.\nImplicit in Kaufman’s post was the “its all too complex” argument.  Social and biological sciences are complex (which is why I find them interesting). However, if we fully accepted Kaufman’s argument that “our understanding of the factors that contribute to the development of human traits … is currently so deficient that we typically do not know if the environmental factors important in the development of a particular trait are stable across testing situations”, it would put into question most of the data analysis in economics, sociology and biology. Econometrics operates on the idea of all other things being equal.\nFortunately, Kaufman has not taken the Gladwell-esque approach of suggesting that we forget about genetic factors. Kaufman suggests further research into how nature and nurture are intertwined. If it is all too complex, we should start unwinding the complexity. However, I believe that, in the meantime, this complexity does not mean that we should throw out all the results that have previously been obtained."
  },
  {
    "objectID": "posts/the-genetic-basis-of-social-mobility.html",
    "href": "posts/the-genetic-basis-of-social-mobility.html",
    "title": "The genetic basis of social mobility",
    "section": "",
    "text": "In 2007’s A Farewell to Alms: A Brief Economic History of the World, Gregory Clark argued that the higher fertility of the rich in pre-industrial England sowed the seeds for the Industrial Revolution. As children resemble their parents, the increased number of prudent, productive people made possible the modern economic era.\nPart of the controversy underlying Clark’s argument – made stark by Clark in articles and speeches following A Farewell to Alm’s publication - was that he considered there may be a genetic basis to the transmitted traits. The higher fertility of the rich and changing character of the population was natural selection at work.\nClark’s new book The Son Also Rises: Surnames and the History of Social Mobility, also makes a new and unique argument. And like A Farewell to Alms, there is a genetic underlay.\nClark’s primary argument is that across a range of societies and eras – from pre-Industrial to modern England, from pre- to post-revolution China, and across the centuries in the United States, Sweden and India – social mobility is low. The correlation in social status between one generation and the next is around 0.7 to 0.8, meaning we can find the echoes of high status 10 or more generations later. Status does “regress to the mean”, but it does so slowly.\nTo put this in context, Norman surnames are overrepresented at Cambridge and Oxford today by around 25 per cent, 950 years after the Norman conquest. The descendants of the samurai, who lost any legal privileges in 1871, are today several times overrepresented in high status occupations in Japan. The eighteenth century elite of egalitarian Sweden are still a privileged group.\nWhat makes Clark’s finding particularly striking is that most studies of intergenerational mobility have found lower numbers – often an intergenerational correlation of around 0.2 to 0.4. A correlation of that order would erase the traces of social status in a few generations.\nClark suggests one reason for his finding of lower social mobility is that previous studies measured noise. Suppose status is a result of two components - a fixed factor transmitted between generations, and a part determined by luck. Measuring across a single generation, the luck disguises the underlying correlation. If measured across multiple generations, bad lack in one generation will typically be followed by reversion to the underlying status the next. Status across multiple generations provides a better measure of the persistence of social status and of the effect of the fixed factor transmitted between them.\nClark directly relates this point to the biological concepts of genotype and phenotype. The underlying base social status is the genotype. If you observe someone’s external characteristics, you observe the phenotype, which reflects genotype plus some degree of noise. While this might be seen as an analogy, Clark argues that a genetic explanation is the best explanation of what he observes.\nA second reason for the difference between Clark’s results and the previous studies is that most studies of intergenerational transmission of status focus on a single measure of status such as correlation in income between parents and their children. But people often trade off one type of status for another. A political leader or leading academic typically receives income that would barely scrape them into the top one per cent. People take lower pay for more prestigious or interesting jobs. Looking a single measure of status will overestimate the change in status as it cannot capture the trade-offs across different domains.\nSo how does Clark avoid this problem? Clark’s trick is to use rare surnames, and treat them as large families. By finding rare surnames with high or low status, Clark and a range of colleagues tracked the average status of these surnames across the generations through measures such as relative representation in legal practitioner and medical rolls or lists of the wealthy.\nAs a rare surname comprises many people, the noise and trade-offs across different types of status is averaged out across the “family”. Surname families have an underlying status genotype that can be tracked more faithfully that the individual phenotypes observed in typical studies.\nThe body of research Clark presents through the book is impressive, although it is not always the most exhilarating reading. As he works through one society to the next, looking at various measures of status, the story is usually the same. Status is persistent and surprisingly similar across times, countries and different measures of status. One interesting finding presented by Clark is over-representation of Norman surnames in the military – and this is not a finding that can be explained by persistence in status. Clark suggests that 10 generations after the Norman conquest, the descendants of the Norman conquerors still had a taste and facility for organised violence.\nClark’s results suggest that while 100 years of Swedish social democracy may have created a more economically equal society, it is no more mobile. The Scientific Revolution, Enlightenment and Industrial Revolution did little to change social mobility in England, and the persistence of status has been almost unaffected by massive changes in inheritance tax. China’s Cultural Revolution had little effect. And across all these countries, government interventions from universal education to progressive taxation have failed to budge social mobility. As Clark states:\n\nEvents that at the time seem crucial, powerful and critical determinants of the fate of societies leave astonishingly little imprint in the objective records of social mobility rates.\n\nThat social mobility is low but still occurring is a combination of some interesting factors. First, Clark argues that low mobility is affected by assortative mating. If people mate with people of similar status, their children will better reflect their status. However, assortative mating is not perfect. People do not precisely assort by status. And even if they did, observed status (phenotype) does not perfectly reflect the underlying genotype. Both of these factors mean that high or low status people will tend to partner with people closer to the mean, meaning that their children will similarly be closer to the mean.\nOne way to preserve a group’s status is to have marital endogamy – people only marry within the group – preventing mistakes in marrying low status genotypes that had good luck. This is observed is the highest status castes in India. High status individuals within that group will regress to the mean of that group, but it is regression to a higher mean than that of the rest of the population. The corollary of this point is that to maximise social mobility, you would encourage marriage across groups and status levels.\nThe inability to observe your potential partner’s status perfectly suggests that if you want to achieve high status, you should not look not just at your potential partner but also at their relatives. Since luck may have affected your potential partner’s status, you gain more information from the status of their relatives. However, these relatives will not contribute anything to the success of your child – transfers of wealth do not make status more persistent. But the status of relatives provides evidence of the social status underlying your potential mate. There are high rewards to this choice. Through appropriate choice of mates, a lineage can avoid downward mobility forever.\nIn addition to marital endogamy, another apparent exception to regression to the mean is through the loss of low or high status members from high or low status groups respectively. In Ireland, high status Catholics are more likely to change to become Protestant than Catholics of low status, and low status Protestants tend to drift the other way. The expected regression to the mean of these groups does not occur, although if you track Irish surnames, there is still the typical low level of social mobility. Gypsies likely maintain their low status through similar dynamics.\nOne of Clark’s obvious in hindsight findings is that the social mobility he describes works both ways. In the same way that regression to the mean is slow, the path to the top or bottom follows a similar process. Increases in status are largely driven by random shuffling of genes and good luck in marrying people of better genotype than phenotype. Rags to riches stories and vice versa are rare for whole groups of surnames. The super rich tend to be children of moderately rich, and the poorest children are the children of the moderately poor. This does not mean that you can predict which families will rise to the top - but you can predict the long and slow process.\nSo why does Clark feel that genetics must be involved, and not transmission of resources or other advantages that accrue to those with high status? As a start, the genetic story is consistent with the mountain of twin and adoption studies that demonstrate a strong role for genetics and a limited role for family environment.\nOne of his more interesting arguments is that genetics is required to explain regression to the mean. In modern societies, high status people typically have lower fertility and are able to make much larger investments in each child. If this transfer to children mattered, status should persist or these groups should move even further above the mean.\nA weakness with A Farewell to Alms was that Clark did not seem ready to bite the population genetic bullet. The tools of population genetics could have helped Clark nail his points and put to bed many of the criticisms that were made of his work. Since then Clark has spent a lot more time in the company of geneticists, and this is reflected in his arguments. His use of genetic data in interpreting the social mobility in Ashkenazi Jews helped his argument cut through. He still does not use population genetic tools in the way he could, but it seems he is much more prepared to fight on the genetic front.\nSo what do Clark’s arguments mean for how we should think about inequality or social mobility? Clark points out that a genetic basis to social mobility means that people do not achieve what they do because of family background. Instead, it is their ability, their propensity to work hard, and their resilience to failure that leads to success. We can predict success based on family background, but family background is not the cause.\nClark suggests (and I agree) that world is actually fairer than we believe if there is a genetic basis to social outcomes. Large investments by the upper class are doomed to failure. Do not worry that you cannot afford that expensive preparatory school for your kids. People still need to struggle and put in effort to succeed. Genetics just suggests which people will be most likely to struggle and invest that effort.\nAlso on the optimistic front, the lower fertility of high status people means that social mobility in the modern world is predominantly upward. Groups tend to move up to fill the space at the top – which is the opposite of the dynamic that existed in pre-Industrial England. (Although I am not convinced that lower fertility of the rich is either a general or a long-term dynamic)\nHaving slashed through the idea that government policy might promote social mobility, Clark is still relatively progressive in his policy recommendations. As he states, why do we want to multiply the awards to the genetic lottery winners? He prefers a Nordic model where, even though social mobility is low, the gap between those of high and low status is not as large. Clark argues the persistence of status, despite the range of taxation and other measures people have been subject to, suggests reward is not required to stimulate achievement.\nI disagree with Clark on this point. Reward is important, but the reward just happens to be status itself. A world with no difference in economic outcomes as opposed to reduced difference could see marked changes in effort. Clark also spends little time discussing the other trade-offs involved in the Nordic model, such as the effect on overall wealth.\nFinally, Clark does not ask whether the Nordic countries have lower underlying (genetic) variation in their status. It may be that the Nordic institutions reflect the characteristics of the population, rather than being the cause."
  },
  {
    "objectID": "posts/the-genetic-and-social-lottery.html",
    "href": "posts/the-genetic-and-social-lottery.html",
    "title": "The genetic and social lottery",
    "section": "",
    "text": "As opportunity is equalised, more of the variation in outcomes between people will be due to genetic factors. This may have the somewhat ironic result of reducing social mobility.\nI generally take the view that assortment by genetic lottery is no more fair than assortment based on social factors such as being born to low socio-economic status parents. An alternative view is given by Garret Hardin in his famous article The Tragedy of the Commons, when he writes of the private property solution to the tragedy:\n\nWith real  estate and other material goods, the alternative we have chosen is the institution of private property coupled with legal inheritance. Is this system perfectly just? As a genetically trained biologist I deny that it is. It seems  to me that, if there are  to be differences in individual inheritance, legal possession should be perfectly correlated with biological inheritance-that those who are biologically more fit  to be the custodians of property and power should legally inherit more. But genetic recombination continually makes a mockery of the doctrine of “like father, like son” implicit in our laws of legal inheritance. An idiot can inherit millions, and a trust fund can keep his estate intact. We must admit that our legal system of private property plus inheritance is unjust-but we put up with it because we are not convinced, at the moment, that anyone has invented a better system. The alternative of the commons is  too horrifying to contemplate. Injustice is preferable to total ruin.\n\nHardin’s observation that genetic recombination can result in an idiot inheriting millions does not bother me, as the luck involved in everyday existence can have the same effect. While the correlation between income and genetic factors is strong, there is plenty of luck involved, particularly at the high-end of the tail.\nHowever, his proposed alternative, inheritance by the biologically fit, is more problematic. How would we determine who is biologically fit? In the future, the ability to assess traits through genetic testing of the foetus may allow inheritance scales to be set. In the meantime, the correlation between parental and child traits are likely to offer a reasonable match, particularly with the degree of assortive mating that occurs. Further, the accumulation of capital itself is a better test than could otherwise be developed. Why test for the ability to be productive in an economy when you can simply observe it, with the result the reward?\nMore importantly, what of the incentive effects? Do you want the biologically fit to be given their “winnings” as an inheritance, or would you prefer that they have to work hard to receive it - as they would do if they received no inheritance? The value to society of the biologically fit comes from what they do with their endowed traits. Further, does their value come from prudent use of the capital they hold or from the hard work they undertake to amass it? I tend towards the latter.\n\nAs a footnote, Hardin’s use of the term biologically fit is somewhat different to how the term might be used in biology (ability to survive and reproduce) - given Hardin’s views on population control, those who are biologically fit to hold property are probably not biologically fit in the strict sense at all."
  },
  {
    "objectID": "posts/the-gender-gap.html",
    "href": "posts/the-gender-gap.html",
    "title": "The gender gap",
    "section": "",
    "text": "This month’s Cato Unbound has another interesting subject, this time on the decline of men. In the lead essay, Kay Hymowitz runs through the mass of ways men are starting to fall behind women. Many of the statistics were a surprise to me. Take the following:\n\nIn an analysis of recent census data, Reach Advisors found that childless twentysomething men now earn 8% less than their female counterparts in 147 out of 150 of American cities. That’s despite the fact that college-going women major in subjects that tend to lead to lower paying jobs. Young single men are less likely to own a home than women. While on average men continue to earn more than women, their wages, unlike those of women, have stalled.\n\nIn her response essay, Jessica Bennett counters that women are still behind in many areas:\n\nBy the time women enter college, studies show they’ll have given up many of their leadership roles. The rise of the knowledge economy may have multiplied opportunities in other fields (Hymowitz sites public relations, graphic design, and management). But women will still make up just a third of business-school students and barely a quarter of law firm partners. …\nWomen still have trouble penetrating the highest rungs of the corporate world: they are also just 3 percent of Fortune 500 CEOs, less than a quarter of politicians, and just 22 percent of the leadership positions in journalism.\n\nBetween the two essays, there is a thread that young women are overtaking men in many areas, while not yet penetrating the top.\nBryan Caplan notes one potential explanation, namely that women seem to have less variance in traits than men in many dimensions:\n\nBut isn’t the obvious explanation just that men have higher variance in general?  This is easiest to prove for cognitive ability - see Garett Jones’ review of the evidence.  But it also seems very plausible for interests and obsessiveness.  Anyone can start a blog, but men are much more likely to do so.  The reason, I’ll warrant, is that the male distribution of ego has a right tail that stretches far into the horizon.&lt;\n\nFrom an evolutionary perspective, the higher variance for men makes sense. In a winner takes all competition for women, men at the top can have many children. Meanwhile, women are constrained in the maximum of number of children they can have. The jackpot of being the best male far outweighs the cost of being the worst. Genghis Khan was so reproductively successful (as was his grandson Kublai Khan) that one in 200 men globally are direct descendants along the male line and hence carry his y-chromosome. Such success would be impossible for a woman. Being a rich, successful male could buy additional access to mates and reproductive success that being a rich, successful woman does not. The evolutionary incentives are stacked in favour of the man seeking those top positions.\nAn evolutionary perspective also assists in the following, in which Hymowitz asks whether the loss of the historical male role as provider has played a part in the “decline of men”:\n\nConsider another recent study by S. Alexandra Burt at Michigan State University. Burt followed 289 pairs of male twins for 12 years, between the ages of 17 and 29. More than half of the twins were identical. She found that men who had shown less antisocial behavior as adolescents were more likely to marry as they got older, which argues for self-selection. But she also found that a married twin had fewer antisocial behaviors—aggression, irritability, financial irresponsibility, and criminal involvement—than his unmarried brother. This suggests there is some truth to the very unfashionable idea that marriage helps to discipline men.\n\nLess anti-social men have a higher probability of getting married - they are better at providing something women want. In the case of the anti-social, unmarried brother, their risk taking activity reflects that, having failed to initially get a mate, they move to progressively riskier activity until they succeed, die or are jailed, or they somehow survive as their testosterone fades into old age. Marriage helps discipline the man by providing the man with his objective.\nUltimately, I wonder how long the trend that triggered the essays will last. Many women, as they have for the last 40 years, are choosing to remain childless. The genes of these women, and any inherent traits that result in a predisposition to remain childless, disappear from the gene pool. The women that form future generations are the children of mothers who chose to take time out to have children despite the growing options for their own careers. As Hymowitz notes:\n\nThe point is that today, with the important exception of the technical and financial sector, younger women (that is, childless women, an important caveat) have shown they can easily be men’s equals, and possibly even their superiors, in the knowledge economy.\n\nThat important caveat will be applied more and more in the future. In that case, some of the gap will remain. (And as an aside, I recommend that you read all of Hymowitz’s essay - there are a lot of interesting ideas in it)\nUpdate: A follow-up post is here."
  },
  {
    "objectID": "posts/the-evolutionary-foundations-of-economics.html",
    "href": "posts/the-evolutionary-foundations-of-economics.html",
    "title": "The Evolutionary Foundations of Economics",
    "section": "",
    "text": "I released this working paper a few months ago, but neglected to blog about it - I’ve written (with my supervisors) a review of the literature incorporating evolutionary theory into economics. The abstract:\n\nThe Evolutionary Foundations of Economics\nAs human traits and preferences were shaped by natural selection, there is substantial potential for the use of evolutionary biology in economic analysis. In this paper, we review the extent to which evolutionary theory has been incorporated into economic research. We examine work in four areas: the evolution of preferences, the molecular genetic basis of economic traits, the interaction of evolutionary and economic dynamics, and the genetic foundations of economic development. These fields comprise a thriving body of research, but have significant scope of further investigation. In particular, the growing accessibility of low cost molecular data will create more opportunities for research on the relationship between molecular genetic information and economic traits.\n\nThe paper is fairly flat in tone as I wrote it as the introductory review chapter for my thesis. If you’re familiar with the blog, you will have read some more critical pieces on the papers covered in my article before. Links to some of those critiques can be found down the bottom of my evolutionary biology and economics reading list page.\nAnd the disclaimer - this paper isn’t about “evolutionary economics” in the way that term is typically used. I’m interested in the biological angle:\n\nThe subject matter of this paper needs to be distinguished from what is commonly called “evolutionary economics”. Evolutionary economics uses biological concepts, such as natural selection, and applies them to the dynamics of firms, business processes and institutions. The economy is seen as a complex adaptive system in which innovation and change are central considerations. The origin of evolutionary economics is often traced to Veblen (1898), and was revived by Alchian (1950) and later Nelson and Winter (1982), whose seminal work inspired a vast literature. The subject matter of this paper differs from evolutionary economics in that we focus on human biology rather than seeking to apply a biological analogy to higher levels such as firms. This paper is about the application of evolutionary biology to economic processes at the level of humans and their genes and their interactions at the population level."
  },
  {
    "objectID": "posts/the-evolution-of-happiness.html",
    "href": "posts/the-evolution-of-happiness.html",
    "title": "The evolution of happiness",
    "section": "",
    "text": "When we experience positive events, we feel happy. But happiness adjusts, with the effects of a positive event normally short-lived. Over the long-term, happiness tends to float around a stable mean. Happiness is also strongly related to our position relative to our peers. How happy we are with our income depends on everyone else’s income.\nIn line with the first law of behavioural genetics, it is worth looking for an evolutionary foundation to this pattern. How does happiness motivate us to do things in our evolutionary interest?\nEvolution did not shape our happiness to simply increase or decrease in line with how events affect our fitness. We need a more nuanced explanation, which Luis Rayo and Gary Becker offered in two papers published in 2007 by asking how our ability to feel happiness would be affected if it is constrained. The long form of their argument is contained in the Journal of Political Economy, with a shorter version published in the American Economic Review Proceedings and Papers.\nRayo and Becker propose two potential constraints. First, there are limits to a person’s sensitivity to happiness. A person can only determine which of two alternative choices they should make if there is more than a certain size difference in happiness for the two choices. Second, there is a bound on the range of happiness that a person can experience (say, limits to nervous system signal strength).\nTo overcome limits to a person’s sensitivity, evolution could amplify the happiness response to make sure that we knew which of two choices made us happy. But if there is a limit as to how happy we can feel, this solution will not always work. Combining the two constraints, the strength of the happiness signal should be strong where it matters most - over the current relative decisions.\nRayo and Becker relate a couple of cases that are similar. If you move from the sunlight into a dark room, you initially can’t see anything, but your eyes adjust until you can distinguish between the relative shades. Another example comes from Arthur Robson, who likens happiness to a voltmeter. When you are about to measure an electric current, you must first set the voltmeter to the range in which you want to measure the current. If you set the range too high, the meter will barely move. If you set it too low, the reading will go instantly to the maximum value. The voltmeter must first be calibrated to the problem at hand.\nTo formalise this idea, Rayo and Becker developed several “happiness functions”. In one function, the agents first compare their income against their peers to determine their current social position. They then contrast their current social position against their relative social position in the last period. An advance in social position leads to happiness but it is only short-lived.\nUnder this function  a general increase in income across society does not increase happiness (consistent with the Easterlin paradox), and happiness will tend to revert to a mean. However, given recent arguments that the Easterlin paradox is an artefact of having happiness measured on a bounded scale, Rayo and Becker’s argument may need to more finely tuned.\nThis happiness function is also consistent with the positive correlation between income and happiness sometimes observed in cross-section data. People are subject to random shocks and those who have higher income are more likely to have received a recent positive shock.\nOne thing I didn’t enjoy about the Journal of Political Economy article is that it follows a tradition in much work on the evolution of economic preferences by using a metaphorical principal-agent approach to the analysis. The principal is nature, while the agent is the individual being shaped by evolution.  I’ve never been a fan of this approach, which is generally not adopted in evolutionary biology. It lessens the accessibility of what is often already hard to access work, and I am not convinced that any pay-off from the additional complication is worth it. I’ll post some longer thoughts on this soon."
  },
  {
    "objectID": "posts/the-evolution-of-conscientiousness.html",
    "href": "posts/the-evolution-of-conscientiousness.html",
    "title": "The evolution of conscientiousness",
    "section": "",
    "text": "For most of Geoffrey Miller’s Spent: Sex, Evolution, and Consumer Behavior, Miller treats the genetic influences on human preferences as relatively static over human history. However, in his discussion of the big-five personality trait of conscientiousness, Miller suggests that high conscientiousness was only selected for after the Neolithic Revolution:\n\nIn several respects, conscientiousness is an unusual personality trait. Because hunter-gatherer life did not require as much planning and memory for debts and duties as life in larger-scale societies with more complex divisions of labor, conscientiousness may have evolved to higher average levels only recently, and perhaps to a greater degree in some populations than others. Only with the rise of activities like agriculture and animal herding would our ancestors have needed the sort of anxious obsessiveness and future-mindedness that characterize the highly conscientious. Only in the past ten thousand years did our ancestors prosper by continually asking themselves: Have I plowed enough yet? Did I sow the seeds early enough? Is one of the lambs missing? Did my cousin pay me for those olives? Am I teaching my children the skills they will need in twenty years? Thus were born the sleepless predawn ruminations of the middle-aged conscientious.\n\nEconomically, conscientiousness is a positive trait (both individually and in the aggregate). While I would argue that other traits would have also faced selection pressures since the Neolithic Revolution, it is probably easier to build a case for conscientiousness evolving in an economically useful manner than for the other big-five traits.\nMiller couched most of his discussion in Spent in terms of the big-five (openness, conscientiousness, agreeableness, stability and extraversion) plus general intelligence (the central six as he calls them). Rob Brooks did similarly in Sex, Genes & Rock ‘n’ Roll. It’s probably not a bad habit to get into, as Miller suggests most of the studies about the heritability of preferences are simply reflections of the central-six. In their favour, the central-six are near statistically independent, apart from a slight correlation between openness and intelligence, and have survived in various forms through several decades of psychological research.\nAs for the trait I often talk about, time preference or patience, I expect that this is a combination of intelligence and conscientiousness."
  },
  {
    "objectID": "posts/the-eugenics-of-contraception.html",
    "href": "posts/the-eugenics-of-contraception.html",
    "title": "The eugenics of contraception",
    "section": "",
    "text": "After copping some criticism for his comments on the coverage of female contraception in health insurance, Steven Landsburg has noted that some arguments in its favour may have merit. Two of the more interesting he notes are as follows:\n\nWe might not want to discourage parenthood in general, but surely we want to discourage parenthood by the sort of woman who won’t use contraception unless it’s subsidised. Ideally we’d tax childbirth among that class of women, but since they’re hard to identify, the best available policy is to subsidize contraception.\nWe might not want to discourage parenthood in general, but surely we want to discourage unwanted parenthood, because unwanted babies are far less socially valuable than wanted babies.\n\nOn the first argument, Landsburg is not convinced that the primary group who will change their behaviour are the poor or the dumb, or that we do not want them reproducing. However, he notes the second, which in some ways reflects the Levitt-Donohue abortion-crime theory, is an argument worth taking seriously. The two arguments are quite similar, however, as while Levitt and Donohue explained the abortion-crime effect in terms of unwanted parenthood, the theory is often quoted in the context of what type of people had the abortions.\nHowever, what struck me about these arguments is that they are representative of an increasing tendency for discussions about tax or family policy, contraception and immigration to refer to the effects on the composition of the next generation. If government started to actively consider factors such as these, I can only imagine the unintended consequences."
  },
  {
    "objectID": "posts/the-effect-is-too-large-heuristic.html",
    "href": "posts/the-effect-is-too-large-heuristic.html",
    "title": "The “effect is too large” heuristic",
    "section": "",
    "text": "Daniel Lakens writes:\n\nI was listening to a recent Radiolab episode on blame and guilt, where the guest Robert Sapolsky mentioned a famous study [by Danziger and friends] on judges handing out harsher sentences before lunch than after lunch. The idea is that their mental resources deplete over time, and they stop thinking carefully about their decision – until having a bite replenishes their resources. The study is well-known, and often (as in the Radiolab episode) used to argue how limited free will is, and how much of our behavior is caused by influences outside of our own control. I had never read the original paper, so I decided to take a look.\nDuring the podcast, it was mentioned that the percentage of favorable decisions drops from 65% to 0% over the number of cases that are decided upon. This sounded unlikely. I looked at Figure 1 from the paper (below), and I couldn’t believe my eyes. Not only is the drop indeed as large as mentioned – it occurs three times in a row over the course of the day, and after a break, it returns to exactly 65%!\n\n…\nI think we should dismiss this finding, simply because it is impossible. When we interpret how impossibly large the effect size is, anyone with even a modest understanding of psychology should be able to conclude that it is impossible that this data pattern is caused by a psychological mechanism. As psychologists, we shouldn’t teach or cite this finding, nor use it in policy decisions as an example of psychological bias in decision making.\n\nI was aware of one explanation for why the effect reported by Danziger and friends was so large. Andreas Glockner explored what would occur if favourable rulings took longer than unfavourable rulings, and the judge (rationally) plans ahead and stops for their break if they believe the case will take longer than there is time left in the session. Simulating this scenario, Glockner generated an effect of similar magnitude to the original paper.\nHowever, I was never convinced the case ordering was random, a core assumption behind Danziger and friends’ finding. In my brief legal career I often attended preliminary court hearings where matters were listed in a long (possibly random) court list. Then the order emerged. Those with legal representation would go first. Senior lawyers would get priority over junior lawyers. Matters for immediate adjournment would be early. And so on. There was no formal procedure for this to occur other than discussion with the court orderly before and during the session.\nIt turns out that these Israeli judges (or, I should say, a panel of a judge, a criminologist and a social worker) experienced a similar dynamic. Lakens points to a PNAS paper in which Keren Weinshall-Margela (of the Israeli Supreme Courts research division) and John Shapard investigated whether the ordering of cases was actually random. The answer was no:\n\nWe examined data provided by the authors and obtained additional data from 12 hearing days (n = 227 decisions). We also interviewed three attorneys, a parole panel judge, and five personnel at Israeli Prison Services and Court Management, learning that case ordering is not random and that several factors contribute to the downward trend in prisoner success between meal breaks. The most important is that the board tries to complete all cases from one prison before it takes a break and to start with another prison after the break. Within each session, unrepresented prisoners usually go last and are less likely to be granted parole than prisoners with attorneys.\n\nDanziger and friends have responded to these claims and attempted to resuscitate their article, but here is something to be said for the “effect is too large” heuristic proposed by Lakens. No amount of back and forth about the finer details of the methodology can avoid that point.\nThe famous story about the effect of defaults on organ donation provides another example. When I first heard the claim that 99.98% of Austrians, but only 12% of Germans, are organ donors due to the default organ donation option in their driver licence renewal, I simply thought the size of the effect was unrealistic. Do only 2 in 10,000 Austrians tick the box? I would assume more that 2 in 10,000 would tick it by mistake, thinking that would make them organ donors. So when you turn to the original paper or examine the actual organ donation process you will see this has nothing to do with driver’s licences or ticking boxes. The claimed effect size and the story simply did not line up.\nAndrew Gelman often makes a similar point. Much research in the social sciences reflects an attempt to find tiny effects in noisy data, and any large effects we find are likely gross overestimates of the true effect (to the extent the effect exists). Gelman and John Carlin call this a Type M error.\nFinally, I intended to include Glockner’s paper in my critical behavioural economics and behavioural science reading list, but it slipped my mind. I have now included it and these other articles for a much richer story."
  },
  {
    "objectID": "posts/the-deep-roots-of-economic-development.html",
    "href": "posts/the-deep-roots-of-economic-development.html",
    "title": "The deep roots of economic development",
    "section": "",
    "text": "I first flagged this article a year or so ago when it was released as a working paper, but the new Journal of Economic Literature paper How Deep Are the Roots of Economic Development (ungated pdf) by Enrico Spolaore and Romain Wacziarg has so much good material in it that it is worth a revisit.\nI am going to cover the article over two posts. This first post covers the major theme of the first half of the paper - that it is populations, not institutions, that underlie persistent differences in economic development. In a second post, I comment on Spolaore and Wacziarg’s analysis of the genetic and cultural intergenerational transmission of development.\nTo start, I will let Spolaore and Wacziarg (or the authors they reference) do most of the talking. First, from Glaeser and colleagues (2004):\n\n[Acemoglu, Johnson, and Robinson’s] results do not establish a role for institutions. Specifically, the Europeans who settled in the New World may have brought with them not so much their institutions, but themselves, that is, their human capital. This theoretical ambiguity is consistent with the empirical evidence as well.\n\nOn Michalopoulos and Papaioannou’s (2010) analysis of institutions in Africa:\n\nMichalopoulos and Papaioannou (2010) find that national institutions have little effect when one looks at the economic performance of homogeneous ethnic groups divided by national borders. …\n\nOverall, their findings suggest that long-term features of populations, rather than institutions in isolation, play a central role in explaining comparative economic success.\nOn Putterman and Weil (2010):\n\nPutterman and Weil’s results strongly suggest that the ultimate drivers of development cannot be fully disembodied from characteristics of human populations. When migrating to the New World, populations brought with them traits that carried the seeds of their economic performance. This stands in contrast to views emphasizing the direct effects of geography or the direct effects of institutions, for both of these characteristics could, in principle, operate irrespective of the population to which they apply. A population’s long familiarity with certain types of institutions, human capital, norms of behavior or more broadly culture seems important to account for comparative development.\n\nFinally, on Easterly and Levine (2012):\n\nEasterly and Levine (2012) confirm and expand upon Putterman and Weil’s finding, showing that a large population of European ancestry confers a strong advantage in development, using new data on European settlement during colonization and its historical determinants. They find that the share of the European population in colonial times has a large and significant impact on income per capita today, even when eliminating Neo-European countries and restricting the sample to countries where the European share is less than 15 percent—that is, in non-settler colonies, with crops and germs associated with bad institutions.\n\nThis angle reflects Greg Clark’s analysis in A Farewell to Alms (largely contained in Chapter 8). Clark asks why economic growth took so long to emerge in England when the important institutional backbone for economic development was established well before 1800. Clark points to changes in the characteristics of the population.\nThese articles also reflect the question I tend to ask about institutional explanations of development. Why do good institutions exist in some places and not others? The above evidence suggests that institutions are largely endogenous to the population.\nIn my next post, I address the intergenerational transmission of traits. And I’ve added Spolaore and Wacziarg’s paper to my evolutionary biology and economics reading list."
  },
  {
    "objectID": "posts/the-decline-in-intelligence.html",
    "href": "posts/the-decline-in-intelligence.html",
    "title": "The decline in intelligence?",
    "section": "",
    "text": "Two papers in which Gerald Crabtree argues that human intelligence has declined since a peak thousands of years ago (Part I and Part II) have been the subject of the popular science media rounds over the last week (such as this piece in The Independent).\nCrabtree’s argument has two components. The first is that intelligence is fragile and vulnerable to genetic load. He estimates that within the last three thousand years, the average person would have accumulated at least two mutations that have harmed our intelligence.\nThe second component is that selection pressure on intelligence eased when humans started to live in supportive societies where failures of judgement are no longer fatal. Adaptations relating to immunity would have been more important. Without this selection pressure, the negative mutations are no longer eliminated from the population.\nI’m relatively sympathetic to the first limb, and the idea that thousands of genes influence intelligence (although I’m not convinced that Crabtree has put together the best case for it). For the second, however, Crabtree overestimates the existence of a social support web over recent millennia and underestimates the need for intelligence to survive in agricultural societies. I am not sure what level of social security existed in medieval Europe, but I expect it was minimal and probably little different from the web of family support received in a hunter-gatherer group. Crabtree appears to be projecting today’s supportive conditions back further than they existed.\nCrabtree anticipates one response to his argument, that intelligence can still be favoured by sexual selection:\n\nIntellectual capacity and emotional stability have mating advantages that would reduce the rate at which mutations affecting these traits become fixed in our genome. This is true, but I fear does not take into account the extreme selection required to maintain traits dependent upon thousands of genes with reduced heritability. A hunter–gatherer who did not correctly conceive a solution to providing food or shelter probably died, along with his/her progeny, whereas a modern Wall Street executive that made a similar conceptual mistake would receive a substantial bonus and be a more attractive mate. Clearly, extreme selection is a thing of the past.\n\nThe problem is that Crabtree does not see sexual selection as an “extreme” selective force, when it is. Consider Wade and Shuster’s estimate that sexual selection accounts for 55 per cent of total selection in homo sapiens. Or take Greg Clark’s data from A Farewell to Alms, with the rich having twice the children of the poor. The link between resources and reproductive success is strong across societies, and assuming a link between resources and intelligence (which if anything appears to be getting stronger), the intelligent have been reaping a reproductive bounty for some time. For those less fortunate, survival without reproduction is still a genetic dead-end.\nAt the opening of the first paper, Crabtree offers a wager on his hypothesis.\n\nI would wager that if an average citizen from Athens of 1000 BC were to appear suddenly among us, he or she would be among the brightest and most intellectually alive of our colleagues and companions, with a good memory, a broad range of ideas, and a clear-sighted view of important issues. Furthermore, I would guess that he or she would be among the most emotionally stable of our friends and colleagues. I would also make this wager for the ancient inhabitants of Africa, Asia, India, or the Americas, of perhaps 2000–6000 years ago.\n\nCrabtree does not seem to realise that this experiment has been run before. Over the last couple of hundred years, members of previously isolated hunter-gatherer tribes have been incorporated into modern societies. Those hunter-gatherers’ lives of day-to-day danger, by Crabtree’s theory, should have kept them out of the downward intelligence spiral that the rest of us has been subject to. We have a reference point that we can use.\nPart of the problem with Crabtree’s argument is that he has a different conception of intelligence to that in psychometry. He talks of spatial abilities, which are hard to replicate using robots, as being of a different order to playing “superficially intellectual” chess. I’d be more sympathetic to Crabtree’s argument if he limited his argument to spatial abilities, and we could look at the spatial abilities of hunter-gatherers. Different skills were almost certainly required post-agriculture. But trying to turn questions around the need for spatial abilities into an argument of general intellectual decline is stretching it too far.\nCrabtree also suggests that genomic sequencing may also shed some light on this question. I agree, but I expect that the increasing evidence of accelerating adaptive evolution of humans will only grow stronger with that genomic evidence, and that intelligence is likely to be one of those selected traits."
  },
  {
    "objectID": "posts/the-costs-of-polygamy.html",
    "href": "posts/the-costs-of-polygamy.html",
    "title": "The costs of polygamy",
    "section": "",
    "text": "From The Economist’s Free Exchange:\n\nEconomists often argue that polygamy … benefits women because it enhances their market power. That’s because it means more marriageable men for every women. …\nBut once a woman enters into a polygamous arrangement, it seems she’d have less power. Bargaining power in a household is often based on who contributes what to household production and utility. Each person provides certain services and resources to make the household function and this keeps the marriage balanced. But the power structure is different when you have one man and several women. The marginal value each woman can uniquely provide diminishes the more women that are added to the family. …\nYou might argue that a woman retains some power because she can leave marriage and find someone else. After all, she has exceptional market power in the dating market. But terminating a marriage is costly, especially if the woman has children and is dependent on the man financially (and has no legal recognition). … Under these circumstances her market value declines as she ages.\n\nI find the argument about declining power unconvincing. It appears to be an argument based on irrationality of the woman. These potential costs are not hidden and as for other marriages in liberal democracies, the woman can enter the marriage when and on the terms she sees fit. Yes, power may relatively decline, but that is part of the consideration of whether to marry. If there are any restrictions on her bargaining power, they are often in the form of state regulation of marriage and restrictions on pre-nuptial agreements. If polygamy was legal, a term of the marriage contract could concern other or potential wives.\nThere is another argument against polygamy that I find more convincing. In a polygamous society, there is an increase in the number of low-status men who will be unable to find a partner. Evolutionary theory would predict that those men are going to take whatever actions necessary to gain access to mates. In a June opinion piece, evolutionary biologist Rob Brooks wrote of the similar problem of excessive men due to sex selection:\n\nIn many animal species, when males overabound, they often compete so fiercely to court, win and even coerce the few available females into mating that everybody suffers. The same is true when the supply of men on the marriage market exceeds the demand from females.\nIn Bare Branches: The Security Implications of Asia’s Surplus Male Population, political scientists Valerie Hudson and Andrea den Boer argue that violent crime, gambling, drugs and the kidnapping and trafficking of women are rampant in places where there are too many men.\n\nIn his book Sex, Genes and Rock & Roll, Brooks addresses polygamy more directly:\n\nThe first possibility is that legal polygyny is incompatible with mature democracy. More than a century ago, George Bernard Shaw observed that ‘Any marriage system which condemns a majority of the population to celibacy will be violently wrecked on the pretext that it outrages morality’. He had the then-recent Mormon experience in mind when he observed that ‘Polygamy, when tried under modern democratic conditions…is wrecked by the revolt of the mass of inferior men who are condemned to celibacy by it’. Late in the twentieth century, biologists picked up on this idea. Richard Alexander argued that it becomes ever more difficult for wealthy polygynous leaders to retain the political support of lower-status men when those men cannot marry and reproduce.\nPolygyny is, in the long term, incompatible with a smooth-functioning democracy because it promotes the deepest evolutionary interests of the wealthiest and most powerful men at the expense of all other men and all women. Despotic and bloody rule allowed kings and emperors of old to amass phenomenal wealth, marry prolifically and keep harems. But wherever circumstances have made it more difficult for despots to rule, the elites found it easier to gain the loyalty and support of their subjects if those subjects had the stake in society that marriage and family brings, and if the elites were themselves visibly monogamous. Most of the countries where polygyny remains legal are countries where democratic governance, if it is present at all, has only recently superseded dictatorship or monarchy. We can predict that in those countries where democracy matures, the state will cease to sanction polygyny.\n\nIn some ways, there is already some monopolisation of females by high-status men through serial marriages and mistresses. Many low-status men are unable to obtain a partner regardless of the legality of polygamy. However, policies that increase that tendency can lead to trouble. Mating is not a zero-sum game, and this is one area where, despite leaning towards the legalisation of polygamy (or more precisely, the exit of the state from regulation of marriage), I would be watching what occurs very closely.\nIf we ran that experiment, my gut instinct is that in developed nations, legalised polygamy would see limited occurrence of multiple wives (or husbands) and would hardly change the probability of most men finding partners. How many men do you know that could hold two wives?"
  },
  {
    "objectID": "posts/the-case-against-loss-aversion.html",
    "href": "posts/the-case-against-loss-aversion.html",
    "title": "The case against loss aversion",
    "section": "",
    "text": "Summary: Much of the evidence for loss aversion is weak or ambiguous. The endowment effect and status quo bias are subject to multiple alternative explanations, including inertia. There is possibly better evidence for loss aversion in the response to risky bets, but what emerges does not appear to be a general principle of loss aversion. Rather, “loss aversion” is a conditional effect that most typically emerges when rejecting the bet is not the status quo and the stakes are material.\n[As a postscript, a week after publishing this post, a working paper for a forthcoming Journal of Consumer Psychology article was released. That paper addresses some of the below points. A post on that paper is in the works.]\nIn a previous post I flagged three critiques of loss aversion that had emerged in recent years. The focus of that post was Eldad Yechiam’s analysis of the assumption of loss aversion in Kahneman and Tversky’s classic 1979 prospect theory paper.\nThe second critique, and the focus of this post, is an article by David Gal and Derek Rucker The Loss of Loss Aversion: Will It Loom Larger Than Its Gain (pdf). Its abstract:\nThe release of Gal and Rucker’s paper was accompanied by a Scientific American article by Gal, Why the Most Important Idea in Behavioral Decision-Making Is a Fallacy. It uses somewhat stronger language. Here’s a snippet:\nThis critique of loss aversion is not completely new. David Gal has been making related arguments since 2006. In this more recent article, however, Gal and Rucker draw on a larger literature and some additional experiments to expand the critique.\nTo frame their argument, they describe three potential versions of loss aversion:\nThe strong version appears to be a straw man that few would defend, but there is some subtlety in Gal and Rucker’s definition. They write:\nAn interesting point by Gal and Rucker is that for most research on the boundaries or moderators of loss aversion, loss aversion is the general principle around which the exceptions are framed. If people don’t exhibit loss aversion, it is usually argued that the person is not enoding the transaction as a loss, so loss aversion does not apply. The alternative that the gains have equal weight to (or greater weight than) the loss is not put forward. So although few would defend a blunt reading of the strong version, many researchers take it as though people are loss averse unless certain circumstances are present.\nEstablishing the weak version seems difficult. Tallying studies in which losses loom larger and where gains dominate would provide evidence more on the focus of research than the presence of a general principle of loss aversion. It’s not even clear how you would compare across different contexts.\nDespite this difficulty (or possibly because of it), Gal and Rucker come down firmly in favour of the contextual version. They do this not through tallying or comparing the contexts in which losses or gains loom larger, but by arguing that most evidence of loss aversion is ambiguous at best.\nLoss aversion as evidence for loss aversion\nThe principle of loss aversion is descriptive. It is a label applied to an empirical phenomena. It is not an explanation. Similarly, the endowment effect, our tendency to ascribe more value to items that we have than to those we don’t, is a label applied to an empirical phenomena.\nDespite being descriptive, Gal and Rucker note that loss aversion is often used as an explanation for choices. For example, loss aversion is often used as an explanation for the endowment effect. But using a descriptive label as an explanation provides no analytical value, with what appears to be an explanation simply application of a different label. (Owen Jones suggests that stating the endowment effect is due to loss aversion is no more useful than labelling human sexual behaviour as being due to abstinence aversion. I personally think it is marginally more useful, if only for the fact there is now a debate as to whether loss aversion and the endowment effect are related. The transfer of label shows that you believe these empirical phenomena have the same psychological basis.)\nGal and Rucker argue that the application of the loss aversion label to the endowment effect leads to circular arguments. The endowment effect is used as evidence for loss aversion, and, as noted above, loss aversion is commonly used to explain the endowment effect. This results in an unjustified reinforcement of the concept, and a degree of neglect of alternative explanations for the phenomena.\nI have some sympathy for this claim, although am not overly concerned by it. The endowment effect has multiple explanations (as will be discussed below), so it is weak evidence of loss aversion at best. However, it is rare that the endowment effect is the sole piece of evidence presented for the existence of loss aversion. It is more often one of a series of stylised facts for which a common foundation is sought. So although there is circularity, the case for loss aversion does not rest solely on that circular argument."
  },
  {
    "objectID": "posts/the-case-against-loss-aversion.html#risky-versus-riskless-choice",
    "href": "posts/the-case-against-loss-aversion.html#risky-versus-riskless-choice",
    "title": "The case against loss aversion",
    "section": "Risky versus riskless choice",
    "text": "Risky versus riskless choice\nMuch of Gal and Rucker’s examination of the evidence for loss aversion is divided between riskless and risky choice. Riskless choice involves known options and payoffs with certainty. Would you like to keep your chocolate or exchange it for a coffee mug? In risky choice, the result of the choice involves a payoff that becomes known only after the choice. Would you like to accept a 50:50 bet to win $110, lose $100?\nBelow is a collection of their arguments as to why loss aversion is not the best explanation for many observed empirical results sorted across those two categories.\n\nRiskless choice - status quo bias and the endowment effect\nGal and Rucker’s examination of riskless choice centres on the closely related concepts of status quo bias and the endowment effect. Status quo bias is the propensity for someone to stick with the status quo option. The endowment effect is the propensity for someone to value an object they own over an object that they would need to acquire.\nStatus quo bias and the endowment effect are often examined in an exchange paradigm. You have been given a coffee mug. Would you like to exchange it for a chocolate? The propensity to retain the coffee mug (or the chocolate if that was what they were given first) is labelled as either status quo bias or the endowment effect. Loss aversion is often used to explain this decision, as the person would lose the status quo option or their current endowment when they choose an alternative.\nGal and Rucker suggests that rather than being driven by loss aversion, status quo bias in this exchange paradigm is instead due to a preference for inaction over action (call this inertia). A person needs a psychological motive for action. Gal examined this in his 2006 paper when he asked experimental subjects to imagine that they had a quarter minted in one city, and then whether they would be willing to change it for a nickel minted in another. Following speculation by Kahneman and others that people do not experience loss aversion when exchanging identical goods, Gal considered that a propensity for the status quo absent loss aversion would indicate the presence of inertia.\nGal found that despite there being no loss in the exchange of quarters, the experimental subjects preferred the status quo of keeping the coin they had. Gal and Rucker replicated this result on Amazon Turk, offering to exchange one hypothetical $20 bill for another. They took this as evidence of inertia.\nApart from the question of what weight you should give an experiment involving hypothetical coins, notes and exchanges, I don’t find this a convincing demonstration that inertia lies behind the status quo bias. Exchange does involve some transaction costs (in the form of effort, however minimal, even if you are told to assume they are insignificant). In his 2006 paper, Gal reports other research where people traded identical goods when paid a nickel to cover “transaction costs”. The token amount spurred action.\nThose experiments, however, involved transactions of goods with known values. The value of a quarter is clear. In contrast, Gal’s exploration for the status quo bias in his 2006 paper involved goods without an obvious face value. This is important, as Gal argued that people have “fuzzy preferences” that are often ill-defined and constructed on an ad hoc basis. If we do not precisely judge the attractiveness of chocolate or a mug, we may not have a precise ordering of preference between the two that would justify choosing one after another. Under Gal’s concept of inertia, the lack of a psychological motive to change results in us sticking with the status quo mug.\nContrasting this with the exchange of quarters, there the addition of a nickel to cover trading expenses allows for a precise ordering of the two options, as they are easily comparable monetary sums. In the case of a mug and chocolate, addition of a nickel is unlikely to make the choice any easier as the degree of fuzziness extends over a much larger range.\nThe other paradigm under which the endowment effect is explored is the valuation paradigm. The valuation paradigm involves asking someone what they would be willing to pay to purchase or acquire an item, or how much they would require to be paid to accept an offer to purchase an item in their possession. The gap between this willingness to pay and the typically larger willingness to accept is the additional value given to the endowed good. (For some people this is how status quo bias and the endowment effect are differentiated. Status quo bias is the maintenance of the status quo in an exchange paradigm, the endowment effect is the higher valuation of endowed goods in the valuation paradigm. However, many also label the exchange paradigm outcome as being due to the endowment effect. Across the literature they are often used interchangeably.)\nThis difference between willingness to pay and accept in the valuation paradigm is often cited as evidence of loss aversion. But as Gal and Rucker argue, this difference has alternative explanations. Fundamentally different questions are asked when seeking an individual’s willingness to accept (what is the market value?) and their willingness to pay (what is their personal utility?). Only willingness to pay is affected by budget constraints.\nAlthough not mentioned in the 2018 paper, Gal’s 2006 paper suggests this gap may also be due to fuzzy preferences, with the willingness to pay and willingness to accept representing the two end points of the fuzzy range of valuation. Willingness to pay is the lower bound. For any higher amount, they are either indifferent (the range of fuzzy preferences) or would prefer the monetary sum in their hand. Willingness to accept is the upper bound. For any lower amount they are either indifferent (the range of fuzzy preferences) or would prefer to keep the good.\nThere are plenty of experiments in the broader literature seeking to tease out whether the endowment effect is due to loss aversion or alternative explanations of the type above. Gal and Rucker report their own (unpublished) set of experiments where they seek to isolate inertia as the driver of the difference between willingness to pay and willingness to accept. They asked for experimental subjects’ willingness to pay to obtain a good, versus their willingness to retain a good. For example, they compared subjects’ willingness to pay to fix a phone versus their willingness to pay to get a repaired phone. They asked about their willingness to expend time to drive to get a new notebook they left behind versus their willingness to drive to get a brand new notebook. They asked about their willingness to pay for fibre optic internet versus their willingness to pay to retain fibre optic internet that they already had. For each choices the subject needs to act to get the good, so inertia is removed as a possible explanation of a preference to retain an endowed good.\nWith fuzzy preferences under this experimental set up, both willingness to pay and willingness to retain would be the lower bound, as any higher payment would lead to indifference or preference of the monetary sum. Here Gal and Rucker found little difference between willingness to pay and willingnes to accept.\nGal and Rucker characterise each of the options as involving choices between losses and gains, and survey questions put to the experimental subjects confirmed that most were framing the choices in that way. This allowed them to point to this experiment as evidence against loss aversion driving the endowment effect. Remove inertia but leave the loss/gain framing, and the effect disappears.\nHowever, the experimental implementation of this idea is artificial. Importantly, the decisions are hypothetical and unincentivised. Whether coded as a loss or gain, the experimental subjects were never endowed with the good and weren’t experiencing a loss.\nMore convincing evidence, however, came from Gal and Rucker’s application of this idea in an exchange paradigm. In one scenario, people were endowed with a pen or chocolate bar. They were then asked to choose between keeping the pen or swapping for the chocolate bar, so an active choice was required for either option. Gal and Rucker found that regardless of the starting point, roughly the same proportion chose the pen or chocolate bar. This constrasts with a more typical endowment effect experimental setup that they also ran, in which they simply asked people given a pen or chocolate bar whether they would like to exchange. Here the usual endowment effect pattern emerged, with people more likely to keep the endowed good.\nLike the endowment effect experiments they critique, this result is subject to alternative explanations, the simplest (although not necessarily convincing) being that the reference point has been changed by the framing of the question. By changing the status quo, you also change the reference point. (I should say this type of argument involving ad hoc stories about changes in reference points is one of the least satisfactory elements of prospect theory.)\nDespite the potential for alternative explanations, these experiments are the beginning of a body of evidence for inertia driving some results currently attributed to loss aversion. Gal and Rucker’s argument against use of the endowment effect as evidence of loss aversion is even stronger. There are many alternative explanations to loss aversion for the status quo bias and endowment effect. The evidence for loss aversion is better found elsewhere.\n\n\nRisky choice\nGal and Rucker’s argument concerning risky bets resembles that for riskless choice. Many experiments in the literature involve an offer of a bet, such as a 50:50 chance to win $100 or lose $100, which the experimental subject can accept or reject. Rejection is the status quo, so inertia could be an explanation for the decision to reject.\nGal and Rucker describe an alternative experiment in which people can choose between a certain return of 3% or a risky bet with expected value of zero. As they must make a choice, there is not a status quo option. 80% of people allocate at least some money to the risky bet, suggesting an absence of loss aversion. This type of finding is reflected across a broader literature.\nThey also report a body of research where the risky bet is not the sole option to opt into, but rather one of two options for which an active choice must be made. For example, would you like $0 with certainty, or a 50:50 bet to win $10, lose $10. In this case, little evidence for loss aversion emerges unless the stakes are large.\nThis framing of the safe option as the status quo is one of many conditions under which loss aversion tends to emerge. Gal and Rucker reference a paper by Eyal Ert and Ido Erev, who identified that in addition to emerging when the safe option is the status quo, loss aversion also tends to emerge with:\n\nhigh nominal payoffs\nwhen the stakes are large\nwhen there are bets present in the choice list that create a contrast effect, and\nin long experiments without feedback where the computation of the expected payoff is difficult.\n\nErt and Erev described a series of experiments where they remove these features and eliminate loss aversion.\nGal and Rucker also reference a paper by Yechiam and Hochman pdf, who surveyed the loss aversion literature involving balanced 50:50 bets. For experiential tasks, where decision makers are required to repeatedly select between options with no prior description of the outcomes of probabilities (effectively learning the probabilities with experience), there is no evidence of loss aversion. For descriptive tasks, where a choice is made between fully-described options, loss aversion most typically arises for “high-stakes” hypothetical amounts, and is often absent for lower sums (which are also generally hypothetical).\nFor the higher stakes bets, Yechiam and Hochman suggest risk aversion may explain the choices. However, what Yechiam and Hochman call high stakes aren’t that high; for example $600 versus $500. As I described in my previous post on the Rabin Paradox, risk aversion at stakes of that size can only be shoehorned into the traditional expected utility model with severe contortions (although it can be done). Rejecting that bet is a high level of risk aversion for anyone with more than minimal wealth (although these experimental subjects may have low wealth as they are generally students). Loss aversion is one alternative explanation.\nRegardless, under the concept of loss aversion as presented in prospect theory, we should see loss aversion for low stakes bets. Once you are arguing that “loss aversion” will emerge if the bet is large enough, this is a different conception of loss aversion to that in the academic literature."
  },
  {
    "objectID": "posts/the-case-against-loss-aversion.html#other-phenomena-that-may-not-involve-loss-aversion",
    "href": "posts/the-case-against-loss-aversion.html#other-phenomena-that-may-not-involve-loss-aversion",
    "title": "The case against loss aversion",
    "section": "Other phenomena that may not involve loss aversion",
    "text": "Other phenomena that may not involve loss aversion\nAt the end of the paper, Gal and Rucker mention a couple of other phenomena incorrectly attributed to or not necessarily caused by loss aversion.\nThe first of these is the Asian disease problem. In this problem, experimental subjects are asked:\n\nImagine that the U.S. is preparing for the outbreak of an unusual Asian disease, which is expected to kill 600 people. Two alternative programs to combat the disease have been proposed. Assume that the exact scientific estimate of the consequences of the programs are as follows:\nIf Program A is adopted, 200 people will be saved.\nIf Program B is adopted, there is 1/3 probability that 600 people will be saved, and 2/3 probability that no people will be saved.\nWhich of the two programs would you favor?\n\nMost people tend to prefer program A.\nThen ask another set of people the following:\n\nIf Program C is adopted 400 people will die.\nIf Program D is adopted there is 1/3 probability that nobody will die, and 2/3 probability that 600 people will die.\nWhich of the two programs would you favor?\n\nMost people prefer program D, despite C and D being a reframed version of programs A and B. The reason for this change is usually attributed to the second set of options being a loss frame, with people preferring to gamble to avoid the loss.\nThis, however, is not loss aversion. There is, after all, no potential for gain in the second set of questions against which the strength of the losses can be compared. Rather, this is the “reflection effect”.\nTversky and Kahneman recognised this when they presented the Asian disease problem in their 1981 Science article pdf, but the translation into public discourse has missed this difference, with the Asian disease problem often presented as an example of loss aversion.\nGal and Rucker point out some other examples of phenomena that may be incorrectly attributed to loss aversion. The disposition effect - people tend to sell winning investment and retain losing investments - could also be explained by the reflection effect, or by lay beliefs about mean reversion. The sunk cost effect involves a refusal to recognise losses rather than a greater impact of losses relative to gains, as no comparison to a gain is made."
  },
  {
    "objectID": "posts/the-case-against-loss-aversion.html#losses-dont-hurt-more-than-gains",
    "href": "posts/the-case-against-loss-aversion.html#losses-dont-hurt-more-than-gains",
    "title": "The case against loss aversion",
    "section": "Losses don’t hurt more than gains",
    "text": "Losses don’t hurt more than gains\nBeyond the thoughtful argument in the paper, Gal’s Scientific American article goes somewhat further. For instance, Gal writes:\n\nPeople do not rate the pain of losing $10 to be more intense than the pleasure of gaining $10. People do not report their favorite sports team losing a game will be more impactful than their favorite sports team winning a game.\n\nI find it useful to distinguish two points. The first is the question of the psychological impact of a loss. Does a loss generate a different feeling, or level of attention, to an equivalent gain? The second is how that psychological response manifests itself in a decision. Do people treat losses and gains differently, resulting in loss aversion of the type described in prospect theory?\nThe lack of differentiation between these two points often clouds the discussion of loss aversion. The first point accords with our instinct. We feel the pain of a loss. But that pain does not necessarily mean that we will be loss averse in our decisions.\nGal and Rucker’s article largely focuses on the second of these points through its examination of a series of choice experiments. Yet the types of claims in the Scientific American article, as in the above quote, are more about the first.\nThis is the point where I disagree with Gal. Although contextual (isn’t everything), the evidence of the greater psychological impact of losses appears solid. In fact, the Yechiam and Hochman article pdf, quoted by Gal and Rucker for its survey of the loss aversion literature, was an attempt to reconcile the disconnect between the evidence for the effect of losses on performance, arousal, frontal cortical activation, and behavioral consistency with the lack of evidence for loss aversion. Yechiam’s article on the assumption of loss aversion by Kahneman and Tversky (the subject of a previous post) closes with a section reconciling his argument with the evidence of the effect of small stake losses on learning and performance.\nTo be able to make claims that the evidence of psychological impact of losses is as weak and contextual as the evidence for loss aversion, Gal and Rucker would need to provide a much deeper review of the literature. But in the absence of that, my reading of the literature does not support those claims.\nUnfortunately, these points in the Scientific American article have been the focus of the limited responses to Gal and Rucker’s article, leaving us with a somewhat unsatisfactory debate (as I discuss further below)."
  },
  {
    "objectID": "posts/the-case-against-loss-aversion.html#hey-were-overthrowing-the-old-paradigm",
    "href": "posts/the-case-against-loss-aversion.html#hey-were-overthrowing-the-old-paradigm",
    "title": "The case against loss aversion",
    "section": "Hey, we’re overthrowing the old paradigm!",
    "text": "Hey, we’re overthrowing the old paradigm!\nThe third part of Gal and Rucker’s paper concerns what they call the “Sociology of Loss Aversion”. I don’t have much to say on their particular arguments in this section, except that I have a gut reaction against authors discussing Thomas Kuhn and contextualising their work as overthrowing the entrenched paradigm. Maybe it’s the lack of modesty in failing to acknowledge they could be wrong (like most outsiders complaining about their ideas being ignored and quoting Kuhn). Just build your case overthrowing the damn paradigm!\nThat said, the few responses to Gal and Rucker’s paper that I have seen are underwhelming. Barry Ritholtz wrote a column, labelled by Richard Thaler as a “Good takedown of recent overwrought editorial“, which basically said an extraordinary claim such as this requires extraordinary evidence, and that that standard has not been met.\nUnfortunately, the lines in Gal’s Scientific American article on the psychological effect of losses were the focus of Ritholtz’s response, rather than the evidence in the Gal and Rucker article. Further, Ritholtz didn’t show much sign of having read the paper. For instance, in response to Gal’s claim that “people are not particularly likely to sell a stock they believe has even odds of going up or down in price”, Ritholtz responded that “the endowment effect easily explains why we place greater financial value on that which we already possess”. But, as noted above, (a solid) part of Gal and Rucker’s argument is that the endowment effect may not be the result of loss aversion. (It’s probably worth noting here that Gal and Rucker did effectively replicate the endowment effect many times over. The endowment effect is a solid phenomena.)\nAnother thread of response, linked by Ritholz, came from Albert Bridge Capital’s Drew Dickson. One part of Dickson’s 20-tweet thread runs as follows:\n\n13| So, sure, a billionaire will not distinguish between a $100 loss and a $100 gain as much as Taleb’s at-risk baker with a child in college; but add a few zeros, and the billionaire will start caring.\n4| Critics can pretend that Kahneman, Tversky and @R_Thaler haven’t considered this, but they of course have. From some starting point of wealth, there is some other number where loss aversion matters. For everyone. Even Gal. Even Rucker. Even Taleb.\n15| Losses (that are significant to the one suffering the loss) feel much worse than similarly-sized gains feel good. Just do the test on yourself.\n\nBut this idea that you will be loss averse if the stakes are high enough is not “loss aversion”, or at least not the version of loss aversion from prospect theory, which applies to even the smallest of losses. It’s closer to the concept of “minimal requirements”, whereby people avoid bets that would be ruinous, not because losses hurt more than gains.\nThaler himself threw out a tweet in response, stating that:\n\nNo minor point about terminology. Nothing of substance. WTA &gt; WTP remains.\n\nThat willingness to accept (WTA) is greater than willingness to pay (WTP) when framed as the status quo is not a point Gal and Rucker would disagree with. But is it due to loss aversion?\nThankfully, the publication of Gal and Rucker’s article was accompanied by two responses, one of which tackled some of the substantive issues (the other response built on rather than critiqued Gal and Rucker’s work). That substantive response (pdf), by Itamar Simonson and Ran Kivetz, would best be described as supporting the weak version of loss aversion.\nSimonson and Kivetz largely agreed that status quo bias and the endowment effect do not offer reliable support for loss aversion, particularly given the alternative explanations for the phenomena. However, they were less convinced of Gal and Rucker’s experiments to identify inertia as the basis of these phenomena, suggesting the experiments involved “unrealistic experimental manipulations that are susceptible to confounds and give rise to simple alternative explanations”, although they leave those simple alternative explanations unspecified.\nSimonson and Kivetz also disagreed with Gal and Rucker on the evidence concerning risky bets, describing as ad hoc and unsupported the assumption that not accepting the bet is the status quo. It’s not clear to me how they could describe that assumption as unsupported given Gal and Rucker’s experimental evidence (nor the evidence Gal and Rucker cite) about the absence of loss aversion for small stakes when rejecting the bet is not framed as the status quo. Loss aversion only emerges for larger bets.\nI should say, however, that I do have some sympathy for Simonson and Kivetz’s resistance to accepting Gal and Rucker’s sweeping of the risky bet premium into the status quo bucket. Even those larger bets for which loss aversion arises aren’t that large (as noted above, they’re often in the range of $500). Risk aversion is a somewhat unsatisfactory alternative explanation (a topic I discuss in my post on Rabin’s Paradox), and I sense that some form of loss aversion kicks in, although here we may again be talking about a minimal requirements type of loss aversion, not the loss aversion of prospect theory.\nDespite their views on risky bets, Simonson and Kivetz were more than willing to approve of Gal and Rucker’s case that loss aversion was a contingent phenomena. They would simply argue that loss aversion occurs “on average”. As noted above, I’m not sure how you would weight the relative instances of gains or losses having greater weight, so I’ll leave that debate for now.\nFunnily enough, a final comment by Simonson and Kivetz on risky bets is that “the notion that losses do tend to loom larger than gains is most likely correct; it certainly resonates and “feels” consistent with personal experience, though intuitive reactions are a weak form of evidence.” As noted above, we should distinguish feelings and a decision exhibiting loss aversion.\nUnfortunately, I haven’t found anything else that attempts to pick apart Gal and Rucker’s article, so it is hard to gauge the broader reception to the article or whether it has resonated in academic circles at all."
  },
  {
    "objectID": "posts/the-case-against-loss-aversion.html#where-does-this-leave-us-on-loss-aversion",
    "href": "posts/the-case-against-loss-aversion.html#where-does-this-leave-us-on-loss-aversion",
    "title": "The case against loss aversion",
    "section": "Where does this leave us on loss aversion?",
    "text": "Where does this leave us on loss aversion?\nPutting this together, I would summarise the case for loss aversion as follows:\n\nThe conditions for loss aversion are more restrictive than is typically thought or stated in discussion outside academia\nSome of the claimed evidence for loss aversion, such as the endowment effect, have alternative explanations. The evidence is better found elsewhere\nThere is sound evidence for the psychological impact of losses, but this does not necessarily manifest itself in loss aversion\nMost of the loss aversion literature does a poor job of distinguishing between loss aversion in its pure sense and what might be called a “minimal requirements” effect, whereby people are avoiding the gamble due to the threat of ruin.\n\nThis is a more restricted conception of loss aversion than I held when I started writing this post."
  },
  {
    "objectID": "posts/the-case-against-loss-aversion.html#the-loss-aversion-series-of-posts",
    "href": "posts/the-case-against-loss-aversion.html#the-loss-aversion-series-of-posts",
    "title": "The case against loss aversion",
    "section": "The loss aversion series of posts",
    "text": "The loss aversion series of posts\nMy next post will be on the topic of ergodicity, which involves the concept that people are not maximising the expected value of a series of gambles, but rather the time average (explanation on what that means to come). If people maximise the latter, not the former as many approaches assume, you don’t need risk or loss aversion to explain their decisions.\nMy other posts on loss aversion can be found here:\n\nKahneman and Tversky’s debatable loss aversion assumption\nWhat can we infer about someone who rejects a 50:50 bet to win $110 or lose $100? The Rabin paradox explored\nThe case against loss aversion (this post)\nErgodicity economics - a primer"
  },
  {
    "objectID": "posts/the-blogs-i-read.html",
    "href": "posts/the-blogs-i-read.html",
    "title": "The blogs I read",
    "section": "",
    "text": "Although RSS seems to be on the way out, I’ve found myself explaining feed readers to a few people recently. They asked for some suggestions of blogs to follow, so below are some from my reading list.\nI try not to live in a bubble, but you can see a libertarian bent to these recommendations. My full reading list (as at 4 January 2015) is here - unzip and upload it into your favourite feed reader - and is a bit broader than the below might suggest.\nStatistical Modeling, Causal Inference, and Social Science: My favourite blog. Regularly skewers statistical papers of all types. I’ve learnt more about the practical use of statistics from Andrew Gelman than I have in any statistics or econometrics class.\nOffsetting Behaviour: Eric Crampton’s regular dismantling of those who want to protect us from ourselves is always worth reading.\nGene Expression: Still the best evolutionary biology and genetics blog.\nBleeding Heart Libertarians: The blog at which I feel most at home politically.\nEconlog: I have only Bryan Caplan’s posts in my feed, although Caplan is possibly the most infuriating thinker I regularly read.\nAskblog: Arnold Kling’s post-Econlog blog is always a source of sharp comment on interesting material.\nMarginal Revolution: One of the most popular economics sites, but possibly the best aggregator of interesting content.\nEcontalk: Not a blog but a podcast. Russ Roberts has an impressive guest list and is rarely dull. There is a massive back catalogue worth working through.\nClub Troppo: A centrist Australian political blog. I don’t have any Australian “libertarian” or “free market” blogs in my feed, as they are generally horrible - conservative at best (rare), corporatist at worst, with posts closer to trolling than informative and comment sections that make the eyes bleed.\nInformation Processing: Stephen Hsu provides plenty of material at the cutting edge of research into genetics and intelligence.\nSanta Fe Institute News: The best feed of complexity related stories and ideas.\nMatt Ridley’s Blog: Hit and miss (a bit like The Rational Optimist), but more than enough good material.\nThe Enlightened Economist: A constant source of additions to my book reading list."
  },
  {
    "objectID": "posts/the-biological-basis-of-preferences-and-behaviour-conference.html",
    "href": "posts/the-biological-basis-of-preferences-and-behaviour-conference.html",
    "title": "The Biological Basis of Preferences and Behaviour conference",
    "section": "",
    "text": "I have just attended The Biological Basis of Preferences and Behaviour conference at the Becker Friedman Institute at the University of Chicago. It was a good conference with some high quality presentations. I will post on some of them over the next few months once I digest the presentations and papers (or they exit embargo).\nIn the meantime, the conference has triggered some thoughts on how economics will contribute to the evolutionary sciences, and how biology will be integrated into economics.\nMost work generated by economists on the evolution and biological basis of preferences uses beautiful (to some beholders) but complicated mathematical models. As a gross generalisation, most presentations at the conference consisted of multiple slides of equations and proofs. Many questions and clarifications were required as the presenter progressed through the slides. The presentation usually ended with an attempt to explain the intuition of the model in plain language.\nAfter being walked through one of these models, the first question to ask is whether the finding of the model might be true (noting the problems of defining truth). Are the assumptions even ballpark realistic? Is there any evidence that the process described in the model occurs or has occurred?\nThe next question is about insight. Did this model, even if not “true”, provide useful insight into the process or phenomena being explored?\nFinally, does the model communicate the idea in a way that will be understood by people who might care about or should know about the work?\nMy impression was that many of the conference presentations fell at the first two hurdles. I sense that in many instances this was because the presenters were more interested in the mathematical dynamics and game theory than the final application of the model. This is, of course, a broader issue through much of economics, where the incentives push researchers towards beautiful but complicated models. Generally, these models will not be subject to testing and as a result, do not face the threat of being discarded if they are not supported by the empirical evidence. Having robust mathematics that supports the story being told has greater weight than the truth of the model or the manner in which it might communicate an insight. There is little direct competition between models.\nEven where the first two hurdles were cleared, I am uncertain how often the product of the research is likely to be consumed outside of a small circle, generally consisting of other members of the field. Mathematical models are an excellent way to communicate to some people (my impression is that many in the conference audience prefer this approach), but simple, verbal explanations are the way most communication occurs.\nIn contrast, at last week’s Consilience Conference there was barely an equation in sight. The presentations provided me with a mountain of ideas that I have communicated in various ways ever since. Those presentations from this week’s conference that were based on a detailed mathematical model, while providing some fodder for thought, are unlikely to feature in many conversations. I am not sure the models presented are true or insightful, and even where they are, my first thought is how to come up with a different way of communicating the point. My posts over the next couple of weeks will largely feature those presentations that were the exception to the rule.\nHaving dug myself into a hole through some broad, over-generalised criticism, this critique is not to say that there are no useful and important ideas coming out of this work. My economics and evolutionary biology reading list has a sample of what I consider to be some of the more important contributions. Rather, my critique reflects my instinct that the field will remain a world unto itself, without being a significant contributor to the congruence of biology and economics. Research into the biological basis of preferences and behaviour has massive potential to affect the broader field of economics. It would be disappointing if this research did not achieve that result.\n*The videos are many of the presentations are now up."
  },
  {
    "objectID": "posts/the-benefits-of-math-skills-to-forager-farmers.html",
    "href": "posts/the-benefits-of-math-skills-to-forager-farmers.html",
    "title": "The benefits of math skills to forager-farmers",
    "section": "",
    "text": "In a new article by Eduardo Undurraga and colleagues (HT: Neuroskeptic):\n\nMath skills and market and non-market outcomes: Evidence from an Amazonian society of forager-farmers.\nResearch in industrial nations suggests that formal math skills are associated with improvements in market and non-market outcomes. But do these associations also hold in a highly autarkic setting with a limited formal labor market? We examined this question using observational annual panel data (2008 and 2009) from 1,121 adults in a native Amazonian society of forager-farmers in Bolivia (Tsimane’). Formal math skills were associated with an increase in wealth in durable market goods and in total wealth between data collection rounds, and with improved indicators of own reported perceived stress and child health. These associations did not vary significantly by people’s Spanish skills or proximity to town. We conclude that the positive association between math skills and market and non-market outcomes extends beyond industrial nations to even highly autarkic settings.\n\nOne nice element of this study is that the effect of math skills on economic outcomes is not through a “sheepskin effect”, whereby the benefits accrue to a degree or diploma, as there is no formal labour market where signals of that kind are used. In modern economies, it is hard to separate sheepskin effects from gains due to skills. In this case, the absence of a sheepskin effect means that it is the math skills themselves (or the general intelligence underlying those skills) that are paying off."
  },
  {
    "objectID": "posts/the-benefits-of-cognitive-limits.html",
    "href": "posts/the-benefits-of-cognitive-limits.html",
    "title": "The benefits of cognitive limits",
    "section": "",
    "text": "Cleaning up some notes recently, I was reminded of another interesting piece from Gerd Gigerenzer’s Rationality for Mortals:\n\nIs perfect memory desirable, without error? The answer seems to be no. The “sins” of our memory seem to be good errors, that is, by-products (“spandrels”) of a system adapted to the demands of our environments. In this view, forgetting prevents the sheer mass of details stored in an unlimited memory from critically slowing down and inhibiting the retrieval of the few important experiences. Too much memory would impair the mind’s ability to abstract, to infer, and to learn. Moreover, the nature of memory is not simply storing and retrieving. Memory actively “makes up” memories—that is, it makes inferences and reconstructs the past from the present. This is in contrast to perception, which also makes uncertain inferences but reconstructs the present from the past. Memory needs to be functional, not veridical. To build a system that does not forget will not result in human intelligence.\n…\nCognitive limitations both constrain and enable adaptive behavior. There is a point where more information and more cognitive processing can actually do harm, as illustrated in the case of perfect memory. Built-in limitations can in fact be beneficial, enabling new functions that would be absent without them (Hertwig & Todd, 2003). …\nNewport (1990) argued that the very constraints of the developing brain of small children enable them to learn their first language fluently. Late language learners, in contrast, tend to experience difficulties when attempting to learn the full range of semantic mappings with their mature mental capacities. In a test of this argument, Elman (1993) tried to get a large neural network with extensive memory to learn the grammatical relationships in a set of several thousand sentences, yet the network faltered. Instead of taking the obvious step of adding more memory to solve the problem, Elman restricted its memory, making the network forget after every three or four words—to mimic the memory restrictions of young children who learn their first language. The network with the restricted memory could not possibly make sense of the long complicated sentences, but its restrictions forced it to focus on the short simple sentences, which it did learn correctly, mastering the small set of grammatical relationships in this subset. Elman then increased the network’s effective memory to five or six words, and so on. By starting small, the network ultimately learned the entire corpus of sentences, which the full network with full memory had never been able to do alone.\n\nGigerenzer also makes the case that most visual illusions are “good errors” necessary in an intelligent animal. Assumptions used to create the illusions, such as “light tends to come from above”, inform what we “see”.\n\nPerceptual illusions are good errors, a necessary consequence of a highly intelligent “betting” machine (Gregory, 1974). Therefore, a perceptual system that does not make any errors would not be an intelligent system. It would report only what the eye can “see.” That would be both too little and too much. Too little because perception must go beyond the information given, since it has to abstract and generalize. Too much because a “veridical” system would overwhelm the mind with a vast amount of irrelevant details. Perceptual errors, therefore, are a necessary part, or by-product, of an intelligent system. They exemplify a second source of good errors: Visual illusions result from “bets” that are virtually incorrigible, whereas the “bets” in trial- and-error learning are made in order to be corrected eventually. Both kinds of gambles are indispensable and complementary tools of an intelligent mind.\nThe case of visual illusions illustrates the general proposition that every intelligent system makes good errors; otherwise it would not be intelligent. The reason is that the outside world is uncertain, and the system has to make intelligent inferences based on assumed ecological structures. Going beyond the information given by making inferences will produce systematic errors. Not risking errors would destroy intelligence.\n\nIn other parts of his work Gigerenzer builds the case that many of the “biases” identified by Kahneman and friends fall into the “good errors” camp."
  },
  {
    "objectID": "posts/the-benefit-to-being-right.html",
    "href": "posts/the-benefit-to-being-right.html",
    "title": "The benefit to being right",
    "section": "",
    "text": "In all the debates about human biases, I like to believe that there is some benefit to being right. There must be some evolutionary benefit to knowing the true state of the world (on average). That is not to say that the benefits will always outweigh the costs, as the aim is to reproduce. Attracting a mate or surviving within a group may require holding some beliefs, such as religion. However, knowing that the snake will kill you will have fitness benefits.\nHitting on this subject, John Wilkins of Evolving Thoughts has a guest post up at Scientific American on the evolution of common sense. He puts the problem nicely:\n\nWith philosopher Paul Griffiths, I call this the Darwin’s Monkey Brain Problem: how can we rely upon a cognitive apparatus which had not evolved for finding out about the world, but instead for the purpose of getting primates laid?\n\nWilkins splits beliefs into moral, religious and environmental. On moral and religious beliefs, he states:\n\nIn the case of moral values, fitness is clearly at least in part down to our behaviour being acceptable to those around us, so that we do not suffer sanctions, and gain assistance when we need it. We are adapted to interpersonal and social interactions.\nIn the case of religious claims, as Griffiths and I argue, our beliefs are more likely to be fitness enhancing for much the same reasons as moral beliefs – they avoid our being censured, perhaps even executed as apostates or heretics, and increase our likelihood of receiving aid when we are in dire straits.\n\nBeyond the benefits arising within a group, some religious or moral beliefs directly increase fitness. For example, a belief that contraception is not allowed may be fitness enhancing. Even activities that may be wasteful, such as participating in religious observances, might play a role as conspicuous leisure or consumption. The waste signals wealth or status to potential mates.\nFor environmental beliefs, Wilkins notes that there is generally direct feedback:\n\nIt seems likely that some beliefs – let us call them environmental beliefs – gain fitness because they track, if not exactly truth, then satisfactory ecological correlations. Obviously, if you believe there is a cliff in front of you, and there is, then you will tend not to leap over it, and your fitness is thereby enhanced. If you believe that rustling in the undergrowth is a leopard, and take evasive action, you are fitter than the poor thinker who takes a Plantingan line and treats it as a mere Kantian construct.\n\nSome interesting questions arise when we come to debates about issues such as climate change. How are our minds shaped to deal with these questions and what are the fitness consequences of being right? Issues such as climate change seem to fall increasingly in the religious or moral categories. There are benefits of fitting in with a group. Truth seeking may not be the best fitness enhancing strategy. Further, it is hard to find any direct fitness consequences of being on either side of the climate debate. Our belief is unlikely to have fitness consequences, unless someone decides they have a particular interest in beach-side real estate.\nI don’t find that very encouraging for my hope that there are benefits to being right (apart from personal smugness - although even that does not require that you are right, only that you believe you are right). I think the best bet is that, eventually, the consequences of climate change are so obvious that some people amend their beliefs to avoid signalling low intelligence. However, given how long people have held out in the evolution and creationism debates, this might be a rather long shot."
  },
  {
    "objectID": "posts/the-benefit-of-doing-nothing.html",
    "href": "posts/the-benefit-of-doing-nothing.html",
    "title": "The benefit of doing nothing",
    "section": "",
    "text": "From Tim Harford:\n\n[I]n many areas of life we demand action when inaction would serve us better.\nThe most obvious example is in finance, where too many retail investors trade far too often. One study, by Brad Barber and Terrance Odean, found that the more retail investors traded, the further behind the market they lagged: active traders underperformed by more than 6 percentage points (a third of total returns) while the laziest investors enjoyed the best performance.\nThis is because dormant investors not only save on trading costs but avoid ill-timed moves. Another study, by Ilia Dichev, noted a distinct tendency for retail investors to pile in when stocks were riding high and to sell out at low points. …\nThe same can be said of medicine. It is a little unfair on doctors to point out that when they go on strike, the death rate falls. Nevertheless it is true. It is also true that we often encourage doctors to act when they should not. In the US, doctors tend to be financially rewarded for hyperactivity; everywhere, pressure comes from anxious patients. Wiser doctors resist the temptation to intervene when there is little to be gained from doing so — but it would be better if the temptation was not there. …\n\nHarford also reflects on the competition between humans and computers, covering similar territory to that in my Behavioral Scientist article Don’t Touch the Computer (even referencing the same joke).\n\nThe argument for passivity has been strengthened by the rise of computers, which are now better than us at making all sorts of decisions. We have been resisting this conclusion for 63 years, since the psychologist Paul Meehl published Clinical vs. Statistical Prediction. Meehl later dubbed it “my disturbing little book”: it was an investigation of whether the informal judgments of experts could outperform straightforward statistical predictions on matters such as whether a felon would violate parole.\nThe experts almost always lost, and the algorithms are a lot cleverer these days than in 1954. It is unnerving how often we are better off without humans in charge. (Cue the old joke about the ideal co-pilot: a dog whose job is to bite the pilot if he touches the controls.)\n\nThe full article is here."
  },
  {
    "objectID": "posts/the-behaviour-genetics-to-eugenics-to-nazi-manoeuvre.html",
    "href": "posts/the-behaviour-genetics-to-eugenics-to-nazi-manoeuvre.html",
    "title": "The behaviour genetics to eugenics to Nazi manoeuvre",
    "section": "",
    "text": "Recently, I’ve tended to roll my eyes rather than respond to poor commentary on behaviour genetics. But areview by Kate Douglas at New Scientist, in which she pulls the behaviour genetics to eugenics to Nazi manoeuvre, has pointed out a potentially interesting book.\nFirst, from the conclusion to Douglas’s review (actually, not so much a review but a launchpad):\n\nBehaviour geneticists came to see finding high heritability as a justification for their work. But heredity changes depending on the environment. Grow those tomatoes in a regulated greenhouse and almost all the difference in their height will be thanks to their genes; grow them on a sloping, partly shaded field and the effect of heritability is lower.\nNature and nurture are not distinct, and the complexity of their interactions is increasingly apparent in this genomic age. Heritability can’t even be reliably estimated in humans using twin and adoption studies, the method of choice for behaviour geneticists.\nAll this undermines the supposition that heritability tells us about the cause of a behaviour. In fact, heritability is almost entirely meaningless.\n\nI haven’t yet met a behaviour geneticist who doesn’t understand that heritability can vary with environment. Just look at Eric Turkheimer and friends’ work on heritability of IQ among different socioeconomic groups. The change in heritability across environment tells us something. And if heritability measures are robust across environments (as it is for IQ for most socioeconomic groups), that tells you something too.\nBut moving on, Douglas’s review is of a new book, Misbehaving Science: Controversy and the Development of Behavior Genetics by Aaron Panofsky. The blurb on the book suggests it might be better than the review, and could contain some interesting history on the debates in the field.\n\nBehavior genetics has always been a breeding ground for controversies. From the “criminal chromosome” to the “gay gene,” claims about the influence of genes like these have led to often vitriolic national debates about race, class, and inequality. Many behavior geneticists have encountered accusations of racism and have had their scientific authority and credibility questioned, ruining reputations, and threatening their access to coveted resources.\nIn Misbehaving Science, Aaron Panofsky traces the field of behavior genetics back to its origins in the 1950s, telling the story through close looks at five major controversies. In the process, Panofsky argues that persistent, ungovernable controversy in behavior genetics is due to the broken hierarchies within the field. All authority and scientific norms are questioned, while the absence of unanimously accepted methods and theories leaves a foundationless field, where disorder is ongoing. Critics charge behavior geneticists with political motivations; champions say they merely follow the data where they lead. But Panofsky shows how pragmatic coping with repeated controversies drives their scientific actions. Ironically, behavior geneticists’ struggles for scientific authority and efforts to deal with the threats to their legitimacy and autonomy have made controversy inevitable—and in some ways essential—to the study of behavior genetics."
  },
  {
    "objectID": "posts/the-academic-experiment.html",
    "href": "posts/the-academic-experiment.html",
    "title": "The academic experiment",
    "section": "",
    "text": "This week I finally took the plunge and joined academia. It’s a possibility that has been lurking over me for close to ten years, although recently I had been of the view that the time had passed.\nAs I wound up my PhD in 2014, I nosed around the Australian academic job market. With twins on the way, I was somewhat reluctant to nose further afield, so only applied for two foreign opportunities that were a particularly good fit.\nIt is fair to say I didn’t generate much interest. Australian economics departments typically have a preference for new hires from name universities overseas. (I think a global perspective is an OK starting point but that they err in their balance and don’t capitalise on their potential information advantage.) The Australian PhD is absent the often hellish coursework indoctrination that occurs in most US PhD programs, so is seen as less rigorous. (Several Australian universities including my new home have moved to mimic the US structure over the last decade.) An Australian PhD also isn’t likely to generate connections with the cabal that monopolises the top journals.\nMy idiosyncratic PhD topic combining economics with evolutionary biology didn’t help. I thought it was a good PhD (albeit not the one I would write now). My reviewer feedback was strong and my university agreed. Alas…a dozen or so applications, one interview (thanks UQ!) and that was that. Funnily enough, my job market paper is the one chapter from my PhD that hasn’t been published. I think it’s the most interesting idea in the PhD, but maybe I picked the wrong one. I’d present the idea much differently now - rewrite coming up! - so maybe it will have second legs.\nRather than the academic path, I’ve since worked in a mix of behavioural and data science roles, and otherwise kept myself amused with this blog and other fora such as Behavioral Scientist.\nBut at the time I was moving on from my attempt to get a foothold in academia, this blog generated a random connection. After a post on a working paper, I received an email from an academic, Lionel Page, suggesting I present to his department at QUT. We ended up writing a follow-up paper together. Then when he started a new Graduate Certificate in Behavioural Economics at University of Technology Sydney, I agreed to teach a few units as a side-gig from my day job.\nAnd there arises the opportunity. The Graduate Certificate is now part of a new broader Masters programme, but Lionel had decided to leave UTS. My teaching and experience made me a logical plug for the gap. Now I’m part of the Economics Discipline Group at the University of Technology Sydney with a mission to drive the Masters.\nIn my ‘sour grapes’ moments over the last few years, I had questioned whether I wanted to join academia. The blog receives several orders of magnitude more readers in the course of a year than my journal articles will ever get. Why would I want to join a game fighting to get material published in arcane places that nobody reads when I can play with ideas, write them up in a way that suits me, and publish them in a forum where there are actual readers? I could even work just a couple of days a week, earn more than an academic salary and use my remaining time as I please.\nA few things tipped me.\nFirst, the teaching interests me. In the same way that I understand something an order of magnitude more after I’ve blogged it, teaching has the same effect. And I like trying to get people across the line with new ideas.\nThe broader challenge of creating a great Masters program is also interesting. As regular readers of this blog would know, I’m critical of a lot of academic and applied behavioural science. How do we create a pipeline of graduates who know the science, can determine what’s bullshit, and who won’t just wave around the latest shiny thing? I deliberately shaped the units I have taught over the last couple of years to give the highs and the lows of behavioural economics. It’s now a case making sure the whole package works.\nFinally, I do have the freedom to pick up interesting external projects. If I could be useful, let me know!\nI’m not planning to play the academic publishing game to maximise impact points or whatever the benchmark is. I’m old and grumpy enough to be willing to travel my own path and that involves exploring interesting ideas in places where people will engage with them. That said, I do have some paper ideas that I think will be successful. There is a role for the sceptical behavioural economics aggregator and critic. I’ve got some thoughts to build on the ergodicity posts I’ve written. There are some lingering evolutionary ideas from my earlier research. And I want to get that damned job market paper published!\nThe other beneficiary of the academic gig will be this blog. You may have noticed that I have awoken from my pandemic-induced slumber of the last two years and been posting on a weekly basis since the beginning of the year. My plan is that will continue. The blog is a major way I develop my thoughts. I hope you also get something out of it.\n\nPostscript: I’m halfway through David Epstein’s Range: Why Generalists Triumph in a Specialized World. It’s excellent, almost on par with the fantastic The Sports Gene. A central thread is that you shouldn’t specialise too early. Have a long sampling period. Because of improved match and a bigger toolkit, those who specialise late are often the most successful.\nThe book is almost tailor made to equip me to justify my career path. I dropped out of army officer training (there’s a chapter about that very topic!) and out of university three times. I’ve been a lawyer, environmental campaigner, policy adviser in Treasury, environmental consultant, economic consultant, behavioural economist and data scientist. I have five degrees (from nine course starts…I don’t count one of the times as “dropping out”). And now I am on a new venture. Here’s hoping I’m a case study on the payoffs of a very long sampling period."
  },
  {
    "objectID": "posts/that-chart-doesnt-match-your-headline-fertility-edition.html",
    "href": "posts/that-chart-doesnt-match-your-headline-fertility-edition.html",
    "title": "That chart doesn’t match your headline - fertility edition",
    "section": "",
    "text": "Under the heading “Japan’s birth rate problem is way worse than anyone imagined”, Ana Swanson at The Washington Post’s Wonkblog shows the following chart:\n\n\nJapan fertility rate\n\n\n\nSo, the birth rate problem is worse than forecast in 1976, 1986, 1992 and 1997. However, the birth rate is higher than was forecast in 2002 and 2006 - so has surprised on the upside. It’s only “worse than anyone imagined” if you’ve had your head in the sand for the last 10 or so years. As Noah Smith asks, didn’t any of the people tweeting the graph (it appeared at least half a dozen times in my feed) look at it?\nThat said, the chart demonstrates the lack of robust conceptual models that might be used to forecast fertility. As another example, the below figure comes from Lee and Tuljapurkar’s Population Forecasting for Fiscal Planning: Issues and Innovations and shows US Census Bureau forecasts through to 1996. As for the Japan forecasts, the tendency is to assume a slight reversion toward replacement fertility followed by constant fertility.\n\n\nUS Census forecasts\n\n\n\nThe Bureau of the Census produced high and low estimates (as in the figure below), but these don’t make the forecasting look any better. For many forecasts, the fertility rate was outside the range within 3 years. In 1972, fertility fell outside the range before the forecast was even published.\n\n\nUS Census High-Low forecasts\n\n\n\nOver the last ten years, fertility “surprises” on the upside are typical in developed countries. Japan is not an outlier. Below are three consecutive projections from the Australian Government’s Intergenerational Report. IGR1 was published in 2002, IGR2 in 2007 and IGR 2010 in 2010 (obviously). As you can see, they’ve been chasing an upward trend in fertility. The fertility problem is less severe than once thought. Long-term fertility is assumed to be 1.60 in the 2002 forecast, but 1.90 in 2010. A new IGR is due out this year, so it will be interesting to see where that forecast goes.\n\n\nIGR_2007 fertility chart\n\n\n\n\n\nIGR_2010 fertility chart\n\n\n\nAs for building better conceptual models of fertility, I don’t envy anyone attempting that task. But as I argue in a working paper, evolutionary dynamics will tend to drive fertility rates up from recent lows. Is that part of the story behind what we are seeing in Japan and elsewhere?"
  },
  {
    "objectID": "posts/tetlocks-expert-political-judgment-how-good-is-it-how-can-we-know.html",
    "href": "posts/tetlocks-expert-political-judgment-how-good-is-it-how-can-we-know.html",
    "title": "Tetlock’s Expert Political Judgment: How Good Is It? How Can We Know?",
    "section": "",
    "text": "A common summary of Philip Tetlock’s Expert Political Judgment: How Good Is It? How Can We Know? is that “experts” are terrible forecasters. There is some truth in that summary, but I took a few different lessons from the book. While experts are bad, others are worse. Simple algorithms and more complex models outperform experts. And importantly, forecasting itself is not a completely pointless task.\nTetlock’s book reports on what must be one of the grander undertakings in social science. Cushioned by his recently gained tenure, Tetlock asked a range of experts to predict future events. With the need to see how the forecasts panned out, the project ran for almost 20 years.\nThe basic methodology was to ask each participant to rate three possible outcomes for a political or economic event on a scale of 0 to 10 on how likely each outcome is (with, assuming some basic mathematical literacy, the sum allocated to the three options being 10). An example questions might be whether a government will retain, lose or strengthen its position after the next election. Or whether GDP growth will be below 1.75 per cent, between 1.75 per cent and 3.25 per cent, or above 3.25 per cent.\nOnce the results were in, Tetlock scored the participants on two dimensions - calibration and discrimination. To get a high calibration score, the frequency with which events are predicted needs to correspond with their actual frequency. For instance, events predicted to occur with a 10 per cent probability need to occur around 10 per cent of the time, and so on. Given experts made many judgments, these types of calculations could be made.\nTo score highly on discrimination, the participant needs to assign a score of 1.0 to things that happen and 0 to things that don’t. The closer to the ends of the scale for predictions, the higher the discrimination score. It is possible to be perfectly calibrated but a poor discriminator (fence sitter) through to a perfect discriminator (only using the extreme values correctly).\nFrom Tetlock’s analysis of these scores come the headline findings of the book. I take them as:\n\nExperts, who typically have a doctorate and average 12 years experience in their field, barely outperform “chimps” - the chimps being allocation of equal probability of 33 per cent to each potential outcome.\nHowever - and this point is one you rarely hear in commentary about the book - the experts outperform unsophisticated forecasters (a role filled by Berkeley undergrads), whose performance is truly woeful. So, when people lament about experts after reading this book, be even more afraid of the forecasts of the general population.\nThe experts were not differentiated on a range of dimensions, such as years of experience or whether they are forecasting on their area of expertise. Subject matter expertise translates less into forecasting accuracy than confidence.\nThe one dimension where forecast accuracy was differentiated is on what Tetlock calls the fox-hedgehog continuum (borrowing from Isiah Berlin). Hedgehogs know one big thing and aggressively expand that idea into all domains, whereas foxes know many small things, are skeptical of grand ideas and stitch together diverse, sometimes conflicting information. Foxes are more willing to change their minds in response to the unexpected, more likely to remember past mistakes, and more likely to see the case for opposing outcomes. And foxes outperformed on both measures of calibration and discrimination.\nExperts are outperformed by simple algorithms that predict the continuation of the recent past into the future and vastly so by more sophisticated models (generalised autoregressive distributed lag). Political observers would be better off thinking less, and if they know the base rates of possible outcomes, they should simply predict the most common.\n\nAs Bryan Caplan argues, Tetlock gives experts a harder time than they might deserve. The “chimps” are helped by a combination of hard questions and constrained answer fields. There is no option to predict one million per cent growth in GDP next year. We might expect experts to shine more if there were “dumb” questions. Further, the mentions of the horrible performance of the Berkeley undergrads, the proxy for unsophisticats, are rare. On the flipside, a baseline for assessment should not be the chimp or these undergrads, but the simple extrapolation algorithms - and there experts measure poorly.\nThe expected behaviour of the experts may provide a partial defence. They are filling out a survey, and are unlikely generate a model for every question. Many judgements were likely off the top of the head, with no serious stakes (including no public shaming). This does, however, raise the question of why they were so hopeless in their own fields of expertise where they might have some of these models available.\nSo what is it about foxes and hedgehogs that leads to differences in performance?\nAs a start, the approach of foxes lines up with the existing literature on forecasting. This literature shows that average predictions of forecasters are generally more accurate than the majority of forecasters for whom the averages are computed, trimming outliers further enhances accuracy, and there is opportunity for further improvement through the Delphi technique. In line with this, Tetlock suggests foxes factor in conflicting considerations in a flexible weighted-averaging fashion into their judgements.\nNext, foxes are better Bayesians in that they update their beliefs in response to new evidence and in proportion to the extremity of the odds they placed on possible outcomes. They weren’t perfect Bayesian’s however - when surprised by a result, Tetlock calculated that foxes moved around 59 per cent of the prescribed amount compared to 19 per cent for hedgehogs. In some of the exercises, hedgehogs moved in the opposite direction.\nThere was a lot of evidence that both foxes and hedgehogs were more egocentric than natural Bayesians. A natural Bayesian would consider the probability of the event occurring if their view of the world is correct (which also has a probability attached to it) and the probability of the event occurring if their understanding of the world was wrong. But few spontaneously factored other views into their assessment of probabilities. When Tetlock broke down his experts’ predictions, the odds were almost always calculated based on their interpretation of the world being correct.\nFoxes were also less prone to hindsight effects. Many experts claimed that they assigned higher probabilities to outcomes that materialised than they did. As Tetlock notes, it is hard to say someone got it wrong if they think they got it right. (Is hindsight bias, as suggested by one hedgehog, an adaptive mechanism that unclutters the mind?)\nThe chapter of the book where the hedgehogs wheel out the defences against their poor performance is somewhat amusing. As Tetlock points out, forecasters who thought they were good at the beginning sounded like radical skeptics about the value of forecasting by the end.\nThe experts commonly pointed out that their prediction was a near miss, so the result shouldn’t be held against them. But almost no-one said don’t hold the non-occurrence of event against others who predicted it.\nThey also tended to claim that “I made the right mistake”, as it is better to be safe than sorry. But all of Tetlock’s attempts to adjust the scoring to help hedgehogs in these cases failed to close the gap.\nSome hedgehogs claimed that the questions were not over a long enough time period. There are irreversible trends at work in world today, and while specific events might be hard to predict, the shape of the world in the long-term is clear. But the problem is that hedgehogs were ideologically diverse, and only a few could be right about any long-term trends that exist.\nOne thing that might be said in favour of the hedgehogs is that the accuracy of the average of hedgehog forecasts was similar to the average of fox forecasts. The average fox forecast beats about 70% of foxes, but the average hedgehog forecast beats 95% of hedgehogs. The hedgehogs benefit in that the more extreme mistakes are balanced out. The result is that a team of hedgehogs might curtail each other’s excesses.\nA better angle of defence is that the real goal of forecasting is political impact or reputation, where only the confident survive. Hedgehogs are also good at avoiding distraction in high noise environments, which becomes apparent when examining the major weakness of foxes.\nTetlock put some of his experts through a scenario exercise. In this exercise, the high level forecasts were branched into a large number of sub-scenarios, for which probabilities had to be allocated to each. For example, when given the question of whether Canada would break up (this was around a time of the Quebec separatist referendum), combinations of outcomes involving separatist party success at elections, referendum results, economic downturns and levels of acrimony were presented, rather than the simple question of whether Quebec would succeed or not.\nAs has been show in the behavioural literature, when this type of task is undertaken, the likelihood of the components often sums to more than one. For the Quebec question, the initial probabilities added up to 1.0 for the basic question - as expected - but to an average of 1.58 for the branched scenarios. Foxes, however, suffered the most in this exercise, producing estimates that summed to 2.09.\nTo constrain this problem, it is common to end the branching exercise with a requirement to adjust the probabilities such that they add to one. But the foxes tended not to end up where they started for the simple question, with the branching followed by adjustment reducing their forecasting accuracy down to the level of hedgehogs.\nGiven the net result of the scenario exercise was to confuse foxes and fail to open the mind of hedgehogs, it could be suggested to be a low value exercise. For people advocating scenario development, pre-mortems and red teaming, the possibly deleterious effects on some forecasters needs to be considered.\nIn sum, it’s a grand book. There are some points where deeper analysis would have been handy - such as when he suggests there is disagreement from “psychologists who subscribe to the argument that fast-and-frugal heuristics-simple rules of thumb-perform as well as, or better than, more complex, effort demanding algorithms” without actually examining whether they are at odds with his findings of the forecasting superiority of foxes. But that’s a small niggle in a fine piece of work."
  },
  {
    "objectID": "posts/teaching-evolution-in-economics.html",
    "href": "posts/teaching-evolution-in-economics.html",
    "title": "Teaching evolution in economics",
    "section": "",
    "text": "At the start of the concluding chapter in Gad Saad’s The Evolutionary Bases of Consumption, Saad quotes Kenrick and Simpson as follows:\n\nNisbett introduced the series [on evolution and social cognition at the University of Michigan] by saying that he once thought every psychology department would need to hire an evolutionary psychologist, but he had changed his mind. Instead, Nisbett predicted that evolutionary theory will come to play the same role in psychology as it currently assumes in biology: “Not every psychologist will be an evolutionary psychologist, but every psychologist will be aware of the perspective and will have to address its explanations and constraints in his or her own work” (Nisbett, 1995, personal communication).\n\nI have similar thoughts about biology in economics. If, in 20 years time, there is a small but active research field at the intersection of economics and evolutionary biology, I will be disappointed. Rather, all economists should have the tools to assess whether evolutionary biology is relevant to their work. A unit or two in biology and evolutionary theory should form the basis of early economics education. Only then will economists have the required tools at their disposal."
  },
  {
    "objectID": "posts/tamed-by-an-influx-of-women.html",
    "href": "posts/tamed-by-an-influx-of-women.html",
    "title": "Tamed by an influx of women",
    "section": "",
    "text": "Perusing through some of my bookmarks in Steven Pinker’s The Better Angels of Our Nature: Why Violence Has Declined, I was reminded of the following passage. It’s worth sharing.\n\n\nThe West was eventually tamed not just by flinty-eyed marshals and hanging judges but by an influx of women. The Hollywood westerns’ “prim pretty schoolteacher[s] arriving in Roaring Gulch” captures a historical reality. Nature abhors a lopsided sex ratio, and women in eastern cities and farms eventually flowed westward along the sexual concentration gradient. Widows, spinsters, and young single women sought their fortunes in the marriage market, encouraged by the lonely men themselves and by municipal and commercial officials who became increasingly exasperated by the degeneracy of their western hellholes. As the women arrived, they used their bargaining position to transform the West into an environment better suited to their interests. They insisted that the men abandon their brawling and boozing for marriage and family life, encouraged the building of schools and churches, and shut down saloons, brothels, gambling dens, and other rivals for the men’s attention. Churches, with their coed membership, Sunday morning discipline, and glorification of norms on temperance, added institutional muscle to the women’s civilizing offensive. Today we guffaw at the Women’s Christian Temperance Union (with its ax-wielding tavern terrorist Carrie Nation) and at the Salvation Army, whose anthem, according to the satire, includes the lines “We never eat cookies ’cause cookies have yeast / And one little bite turns a man to a beast.” But the early feminists of the temperance movement were responding to the very real catastrophe of alcohol-fueled bloodbaths in male-dominated enclaves.\n\n\nThe idea that young men are civilized by women and marriage may seem as corny as Kansas in August, but it has become a commonplace of modern criminology. A famous study that tracked a thousand low-income Boston teenagers for forty-five years discovered that two factors predicted whether a delinquent would go on to avoid a life of crime: getting a stable job, and marrying a woman he cared about and supporting her and her children. The effect of marriage was substantial: three-quarters of the bachelors, but only a third of the husbands, went on to commit more crimes. This difference alone cannot tell us whether marriage keeps men away from crime or career criminals are less likely to get married, but the sociologists Robert Sampson, John Laub, and Christopher Wimer have shown that marriage really does seem to be a pacifying cause. When they held constant all the factors that typically push men into marriage, they found that actually getting married made a man less likely to commit crimes immediately thereafter. The causal pathway has been pithily explained by Johnny Cash: Because you’re mine, I walk the line.\n\n\nAn appreciation of the Civilizing Process in the American West and rural South helps to make sense of the American political landscape today. Many northern and coastal intellectuals are puzzled by the culture of their red state compatriots, with their embrace of guns, capital punishment, small government, evangelical Christianity, “family values,” and sexual propriety. Their opposite numbers are just as baffled by the blue staters’ timidity toward criminals and foreign enemies, their trust in government, their intellectualized secularism, and their tolerance of licentiousness. This so-called culture war, I suspect, is the product of a history in which white America took two different paths to civilization. The North is an extension of Europe and continued the court- and commerce-driven Civilizing Process that had been gathering momentum since the Middle Ages. The South and West preserved the culture of honor that sprang up in the anarchic parts of the growing country, balanced by their own civilizing forces of churches, families, and temperance."
  },
  {
    "objectID": "posts/susan-cains-quiet-the-power-of-introverts-in-a-world-that-cant-stop-talking.html",
    "href": "posts/susan-cains-quiet-the-power-of-introverts-in-a-world-that-cant-stop-talking.html",
    "title": "Susan Cain’s Quiet: The Power of Introverts in a World That Can’t Stop Talking",
    "section": "",
    "text": "I have mixed views about Susan Cain’s Quiet: The Power of Introverts in a World That Can’t Stop Talking.\nCain makes an important point that many of our environments, social structures and workplaces are unsuited to “introverts” (and possibly even humans in general). We could design more productive and inclusive workplaces, schools and organisations if we considered the spectrum of personality types who will work, live and learn in them.\nOn the flip side, Cain expanded the definition of introversion to include a host of positive attributes that wouldn’t normally (at least by me) be grouped with introversion. This led to a degree of cheer-leading for introverts that was somewhat off-putting (despite my own introverted nature). The last couple of chapters of the book also fall into evidence-free story-telling.\nBut to the good first. I enjoyed Cain’s filleting of open workplaces. Open plan workplaces or “activity-based working” are often dressed up as a means to seed creativity and collaboration, but they are more accurately described as a shift to lower floor space per employee to save costs. The evidence for increased collaboration or creativity is scant. Innovation may occur in teams, but it also requires quiet.\nCain suggests the trend toward these open workspaces is built on a mis-understanding of some of the classic examples of collaboration associated with the rise of the web. Yes, Linux and Wikipedia were built by teams, not individuals. But these people did not share offices or even countries. Regardless, the collaboration ideal was extended to our physical spaces.\nCain catalogues the research on the poor productivity in open workplaces. I had seen the following research before, but it is a great case study:\n\n… DeMarco and his colleague Timothy Lister devised a study called the Coding War Games. The purpose of the games was to identify the characteristics of the best and worst computer programmers; more than six hundred developers from ninety-two different companies participated. Each designed, coded, and tested a program, working in his normal office space during business hours. Each participant was also assigned a partner from the same company. The partners worked separately, however, without any communication, a feature of the games that turned out to be critical.\nWhen the results came in, they revealed an enormous performance gap. The best outperformed the worst by a 10:1 ratio. The top programmers were also about 2.5 times better than the median. When DeMarco and Lister tried to figure out what accounted for this astonishing range, the factors that you’d think would matter—such as years of experience, salary, even the time spent completing the work—had little correlation to outcome. Programmers with ten years’ experience did no better than those with two years. The half who performed above the median earned less than 10 percent more than the half below—even though they were almost twice as good. The programmers who turned in “zero-defect” work took slightly less, not more, time to complete the exercise than those who made mistakes.\nIt was a mystery with one intriguing clue: programmers from the same companies performed at more or less the same level, even though they hadn’t worked together. That’s because top performers overwhelmingly worked for companies that gave their workers the most privacy, personal space, control over their physical environments, and freedom from interruption. Sixty-two percent of the best performers said that their workspace was acceptably private, compared to only 19 percent of the worst performers; 76 percent of the worst performers but only 38 percent of the top performers said that people often interrupted them needlessly.\n\nOne pillar to the case for quiet spaces comes from how we build expertise. Work by Anders Ericsson, of deliberate practice fame, has identified studying alone or practicing in solitude as the prime way to gain skill. You need to be alone to engage in deliberate practice, as this allows you to go directly to the part that is challenging you. Open workspaces are a poor place to tackle challenging problems.\nCain also includes some interesting material on the extension of this “collaborative” space design to schooling. Children are increasingly schooled in pods as part of a shift to “cooperative learning”. We’re preparing children for the sub-optimal workplaces they are about to enter by replicating that sub-optimal environment in their schools. What is particularly problematic is that there is little opportunity in school to opt out, whereas adults have more opportunity to choose their workplace and shape their environment.\nOne thread in the book, which features in the opening, is that society is in the thrall of an “extrovert ideal”. Cain argues that we have become more interested in how people perceive us than the content of our character - a shift from a culture of character to one of personality. Self-help guides used to focus on concepts such as citizenship, duty, work, honour, morals, manners and integrity. They now focus on being magnetic, fascinating, attractive and energetic. Being quiet is now a problem.\nThis is particularly reflected in what we look for in leaders. People who talk more tend to be rated as more intelligent. Good presenters often get ahead. But talking more or presentation skills might be weak indicators of the actual capabilities you want in your leaders.\nCain briefly touches on the genetics of introversion. Unsurprisingly, as for every behavioural trait, introversion is heritable. Around 40% to 50% of the variation in introversion is due to differences in genes. Cain also hints at cross-racial differences in introversion, noting that the waves of emigrants to a new continent would have the more extroverted traits of world travellers.\nThe least satisfying element to the book was Cain’s definition of introvert. At times, Cain’s definition seemed to expand to capture all that is good. From a typical definition of being reserved, reflective, or interested in one’s own mental self, her definition includes everyone who is thoughtful, cerebral, willing to listen to others, and immune to the pull of wealth and fame. Introverts are needed to save us from climate change. (“Without people like you, we will, quite literally, drown.”) Extroverts, in contrast, are thoughtless risk seekers with no self control. Extroverts caused the global financial crisis.\nCain does note her broad definition of introvert in an appendix to the book, _A Note on the Words Introvert and Extrovert. _It would have helped me a lot if this note had been at the front (or if I had realised it was there before reading the book). There she clarifies that she is not using the standard definition of introversion captured by the well-established Big 5 taxonomy. She states that she is extending introversion to include people with “a cerebral nature, a rich inner life, a strong conscience, some degree of anxiety (especially shyness), and a risk-averse nature”.\nThese traits would normally be considered to relate to the other Big 5 traits of openness, conscientiousness and neuroticism. This is particularly confusing, as in parts of the book she talks about the other big 5 traits as separate concepts. Cain’s definition also appears broader than that used by Carl Jung and in the Myer-Briggs test, which seem to be her foundation. (Although never explicitly endorsed, I get the feeling that Cain is a Myer-Briggs advocate.)\nOnce the definition is expanded to include these other dimensions, it is hard to see how one third of the population can be described as introverts. It also means that many parts of the book feel more an ode to conscientiousness, and possibly even intelligence, than to introversion.\nThis was most stark in the chapter on the differences between Asians and Americans. Cain attributes Asian achievement - such as high scores in international tests and their superior academic results - to the higher introversion of Asians. There is not one mention of the higher conscientiousness of East Asians, nor their higher IQ scores. Instead these seem bundled into the introvert basket of traits.\nI also struggled with the final two substantive chapters of the book - on relationships and children. There Cain shifts from an approach generally built on research to one that is little more than storytelling. The chapters are full of unsourced statements or recommendations. For instance, she recommends that you gradually introduce your kids to new situations. This supposedly produces more confident kids than the alternatives of overprotection or pushing too hard, contrasting somewhat with the established literature on the lack of effect of parents.\n*Disclosure of interest: Here are the percentiles for the last time I did a Big 5 test. I’m not far from Cain’s introvert ideal (possibly a touch low on neuroticism):\n\nOpenness: 88\nConscientiousness: 80\nExtroversion: 29\nAgreeableness: 51\nNeuroticism: 48"
  },
  {
    "objectID": "posts/strength-by-outbreeding.html",
    "href": "posts/strength-by-outbreeding.html",
    "title": "Strength by outbreeding",
    "section": "",
    "text": "I am reading Robert Trivers’s The Folly of Fools: The Logic of Deceit and Self-Deception in Human Life. I will review in the next few days, but these passages on the benefits of outbreeding are particularly interesting:\n\nUS history has many virtues, among which is the fact that the US population is reconstituted every generation through a roughly 10 percent admixture by external immigration from throughout the world. Although in its history rules of immigration have favored some groups over others, all have had some opportunity. And with illegal immigration, such opportunities are sometimes greatly enhanced. From a biological standpoint, the resulting outbreeding (insofar as it takes place, as it inevitably must) will tend to be genetically beneficial. The US population is perpetually heterogeneous, about to be infused with 10 percent more genes from around the world. …\n\n\nThe later history of African Americans was in some ways more dreadful than under slavery, since not counting as property they could be hanged or “lynched” by the thousands as a form of social control. Nevertheless, the subpopulation had become strong enough by the middle of the twentieth century to begin a political and social movement that led to eventual legal liberation, and with this yoke lifted, the intrinsic benefits of strong outbreeding associated with strong selection has produced a vibrant and powerful subgroup. African Americans are the melting-pot population par excellence in the United States, genetically roughly 25 percent European in origin, 70 percent African, and the remainder Amerindian and Chinese. At the same time, social policies such as the war on drugs amount to a war on lower-class African Americans, greatly increasing incarceration rates, with destructive effects on their communities. So the racist attack continues, but in the long run it can only strengthen the biological power of its target."
  },
  {
    "objectID": "posts/sports-team-ownership-as-conspicuous-consumption.html",
    "href": "posts/sports-team-ownership-as-conspicuous-consumption.html",
    "title": "Sports team ownership as conspicuous consumption",
    "section": "",
    "text": "Most of Malcolm Gladwell’s appearances in this blog involve me complaining about his various writings (such as my review of Outliers), but his recent piece on the NBA lock-out is a gem (HT Rob Brooks). I wish I could write like that.\nGladwell argues that owning a sports team is akin to owning a painting - there are benefits beyond the monetary. He writes:\n\nPro sports teams are a lot like works of art. Forbes magazine annually estimates the value of every professional franchise, based on standard financial metrics like operating expenses, ticket sales, revenue, and physical assets like stadiums. When sports teams change hands, however, the actual sales price is invariably higher. … Forbes is evaluating franchises strictly as businesses. But they are being bought by people who care passionately about sports — and the $90 million premium that the Warriors’ new owners were willing to pay represents the psychic benefit of owning a sports team. If that seems like a lot, it shouldn’t. There aren’t many NBA franchises out there, and they are very beautiful.\nThe big difference between art and sports, of course, is that art collectors are honest about psychic benefits. They do not wake up one day, pretend that looking at a Van Gogh leaves them cold, and demand a $27 million refund from their art dealer. But that is exactly what the NBA owners are doing. They are indulging in the fantasy that what they run are ordinary businesses — when they never were. And they are asking us to believe that these “businesses” lose money. But of course an owner is only losing money if he values the psychic benefits of owning an NBA franchise at zero — and if you value psychic benefits at zero, then you shouldn’t own an NBA franchise in the first place. You should sell your “business” — at what is sure to be a healthy premium — to someone who actually likes basketball.\n\nHaving praised Gladwell, I should note that he does miss an obvious explanation for the “psychic benefits” of owning a sports team (although most of the literature on psychic benefits also misses this element). Owning a sports team is a form of conspicuous consumption. It is a signal of wealth.\nThe additional cost of an NBA team over its value as a business is a nice illustration of an important element of a reliable signal - waste. It is only a truly wealthy man who can afford to tie up $90 million of capital in the premium he pays for the Warriors. If the money paid for the team represented a prudent investment, someone of less wealth might buy it (including by convincing a lender to give them the money). That potential return on $90 million invested in productive uses is the amount of waste required to fend off the next most wealthy man in the bidding for the team.\nSo, now that the NBA owners are complaining, we can assume that they are not as rich as their signal might suggest. I have an alternative to Gladwell’s recommendation. Instead of selling the teams to someone who actually likes basketball, the owners should sell their “business” to someone who actually has the wealth representative of the signal they are making."
  },
  {
    "objectID": "posts/spontaneous-order.html",
    "href": "posts/spontaneous-order.html",
    "title": "Spontaneous order",
    "section": "",
    "text": "Another interesting old paper off my reading pile has been Robert Sugden’s Spontaneous Order.\nSugden asks us to picture a scenario where you are travelling down a road towards another car. You have two alternatives - move to the left or right. The other car has the same choice. If you pick the same, you pass safely. A game theoretic analysis tells us that there are three Nash equilibria – that is, three sets of strategies that neither driver would have incentive to deviate from (or put yet another way, each driver’s strategy is a best response to the other driver’s strategy). The three are: both cars move left; both cars move right; or each car moves left or right with 50 per cent probability.\nBut which strategy do they choose? Sugden writes:\n\n[T]he ultimate objective of game theory is to show that rational analysis uniquely prescribes a particular strategy for each player in a game. It is as if each player sits in a room by himself, knowing nothing about the other player except his utility function and that he is rational, and knowing nothing about how the game may have been played by other people. Each player must decide what to do, applying unlimited powers of rationality to this severely restricted information and to nothing else. …\nOne of the major achievements of Thomas Schelling … has been to show that games like Chicken cannot be “solved” in this way. The ideally rational but completely inexperienced players of classical game theory would find they had insufficient data to determine what they should do. In contrast, ordinary people with limited rationality but some degree of experience and imagination might have no difficulty in coordinating their behavior. On this view, the program of classical game theory is a blind alley: it requires us to throw away the information that players need if they are to work out what it is rational for them to do. …\nIf we are to coordinate our behavior, as we both wish to do, we must rely on some shared notion of prominence. Our common experience of English driving provides the clue we need. Steering left is prominent because it is common knowledge that this is what people generally do: we have each observed this, we can each assume the other has observed it, and so on.\n\nBut this raises a new question. Where does this experience come from? Why steering to the left?\n\nIf we are to explain why one convention is found rather than another, it is not very useful to start from a comparison between a world in which everyone follows one convention and a world in which everyone follows the other: either of these worlds, once achieved, would be self-perpetuating. Instead we must consider the process by which conventions evolve. More particularly, we must look at how they start to evolve. Once a convention has started to evolve - once significantly more people are following it than are following any other convention - a self-reinforcing process is in motion. The conventions that establish themselves will be the ones that can take root (biological metaphors are almost unavoidable) most quickly in a convention-free world.\n\nI have always been a fan of arguments that recognise the path dependent nature of evolution. There are many possible outcomes, but various historical contingencies shape the result.\n\nA convention can start to evolve as soon as some people believe that other people are following it. But what gives rise to this initial belief? One possibility is that the same forces are at work as enable people to coordinate their actions without communication in unrepeated games. Some forms of coordination are more prominent than others, and people have a prior expectation of finding the most prominent ones. But, I have argued, prominence is largely a matter of common experience. The implication is that conventions may spread by analogy from one context to another. If it is a matter of common knowledge that a particular convention is followed in one situation, then that convention acquires prominence for other, analogous situations. For example: on my journey to work there is a narrow bridge, not wide enough for two vehicles to pass. If two drivers approach from opposite directions, which of them should give way? Coming on this problem for the first time, my prior expectation was when the drivers came into view of one another, whoever was closer to the bridge would be given the right of way. This expectation-which proved correct-was based on an analogy with the “first come, first served” principle.\n\nThis line of reasoning has some relevance to arguments about the evolution of cooperation.  In A Cooperative Species, Bowles and Gintis note that where there are repeated interactions between people, there can be infinitely many equilibria. This concept is known as the folk theorem. Due to this possibility, Bowles and Gintis suggest that it is unlikely that cooperation could evolve in such a situation purely by self-interested rationality. How would the parties reach equilibrium, and why would it be the cooperative one? But as Sugden’s argument suggests, if certain equilibria have prominence, possibly by historical accident and the shared experience of the actors, there may be more scope for cooperation. Humans are not perfectly rational beings solving a problem from scratch. We are evolved creatures with a history and experience."
  },
  {
    "objectID": "posts/some-perspectives-on-elinor-ostrom.html",
    "href": "posts/some-perspectives-on-elinor-ostrom.html",
    "title": "Some perspectives on Elinor Ostrom",
    "section": "",
    "text": "Below are three passages that capture a small part of the evolutionary flavour of the now late Elinor Ostrom’s work.\nFrom David Sloan Wilson:\n\nLin’s work was like a breath of fresh air compared to the forbidding world of neoclassical economics, which was top-heavy with theory and required assumptions about human preferences and abilities that were manifestly unrealistic. In contrast, Lin’s work was empirically well grounded and her eight design principles were highly congruent with the evolutionary dynamics of cooperation in all species and the biocultural evolution of our own species. Her work might have originated within the field of political science and been applied primarily to common-pool resource groups, but I realized that it could be generalized in two senses. First, it could be placed on a more general theoretical foundation suitable for all human-related disciplines. Second, it could be applied in a practical sense to most human endeavors that involve working in groups to achieve common goals—which means most human endeavors.\n\nFrom Henry Farrell’s response to Ostrom’s Sveriges Riksbank Prize in Economic Sciences in Memory of Alfred Nobel:\n\nHer work implies that both pure marketization and top-down government control can have badly adverse consequences for resource management, because they rob individuals of the capacity to govern themselves, and because they both lead to the depletion of important forms of local collective knowledge. Alex Tabarrok is right to see something Hayekian in Ostrom’s arguments – but it is Hayek against Hayek. Ostrom stresses repeatedly that even the best functioning markets are undergirded by an array of collective institutions which order people’s market interactions, and that in the absence of such rules, self interested behaviour will have highly adverse consequences.\n\nFrom Ostrom’s How do Institutions for Collective Action Evolve (pdf):\n\nIt would be naive to assume that any evolutionary process will always lead to better outcomes. In biological systems, competition among populations of diverse species did lead to the weeding out of many individuals over time that were out-competed for mates and food in a given environment. Evolutionary processes can also lead to equilibria imposing higher costs on some species and eliminating others. The huge investment made by peacocks in their tails is one example. Thus, one should not expect that all locally governed systems will eventually find effective rule configurations. Some will experiment with rule configurations that are far from optimal. And, if the leaders of these systems are somehow advantaged by these rules, they may resist any effort to change."
  },
  {
    "objectID": "posts/social-mobility-across-the-generations.html",
    "href": "posts/social-mobility-across-the-generations.html",
    "title": "Social mobility across the generations",
    "section": "",
    "text": "The Economist reports on Greg Clark’s work using surnames to track social mobility. I have posted about this work before, but The Economist piece makes an important point. When tracking mobility across generations, you cannot simply extrapolate the results of a single generation into the future.\n\nCorak’s work draws on recent studies that compare income levels between just two generations: fathers and sons. That is out of necessity; good data covering three or more generations are scarce. But reliance on limited data could lead to overestimates of social mobility.\nGregory Clark, an economist at the University of California, Davis, notes that across a single generation some children of rich parents are bound to suffer random episodes of bad luck. Others will choose low-pay jobs for idiosyncratic reasons, like a wish to do charitable work. Such statistical noise makes society look more changeable than it is. Extrapolating the resulting mobility rates across many generations gives a misleadingly sunny view of long-term equality of opportunity. Mr Clark suggests that family history has large effects that persist for much greater spans of time. Fathers matter, but so do grandfathers and great-grandfathers. Indeed, it may take as long as 300-500 years for high- and low-status families to produce descendants with equal chances of being in various parts of the income spectrum.\n\nIn line with his earlier work, Clark fingers a genetic basis to the finding.\n\nMr Clark’s conclusion is that the underlying rate of social mobility is both low and surprisingly constant across countries and eras: the introduction of universal secondary education scarcely affects intergenerational mobility rates in Britain, for example. This consistency, he suggests, shows that low mobility may be down to differences in underlying “social competence”. Such competence is potentially heritable and is reinforced by the human tendency to mate with partners of similar traits and ability.\n\nThis work reminds me of an article in which Sam Bowles and Herb Gintis explore the inheritance of inequality. They estimated that intelligence is responsible for a correlation between parent and child income of 0.01, with total correlation due to genetic factors of around 0.12. I played with the results to show that a larger proportion of the observed variation could be explained by genetics if you tweak some of the assumptions. However, Clark’s work shows that a larger limiting factor on their result was a single generation comparison in their analysis.\nPostscript: A debate followed the Economist article, with contributions by Miles Corak, Fransicso Ferreira, Greg Clark (and again), and Jason Long. They are all worth reading."
  },
  {
    "objectID": "posts/social-darwinism-is-back.html",
    "href": "posts/social-darwinism-is-back.html",
    "title": "Social Darwinism is back",
    "section": "",
    "text": "A couple of weeks ago I flagged the Journal of Economic Behavior and Organization’s (JEBO) special issue Evolution as a General Theoretical Framework for Economics and Public Policy. I thought I would open my commentary on the special issue by examining one of the popular press articles that accompanied its launch, a piece by David Sloan Wilson called A good social Darwinism.\nA couple of years ago Wilson wrote a series of posts at his blog Evolution for Everyone called Economics and Evolution as Different Paradigms. I wrote a series of posts in response to Wilson (here, here, here and here) setting out my issues with Wilson’s approach, particularly the caricatured version of economics. I also considered that Wilson sold the potential for evolutionary biology in economics a bit short, and I was somewhat pessimistic about what might come out of the Evolution Institute (This JEBO special issue has seen the Evolution Institute exceed my expectations.)\nWilson’s latest article continues in a similar vein, with a marginally more subtle perspective on economics, although still missing some of the richness. He starts by painting economics as torn between two ideas: Adam Smith’s “invisible hand” that could lead to benevolent outcomes despite no-one intending them; and Smith’s fear of naked self interest. By going too far in either direction, there can be significant costs, which Wilson suggests includes the Industrial Revolution (surely not) and the Great Depression on the one hand, and Communism on the other.\nWhat Wilson sees as lacking is the ability of economics to navigate between these two extremes. He points to some of the attempts of economics to traverse this middle course, in which he includes Walrasian general equilibrium and the development of homo economicus. These concepts survived critiques by the likes of Thorstein Veblen to be adopted by, among others, Milton Friedman (I addressed Wilson’s views on Friedman’s The Methodology of Positive Economics in an earlier post).\nWilson then suggests that evolutionary theory can offer something here (concur), including a less Newtonian view of the world. The most interesting part of the article, however, is Wilson’s desire to rehabilitate the idea of the invisible hand through multi-level selection - that is, natural selection occurring at levels higher than the individual. While lower level units of selection (say, the gene) do not have the higher level units in mind, selection of higher level units shapes the traits of the lower level units such that they contribute to the good of the group. Wilson suggests that the invisible hand can operate in human groups as selection at the level of groups has shaped us that way.\nAs an example of this, Wilson points to the work of Elinor Ostrom, sadly unknown among most of the economics profession until her Nobel Memorial Prize in Economic Sciences in 2009. Ostrom did fantastic work on common-pool resources (such as fisheries, farm land or water) and showed that, under certain conditions, institutional arrangements could emerge without either private ownership or government intervention. Wilson then notes work (the subject of one of the JEBO papers) which suggests that these design principles can be expanded to a broader range of groups than just those managing common-pool resources. As a result, Wilson suggests that an evolutionary approach can offer a basis for “steering an intelligent middle course between extreme laissez-faire and ham-fisted regulation that have proven so disastrous in the past.”\nBut this is where Wilson misses one important interpretation of Ostrom’s work. Ostrom’s work was well-known and highly regarded before her 2009 prize by some economists who researched public choice and institutional development, many of whom were libertarians (and from the Austrian school of economics in particular). The reason they cherished Ostrom’s work was that it showed that the tragedy of the commons does not always require a solution to be imposed from above. Decentralised groups develop the rules that allow a solution to the commons problem to emerge cooperatively through voluntary association. As such, Ostrom’s work could be argued to support the laissez-faire end of the spectrum (although Ostrom was not a libertarian).\nWhile it is possible to argue that it is the group that is autonomous, not the individual, another problem arises where the specific conditions that Ostrom identified are not met, such as clear group boundaries. For many economic questions, those group boundaries simply do not exist, leaving us back where we started in trying to steer the middle ground.\nIt is also not clear that evolutionary theory takes us to the middle. An evolutionary view of the humans in government and bureaucracies may lead to a rather pessimistic view of the ability (or motivation) of government to address “market failures”. The recent meeting of the Mont Pelerin Society (of which none other than Milton Friedman was a founder and past president) on Evolution, the Human Sciences and Liberty saw many make the argument that human nature points to a free society as the optimal state. Paul Rubin wrote the excellent Darwinian Politics: The Evolutionary Origins of Freedom as an argument that liberal society is the best fit for our evolved human natures. It is fair to say evolutionary arguments can and have been used to support all points on the political spectrum.\nBut let me close with some praise for Wilson. Although I find many specific points to disagree with him, from his interpretation of the invisible hand to his use of multi-level selection arguments, I am somewhat in awe of his productivity and energy. My initial pessimism about the Evolution Institute is turning into optimism. Even though Wilson has his perspectives, he seems to have a drive to bring interesting people and ideas together to address some economic questions that could truly benefit from an evolutionary approach.\nMy series of posts on the Journal of Economic Behavior & Organization special issue, Evolution as a General Theoretical Framework for Economics and Public Policy, are as follows:\n\nSocial Darwinism is back (this post) - a post on one of the popular press articles that accompanied the special issue, a piece by David Sloan Wilson called A good social Darwinism.\nFour reasons why evolutionary theory might not add value to economics - a post on David Sloan Wilson and John Gowdy’s article Evolution as a general theoretical framework for economics and public policy\nEconomic cosmology - The rational egotistical individual - a post on John Gowdy and colleagues’ article Economic cosmology and the evolutionary challenge \nEconomic cosmology - The invisible hand - a second post on Economic cosmology and the evolutionary challenge \nEconomic cosmology - Equilibrium - a third post on Economic cosmology and the evolutionary challenge\nDesign principles for the efficacy of groups - a post of David Sloan Wilson, Elinor Ostrom and Michael E. Cox’s article Generalizing the core design principles for the efficacy of groups"
  },
  {
    "objectID": "posts/simple-heuristics-that-make-us-smart.html",
    "href": "posts/simple-heuristics-that-make-us-smart.html",
    "title": "Gerd Gigerenzer, Peter Todd and the ABC Research Group’s Simple Heuristics That Make Us Smart",
    "section": "",
    "text": "I have recommended Gerd Gigerenzer, Peter Todd and the ABC Research Group’s Simple Heuristics That Make Us Smart enough times on this blog that I figured it was time to post a synopsis or review.\nAfter re-reading it for the first time in five or so years, this book will still be high on my recommended reading list. It provides a nice contrast to the increasing use of complex machine learning algorithms for decision making, although it is that same increasing use that makes some parts of the book are seem a touch dated.\nThe crux of the book is that much human (or other animal) decision making is based on fast and frugal heuristics. These heuristics are fast in that they do not rely on heavy computation, and frugal in that they only search for or use some of the available information.\nImportantly, fast and frugal heuristics do not simply trade-off speed for accuracy. They can be both fast and accurate as the tradeoff is between generality versus specificity. The simplicity of fast and frugal heuristics allows them to be robust in the face of environmental change and generalise well to new situations, leading to more accurate predictions for new data than a complex, information-guzzling strategy. The heuristics avoid the problem of overfitting as they don’t assume every detail to be of utmost relevance, and tend to ignore the noise in many cues by looking for the cues that swamp all others.\nThese fast and frugal heuristics often fail the test of logical coherence, a point often made in the heuristics and biases program kicked off by Kahneman and Tversky. But as Gigerenzer and Todd argue in the opening chapter, pursuing rationality of this nature as an ideal is misguided, as many of our forms of reasoning are powerful and accurate despite not being logically coherent. The function of heuristics is not to be coherent. Their function is to make reasonable adaptive inference with limited time and knowledge.\nAs a result, Gigerenzer and Todd argue that we should replace the coherence criteria with an assessment of real-world functionality. Heuristics are the way the mind takes advantage of the structure of the environment. They are not unreliable aids used by humans despite their inferior performance.\nThis assessment of the real-world functionality is also not a general assessment. Heuristics will tend to be domain specific solutions, which means that “ecological rationality” is not simply a feature of the heuristic, but a result of the interaction between the heuristic and the environment.\nBounded rationality\nIf you have read much Gigerenzer you will have seen his desire to make clear what bounded rationality actually is.\nBounded rationality is often equated with decision making under constraints (particularly in economics). Instead of having perfect foresight, information must be obtained through search. Search is conducted until the costs of search balance the benefits of the additional information.\nOne of the themes of the first chapter is mocking the idea that decision making under constraints brings us closer to a model of human decision making. Gigerenzer and Todd draw on the example of Charles Darwin, who created a list of the pros and cons of marriage to assist his decision. This unconstrained optimisation problem is difficult. How do you balance children and the charms of female chit chat against the conversation of clever men at clubs?\nBut suppose a constrained Darwin is starting this list from scratch. He already has two reasons for marriage. Should he try to find another? To understand whether he should continue his search he effectively needs to know the costs and benefits of all the possible third options and understand how each would affect his final decision. He effectively needs to know and consider more than the unconstrained man. You could even go the next order of consideration and look at the costs and benefits of all the cost and benefit calculations, and so on. Infinite regress.\nSo rather than bounded rationality being decision making under constraints, Gigerenzer argues for something closer to Herbert Simon’s conception, where bounded rationality is effectively adaptive decision making. The mind is computationally constrained, and uses approximations to achieve most tasks as optimal solutions often do not exist or are not tractable (think the relatively simple world of chess). The effectiveness of this approximation is then assessed in the environment in which the mind makes the decisions, resulting in what Gigerenzer terms the “ecological rationality” of the decision.\nThe recognition heuristic\nThe first fast and frugal heuristic to be examined in detail in the book is the recognition heuristic. Goldstein and Gigerenzer (the authors of that chapter) define the  recognition heuristic as “If one of two objects is recognized and the other is not, then infer that the recognized object has the higher value.”\nThe recognition heuristic is frugal as it requires a lack of knowledge to work - a failure to recognise one of the alternatives. The lack of computation required to apply it points to its speed. Goldstein and Gigerenzer argue that the recognition heuristic is a good model for how people actually choose, and present evidence that it is often applied despite conflicting or additional information being available.\nRecognition is different from the concept of “availability” developed by Tversky and Kahneman. The availability heuristic works by drawing on the most immediate or recent examples when making an evaluation.  Availability refers to the availability of terms or concepts in memory, whereas recognition relies on the differences between things in and out of memory.\nAs an example application (and success) of the recognition heuristic, American and German students were asked to compare pairs of German or American cities and select the larger. American students comparing pairs of American cities did worse than Germans on those same American cities - the Americans knew too much to apply the recognition heuristic. The Americans do as well comparing less familiar German cities as they do American cities.\nThe success of the recognition heuristic results in what could be described as a “less is more” effect. There are situations where decisions based on missing information can be more accurate than those made with more knowledge. There is information implicit in the failure to recognise something.\nA second chapter on the recognition heuristic by Borges and friends involves the authors using the recognition heuristic to guide their stock market purchases. They surveyed US and German experts and laypeople about US and German shares and invested based on those that were recognised.\nOverall, the authors’ returns beat the aggregate market indices. A German share portfolio based on the recognition of any of the US and German experts or US and German laypeople outperformed the market indices, as did the US stock portfolio based on recognition by Germans. The only group for which recognition delivered lower returns was the US portfolio based on US expert or layperson recognition.\nBorges and friends did note that this was a one-off experiment in a bull market, so there is a question of whether it would generalise to other market conditions (or even if it was more than a stroke of luck). But the next chapter took the question of the robustness of simple heuristics somewhat more seriously.\nThe competition\nOne of the more interesting chapters in the book is a contest across a terrain of 20 datasets between a fast and frugal heuristic, “take-the-best”, and a couple of other approaches, including the more computationally intensive multiple linear regression. In each of these 20 contests, the competitors were tasked with selecting for all pairs of options which has the highest value. This includes predicting which of two schools had the highest drop out rates, which stretches of highway had the highest accident rates, or which people had the highest body fat percentage.\nThe take-the-best heuristic works as follows: Choose the cue most likely to distinguish correctly between the two. If the two choices differ on that cue, select the one with the highest value, and end the search. If they are the same, move to the cue with the next highest validity and repeat.\nFor example, suppose you are comparing the size of two German cities and the best predictor (cue) of size is whether they are a capital city. If neither is a capital city, you then move to the next best cue of whether they have a soccer team. If one does and the other doesn’t, select the city with the soccer team as being the larger.\nThe general story is that in terms of fitting the full dataset, take-the-best performs well but is narrowly beaten by multiple regression (75% to 77% - although multiple regression was only fed cue direction, not quantitative variables). The closeness across the range of datasets suggests that the power of take the best is not just restricted to one environment.\nThe story changes more in favour of take-the-best when the assessment shifts to prediction out-of-sample, with multiple regression suffering a severe penalty. Regression accuracy dropped to 68%, whereas take-the-best dropped less to 71%.\nThere was a model in the competition - the minimalist - which only considered a randomly chosen cue and seeing if it points in one direction or the other. If so, select that choice, otherwise select another cue. The performance of the minimalist suggested frugality can be pushed too far, although it did perform only 3 percentage points below regression in out-of-sample prediction.\nThe results of the challenge suggests that take-the-best tends not to sacrifice accuracy for its frugality. The relative performance of take-the-best is particularly strong when there is a low number of training examples, with regression having less chance of overfitting in larger environments. Regression tended to perform relatively worse when there were less examples per cue. One point that favoured take-the-best is that the trial didn’t have many large environments. Only two had more than 100 examples, and many had between 10 and 30.\nThe restriction of regression to use cue direction rather than the quantitative variable also dampened its effectiveness. If able to use quantitative predictors, regression tied take the best on 76% out of sample, even though take-the-best doesn’t use these quantitative values. There was effectively no penalty for the frugality.\nA later chapter added to the competition computationally expensive Bayesian models. Bayesians networks won the competition on out-of-sample testing by three percentage points over take-the-best. Again, take-the-best did best relatively when there were small numbers of examples. The more frugal naive Bayes also did pretty well - falling somewhere between the two approaches.\nThe results suggest that each approach has its place. Use fast and frugal approaches when you need to be quick with low numbers of examples, and use Bayesian approaches when have time, computational power and knowledge. This is where some of the examples start to feel dated when the size of the datasets in many domains is rapidly growing in combination with cheaper computational power.\nThis dated feel is even more apparent in the competition between another heuristic, categorisation by elimination, and neural networks across 3 datasets.\nCategorisation by elimination is a classification algorithm that walks through examples and cues, starting from the cue with the highest probability of success. If the example can be categorised, categorise it and move to the next example. If not, move to the next cue, with possible categories limited to those possible given earlier cues. Repeat until classified.\nIn measured performance, categorisation by elimination was only a few percentage points behind neural networks, although the datasets contained only 150, 178 and 8124 examples. The performance of neural networks also capped out at 100% on the largest mushroom dataset (not bad when picking what should eat and consequences) and 94 and 96% on the other two. There wasn’t much room for a larger victory.\nA couple of the chapters are also just a touch too keen to show the effectiveness of the simple heuristics. This was one such case. An additional competition was run giving neural networks only a limited number of cues, in which case its performance plunges. But these cues were chosen based on the number of cues used by categorisation by elimination, rather than a random selection.\nThe 37% rule\nOne interesting chapter is on the “secretary problem” and the resulting 37% rule. The basic idea is that you have a series of candidates you are interviewing for the role of secretary (this conception of the problem spread in the 1950s). You view each candidate one by one and must decide on the spot if you will stop your search there and hire the candidate in front of you. If you move to the next candidate, the past candidate is gone forever.\nTo maximise your probability of finding the best secretary, you should view 37% of the candidates without making any choice, and then accept the next candidate who is better than all you have seen to date. This rule gives (coincidentally) a 37% chance of ending up with the best mate.\nBut this rule is not without risks. If the best candidate was in that first 37%, you will end up with the last person you see, effectively a random person from the population. So there is effectively a 37% chance of a random choice. Because of that random choice, the 37% rule leaves you with a 9% chance you will end up with someone in the bottom quartile.\nBut what if, like most people, you have a degree of risk aversion - particularly if you are applying the rule to serious questions such as mate choice. Suppose there are 100 candidates and you want someone out of the top 10%. In that case you only want to look at the first 14% of candidates and choose the next candidate who is better than all previous candidates. That gives you an 83% chance of a top 10% candidate. If you will settle for the top 25%, you only need look at the first 7% for a 92% chance of getting someone in the top quartile.\nIn larger populations, you need to look at even less. With 1000 people, you need only look at only 3% of the candidates to maximise chance of top 10% at 97% probability. For a top 25% mate, you should only check out 1 to 2%.\nThe net result is that the 37% rule sets aspirations too high unless you will settle for nothing but the best. It is less robust than other rules.\nThis exploration points to the potential for a simple search heuristic. Try a dozen will generally outperform the 37% rule across most population sizes for getting a good but not perfect mate. Try a few dozen is a great rule for someone in New York who wants close to the best.\nThen there is the issue that the success of the 37% rule depends on your own value. On finding the mate you will finally propose to, what is the probability that the two-sided choice will end up with them saying yes? In domains such as mate choice, only one or two people could get away with applying that rule - and that leads to a whole new range of considerations.\nOdds and ends\nThe book is generally interesting throughout. Here are a few odds and ends:\n\nOne chapter argues that the hindsight bias is the product of fast and frugal approach to recalling decisions. We update knowledge when it is received. If we cannot recall the original decision, we can approximate it by going through the same process as used to generate the decision last time. But if we have updated our knowledge, we get a new answer.\nAs mentioned, some chapters are a bit out of date. One chapter is on using heuristics to predict intention from motion. I expect neural networks will likely be in another league on domains such as this compared to when the book was written.\nAnother chapter is on investment in offspring. Heuristics such as invest in the oldest do almost as well as the optimal investment rules developed by Becker, despite their lack of relative complexity. The best rule for a particular time will depend on the harshness of the environment."
  },
  {
    "objectID": "posts/silvers-the-signal-and-the-noise.html",
    "href": "posts/silvers-the-signal-and-the-noise.html",
    "title": "Silver’s The Signal and the Noise",
    "section": "",
    "text": "I’d recommend Nate Silver’s The Signal and the Noise: Why So Many Predictions Fail but Some Don’t to anyone looking for a layman’s tour of applied statistics. It is not a “how to” book, although there are plenty of principles (and suggestions to be humble) worth following. It’s also not a book that gets too deep into any subject area, so for those areas I was familiar with, there were no surprises. But where I wasn’t, I generally enjoyed it.\nThe section of the book I enjoyed most was the section on weather forecasting. As a start, weather forecasting is getting a lot better. In the early 1970s, a temperature forecast by the US National Weather Service three days in advance tended to miss the actual temperature by around 6 degrees F. Today, they miss by around 3.5 degrees F. For hurricane forecasts 25 years ago, the prediction of where a hurricane would make landfall three days in advance had an average miss of 350 miles. Today it is marginally over 100 miles. For Hurricane Katrina, the prediction of landfall at New Orleans was made five days in advance.\nIt’s cool to see that there is progress in forecasting in complex systems such as the weather. But the current limits to predictive ability are also interesting. Silver presents a chart comparing the accuracy of temperature forecasts from three sources: climate averages, persistence (the weather tomorrow will be the same as today) and commercial forecasts. Predicting the climate average for any number of days in advance of today results in an error of around 7 degrees F. Persistence outperforms the climate average for the next day, but for more than two days ahead, you are better off assuming the climate average. Finally, commercial forecasts start out as much superior to assuming the climate average, with an average error of around 3 degrees F for the next day. But by day 8, the commercial forecasts barely beat the climate average. After day 10, the commercial forecasts are worse.\nWhat is interesting is that the commercial forecasts rapidly deteriorate to a point where they display negative skill after 10 days. You could simply look to historical averages for a better indication. This is caused by feedbacks in the computer program, which start to build on themselves. With the programs highly sensitive to the initial conditions, the noise ends up dominating (Silver gives a useful simple synopsis of chaos theory).\nThe weather chapter provides an interesting contrast to the chapter on economics. Economic forecasting might be considered similar to weather - a dynamic system sensitive to initial conditions - but with a markedly different record of improvement. And not only is economic forecasting demonstrating little improvement, but the confidence with which economic forecasts are made is far in excess of what they deserve.\nA couple of issues underlie this. First, economics does not have the same fundamental laws underlying it (and as I often argue in posts in this blog, those that exist are too often ignored). But also, the incentives are different. Weather forecasting is tested day after day, so people note errors. There is a small degree of bias where there is something at stake, such as some forecasters exaggerating the probability of rain when it is unlikely to occur, because people remember rain ruining an event when the forecast was for sun. And local TV weather presenters push this wet bias even further. But in general, performance is not too bad.\nFor economics, predictions are rarely tested and often made years in advance. There are strong incentives to herd where being wrong and the only one who is wrong is costly. There are also incentives to make outlandish claims when people will forget all the other times you are wrong (keep predicting the financial crisis, and you will be right sooner or later). It’s hardly an environment where accuracy is the best bet.\nOtherwise, the chapters on disease and chess were also excellent. I wasn’t a big fan of the climate change chapter (neither was Michael Mann) and I still don’t understand why Americans like baseball."
  },
  {
    "objectID": "posts/should-we-tax-education.html",
    "href": "posts/should-we-tax-education.html",
    "title": "Should we tax education?",
    "section": "",
    "text": "Over the last few weeks, Bryan Caplan of Econlog has engaged in a debate with his former teacher Bill Dickens over the social value of education. Brian’s position is that  education is largely used for signalling rather than skill acquisition. While some signalling is good (matches students and employers), it is privately optimal to far exceed the social optimal. This excessive signalling consumes resources for limited social return, so we should stop subsidising it and possibly consider taxing it. I find myself leaning towards Brian’s position - particularly in relation to senior high school and university/college education.\nA recent Economist daily chart reflects the waste from the subsidisation of education, with over 20 per cent of university graduates in the OECD working in low-skilled jobs. In the United States and Canada, it is over 30 per cent. Some of this is choice, and you would expect that group to be the first to cut back on education if they were required to pay the full cost.\nThe element of Brian’s position that interests me is what the signalling environment would look like in the absence of subsidised education. Education is not only costly in terms of money, but it also consumes a large amount of time. Is the time commitment required for an accurate signal? Once education is no longer subsidised, will other signals that are less intensive in the time required emerge? IQ can be determined through tests. How long does someone need to engage in an education to show persistence, courteousness and reliability? Could an intense one-year course substitute?\nI would also expect to see a distinction emerge between those courses with a larger signalling element and those in which skills are genuinely acquired. This would flow on to the costs (salaries) associated with those skills.\nThe last things, and my hope, is that it would give us cheaper plumbers. That is a touch flippant, but if fewer people move into education as the price has increased, perhaps some of them will get those other unsubsidised skills and trades and push the price of them down."
  },
  {
    "objectID": "posts/sexual-selection-on-the-american-frontier.html",
    "href": "posts/sexual-selection-on-the-american-frontier.html",
    "title": "Sexual selection on the American frontier",
    "section": "",
    "text": "It seems obvious that having multiple wives is a good thing for the fitness of a man. Similarly, having the women in a population monopolised by a small number of men is not good for the fitness of those men who miss out on a mate. In such a society, the large difference in fitness between the haves and the have-nots would be expected to result in strong sexual selection.\nHaving noted the obvious, an article in Evolution and Human Behavior by Moorad and colleagues presents an interesting illustration of this situation. They examined the strength of sexual selection in a population of Utah men born between 1830 and 1894, with the rate of polygamy among married men dropping from over 17 per cent among some year groups born in the 1930s to less than 1 per cent among those born at the latest dates. These men faced a ban on polygamy in 1862, among other increasing social pressures against multiple marriages. The authors’ analysis of the population data allowed them to estimate the strength of sexual selection over this period and to isolate which factors contributed to reproductive success.\nThe headline finding from the study is that between 1830 and 1894, the strength of sexual selection in this population dropped by 58 per cent. This authors calculated this reduction from changes in Crow’s Index, which sets an upper limit on the rate of evolutionary change. Crow’s index is an upper limit as, first, not all differences in fitness may be due to phenotype.  An environmental or random cause may be relevant. Second, phenotypic selection is only genetic to the extent that the genotype associated with the higher fitness is passed to the next generation.\nMoorad and colleagues do not attempt to tease out the phenotypic or genetic selection in this sample. I am not sure how they could. However, it is reasonable to assume that changes in total selection are closely related to changes in the underlying level of genotypic and phenotypic selection.\nThe authors also made estimates of the costs and benefits of the polygamous mating system for men and women. They did this by examining the Bateman gradients, which are a measure of the change in reproductive success for a given change in mating success. In the 1830s, the Bateman gradient for men was 5.87, meaning that for each extra mate a male could expect almost six extra offspring. By the 1890s, this had dropped to 1.92. For women the gradient was slightly positive, increasing from 0.195 to 0.671 over the course of the study period.\nThe authors made a more interesting use of the Bateman gradient when they determined the gradient for women for each additional mate that a man has. This comes back to the classic trade-off question - do the reduced resources available to a woman from having to share the male’s resources with more wives outweigh the potentially higher quality of the man that a woman can obtain in the polygamous system? The answer to this question was yes (at least in terms of number of children) - there was a slightly negative gradient in the 1830s (starting at -0.06), which dropped to a low of -1.36, before ending at -1.11 in the 1890s. In the 1890s, each additional wife resulted in the other wives having, on average, one child less. The woman would want to hope that they are particularly high quality children.\nA further piece of information that Moorad and colleagues teased out was how much a man’s increased fitness, due to additional wives, comes from increased reproductive rate or from a lengthened reproductive tenure. At the beginning of the sample, increasing the number of wives increased both the rate and tenure of reproduction. This switched towards the 1890s however, with increased wives only extending the tenure and coming at the cost of the rate of reproduction. This reflects the fact that towards the end of the sample, additional wives were normally the product of serial monogamy. For the few polygamists around at that stage, they still had the benefit of both an increased reproductive rate and tenure.\nI enjoy studies like this. Even though they sometimes seem to be demonstrating the obvious, an empirical illustration adds some colour and robustness to the theory. Given my research interests around the economic consequences of evolutionary change in humans, illustrations of the rate of evolution in human populations are always useful. Could change genetically in a time period short enough to have economic significance. This study does not answer this, but it is another useful example."
  },
  {
    "objectID": "posts/sexual-selection-and-inequality.html",
    "href": "posts/sexual-selection-and-inequality.html",
    "title": "Sexual selection and inequality",
    "section": "",
    "text": "From Matt Ridley in the Wall Street Journal:\n\nBack in the hunter-gatherer Paleolithic, inequality had reproductive consequences. The successful hunter, providing valuable protein for females, got a lot more mating opportunities than the unsuccessful. So it’s possible that men still walk around with a relatively simple equation in their brains, namely that relative success at obtaining assets results in more sexual adventures and more grandchildren.\nIf so, this might explain why it is relative, rather than absolute, inequality that matters so much to people today. In modern Western society, when even relatively poor people have access to transport, refrigeration, entertainment, shoes and plentiful food, you might expect that inequality would be less resented than a century ago—when none of those things might come within the reach of a poor person. What does it matter if there are people who can afford private jets and designer dresses?\nBut clearly that isn’t how people think. They resent inequality in luxuries just as much if not more than inequality in necessities. They dislike (and envy) conspicuous consumption, even if it impinges on them not at all. What hurts is not that somebody is rich, but that he is richer.\n\nFemales care about a male’s resources as they can be used by the female to raise their children, and they may be a signal of the male’s underlying genetic quality.\nThis preference exists despite the apparently declining importance of additional resources in a modern context. In a rich country, all necessities can be easily provided, and investment over a certain threshold likely has limited effect on child quality. However, another less mentioned reason behind the desire for more resources is the potential for an environmental shock. Inequality in luxuries may suddenly become important as those with ample resources are better able to survive a new, constrained environment.\nA commonly used example of this is survivorship on the Titanic (unfortunately of most recent note due to the self-plagiarism issues involving Bruno Frey and colleagues). Of the first, second and third class passengers on the Titanic, 62 per cent, 41 per cent and 25 per cent survived respectively (I pulled these numbers from Gandolfi, Gandolfi and Barash’s Economics as an Evolutionary Science). Similar patterns of survival occur in war, with the wealthy better able to flee or buy their safety. Resources currently allocated to luxuries can be diverted to necessities when required.\nThe Titanic example and other modern equivalents are too recent to have shaped our evolved psyche, but this pattern likely has been around since human’s started to accumulate wealth. Thus, even in times of plenty, the potential for a shock to the system may still lie at the back of people’s minds.\nAs an aside, my work with Juerg Weber and Boris Baer also gets a mention in Ridley’s piece."
  },
  {
    "objectID": "posts/selfish-herding.html",
    "href": "posts/selfish-herding.html",
    "title": "Selfish herding",
    "section": "",
    "text": "Nicholas Gruen writes:\n\n[Y]ou’d think that economics would have a good theory of herding, or at least that it would be a prominent subject within the discipline. Alas, if you thought that, you’d be mistaken. When Oswald looked at the biology of herding, the canonical article was Hamilton, W. D. (1971). “Geometry for the Selfish Herd”. Journal of Theoretical Biology 31 (2): 295–311.\nThis theory models herding as a ‘rational’ strategy to avoid predators. The ‘game’ that gets selected for is for each animal to try to avoid being on the outside of the herd so that the predator gets to eat the outsider. It’s a powerful theory which fits (ie ‘predicts’ a lot of of biological data). …\nYou know how many times it’s been cited in the economics literature? Well it’s never been cited – at least when Oswald looked it up – it probably has now. …\nBy contrast, the prominent theory of herding in economics is herding as informational learning. Thus for instance people imitate others figuring ‘they must know something I don’t’. In a cinema, someone yells “Fire!”. People start running for the exit. Others up the back don’t hear what the yeller yelled, but they figure they could do worse than follow the herd. The idea is illustrated at the end of this famous scene. This idea can help explain financial bubbles.\nBut the biological herding idea seems so much more powerful. Because it suggests that a dominant biological mode is one in which each ‘agent’ seeks the local optimum of their own survival, and that this gives the group some holistic coherence, but that no-one is thinking of the group, and the group’s survival and welfare – the global optimum – is therefore the (arbitrary) result of these individual optimisations. The biological theory of herding spells danger for the herd in many situations.\n\nI had come across Hamilton’s paper before, but this post prompted me to read it properly. It is worth it. Add it to the list of biological examples where the “invisible hand” can lead to an emergent phenomena that is costly to the group.\nAudio and slides for Andrew Oswald’s speech, which triggered Gruen’s post, can be found here."
  },
  {
    "objectID": "posts/selective-sweeps-in-humans.html",
    "href": "posts/selective-sweeps-in-humans.html",
    "title": "Selective sweeps in humans",
    "section": "",
    "text": "From a new paper in PLOS Genetics:\n\n[R]efined analyses of modern human genomic data have changed our view of evolutionary forces acting on our genome. While most people assumed that the out-of-Africa expansion had been characterized by a series of adaptations to new environments leading to recurrent selective sweeps, our genome actually contains little trace of recent complete sweeps and the genetic differentiation of human population has been very progressive over time, probably without major adaptive episodes\n\nJohn Hawks draws a different conclusion:\n\n[I]n fact, we have abundant signs of recent positive selection in the genome, but those signs are nearly all very recent partial sweeps in different human populations. Complete sweeps and near-complete sweeps are indeed few, suggesting that there was relatively little directional adaptive evolution associated with the “origin of modern humans.” Measuring by genetic change, agriculture was many times more important than the appearance of modern humans throughout the world.\n\nIt is obvious in some ways, but if we wish to link economic growth with genetic changes since the appearance of agriculture, those genetic changes may look very different across populations, even comparing two populations that have similar long histories of agricultural life."
  },
  {
    "objectID": "posts/selection-during-pregnancy.html",
    "href": "posts/selection-during-pregnancy.html",
    "title": "Selection during pregnancy",
    "section": "",
    "text": "Carl Zimmer writes about a new paper in Trends in Genetics where the authors argue that natural selection during pregnancy is an important driver of recent evolutionary changes:\n\nWomen nourish their fetuses by raising the level of sugar in their blood. That’s a dangerous game, because it threatens to throw off their own delicate balance between sugar and insulin. If that balance gets out of whack, women may suffer gestational diabetes. The Harvard researchers suggest that the shift to high-carb agriculture in Europe led to more women dying of gestational diabetes. Women with mutations that lowered their blood sugar level during pregnancy were favored by natural selection. And today, European women enjoy the benefits of that suffering: a low risk of gestational diabetes.\nA woman in Bangladesh has a very different history behind her. Her ancestors ate fish, unprocessed rice, and other foods with modest levels of carbohydrates. In that environment, women with mutations that increased their blood sugar during pregnancy might have had healthier children than women without them.  Throw those genes into a modern Western city, and trouble looms. Women with low-sugar genes are now drinking soda and eating bread, ice cream, and lots of other food loaded in carbs. They don’t have the evolved defenses to keep them from developing gestational diabetes.\n\nFrom New York birth data, Women of European descent have less than a 4 per cent chance of developing gestational diabetes during pregnancy, compared to around 20 per cent for Bangladeshi women.\nThe paper also points to recent evolution relating to production of vitamin D, altitude and malaria."
  },
  {
    "objectID": "posts/scarcity-of-time-money-friends-and-bandwidth.html",
    "href": "posts/scarcity-of-time-money-friends-and-bandwidth.html",
    "title": "Scarcity of time, money, friends and bandwidth",
    "section": "",
    "text": "Sendhil Mullainathan and Eldar Shafir’s Scarcity: Why Having Too Little Means So Muchis full of interesting insight and experimental results. It presents a novel way of looking at scarcity that extends beyond the typical analysis in economics, the original “science of scarcity”, and will certainly change the way I think about it.\nBut by the time I reached the end of the book, I was not entirely satisfied. I have new buzzwords and some interesting experiments to think about, but I’m not convinced Mullainathan and Shafir have presented a coherent new perspective on how the world works. Scarcity reminded me of a Malcolm Gladwell book – I got presented with a lot of cool results, they were spun into an interesting narrative that has made me think, but I don’t buy the authors’ main message.\nFirst, the buzzwords, which added to the Gladwell-esque feeling. Scarcity is having less time, money, friends or packing space than you feel you need. (Note the subjective element - scarcity could be caused by relative rather than absolute scarcity_ And it can apply across a range of domains.) Scarcity can have good effects, such as the focus dividend when scarcity captures the mind. It can make us experts in the area in which we are scarce, with poor people better able to judge the value of a dollar saved despite the context in which it appears. However, scarcity also causes us to tunnel, which is to focus single-mindedly on the scarcity at hand, potentially at the neglect of more important or less timely demands. Scarcity also reduces bandwidth, a combination of cognitive function and executive control, by imposing a bandwidth tax. When people suffer from scarcity and tunnel, they are also forced to juggle, as they move from one pressing task to the next. The way to avoid scarcity is to have some slack, an untapped budget we can turn to in times of need.\nThe most salient example of the negative effects of scarcity comes from a study in a New Jersey mall. Mullainathan, Shafir and Jiaying Zhao presented people with a scenario where they need to fund some car repairs. They then gave the participants tests for fluid intelligence and cognitive control. When the cost of the repairs was $150, high and low-income people scored similarly in the tests. But when the shortfall was $1,500, the performance of the poor plunged – the equivalent of losing 13 or so IQ points. The authors point out that this result has been replicated many times, suggesting it is a robust result. [This experiment is on my list of experiments most likely to fail a pre-registered replication.] By causing the poor to focus on their lack of resources through a high and potentially unmanageable repair bill, the poor’s bandwidth for completing the tests was taxed. In another experiment, people were effectively made rich or poor by the flip of a coin, with the poor demonstrating greater present bias. In that case, the bandwidth tax was clearly not due to the inherent traits of the poor.\nMullainathan and Shafir take these results to mean that we don’t need talent or inherent trait-based explanations for differences between the rich and poor. Instead, scarcity makes the poor perform poorly, leaving them in a scarcity trap. They suggest that the effect of scarcity on bandwidth is a good explanation for differences as it can make sense of diverse empirical facts across behaviour, time and place. But it leaves open the question why they imply that scarcity induced and inherent trait-based explanations are mutually exclusive, or why inherently low bandwidth can’t explain an equally diverse set of facts.\nPart of the difference between their and my interpretation is that they are observing a short-term dynamic and extrapolating that to the longer term. My view is that over the longer-term, inherent explanations play a larger role.\nFor example, when we look at twin studies, we find that IQ and other traits inherently differ between people. Adoption studies show long-term outcomes are typically more representative of biological than adoptive parents. Differences in scarcity of financial or other resources due to differences in adoptive parents have almost no effect on IQ, income, obesity or a range of other long-term outcomes.\nGreg Clark’s work on social mobility shows the long-term persistence of status across many generations, despite short-term shocks that would affect scarcity. If Mullainathan and Shafir’s thesis were true, a shock one generation would carry through future generations, rather than seeing the next generation reverting to the underlying status. Under Mullainathan and Shafir’s explanation, we would also see immigrants’ IQ increase when they move country and ease their scarcity (although if scarcity is relative, perhaps their relative position may not have improved). We would see large increases in the IQ of previously poor lottery winners, who would experience a sudden surge in mental resources. In The Son Also Rises, Clark pulls together a few examples of windfalls of this nature and demonstrates the lack of long-term effect. In one case, a lottery of land parcels had no effect on the outcomes of the winners’ children.\nAs a result, I am not convinced that their arguments truly capture the differences between the poor and the rich, or the rushed and the relaxed. Perhaps someone might eat more from time to time due to the bandwidth tax. They might take an ill-advised payday loan when they are stretched for money. But despite this short-term effect, we see little trace of it in long-term outcomes. Something is allowing some people but not others to break the cycle of scarcity.\nI also doubt that Mullainathan and Shafir’s description of the poor as suffering from scarcity is generally true. When it comes to time, the poor watch more television, invest less time in caring for their children, have plenty of free time to think about what they will eat, and yet are more likely to be obese. Their characterisation of the poor having a lot on their mind whereas the rich are relaxed despite their more complex employment does not seem particularly strong.\nThe book is least satisfying when Mullainathan and Shafir start drawing their examples of scarcity from other domains – the padding to turn an interesting idea into a book length argument. Many of them are banal and simply involve scarcity in the way economists might think of it, with no evidence that people were suffering anything more than a lack of time. For example, they talk of the Benihana restaurant, where the chef cooking in front of people sets a quick pace for the meal, allowing more customers to come through. It is a story of scarce time, but provides no interesting insight into their thesis. The fact the Benihana restaurant management took the time to solve their problem suggests they were not overly taxed. Mullainathan and Shafir discuss the crash of NASA’s Mars Orbiter, which had to be launched by a certain date, leading to shortcuts. But what is the evidence that there was tunnelling or taxed bandwidth as opposed to a simple lack of time to do all the checks they would have liked to have done? They don’t present the evidence to distinguish.\nWhen we turn to solutions, many of them do not depend on Mullainathan and Shafir’s argument being true. They are solutions that would be equally useful in dealing with inherently untalented or impatient people. Savings reminders could be useful regardless of the cause of the lack of foresight. Presenting pay day loans in terms of dollars rather than interest rates is standard behavioural economics fare and may work for people lacking bandwidth regardless the cause.\nThe short-term fluctuation of bandwidth does present some interesting possibilities. They suggest that if education fails due to low bandwidth, we should time education when people can best learn. They also give an example of Kenyan farmers failing to use fertiliser and missing out on large gains to their yields. By getting farmers to pre-purchase the fertiliser when they were flush with cash after harvest, more benefited from the yield gains. (Although again, are their decisions because they are inherently shortsighted or taxed?)\nUltimately, the long-term solution to the costs of scarcity is creating bandwidth and a buffer stock of slack. This sounds sensible, but this point drew me back to an example early in their book. They describe an experiment involving Indian vendors who were provided with money to allow them to escape loans with exorbitant interest rates that consumed much of their income. But despite having their debts cleared, bit by bit they fell back into the scarcity trap. Mullainathan and Shafir accredit their return to the trap to shocks. But why did they not save what they would otherwise have been paying in interest to create a buffer from shocks? Why did the new bandwidth not allow at least some of them to escape? Their income had effectively doubled.\nOne interesting idea Mullainathan and Shafir leave lying around is whether the bandwidth of whole economies can fluctuate through good times and bad. As a random idea of my own, is the Flynn effect due to the easing of scarcity in the modern world? [No]\nHaving said the above, the ideas in the book are worth considering, especially for contemplating how short-term mental capacity might be affected by the environment. But extrapolating the results to the long-term despite evidence from twin studies, adoption studies and social mobility analysis needs more. You cannot simply throw the effect of inherent talent and traits out of the window.\n*As a postscript, after writing most of this review I searched for other reviews of the book and came across this piece by Tim Harford. Many of the same points – Harford notes the buzzwords and provides a Gladwell reference too. I should note that, as it seems for Harford, the Gladwell comparison is at least part praise."
  },
  {
    "objectID": "posts/saint-pauls-the-tyranny-of-utility-behavioral-social-science-and-the-rise-of-paternalism.html",
    "href": "posts/saint-pauls-the-tyranny-of-utility-behavioral-social-science-and-the-rise-of-paternalism.html",
    "title": "Saint-Paul’s The Tyranny of Utility: Behavioral Social Science and the Rise of Paternalism",
    "section": "",
    "text": "The growth in behavioural science has given a new foundation for paternalistic government interventions. Governments now try to help “biased” humans make better decisions - from nudging them to pay their taxes on time, to constraining the size of the soda they can buy, to making them save for that retirement so far in the future.\nThere is no shortage of critics of these interventions. Are people actually biased? Do these interventions change behaviour or improve outcomes for the better? Is an also biased government the right agent to fix these problems? Ultimately, do the costs outweigh the benefits of government action?\nIn The Tyranny of Utility: Behavioral Social Science and the Rise of Paternalism, Gilles Saint-Paul points out the danger in this line of defence. By fighting the utilitarian battle based on costs and benefits, there will almost certainly be circumstances in which the scientific evidence on human behaviour and the effect of the interventions will point in the freedom-reducing direction. Arguing about whether a certain behaviour is rational at best leads to an empirical debate. Similarly, arguments about the irrationality of government can be countered by empirical debate on how particular government interventions change behaviour and outcomes.\nAs a result, Saint-Paul argues that:\n\n[I]f we want to provide intellectual foundations for limited governments, we cannot do it merely on the basis of instrumental arguments. Instead, we need a system of values that delivers those limits and such a system cannot be utilitarian.\n\nSaint-Paul argues that part of the problem is that the utilitarian approach is the backbone of neoclassical economics - once (and still in some respects) a major source of arguments in favour of freedom. Now that the assumptions about human behaviour underpinning many neoclassical models are seen to no longer hold, you are still left with utility maximisation as the policy objective. As Saint-Paul writes:\n\nIt should be emphasized that the drift toward paternalism is entirely consistent with the research program of traditional economics, which supposes that policies should be advocated on the basis of a consequentialist cost-benefit analysis, using some appropriate social welfare function. Paternalism then derives naturally from these premises, by simply adding empirical knowledge about how people actually behave …\n\nWhen Saint-Paul describes the practical costs of this increased paternalism, his choice of examples often make it hard to share his anger. One of his prime cases of infringed liberty is a five-times public transport molester who is banned from using the train as a court determined he lacked the self-control to travelling on it. On gun control laws he suggests authoritarian governments could rise in the absence of an armed citizenry.\nStill, some of the other stories (or even these more extreme examples) lead to an important point. Saint-Paul points out that many of these interventions extend beyond the initial cause of the problem and impose responsibility on people for the failings of others. For example, in many countries you need a pool fence even if don’t have kids. You effectively need to look after other people’s children. Similarly, liquor laws can extend to preventing sales to people who are drunk or likely to drive. Where does the chain of responsibility transfer stop?\nOne of the more interesting threads in the book concerns what the objective of policy is. Is it consumption? Or happiness? And based on this objective, how far does the utilitarian argument extend. If it is happiness, should we just load everyone up with Prozac? And then what of the flow on costs if everyone decides to check out and be happy?\n\nWhat if a cardiologist decides that experts and studies are right, that it’s stupid after all to buy a glossy Lamborghini, and dumps a few of his patients in order to take more time off with his family? How is the well-being of the patients affected? What if that entrepreneur who works seventy hours a week to gain market shares calls it a day and closes his factory? In a market society the pursuit of status and material achievement is obtained through voluntary exchange, and must thus benefit somebody else. Owning a Lamborghini is futile, but curing a heart disease is not. The cardiologist may be selfish and alienated; he makes his neighbors feel bad; and he is tired of the Lamborghini. His foolishness, however, has improved the lives of many people, even by the standards of happiness researchers. Competition to achieve status may be unpleasant to my future incarnations and those of my neighbors, but it increases the welfare of those who buy the goods I am producing to achieve this goal.\n\nSaint-Paul’s response to these problems - presented more as suggestions than a manifesto, and thinly summarised in only two pages  at the end of the book - is not to ignore science but to set some limits:\n\nI am not advocating that scientific evidence should be disregarded in the decision-making process. That is obviously a recipe for poor outcomes. Instead, I am pointing out that the increased power and reliability of Science makes it all the more important that strict limits define what is an acceptable government intervention and that it is socially accepted that policies which trespass those limits cannot be implemented regardless of their alleged beneficial outcomes. We are going in the opposite direction from such discipline.\n\nThese limits could involve a minimal redistributive state to rule out absolute poverty - allowing some values to supersede freedom - but these values would not include “statistical notions of public health or aggregate happiness”, nor most forms of strong paternalism.\nBut despite pointing to the dangers of utilitarian arguments against paternalistic interventions, Saint-Paul finds them hard to resist. He regularly refers the biases of government, noting the irony that “the government could well offset such deficiencies with its own policy tools but soon chose not to by having high public deficits and low interest rates.” And when it comes to his picture of his preferred world it has a utilitarian flavour itself.\n\nBeing treated by society as responsible and unitary goes a long way toward eliciting responsible and unitary behavior. The incentives to solve my own behavioral problems are much larger if I expect society to hold me responsible for the consequences of my actions."
  },
  {
    "objectID": "posts/rubins-darwinian-politics.html",
    "href": "posts/rubins-darwinian-politics.html",
    "title": "Rubin’s Darwinian Politics",
    "section": "",
    "text": "The application of evolutionary biology to politics and policy spans the political spectrum. From Peter Singer’s A Darwinian Left to Larry Arnhart’s Darwinian Conservatism to Michael Shermer’s libertarianism, there is something in evolutionary biology for everyone.\nOne of the best of of these applications is by Paul Rubin in Darwinian Politics: The Evolutionary Origin of Freedom. While the arguments lead to conclusions that reflect Rubin’s political leanings, the book reads as though the evidence shapes the result and Rubin gives the evidence fair consideration.\nRubin’s basic position is that the political institutions of Western nations, and particularly the United States, are the best match with evolved human preferences. Humans seek freedom from dominance, with Western society maximizing that freedom. Political freedom allows citizens to form a reverse dominance hierarchy, with public pressure, wealth and constitutional frameworks limiting the ability of Western governments to exercise power. Western institutions also provide a framework that limits negative consequences of our evolved psyche, as the move away from kin based groups reduces xenophobic behaviour.\nRubin does not suggest, however, that humans are perfectly matched to our current environment. Rather, he argues that the current institutional frameworks do a good job of working around them. For example, one of the main threads of Rubin’s argument is that humans have moved from an evolutionary history of living in consumption hierarchies, which are effectively zero sum, to a world of productive hierarchies, whereby participation in hierarchies can boost production and be good for all involved. Humans do not always distinguish between the two (nor academics as Rubin makes clear), and as a result, envy may result as someone acts as though they are still in their zero sum world.\nRubin sees the best political framework as one that can deal with this tendency to envy while not damaging the productivity of the hierarchy. Rubin addresses these concerns through a couple of threads. One is to note that in a free society, movement between hierarchies is possible and people are likely part of many hierarchies. They will not always be at the bottom. However, Rubin paints an overly rosy picture (he should paint a rosy one - it just needs some tempering), as some hierarchies are more important than others and mobility is not a complete solution. If you are at the bottom of an employment hierarchy, your choice is likely to be which hierarchy you wish to be at the bottom of. As shown in the famous Whitehall studies, being at the bottom of a productive hierarchy may have costs (not that I implying that Whitehall itself is a productive hierarchy).\nA more interesting points is when we move from utility to fitness. Rubin writes:\n\nIf an individual is highly productive and creates much wealth, social as well as private benefits will be generated; a productive individual will not normally absorb the entire surplus he will create. Thus, utility or wealth maximisation would imply that all will benefit from such increased productivity and should encourage it. However, if the added productivity is used to engross additional females, or if tastes evolved in an environment where this occurred, then in fact others will become less fit, although wealthier. In this sense, fitness and utility maximisation conflict. This may explain why many utility functions seem to contain elements of envy, even though envy is counterproductive with respect to consumption of wealth maximisation.\n\nRubin’s primary solution to this, already implemented throughout the West, is monogamy. Each man can only monopolise one female regardless of wealth. While monogamy undoubtedly acts as a reproductive leveler, Rubin’s analysis attempts to finesse his case too much. Despite monogamy in Western societies, there is still a large proportion of men at lower socioeconomic status who fail to attract a mate. Higher status women simply choose not to mate with them and lower status women have other avenues of seeking financial support. For example, over 40 per cent of Australian men between the ages of 40 and 44 and with incomes below $20,000 per year remain unpaired. This contrasts with just over 10 per cent of men of that age group with incomes over $83,000 per year. Monogamy levels the reproductive playing field but it is not completely flat.\nFor each of these points, Rubin is essentially right in his argument that productive hierarchies are beneficial and that monogamous societies are more stable and level the reproductive playing field. However, there is still a bottom of the hierarchy and consequences to it, and there will always be some degree of pushback due to this.\nTo assist those at the bottom, Rubin notes the evolved altruistic preferences for assisting those in need. As a result, some wealth redistribution may be supported. But if the focus moves from supporting the poor to clipping the rich, the output of productive hierarchies may be threatened. Further, Rubin considers that social support will remain popular as long as it is not overexposed to free-riding, with humans having strongly refined senses to spot those who are not pulling their weight.\nRubin’s least libertarian finding, apart from his implied support of restrictions on polygamy, relates to restrictions on drugs and other “anti-social” activities. Rubin argues that if consumption of these goods and activities is a form of competition between young males to signal status, restrictions on their use will be required to prevent above optimal use. While Rubin considers that the need to maintain a society’s prime age men at fighting strength is weaker than in our evolutionary past, a case can still be made for this form of control. It was interesting that Rubin chose to use a signaling argument at this point as he does not address the role of signaling in most of his analysis, such as in his discussion of “altruistic” gifts of game in ancestral societies or donations to charity.\nOverall, Rubin’s arguments are clear, transparent and generally persuasive. It was an omission by oversight from my economics and evolutionary biology reading list, as I last read it a number of years ago, but it has now been included (thanks to Eric Crampton for the nudge)."
  },
  {
    "objectID": "posts/rosenzweigs-the-halo-effect-and-the-eight-other-business-delusions-that-deceive-managers.html",
    "href": "posts/rosenzweigs-the-halo-effect-and-the-eight-other-business-delusions-that-deceive-managers.html",
    "title": "Rosenzweig’s The Halo Effect … and the Eight Other Business Delusions That Deceive Managers",
    "section": "",
    "text": "Phil Rosenzweig’s The Halo Effect … and the Eight Other Business Delusions That Deceive Managers is largely an exercise of shooting fish in a barrel, but is an entertaining read regardless.\nThe central premise of the book is that most blockbuster business books (think Good to Great), for all the claims of scientific rigour, are largely exercises in storytelling.\nThe problem starts because it is difficult to understand company performance, even as it unfolds before our eyes. Most people don’t know good what good leadership looks like. It is hard to know what makes good communication or optimal cohesion or good customer service. The result of this difficulty is that people tend to allow good performance in the areas that they can measure (such as profits) to contaminate their assessment of other company attributes. They endow the company with a halo.\nSo when a researcher asks people to rate company attributes when they know the business outcome, those ratings are contaminated. If profits are up, people will assign positive attributes to that company. If times are bad, they will assign negative attributes. We exaggerate strength during booms, and faults during falls. All the factors responsible for a company’s rise might suddenly became the reasons for the fall, or be claimed to have never existed in the first place.\nAs an example, Apple currently has a clean sweep of all nine attributes in Fortune’s “World’s Most Admired Companies” poll - everything from social responsibility to long-term investment value. Is there not a single company in the world that is better than Apple on any of these nine? As Rosenzweig notes, when asked nine questions, people don’t have nine different opinions. They just give their general impression nine times.\nRosenzweig points to one nice experiment by Barry Straw (replicated?), in which Straw asked groups to projects sales and earnings based on financial data. These groups were then given random feedback on their performance. Those with better feedback described their groups as cohesive, motivated and open to change, while those who the experimenter they performed poorly said there was a lack of communication, poor motivation and so on.\nMany of the other delusions in the book are likely familiar to someone who knows a bit about stats or experimental design. Don’t confuse correlation and causation. Rigour is not defined by quantity of data. Do not use samples comprising only successes. Social science isn’t physics.\nOther delusions are less often stated. Don’t be deluded into believing single explanations. If you added up the explained variance across the various single explanation business studies, you’ll explain 100% of the variance many times over. The explanations are likely correlated. And following a simple formula won’t necessarily work for a business as performance is relative. What if all your competitors also follow the same formula?\nThe book closes with Rosenzweig’s spin on what leads to company success, which seems out-of-place after the preceding chapters. Some of it makes sense, such as when Rosenzweig points to the need to acknowledge the role of chance, which is almost never threaded into stories of business success. But Rosenzweig’s punchline of the need for strategy and execution feels just like the type of storytelling that he critiques.\nFurther, when Rosenzweig assesses the performance of three models of good managers - who he approvingly notes share a probabilistic view of the world, realise the role of luck, can make deals under uncertainty, and recognise the need to be vigilant on the changing competitive landscape - it is hard to even agree that all of their actions were successes. Robert Rubin was one of the three. Rosenzweig classes Rubin’s support of the decision to bail out Mexico during the 1995 peso crisis (or more like the exposed US banks - moral hazard anyone?) as a good decision based on the outcome. What is the objective fact not contaminated by a halo? Rosenzweig ends by defending Rubin - who supported deregulation of derivatives trading and was on the board of Citigroup when it was bailed out during the financial crisis - as being more often right than wrong. If nothing else, the strange close to an otherwise good book did give me one more book for the reading pile - Rubin’s In an Uncertain World."
  },
  {
    "objectID": "posts/robert-sugdens-the-community-of-advantage-a-behavioural-economists-defence-of-the-market.html",
    "href": "posts/robert-sugdens-the-community-of-advantage-a-behavioural-economists-defence-of-the-market.html",
    "title": "Robert Sugden’s The Community of Advantage: A Behavioural Economist’s Defence of the Market",
    "section": "",
    "text": "There are few books critiquing behavioural economics that I find compelling. David Levine’s Is Behavioral Economics Doomed? attacks too many straw men. Gilles Saint-Paul’s The Tyranny of Utility: Behavioral Science and the Rise of Paternalism is more an attack of the normative foundations of economics than of behavioural science. And in most of Gerd Gigerenzer’s books, while making a strong case that many of the so-called “biases” are better described as good decision-making under uncertainty, Gigerenzer often extends his defence of human decision making too far.\nIn The Community of Advantage: A Behavioural Economist’s Defence of the Market, Robert Sugden finds a nice balance in his critique. Sugden starts by taking the evidence of behavioural anomalies seriously, reflecting his four decades working in the field. His critique focuses on how the behavioural research has been interpreted and used as part of the “nudge” movement to develop recommendations for the “planner”, “benevolent autocrat” or “choice architect”.\nSugden’s critique has two main thrusts. The first relates to how behavioural economists have interpreted the experimental evidence that our decisions don’t conform with rational choice theory. In the preface, Sugden writes:\n\nI have to say that I have been surprised by the readiness of behavioural economists to interpret contraventions of rational choice theory as evidence of decision-making error. In the pioneering era of the 1980s and 1990s, this was exactly the interpretation of anomalies that mainstream economists typically favoured, and that we behavioural economists disputed. As some of us used to say, it is as if decision-makers are held to be at fault for failing to behave as the received theory predicts, rather than that theory being faulted for failing to make correct predictions.\n\nIn particular, Sugden sees behavioural economists as having adopted an approach whereby they see people as having inner-preferences that conform with the rational choice model (“latent preferences”), contained within a “psychological shell”. This shell distorts our decisions through lack of attention, limited cognitive abilities and incomplete self control. As Sugden points out, this approach has almost no relationship with actual psychological processes, and it is questionable whether these latent preferences exist.\nThe second thrust of Sugden’s critique relates to how the behavioural findings have triggered a public policy response that is largely paternalistic. In the preface, he continues:\n\nI have been less surprised, but still disappointed, by the willingness of behavioural economists to embrace paternalism. And I have felt increasingly uneasy that, in public discourse, ideas from behavioural welfare economics are appealing to a sensibility that is hostile to principles of economic freedom—principles that, for two and a half centuries, have been central to the liberal tradition of economics.\n\nHere Sugden undertakes the rather large task of seeking to displace the dominant normative basis of economics - utilitarianism - with “contractarianism”.\nI’ll now cover each of these two arguments in the depth they deserve.\nThe concept of latent preferences\nDecades of behavioural research have presented a challenge to neoclassical economics. Many of its underpinning assumptions about human preferences and decision making simply do not hold. So how can we reconcile the two?\nSugden makes the case that behavioural economists typically approach this problem by thinking of humans as rational beings wrapped in a layer of irrationality. (He draws heavily on his work with Gerardo Infante and Guilhem Lecouteux in Preference purification and the inner rational agent: A critique of the conventional wisdom of behavioural welfare economics (working paper pdf) in making this case.) Sugden pulls apart a number of the seminal papers on nudging, including Thaler and Sunstein’s Libertarian Paternalism (pdf) (the precursor to Nudge) and Colin Camerer, Samuel Issacharoff, George Loewenstein, Ted O’Donaghue, and Matthew Rabin’s Regulation for Conservatives (pdf), in arguing that there is this common approach. For each of them, the underlying “latent preferences” are the benchmark that against which utility is measured, with decisions that do not meet this criteria attributed to error.\nGiven this, the role of the planner (or “choice architect” as Thaler and Sunstein rebranded the planner in Nudge) is to try to reconstruct a person’s latent preferences. These latent preferences would have been revealed if they had not been affected by limitations of attention, information, cognitive ability or self-control. Sugden calls this reconstruction of latent preferences “preference purification”.\nOne of Sugden’s central points concerns whether preference purification is possible. For instance, it is only possible if the latent preferences are context independent.\nTo illustrate the problem of context independence, Sugden asks us to consider Thaler and Sunstein’s famous cafeteria story. Imagine that the relevant prominence or ordering of food in a cafeteria affects people’s choices (and experimental evidence suggests that it does). The cafeteria director could place the fruit more prominently, with the cakes at the back, increasing purchases of fruit and “nudging” the customers to the healthy option.\nSuppose the cake is at the front of the display. When the ordinary human “Joe” goes to the cafe, he selects the cake. If the fruit had been at the front, he would have selected the fruit. Has Joe made an error in his choice? We need to ask what his latent preference is. But suppose Joe is indifferent between cake and fruit. He is not misled by labelling or any false beliefs about the products or their effects on their health. He simply feels a desire to eat whatever is at the front of the display. What is the nature of the error?\nTo help answer this, imagine that SuperReasoner also goes to the cafe. SuperReasoner is just like Joe except that he “has the intelligence of Einstein, the memory of Deep Blue, and the self-control of Gandhi”. (Sugden borrows this combination of traits from Nudge). What happens when SuperReasoner encounters cake and fruit that vary in prominence? Since he is just like Joe, he is indifferent between the two. He also has the same feelings as Joe, so feels a desire to eat whatever is at the front. This is not a failing of intelligence, memory or self-control. There is no error. Rather, the latent preference itself is context dependent. But if latent preferences themselves are context dependent, how do you ever determine what a latent preference is? What is the right context?\nWhen I first read this example, I was unclear how important it was. It was clear that Sugden had found a weakness in the latent preferences approach, but was this something practically important?\nI think the answer to that question is yes, and it comes down to the disconnect between the latent preferences approach and the actual decision making processes of humans. Context independent latent preferences in many cases simply do not exist. They only come into existence in certain contexts. And whatever the psychological approach actually is, latent preferences in an inner shell isn’t it.\nEven if there were an inner rational agent, advocates of the preference purification approach don’t attempt to understand or explain the decision making process of this inner agent. There is simply an assumption that there is some mode of latent reasoning that satisfies the economists’ principles of rationality, free from the imperfections created by the external psychological shell. (This is also a problem with rational choice theory. As Sugden writes “rational choice is not self-explanatory: if there are circumstances in which human beings behave in accordance with the theory of rational choice, that behaviour still requires a psychological explanation.”)\n(As an aside that I won’t go further into today, Sugden and Sunstein continue this debate in a series of papers that are worth reading. See Sugden’s Do people really want to be nudged towards healthy lifestyles?, Sunstein’s response (pdf) and Sugdens rejoinder. Sugden’s rejoinder has another great example of the problems created by context dependent latent preferences that I’ll discuss in another post.)\nSugden does see that one possible defence of the latent preference approach is to define latent preferences as the preferences that this same person will endorse in independently definable circumstances. People will acknowledge these latent preferences even when a lack of self control (akrasia) leads them to act against their better judgment.\nSunstein and Thaler draw on this interpretation in Nudge in their New Year’s resolution test. How many people vow to drink or smoke more when making their resolutions?\nAs Sugden points out, this is a context dependent preference. People are using the cue of the New Year to make their resolution. In the same way, if they decide later to have an extra glass of wine in a restaurant, they are responding to that particular context.\nThe issue then becomes which of these are the true preference. I presume Sunstein and Thaler would take the New Year’s resolution. Sugden is less sure. As he writes:\n\n[J]ust as the restaurant gives cues that point in the direction of drinking, so the traditions of New Year give cues that point in the direction of resolutions for future temperance. If an argument based on akrasia is to work, we need to be shown that in the restaurant, the individual acknowledges that her true preferences are the ones that led to her New Year’s resolution and not the ones she is now acting on. In many cases that fit the story of the resolution and the restaurant, the individual in the restaurant will be thinking that resolutions should not be followed too slavishly, that there is a place in life for spontaneity, and that having an extra glass of wine would be an appropriately spontaneous response to the circumstances. A person who thinks like this as she breaks a previous resolution is not acting contrary to what, at the moment of choice, she acknowledges as her true preferences.\n\nSo why do behavioural economists tend to see problems such as this as self-control problems? Sugden suggests this is because of their commitment to the model of the inner-rational agent. Any context dependent choice needs to be seen as an error. Sugden has a different view:\n\nIf one has no prior commitment to the idea of latent preferences, there is no reason to suppose that Jane has made any mistake at all. The question of how much she should drink may have no uniquely rational answer. Both when she was making New Year’s resolutions and when she was in the restaurant, she had to a strike a balance between considerations that pointed in favour of alcohol and considerations that pointed against it. The simplest explanation of her behaviour is that she struck one balance in the first case and a different balance in the second. This is not a self-control problem; it is a change of mind.\n\nThe contractarian perspective\nWhile I have opened with Sugden’s critique of the nudging approach that emerged from his own field of behavioural economics, his agenda is The Community of Advantage is much broader - an alternative normative basis for economics that is consistent with the psychological evidence.\nThis normative basis is not new. At the beginning of the book Sugden sources it to John Stuart Mill - the book’s title comes from Mill’s description of the market as a “community of advantage”. Mill considered that economic life is, or should be, built on mutually beneficial cooperation. If people participate in relationships of mutual benefit, they will come to understand that they are cooperative partners, not rivals.\nSugden’s uses the term “contractarianism” to describe this normative foundation. Sugden is inspired by James Buchanan in this approach. Buchanan saw economics as not being about how the market should achieve certain means, but rather how the market is a forum by which people can enter into voluntary exchange.\nThe question for the economist thus becomes what institutional arrangements will maximise the opportunity for mutually beneficial cooperation, or more specifically, what institutional arrangements are in the interest of each individual to accept if everyone else accepts the same. As Sugden shows (through some rather technical proofs), markets tend to intermediate mutually beneficial transactions, so like neoclassical economics, contractarianism provides support for the use of markets. In this argument, he does not rely on the preferences of the agents being integrated, so he avoids the problems of the inadequacy of rational choice theory. In fact, opportunity is defined independently of people’s preferences, so it does not rely on preferences at all.\nThe contractarian approach does not result in a blunt call for no government action. Possibly the most stark example of this is Sugden’s suggestion that retirement savings might be mandated. Sugden is sceptical that savings shortages are driven by short-term desires to spend, and asks whether the large economic, political and personal uncertainties involved in saving for a retirement decades away are more important. Among other things, people may simply believe that their collective voting power might enable them to secure sufficient transfers from the working population whatever they do.\nIn this case, Sugden suggests the problem is a collective action problem. What is the credibility of a policy regime in which private savings play a major part if a large proportion of people simply won’t play ball? In a society where the imprudent have votes, some form of mandatory saving might be required to create the sustainable institutional structure to guarantee some minimum living standard.\nI struggled through my first read of the book to understand exactly what a contractarian would think about nudging (I am no philosopher), but there was one passage that I felt gave me the closest glimpse:\n\nA typical questioner will describe some case in which (as judged by the questioner) a mild but unchosen nudge would be very beneficial to its nudgees. Perhaps the nudgees are morbidly obese, and the nudge is a government policy that will make unhealthy fast food less readily available. The questioner asks me: What would you do in this case? To which my reply is: What do you mean, what would I do? What is the hypothetical scenario in which I am supposed to be capable of doing something about the diets of my morbidly obese fellow-citizens?\nIf the scenario is one in which Robert Sugden is in a roadside restaurant and a morbidly obese stranger is sitting at another table ordering a huge all-day breakfast as a mid-afternoon snack, the answer is that I would do nothing. I would think it was not my business as a diner in a restaurant to make gratuitous interventions into other diners’ decisions about what to eat. But of course, this is not the kind of scenario the questioner has in mind. What is really being asked is what I would do, were I a benevolent autocrat. My answer is that I am not a benevolent autocrat, nor the adviser to one. As a contractarian economist, I am not imagining myself in either of those roles. I am advising individuals about how to pursue their common interests, and paternalism has no place in such advice.\n\nAnother section of the mindset of the contractarian was also helpful:\n\nSunstein and Thaler devote a chapter of Nudge to the issue of retirement savings. The content of this chapter is summarized in the final paragraph:\n\nSaving for retirement is something that Humans [as contrasted with ideally rational agents] find difficult. They have to solve a complicated mathematical problem to know how much to save, and then they have to exert a lot of willpower for a long time to execute this plan. This is an ideal domain for nudging. In an environment in which people have to make only one decision per lifetime, we should surely try harder to help them get it right. (Thaler and Sunstein, 2008: 117)\nLook at the final sentence. Sunstein and Thaler are telling their readers that we should try harder to help them get their decisions right. But who are the ‘we’ and who are the ‘they’ here? What ‘we’ are supposed to be doing is designing and implementing choice architecture that nudges individuals to save more for retirement; so presumably ‘we’ refers to government ministers, legislators, regulators, human resource directors, and their respective assistants and advisers; ‘they’ are the individuals who should be saving. As an expert adviser on the design of occupational pension schemes, Thaler is certainly entitled to categorize himself as one of the ‘we’. But where do his readers belong? Very few of them will be in any position to design savings schemes, but just about all of them will face, or will have faced, the problem of saving for retirement. From a reader’s point of view, Sunstein and Thaler’s conclusion would be much more naturally expressed as: They should try harder to help us get it right.\n\nSunstein and Thaler are writing from the perspective of insiders to the public decision-making process: they are writing as if they were political or economic decision-makers with discretionary power, or the trusted advisors of such decision-makers. And they are inviting their readers to imagine that they are insiders too—that they are the people in control of the nudging, not the people who are being nudged.\nI suggest that the benevolent autocrat model appeals to people who like to imagine themselves as insiders in this sense.\n…\nIn contrast, the contractarian approach appeals to people who take an outsider’s view of politics, thinking of public decision-makers as agents and themselves as principals. The sort of person I have in mind does not think that he has been unjustly excluded from public decision-making or debate; he is more likely to say that he has (what for him are) more important things to do with his time. He does not claim to have special skills in economics or politics, and is willing to leave the day-to-day details of public decision-making to those who do—just as he is willing to leave the day-to-day maintenance of his central heating system to a trained technician. But when public decision-makers are dealing with his affairs, he expects them to act in his interests, as he perceives them. He does not expect them to set themselves up as his guardians.\n\nSugden states - and I agree - that he takes the psychological evidence more seriously than most nudge advocates. But his approach - which doesn’t rely on integrated preferences - does on first glance seem to have some weaknesses. How do people identify these mutually beneficial advantages? If we increase opportunity, do we end up with choice overload?\nI covered some of Sugden’s views on choice overload in a previous post, whereby he stated that much concern for choice overload was condescension towards other people’s preferences. But he does take some of the issues with choice overload seriously. For instance, he notes that long menus of retirement or insurance plans lead to poorer choices through the lack of pre-existing preferences and lack of navigational aids (partly the result of public programs needing to be impartial in the way they present options).\nSugden also sees problems with “obfuscation” in the market, whereby firms deliberately price their products or present the pricing information in overly complex ways. They might bait, whereby only a small quantity of stock is available at the advertised price, or provide exploding offers, whereby a decision must be made in a certain timeframe.\nHere the contractarian does not seek to close opportunities for exchange, but rather to provide a better institutional structure. This might involve requiring transparency in pricing, such as requiring pricing to be given for complementary bundles of goods (e.g. printers and print cartridges). However, they should not be required to be purchased together. Exploding offers designed to induce quick decisions might be tempered by cooling-off periods. Other product information such as calorie counts might be required on menus. Importantly, these measures are not then taken to have failed if someone continues to purchase the high calorie food.\nThe question about how effective people are at identifying and capitalising on opportunities for mutual benefit, outside of the choice overload issue, was less clearly addressed. Sugden reviews some of the experimental evidence relating to fairness, and suggests it points to a preference for mutually beneficial exchange (also a subject for another post). But this does not extend to the question of our effectiveness at seeing these opportunities for exchange.\nA discussion\nIf you would prefer to get a flavour of the book in a different manner, below is a video discussion between Sugden, Henry Leveson-Gower and myself on some of the topics in the book."
  },
  {
    "objectID": "posts/risk-aversion-is-not-irrational.html",
    "href": "posts/risk-aversion-is-not-irrational.html",
    "title": "Risk aversion is not irrational",
    "section": "",
    "text": "Several times over the last few years, I have come across someone willing to claim that risk aversion is a bias or that standard economics cannot explain it (such as this claim by David Sloan Wilson - although he mistakenly named it the Allais paradox).\nYesterday, I saw the other half of the equation. I was looking for reviews of Richard McKenzie’s Predictably Rational?: In Search of Defenses for Rational Behavior in Economics,when I came upon an article by McKenzie containing a snapshot of the argument from his book. He writes:\n\n[C]onsider one of the main experiments that behavioralists Kahneman and Amos Tversky use as evidence for the limitations of perfect rationality as a behavioral premise. They offer their subjects two options: Option A is a “sure thing,” carrying a payoff of, say, $800. Option B is a gamble with an expected payoff of $850: The subjects have an 85-percent chance of receiving $1,000 and a 15-percent chance of getting nothing. The behavioralists report that a “large majority” of subjects choose Option A, in spite of its having an expected value $50 lower than Option B. According to behavioralists, this majority choice demonstrates a form of “bounded rationality.” In other words, the subjects’ rational decision making is impaired by mental constraints on information processing and calculating capacity, not the least of which is risk aversion (with risk aversion evident in people heavily favoring Option A).\n\nMcKenzie then provides a “rational” explanation for the phenomena:\n\n I have repeated this exact choice experiment with my fully employed and executive (business-seasoned) MBA students for several years at the start of their first class—before we discuss rationality, decision making, or any microeconomic concepts and lines of analysis. Just as Kahneman and Tversky report, a “large majority”—between 70 and 85 percent—of my MBA students choose Option A, the sure thing. But would conventional economic thinking fail to predict such an outcome? Not really. As Dwight Lee explained four decades ago (and economists in earlier epochs have presumed), expected value is not all that matters for rational decision making. What the behavioralists miss is that variance in outcomes is also consequential in assessing options. Option A has no variance; Option B has a substantial variance, with the outcome ranging from zero to $1,000. Hence, for many choosers, Option A can be more valuable than Option B. Indeed, if expected value were all that mattered, people would never buy insurance. Is the purchase of insurance irrational?\n\nIn economics, the trade-off between expected value and variance is often discussed through the concepts of expected utility and risk aversion. As utility is assumed to diminish with higher rewards, the expected utility of a gamble is less than the utility of the expected outcome with certainty. People balance risk and reward.\nBut while McKenzie is correct to claim that the experimental result may not be irrationality, I am not aware of anyone active in the field who claims that it is irrational. In regard to McKenzie’s example, Kahneman and Tversky did not claim that the preference for a sure outcome of lower expected value is irrational. It is the universality of the risk-reward trade-off that they challenged. Kahneman and Tversky noted that if framed as a potential loss (an 85 per cent chance of a $1,000 loss versus a sure $800 loss), the experimental subjects became risk seeking. They prefer the gamble. The subjects prefer both higher variance and a higher expected loss. A simple model of expected utility does not show this result, nor does an intuitive explanation of a variance-value trade-off.\nKahneman discusses the history, implications and flaws of expected utility theory in more detail in his fantastic book, Thinking, Fast and Slow. (He also points out the flaws with the alternative approach of prospect theory). His discussion of expected utility is one of the best there is, and shows that he does not consider the existence of a trade-off between variance and expected outcome to be irrational in itself. It is the lack of consistency in how people make this trade-off that is the interesting result.\nDespite the article, I will still read McKenzie’s book. The chapter list looks good, including some consideration of the evolutionary foundations of behaviour. and it has the odd good review. The title also plays to my dislike of calling behavioural biases and heuristics “irrational”. I will assume for the moment that the article is not representative of the book.\nPostscript: After writing this post, I found this on Less Wrong."
  },
  {
    "objectID": "posts/revised-course-notes-on-consumer-financial-decision-making.html",
    "href": "posts/revised-course-notes-on-consumer-financial-decision-making.html",
    "title": "Revised course notes on Consumer Financial Decision Making",
    "section": "",
    "text": "Last year I posted some notes for a course on Consumer Financial Decision Making.\nI have now refreshed those notes, but probably more usefully, also put them into book form using Quarto. Relative to this blog, which is built using markdown and Hugo, Quarto allows the incorporation of computation. It also allows for richer formatting and cross-referencing (without becoming the pain that is LaTeX), and easy production of an associated pdf. I am in the process of converting some of the my other lecture materials into Quarto books.\nYou can access the new online book on Consumer Financial Decision Making here."
  },
  {
    "objectID": "posts/return-to-equilibrium.html",
    "href": "posts/return-to-equilibrium.html",
    "title": "Return to equilibrium",
    "section": "",
    "text": "A post on Cheap Talk reminded me about an old paper of Bill Hamilton’s on the potential for extraordinary sex ratios. Apart from its importance for the particular topic (Hamilton considered it to be one of his best papers), it is one of the more interesting expositions that what is good for the individual (or more specifically, the gene) may not be good for the species. It also raises the implicit question of how quickly something can return to equilibrium.\nFirst, take Fisher’s argument for the equality of sex ratios, as stated by Hamilton:\n\n\nSuppose male births are less common than female.\nA newborn male then has better mating prospects than a newborn female, and therefore can expect to have more offspring.\nTherefore parents genetically disposed to produce males tend to have more than average numbers of grandchildren born to them.\nTherefore the genes for male-producing tendencies spread, and male births become commoner.\nAs the 1:1 sex ratio is approached, the advantage associated with producing males dies away.\nThe same reasoning holds if females are substituted for males through out. Therefore 1:1 is the equilibrium ratio.\n\n\nAll well and good. Then Hamilton frames the following situation (remembering that for humans, men have an X and a Y chromosome, while women have two X chromosomes):\n\nSuppose the Y chromosome has mutated in a way which causes it always to win in the race to fertilize. A male with the Y mutant then produces nothing but sons. Provided these sons, who also carry the mutant, cannot be in any way discriminated against in the unrestricted competition for mates (a situation which is implied if mating is random for the whole population), the Y mutant will have a constant selective advantage. As the mutant spreads, the population sex ratio will become more and more male-biased and the population itself will become smaller and smaller; finally the population will be extinguished, after the last female has chanced to mate with a male carrying the mutant.\n\nStarting from a population of 1,000 with one mutant male, it takes only 15 generations to drive the expected number of females below one. A similar situation can arise with an X-linked mutation, although the path to all females and species extinction is slower.\nWhile this is a theoretical example and rests on the assumption that there is no discrimination against the mutant males or females, there are a few real-world cases of the X-linked drive to all females. At the time of Hamilton’s paper (I’m not sure if this has changed) only one case of the Y-linked drive was known. In that case, a mosquito has a sex-determining gene (not a whole chromosome) but the path to all males has been restrained by several other genes.\nThat brings me to the economics. Fisher’s basic principle, which is the best starting point for discussions of sex ratios, sounds much like a neo-classical economic description of the world. If things tend away from equilibrium, there is a clear strategy that can be exploited - and we would expect it to exploited - that will return the system to equilibrium. However, when someone plays a novel, fast acting strategy, things can move quickly. The question is whether the responding strategy is played immediately or may take some time. In the above case of the mosquito, restraining mutations have occurred, preventing extinction.\nSome evolutionary or neo-Schumpeterian economics seeks to deal with this, particularly in the form put forward by Nelson and Winter. Firms search for technological and organisational solutions based on habitual methods and those which succeed in finding them replicate and spread. The process is not immediate and can be lumpy and crude. Arnold Kling, with his patterns of sustainable specialisation and trade (PSST), also notes that time is an important consideration. Take the following:\n\nIt is the task of entrepreneurs to organize the economy so that people produce stuff that has value. Sometimes, entrepreneurs are not quite up to that task. Then you get unemployment. There is an incentive for entrepreneurs to try to figure out ways to create patterns of sustainable specialization and trade that utilize workers who are currently unemployed.\n\nThe system gets a bump. How long does it then take for a strategy to be developed that deals with it?"
  },
  {
    "objectID": "posts/religion-personality-and-fertility.html",
    "href": "posts/religion-personality-and-fertility.html",
    "title": "Religion, personality and fertility",
    "section": "",
    "text": "Tomas Rees points to an interesting paper by Marcus Jokela, who examined how the fertility rates of Americans born between 1920 and 1960 were affected by their personality.\nUsing the big five personality traits - openness, conscientiousness, extraversion, agreeableness and neuroticism - Jokela found that higher levels of conscientiousness in women and higher levels of openness in both sexes became more strongly related to low fertility as time went on. Effectively, cultural conservatives are now more likely to have higher fertility.\nUsing this result, Rees argued that religiosity is not evolutionarily advantageous due to higher fertility, even though current fertility trends may make it seem so. Rather, the link is between conservative values and fertility, and this link is relatively recent.\nI appreciate this argument, largely because it highlights the inconsistency and looseness in the way many studies define phenotypes. Is religiosity just a manifestation of someone’s big five personality traits? We consistently see new papers about the heritability and evolutionary advantage of various political views, happiness, religious persuasions, income and so on. But ultimately, many of these analyses are versions of the same theme, and the analysis could be hardened by deciding on some firm phenotypic traits on which to conduct the analysis. For example, most religious views can probably be captured through the big five personality traits and IQ. I have previously made a similar point about the heritability of political views. If the analyses were linked to a consistent set of phenotypic traits, we would be better positioned to compare results, look at them through time and to explore the underlying causal mechanisms.\nJokela’s paper also highlights that a trait may become advantageous or not with changes in the environment. Heritable variation in reproductive outcomes is often evidence that the evolutionary advantage is relatively recent. After all, evolution eliminates variation. As a result, particularly in modern contexts, we need to consider what environmental shocks have played out. We can make a coherent story about Jokela’s findings using that framework, with changes in the social environment between 1920 and 1960 allowing those who were more open to experience to choose the alternative low fertility option."
  },
  {
    "objectID": "posts/re-reading-kahnemans-thinking-fast-and-slow.html",
    "href": "posts/re-reading-kahnemans-thinking-fast-and-slow.html",
    "title": "Re-reading Kahneman’s Thinking, Fast and Slow",
    "section": "",
    "text": "A bit over four years ago I wrote a glowing review of Daniel Kahneman’s Thinking, Fast and Slow. I described it as a “magnificent book” and “one of the best books I have read”. I praised the way Kahneman threaded his story around the System 1 / System 2 dichotomy, and the coherence provided  by prospect theory.\nWhat a difference four years makes. I will still describe Thinking, Fast and Slow as an excellent book - possibly the best behavioural science book available. But during that time a combination of my learning path and additional research in the behavioural sciences has led me to see Thinking, Fast and Slow as a book with many flaws.\nFirst, there is the list of studies that simply haven’t held up through the “replication crisis” of the last few years. The first substantive chapter of Thinking, Fast and Slow is on priming, so many of these studies are right up the front. These include the Florida effect, money priming, the idea that making a test harder to read can increase test results, and ego depletion (I touch on each of these in my recent talk at the Sydney Behavioural Economics and Behavioural Science Meetup).\nIt’s understandable that Kahneman was somewhat caught out by the replication crisis that has enveloped this literature. But what does not sit so well was the confidence with which Kahneman made his claims. For example, he wrote:\n\nWhen I describe priming studies to audiences, the reaction is often disbelief . . . The idea you should focus on, however, is that disbelief is not an option. The results are not made up, nor are they statistical flukes. You have no choice but to accept that the major conclusions of these studies are true.\n\nI am surprised at the blind spot I had when first reading it - Kahneman’s overconfidence didn’t register with me.\nAs I was also, Kahneman is a fan of the hot hand studies. Someone who believes in the hot hand believes that a sportsperson such as a basketball player is more likely to make a shot if they made their previous one. Kahneman wrote:\n\nThe hot hand is entirely in the eye of the beholders, who are consistently too quick to perceive order and causality in randomness. The hot hand is a massive and widespread cognitive illusion. [Could the same be said about much of the priming literature?]\nThe public reaction to this research is part of the story. The finding was picked up by the press because of its surprising conclusion, and the general response was disbelief. When the celebrated coach of the Boston Celtics, Red Auerbach, heard of Gilovich and his study, he responded, “Who is this guy? So he makes a study. I couldn’t care less.” The tendency to see patterns in randomness is overwhelming - certainly more impressive than a guy making a study.\n\nAnd now it seems there is a hot hand. The finding that there was no hot hand the consequence of a statistical error (also covered in my recent talk). The disbelief was appropriate, and Auerbach did himself a favour by ignoring the study.\nAs I’ve picked on Dan Ariely for the way he talks about organ donation rates, here’s Kahneman on that same point:\n\nA directive about organ donation in case of accidental death is noted on an individual’s driver licence in many countries. The formulation of that directive is another case in which one frame is clearly superior to the other. Few people would argue that the decision of whether or not to donate one’s organs is unimportant, but there is strong evidence that most people make their choice thoughtlessly. The evidence comes from a comparison of organ donation rates in European countries, which reveals startling differences between neighbouring and culturally similar countries. An article published in 2003 noted that the organ donation rate was closer to 100% in Austria but only 12% in Germany, 86% in Sweden but only 4% in Denmark.\nThese enormous differences are a framing effect, which is caused by the format of the critical question. The high-donation countries have an opt-out form, where individuals who wish not to donate must check an appropriate box. Unless they take this simple action, they are considered willing donors. The low-contribution countries have an opt-in form: you must check a box to become a donor. That is all. The best single predictor of whether or not people will donate their organs is the designation of the default option that will be adopted without having to check a box. …\nWhen the role of formulation is acknowledged, a policy question arises: Which formulation should be adopted. In this case, the answer is straightforward. If you believe that a large supply of donated organs is good for society, you will not be neutral between a formulation that yields almost 100% donations and another formulation that elicits donations from 4% of drivers.\n\nAs Ariely does, Kahneman describes the difference between European countries as being due to differences in form design, when in fact those European countries with high “donor rates” never ask their citizens whether they wish to be donors. The form described does not exist in the high-donation countries. They are simply presumed to consent to donation. (The paper that these numbers come from, Do Defaults Save Lives?, might have been better titled “Does not asking if you can take people’s organs save lives?”. That could have saved some confusion.)\nFurther, Kahneman talks about the gap between 100% and 4% as donation rates, when these numbers refer to those who are presumed to consent in the high-donation countries. Actual donation rates and the gap between the different types of countries are much lower.\nAll the above points are minor in themselves. But together the shaky science, overconfidence and lazy storytelling add up to something substantial.\nWhat I also find less satisfying now is the attempt to construct a framework around the disparate findings in behavioural science. I once saw prospect theory as a great framework for thinking about many of the findings, but it is as unrealistic a decision making model as that for the perfectly rational man - the maths involved is even more complicated. It’s might be a useful descriptive or predictive model (if you could work out what the reference point actually is) but no one makes decisions in that way. (One day I will write a post on this.)\nIt will be interesting to see how Thinking, Fast and Slow stands up after another five years."
  },
  {
    "objectID": "posts/ration-information-and-avoid-news.html",
    "href": "posts/ration-information-and-avoid-news.html",
    "title": "Ration information and avoid news",
    "section": "",
    "text": "I am rereading Nassim Taleb’s Antifragile: Things That Gain from Disorder. The first time I read it was during a series of long-haul flights, so some parts of the book are almost unfamiliar.\nThe below passage is among my favourites. I try to avoid getting sucked into the news cycle and am constantly looking for useful ways to control the flow of information I consume.\n\n[T]hose in corporations or in policy making (like Fragilista Greenspan) who are endowed with a sophisticated data-gathering department and are therefore getting a lot of “timely” statistics are capable of overreacting and mistaking noise for information—Greenspan kept an eye on such fluctuations as the sales of vacuum cleaners in Cleveland to, as they say, “get a precise idea about where the economy is going,” and of course he micromanaged us into chaos.\nIn business and economic decision making, reliance on data causes severe side effects—data is now plentiful thanks to connectivity, and the proportion of spuriousness in the data increases as one gets more immersed in it. A very rarely discussed property of data: it is toxic in large quantities—even in moderate quantities. …\nThe more frequently you look at data, the more noise you are disproportionally likely to get (rather than the valuable part, called the signal); hence the higher the noise-to-signal ratio. And there is a confusion which is not psychological at all, but inherent in the data itself. Say you look at information on a yearly basis, for stock prices, or the fertilizer sales of your father-in-law’s factory, or inflation numbers in Vladivostok. Assume further that for what you are observing, at a yearly frequency, the ratio of signal to noise is about one to one (half noise, half signal)—this means that about half the changes are real improvements or degradations, the other half come from randomness. This ratio is what you get from yearly observations. But if you look at the very same data on a daily basis, the composition would change to 95 percent noise, 5 percent signal. And if you observe data on an hourly basis, as people immersed in the news and market price variations do, the split becomes 99.5 percent noise to 0.5 percent signal. That is two hundred times more noise than signal—which is why anyone who listens to news (except when very, very significant events take place) is one step below sucker. …\nThere is a biological dimension to this story. I have been repeating that in a natural environment, a stressor is information. Too much information would thus be too much stress, exceeding the threshold of antifragility. In medicine, we are discovering the healing powers of fasting, as the avoidance of the hormonal rushes that come with the ingestion of food. Hormones convey information to the different parts of our system, and too much of them confuses our biology. Here again, as with news received at too high a frequency, too much information becomes harmful—daily news and sugar confuse our system in the same manner. …\nTo conclude, the best way to mitigate interventionism is to ration the supply of information, as naturalistically as possible. This is hard to accept in the age of the Internet. It has been very hard for me to explain that the more data you get, the less you know what’s going on, and the more iatrogenics you will cause. People are still under the illusion that “science” means more data.\n\nAs a random personal story, in 2010 when mineral and energy prices were having a slight post-GFC decline (before climbing up to new highs over the following couple of years), a lot of Perth-based mining firms were shutting down projects, cutting costs and generally panicking.\nI asked a friend who was working for a major oil and gas producer whether the shifts in prices were affecting his world. He replied that as they were building an asset with a 40-year life, with another five years before it would start production, why would they even look at the day-to-day fluctuations in energy prices? A lot of the firms caught in the panic of the time had projects of a similar lifespan, but they were drowning in noise."
  },
  {
    "objectID": "posts/pulling-apart-a-classic-nudge-story-the-loft-insulation-trial.html",
    "href": "posts/pulling-apart-a-classic-nudge-story-the-loft-insulation-trial.html",
    "title": "Pulling apart a classic nudge story: the loft insulation trial",
    "section": "",
    "text": "In the early days of the Behavioural Insights Team (aka The Nudge Unit), their successes quickly became part of the behavioural economics canon. They were the only behavioural team publishing results publicly, so those became the stories everyone told.\nThese stories included how adding social norms to tax letters increased on-time submission, giving sweets and a personalised message from the CEO tripled charitable giving rates, and messages crafted around reciprocity and fairness would boost organ donation registrations by almost 100,000 a year. These stories are still told now.\nOne story I heard a lot concerned loft insulation as an energy efficiency measure. It is one of the four projects highlighted on the (promotional puff-piece) Behavioural Insights Team Wikipedia page. Could removing the hassle factor, by offering a loft cleanout in conjunction with the installation, increase takeup?\nHere’s how David Halpern describes the results in Inside the Nudge Unit. (When I searched for other tellings of this story, the examples I found almost always involved David Halpern. It must be one of his favourites.)\n\nAfter looking at the issue, and talking to industry and householders, we concluded that the problem was less about money and more about hassle. Specifically, people couldn’t bear the thought of having to clear out the contents of their lofts.\nTo see if this was the case … we ran a leaflet trial with a commercial provider offering households in different areas of London one of three offers:\n\nA home insulation service at a low, but standard cost.\nA home insulation service with a substantial extra discount if any of your neighbours also booked the same service …\nA home insulation service combined with a loft clearance service, albeit at a significant extra cost to insulation alone.\n\n\n\nThis was not a perfect trial in methodological terms. The different leaflets were tried in similar but separate local areas, and not therefore perfectly randomised and total numbers were small. Nonetheless, the effects were still striking. … the loft clearance offer, despite costing several hundred pounds extra, had a three-times higher level of take-up.\nIt wasn’t that cost did not matter. In a later variation, which involved offering the loft clearance scheme at cost in order to lower the price, uptake was even higher (at around five times the standard offer). But the trial powerfully illustrated that, at least for the minority of households that had yet to insulate their lofts, it wasn’t the price of the insulation that was the barrier, but the fact that they couldn’t face the hassle of clearing their lofts.\n\nI’ll leave the discount for neighbours aside - this was from a separate trial - and focus on the “striking” effect of the loft clearance offer.\nThe trial and its outcomes are described in a Department of Energy & Climate Change report Removing the hassle factor associated with loft insulation: Results of a behavioural trial.\nThe trial concerned three different Boroughs - Kingston, Merton and Sutton - so it’s not a randomised trial as such. In each Borough, around 24,000 leaflets were distributed advertising either loft insulation for $179 (Kingston), loft insulation and clearance for £369 (Merton) or insulation and clearance for £450 (Sutton). The leaflets included a phone number and led to 8, 17 and 11 calls respectively. That’s right, a response rate of around 0.05%\nFrom there, there was an audit to check whether the loft was suitable for insulation, which led to a total of 3, 16 and 9 loft installations. The “three-times” in Halpern’s story is the difference between 3 and 9, and the “five times” the difference between 3 and 16. When Halpern said the “numbers were small”, he meant it.\nHere’s a screenshot of the data table:\n\nBut with three to nine installations from 24,000 letters, you can’t draw any statistically sound conclusions. And that is exactly what the report states:\n\nOn the basis of the level of uptake in this trial, the numbers are too small to provide any firm conclusions.\n\nSo, package this trial under “interesting idea for which we have almost no evidence” rather than a story of striking success. The three- and five-fold increases might just be noise. Further, most of the difference occurred between audit and installation, not in initial response rates. It’s not clear what happened there.\nThus, the main thing learned from this trial is that people don’t respond to leaflets offering various loft installation packages.\n\nAs an aside, as of the date of this post, the Wikipedia content on this trial reads:\n\nAlthough loft insulation is essentially a zero-risk proposition, there were very few people installing it. The team discovered that people’s lofts were full of junk, and provided low-cost labour to clear them; this caused a fivefold increase in the proportion of installed insulation.\n\nThe reference is to a Telegraph article by Chris Bell. The opening of the article, Inside the Coalition’s controversial ‘Nudge Unit’, read:\n\nBack in the dark ages of early 2011, the Cabinet Office began grappling with one of the most serious issues facing our age: loft insulation. A curious anomaly was emerging in official statistics — that no one wanted it. Huge subsidies had been ploughed into lagging and rolls of fibreglass; there were generously-incentivised installation schemes, that would pay for themselves within months. It was, to use non-Whitehall patois, a no-brainer. And yet public adoption rates were minuscule. Policymakers were baffled.\nStep forward, then, a new Government team, with a new way of thinking.\nUsing research, plus a smidgen of common sense, they quickly identified the problem: laziness. More specifically, the sheer hassle of clearing an attic before you can insulate it. This alone was deterring us from taking up, effectively, a free lunch. And so, in a pilot trial in September 2011, they suggested a simple solution: that insulation firms offer to clear the lofts first, and dispose of our unwanted junk. In weeks, the uptake increased threefold, even though it cost the customer more. And when this service was subsidised to cost price, there was a fivefold increase.\n\nAnd where did this come from. David Halpern!\n\nAnd David Halpern, director of the nine-person unit, has been tasked with turning theory into government method.\n“Loft insulation is a fascinating example,” he says. “If there is ‘friction cost’ in the way of doing something, it’ll never happen. We’ll put it off. So a lot of what we do is about making life easier for people.” The unit’s Whitehall office might be minuscule and recession-frugal — and Halpern himself has the bookish appearance of, as he puts it, “a humble policy wonk”.\n\nI wonder if there are a few BIT people who occasionally say “David, you know that loft story you keep telling, maybe lay off it a bit…”."
  },
  {
    "objectID": "posts/procrastination.html",
    "href": "posts/procrastination.html",
    "title": "Procrastination",
    "section": "",
    "text": "Procrastination bothers me. Not in the sense that I want to procrastinate, but biologically. Why would a tendency to procrastinate evolve?\nEven without considering evolution, time inconsistency is a subject of debate in economics. The problem of “hyperbolic discounting”, in which people rapidly discount events in the near future but discount more slowly for further delay, has been well established in experiments. This also accords with our experience (For an example of hyperbolic discounting, as well as some general thoughts on procrastination, it is worth reading James Surowiecki’s recent article). From a rational perspective, why would someone make a rational decision about the future but then change their mind when that time nears?\nBiologically this shortcoming is more serious. The individual that is able to make the rational choice at all times should have higher fitness and come to dominate the population.\nOne explanation for hyperbolic discounting may be uncertainty. A bird in the hand is better than two in the bush. But that explanation is only satisfactory where there is immediate reward. What of situations where we know something is good for us and yet we delay the up-front cost?"
  },
  {
    "objectID": "posts/post-crisis-economics.html",
    "href": "posts/post-crisis-economics.html",
    "title": "Post-crisis economics",
    "section": "",
    "text": "Over the weekend I listened to the three-part Lionel Robbins Memorial Lectures 2010, this year given by Lord Adair Turner.\nLord Turner gives a fantastic and balanced critique of the practice of economics and its use in public policy and regulation of the financial sector. Most refreshingly, he does not end his critique with a demand to regulate and restrict, but rather lays out a range of considerations in setting policy and which arguments he considers should be given most weight. Compared to the caricature of modern markets given by Raj Patel (that I also listened to in the last week), it was a nuanced, thoughtful presentation.\nA few of the features that have stuck in my mind:\n\nLord Turner used Roger Bootles distinction between creative and distributive activities in the economy, with creative referring to activities that create welfare and distributive simply transferring welfare (divorce lawyers being the classic case of distributive activity). The question arises of how much financial activity is on the distributive side of the ledger.\nTurner questioned the benefit from increasingly “perfect” markets. For example, does  the improved price discovery from algorithmic trading (that may trade in fractions of a second) increase welfare much beyond that delivered by share markets now, with trades occurring each minute? He considered that there must be diminishing marginal returns to such activities.\nZero growth is not possible in a free society without increasing voluntary employment. If people are free to increase the efficiency and manner in which they conduct activities, there will inevitably be surplus resources.\nA strong thread through the presentation was the questions of relative versus absolute wealth. As positional goods are only available to those with relatively higher incomes, relative position affects welfare. Considerations such as this mean that inequality cannot be dismissed as a policy consideration."
  },
  {
    "objectID": "posts/population-technological-progress-and-the-evolution-of-innovative-potential.html",
    "href": "posts/population-technological-progress-and-the-evolution-of-innovative-potential.html",
    "title": "Population, technological progress and the evolution of innovative potential",
    "section": "",
    "text": "In his seminal paper Population Growth and Technological Change: One Million B.C. to 1990, Michael Kremer combined two basic concepts to explain the greater than exponential population growth in human populations over the last million years.\nThe first concept is that more people means more ideas. A larger population will generate more ideas to feed technological progress.\nThe second concept is that, in a Malthusian world, population is constrained by income, with income a function of technology. Population can only increase if there is technological progress, with any increase in income generated by technological progress rapidly consumed by population growth.\nWhen you combine these two concepts, a larger population generates more ideas, which in turn eases the constraint on additional population growth, which further accelerates the production of ideas. The result is population growth being in proportion to the population size. The following diagram illustrates the feedback loop.\n\nWhen I first read Kremer’s paper, the title caught my attention, particularly the reference to One Million B.C. Humans have evolved markedly in the last one million years. One million years ago, Homo sapiens did not exist as a distinct species, with Homo erectus found in Africa, Europe and Asia. Since then, cranial capacity (a proxy for brain size) has increased from around 900 cubic centimeters to 1,350 cubic centimeters. And not only have humans evolved, but adaptive human evolution appears to be accelerating. As more people means more mutations, natural selection has greater material on which it can act.\nIt was this consideration that forms the basis of my latest working paper, Population, Technological Progress and the Evolution of Innovative Potential, co-authored with my supervisors Juerg Weber and Boris Baer.\nIn the spirit of Kremer’s original paper, we develop a model of population growth and technological progress, but add an extra element, which we call “innovative potential”. Innovative potential is any trait that results in the production of ideas that advance the technological frontier. Innovative potential might incorporate IQ, willingness to invest in innovation, participation in productive activities in which innovation may occur, risk preference, time preference and so on. At this stage, we do not specify the precise trait, but it is not hard to see what the likely traits are.\nAs more people means more mutations, mutations that increase the innovative potential of the population will occur with greater frequency in a larger population. As the population grows, so too does the rate of evolution of innovative potential.\nIncorporating the evolution of innovative potential into the model creates a second element to the feedback loop, as is shown below. Population growth is now proportional to both the size and innovative potential of the population.\n\nOne of the more interesting results of the model can be seen when we partition the drivers of the acceleration of population growth between increasing population size and the increasing innovative potential of the population. As the population evolves, the relative contribution of continuing growth in innovative potential to the acceleration of population growth declines. Continuing population growth becomes the main driver of technological progress and further population growth. However, this does not mean that innovative potential is not important, as the level of innovative potential continues to have a material effect. Populations with higher innovative potential will have much faster population growth.\nThe reason this change occurs is that population growth is driven by both increasing population size and the increasing innovative potential of the population, whereas innovative potential only increases with population size. As the innovative potential reaches a higher level, each new person is more innovative and generates more ideas, but they will only generate the same number of mutations as they always have.\nOne issue with introducing innovative potential into a model of this kind is that ideas are non-excludable. Suppose I invent some new technology that increases my ability to procure resources. If someone else sees and copies this idea, I wont have an evolutionary edge. In the first version of the model presented in the paper, we handwave around this issue, and suggest that innovative people may have higher fitness due to prestige, the ability to keep secrets or some other avenue of reaping the benefits of the innovation. Although this handwaving likely has an element of truth, we introduced a version of the model in which those who are more innovative are also more productive in using those ideas. The results are robust to inclusion of this element.\nOne other observation from the model is the robustness of the population to technological shocks. Through human history, population did not undergo a simple increase, but underwent shocks and bottlenecks. For example, a change in climate could reduce the carrying capacity of the land (through reducing the effective level of technology), reducing population size.\nIn Kremer’s model, shocks of this nature are a strong setback to population growth and technological progress. As the population is smaller, idea production will be slower. In fact, population growth and technological progress will resemble the levels of growth when the population was last of that size. A population experiencing consistent technological shocks may never grow to a substantial size.\nWhere there is evolution of innovative potential, a technological shock is a setback to population growth, but the clock is not fully wound back to the time when the population was last of that size. The population now has higher innovative potential and the population recovers faster from each successive technological shock. This effect is particularly strong where higher innovative potential also increases the productivity of the population in using the new technologies.\nFinally, two assumptions that we include in the model are that population instantaneously adjusts to the carrying capacity of the land, and that the spread of mutations is instantaneous. The first is a weak assumption given the time spans over which the model operates. The second is much stronger. As a result, we also consider the time it takes for a mutation to spread through the population in a dynamic model and an agent-based simulation. Delaying the spread of a mutation does not substantively change the model results, although it prevents an explosion in the innovative potential of the population at the time that the population explodes. But as noted above, even where mutations spread instantaneously, the contribution of continuing evolution of innovative potential to the acceleration of population growth drops to near zero when the population explodes. The delay in the spread of mutations simply strengthens that result.\nIf you would like to play with the agent-based model, code for the model is contained at the end of the working paper, or you can download the model here. I developed the model in NetLogo, an open source agent-based programming environment, which you can download from here."
  },
  {
    "objectID": "posts/population-connectivity-and-innovation.html",
    "href": "posts/population-connectivity-and-innovation.html",
    "title": "Population, connectivity and innovation",
    "section": "",
    "text": "Near the close of his acceptance speech for the Competitive Enterprise Institute’s Julian Simon Memorial Award, Matt Ridley suggests that the total number of people is not the major driver of technological progress:\n\n[W]hat counts is not how many people there are but how well they are communicating. … [I]t’s trade and exchange that breeds innovation, through the meeting and mating of ideas. That the lonely inspired genius is a myth, promulgated by Nobel prizes and the patent system. This means that stupid people are just as important as clever ones; that the collective intelligence that gives us incredible improvements in living standards depends on people’s ideas meeting and mating, more than on how many people there are. That’s why a little country like Athens or Genoa or Holland can suddenly lead the world.\n\nBryan Caplan takes on Ridley’s argument:\n\nIsn’t the correct position clearly that both population and communication matter?  A two-person world linked by Skype wouldn’t be very creative.  Neither would a world of a trillion people in solitary confinement.  Creativity requires minds to generate ideas, and mouths to share them. …\n\nI agree that we need to consider both population and communication, but there is a third element: quality. Caplan nods to this in his response to Ridley’s statement that stupid people are just as important as clever ones:\n\nThis is frankly an absurd leap.  Geniuses are overrated?  Maybe.  Stupid people are “just as important” for progress as clever ones?  Come on.  Question for Ridley: Whose the most creative person alive with an IQ under 100?  Under 80?\n\nWithout quality, quantity or connectivity, technological progress of the type we see today would not be possible.\nTake an example Ridley uses in the speech - that the internet and mobile telephony had no inventor. True, they were collective enterprises involving many networked people using many accumulated technologies. But what was the average IQ of the inventors of the technology used in creating them? Or the average level of education?\nOr consider the comment on Caplan’s post where Ridley notes the success of Athens, Genoa, Holland, New York, San Jose, Singapore and Hong Kong. They do not look like a random sample. They comprise intelligent, educated populations.\nRidley’s agnosticism about whether people are smart is reflected in his recent post dismissing concern about shrinking brains. As I mentioned then, there are few better predictors of a country’s wealth than the IQ of the population. There are significant benefits to a high average IQ.\nFinally, connectivity is at least partly a consequence of quality. Higher IQ people are more trusting and more likely to trade. Those with higher IQ are more likely to be connected and share the ideas they have created."
  },
  {
    "objectID": "posts/please-not-another-bias-take-two.html",
    "href": "posts/please-not-another-bias-take-two.html",
    "title": "Please not another bias: Take two",
    "section": "",
    "text": "In my last post I discussed how I would like to redo my article “Please Not Another Bias! An Evolutionary Take on Behavioural Economics”. Apart from the removing the weak experimental evidence that I referenced, I wanted to make a few points more explicitly, such as the need for theory.\nWell, that re-write is here in the latest edition of Works in Progress.\nNot much has survived from the original except the opening framing and the continued belief that evolutionary theory will play a role in developing that theory. Otherwise, it’s largely new.\nWriting this article has me half-convinced that I should create the academic version - or at least the version addressing all the “what about X?” that I have heard or anticipate. I also want to sketch more of a picture about what the theory might look like (as in contribute to addressing the problem rather than just complaining about it).\nIn the meantime, here are a few other academic pieces worth reading that make the case for more theory in behavioural economics or the behavioural sciences:\n\nMichael Muthukrishna and Joe Henrich’s A problem in theory (pdf): In addition to arguing for more theory, they nominate dual-inheritance theory as one general theoretical framework that could do the job.\nGigi Foster’s Towards a living theoretical spine for (behavioural) economics: To highlight one paragraph as to why there has been little progress on theory:\n\n\nEconomists today are incentivized to care about publishing in journals that have high citation counts and otherwise- measured “impact”, meaning in large part journals that are popular in the circle of economists. This system of incentives creates both a clique of self-referencing scholars and a very strong personal incentive for anyone outside that group to become a member of it if he wishes to gain status as an economist. To become a member of the clique of present-day economic theorists, in particular, requires that one have a great deal of expertise in mathematical techniques and a bank of knowledge about how to build mathematical models that is very difficult to acquire except through a heavy investment in apprenticeship. Once such an investment is made, the investor’s continued career depends upon the very system that originally challenged him, and the cycle begins again, with renewed commitment by the existing clique of theorists to the standard model of choice. It is in part from this socially-mediated situation – ironically or not, one that itself is rarely modelled in economics –that the inertia of our model of individual decision-making arises.\n\n\nAndreas Ortmann’s On the foundations of behavioural and experimental economics (pdf of draft): Ortmann contrasts the theory of economics and the lack of theory in psychology, pinning the struggles in behavioural economics on the sourcing of new insight from the “untheorized mess” that is psychology.\n\nOtherwise, I haven’t had a chance to read the rest of the new Works in Progress issue yet. But the couple of articles I have read are great. Saloni Dattani’s discussion of peer review felt very relevant. I’ve just kicked off the academic phase of my career and am spending a lot of time wondering about how to engage with the publishing game. What’s the best way to put my work out there (actually have it read), get good feedback, play a useful role in giving feedback to others and not get fired? The answer to those questions does not involve playing the traditional publishing game.\nI’m not a sailor, but Stewart Brand’s The Maintenance Race was also good."
  },
  {
    "objectID": "posts/please-not-another-bias-an-evolutionary-take-on-behavioural-economics.html",
    "href": "posts/please-not-another-bias-an-evolutionary-take-on-behavioural-economics.html",
    "title": "Please, not another bias! An evolutionary take on behavioural economics",
    "section": "",
    "text": "Below is a transcript of my planned presentation at today’s Marketing Science Ideas Xchange. The important images from the slide pack are below, but the full set of slides is available here.\n\nPlease, not another bias! An evolutionary take on behavioural economics\nThank you for the invitation to speak today.\nI accepted the invite because natural selection has shaped the human mind to take actions that have, in our past, tended increase reproductive success.\nThat statement isn’t as creepy as it sounds. I did not calculate the direct reproductive opportunity of this speaking engagement. Rather, our evolutionary past means that we are inclined to pursue proximate objectives that lead to the ultimate goal.\nFor example, we seek status – and what could be more status-enhancing than speaking here. And we engage in the costly signalling of our traits – such as intelligence - to the opposite sex, allies or rivals.\nAnother place where I signal is my blog at jasoncollins.blog. A copy of these slides and the text of what I plan to speak about today – which should approximate what I actually will speak about – will be posted onto jasoncollins.blog before the end of today’s talk. That text includes links to the studies I will refer to.\nTo explain why I engage in this costly signalling – conference speaking, blogging and the like - I will first take a step back and explain how the evolutionary approach to decision making relates to other approaches, starting with behavioural economics.\nAnd I should say that I am going to refer to “behavioural economics” today, even though what I am going to talk about is more rightfully called “behavioural science”.\nI once had an online discussion about this point with last year’s MSiX headlining speaker Rory Sutherland. I was in the behavioural science camp, but he said that the term behavioural economics was fantastic marketing and is effective in getting the attention of economists. Even though calling it behavioural economics is a slight to the psychological foundations of this work, we should live with it.\nNow that I work in this space and I have used the terms behavioural science and behavioural economics with a range of clients and colleagues, I am convinced that Rory was right. I receive blank looks when I use the term behavioural science. I attract immediate interest when I use the term behavioural economics.\nSo, to content. And I am going to start with a complaint. In some ways I am following the traditional format of a behavioural economics talk, which sets up the rational homo economicus straw man, and beats it to death with a series of examples of how irrational we really are. But for a change, I am going to start by beating up on behavioural economics.\nAnd I should say that, despite this bit of bashing, I have a soft spot for behavioural economics. It’s my day job for a start – helping clients in the private and public sectors better understand how their customers, employees and citizens make decisions, and how they can help them to make better ones. It’s just that behavioural economics could be so much more.\nThere are not 165 human biases\nSo, I want to take you to a Wikipedia page that I first saw when someone tweeted that they had found “the best page on the internet”. The “List of cognitive biases” was up to 165 entries on the day I took this snapshot, and it contains most of your behavioural science favourites … the availability heuristic, confirmation bias, the decoy effect – a favourite of marketers, the endowment effect and so on ….\n\n\nWikipedia\n\n\n\nBut this page, to me, points to what I see as a fundamental problem with behavioural economics.\nLet me draw an analogy with the history of astronomy. In 1500, the dominant model of the universe involved the sun, planets and stars orbiting around the earth.\nSince that wasn’t what was actually happening, there was a huge list of deviations from this model. We have the Venus effect, where Venus appears in the evening and morning and never crosses the night sky. We have the Jupiter bias, where it moves across the night sky, but then suddenly starts going the other way.\nPutting all the biases in the orbits of the planets and sun together, we end up with a picture of the orbits that looks something like this picture – epicycles on epicycles.\nBut instead of this model of biases, deviations and epicycles, what about an alternative model?\nThe earth and the planets orbit the sun.\nOf course, it’s not quite as simple as this picture – the orbits of the planets around the sun are elliptical, not circular. But, essentially, by adopting this new model of how the solar system worked, a large collection of “biases” was able to become a coherent theory.\nBehavioural economics has some similarities to the state of astronomy in 1500 – it is still at the collection of deviation stage. There aren’t 165 human biases. There are 165 deviations from the wrong model.\nSo what is this unifying theory? I suggest the first place to look is evolutionary biology. Human minds are the product of evolution, shaped by millions of years of natural selection.\nA hierarchy of decision making\nTo help you understand what an evolutionary lens adds to our understanding of human decision making, I am going to place evolutionary biology in a hierarchy of possible ways to consider the mind.\nThe first four reflect a hierarchy presented by Gerd Gigerenzer in his book Rationality for Mortals (if you haven’t read any Gigerenzer, do).\nFirst, we have the perfectly rational decision maker, homo economicus, who exhibits unbounded rationality. If you have been to enough behavioural economics presentations, you have already seen this model beaten to death.\nThe next is a model provided by economists in response to some of the behavioural critiques – a model of decision making under constraints. If you add costs to information search – there is your role for advertising and marketing – and possibly some limits to computational power, we get different decisions. It is a nice idea, but an even less realistic version of how people actually think. If you have done any late secondary or early tertiary mathematics, you will know it’s typically harder to make calculations with constraints than it is to be the unbounded rationaliser.\nThe third model is the heuristics and biases program of behavioural economics. Gigerenzer calls this work the search for “cognitive illusions.” I have already complained about that.\nNext comes what Gigerenzer calls ecological rationality. I want to spend a moment or two talking about this as it is very similar to an evolutionary approach, minus one important feature.\nEcological rationality\nThe ecological rationality approach involves asking what decision making tools the user possesses. You then look at the environment in which those tools are used, and then you can assess how those tools perform in that environment. The decision making tools and environment in which they are used are two blades of the same scissors (Herbert Simon used this description) – and you need to examine both the tool and the environment to understand the nature of the decision that has been made.\nThrough this approach you might see what are called “biases” emerge, but an ecological rationality approach allows you to understand the basis of the bias. Instead of just noting someone has made a poor decision, you might note why they were wrong and in what alternative environments those decision rules might be more effective.\nLet me give you an example – the gaze heuristic (a heuristic is a mental shortcut). The gaze heuristic is a tool that people – and dogs – use to catch balls. The heuristic is simply this – maintain the ball at a constant angle of gaze. If you move to keep this angle constant, you will end up where the ball lands. Obviously, this is easier than calculating where you should be from the velocity of the ball, angle of flight, the effect of wind resistance and so on.\nBut it results in a strange pattern of movement. Suppose you are close to the point where the ball is first hit into the air. As it rises you will tend to back away from the ball. As it then starts to fall, you will move back in. If it is hit up to the side of you, you will move to the ball in a curve. Now, if you had a behavioural economist look at the path you took to catch the ball, they might call it the curve bias or something like that – but it is actually the result of a very effective decision making tool.\nThere are also some circumstances where it works better, and some where it fails. It tends to work best when the ball is already high in the air. If you catch sight of a ball hit straight up before it has risen far, using the heuristic for its entire flight could require an impossible feat of first running away from the ball and then toward it. When we see fielders messing up a catch when the ball is hit straight up, it can be the backfire of this heuristic.\nUnderstanding this is a much richer understanding than saying that the fielder is biased because he did not run straight to where the ball was going to land. It also points to the power of heuristics. Try to train someone to run straight to where a ball will land and watch them fail. Don’t see these decision making shortcuts as poor cousins of the “more rational” approaches.\nLet me give another more marketing orientated example - the recognition heuristic. The heuristic runs along the line of “If I recognise one of two objects and not the other, then infer that the object I recognise has higher value.”\nObviously, people might use the recognition heuristic when shopping for a product. If I recognise one brand but not the other, I might assume the brand I know is superior.\nThe recognition heuristic will work when recognition is correlated with the quality of the product. I am sure you know plenty of products where brand strength is a good indicator of quality. And of course, one of the jobs of marketers is to make sure the recognition heuristic delivers success for their client – you are trying to achieve brand recognition. Then again, there are other products where brand strength probably leads people to make some poor decisions. My personal view is that the recognition heuristic works particularly poorly when it comes to beer.\nEvolutionary rationality\nNow, I consider Gigerenzer’s approach to be superior to the biases and heuristics or “cognitive illusions” approach. But it still leaves open the question of where these heuristics and other decision making tools come from. And this is where we get to the fifth level – what I will call evolutionary rationality. The toolbox that we use today has been honed by millennia of natural selection.\nAnticipating two common responses to this point, I am not going to spend today trying to convince the doubters in the audience that the human mind is a product of evolution – although I am happy to do that over a drink later.\nAnd I will highlight that humans are cultural and well as biological creatures. That we have a range of universal instincts and preferences shaped by natural selection does not say that culture is not important. What we see is a combination of evolved preferences, social norms, technologies and the like, each interacting with and shaping the others. Yes environment matters, but if you ignore the biology, you will do a poor job of understanding why consumers act the way they do.\nSo what does an evolutionary approach tell us about the human mind?\nFor a start, it tells us something about our objectives. Those who are in the audience today – all of your ancestors, without fail, have managed to do two things: survive to reproductive age, and reproduce. As little as you might like to think about it, your parents, grandparents and so on all the way back until the evolution of sex have always successfully attracted a partner to reproduce with.\nThis does not mean that we literally walk around assessing every action by whether it aids survival or reproduction. Instead, evolution shapes proximate mechanisms that lead to that ultimate goal. And consumer preferences are manifestations of our innate needs and preferences.\nFor example, on survival - we are obsessed with food – and in particular, crave sweet and fatty foods – which in historical times increased survival. Most of the successful global fast-food restaurants target those evolved tastes (in fact, you could say that the market has evolved to match those propensities).\nWe have an innate sense of danger – for example, we (and other animals) are quicker at detecting snakes than other stimuli, even when we have never seen them before.\nOn reproduction, we enjoy sex – which has obvious reproductive benefits, at least before the spread of effective contraception. We accumulate resources far beyond those required for survival. And so on.\nBefore going on, however, I should say that the shaping of proximate rather than ultimate mechanisms for survival and reproduction has some interesting consequences. Our evolved traits and preferences were shaped in times vastly different to today. Our taste for food was shaped at a time when calories were generally scarce and provided in the form of meat, tubers, nuts, vegetables and Glyptodons. The gorging that would occur after the occasional slaughter of a large prey is very different to the eating that occurs in today’s age of grain and calorie abundance. Today, we are effectively calorie unconstrained.\nAnd the joy of sex that once led us to have children clearly isn’t working as efficiently as it once did. Fertility across the developed world has plunged – although I’d be happy argue later over a drink that evolutionary forces will tend to drive fertility back up.\nThis backfiring of our evolved traits and preferences is known as mismatch. Our evolved traits do not always match the new modern environment – and this is something that makes Gigerenzer’s model of looking at the interaction of the decision making tools with the environment such a useful tool for analysis. Sometimes the tool works. Sometimes it doesn’t.\nSo what does evolutionary biology tell us about human decision making, behavioural economics and marketing?\nSex\n\n\nFerrari\n\n\n\nSo, let’s do a quick quiz. Tell me two things about the driver of this Ferrari (I have stolen this example from University of New South Wales evolutionary biologist Rob Brooks).\nFirst, the driver was male.\nSecond, the driver is likely young (in this case, 25).\nSo why is this the case?\nFemales – and in biology, this is in part how females are defined – produce a large immobile egg. Males produce a smaller gamete – sperm. The egg is the scarce resource. Women are born with a million or so eggs, but they release only one or so a month. Men produce 1,500 sperm a second. Each man in this room will produce enough sperm during this talk to fertilise every egg the women in this room will ever produce.\nThen there is what happens when a sperm and an egg are joined. The woman spends nine months carrying the baby – and is unable to reproduce during that time. She then provides the majority of infant care. Men are less constrained by any such barriers.\nThen throw in that women are certain of maternity, whereas men may not be certain of paternity, and you have vastly different patterns of reproduction between the sexes.\nMore men than women have zero children - the worst possible evolutionary outcome. A man who applies no standards to a mate choice may still go without. A woman would never have that problem.\nThen, for a few men, the rewards are vast.\nAs one example, approximately 16 million men in central Asia carry the same Y chromosome – the Y chromosome is passed from down the male lineage from father to son. This chromosome originated in Mongolia around 1000 AD with around 8 per cent of the men in the region carrying it (0.5% of the world’s male population) – they all trace their male lineage back to the same man.\nOne possibility is that this chromosome was so successful as it was carried by Genghis Khan and his close relatives. Genghis had multiple wives and a harem. He may have fathered thousands of children. His grandson Kublai Khan was famous for the size of his harem – I have seen some estimates that it contained 7,000 women (although I haven’t been able to source those estimates reliably). Whether that number is accurate or not, it is feasible that Kublai Khan could have been having hundreds of children a year.\nNo woman could ever have that level of success – but for men, the evolutionary rewards to success can be vast.\nThis brings us back to our Ferrari driver. As a male, the risk-reward calculation in evolutionary terms is quite different from women. Men face a higher probability of evolutionary oblivion and small chance of an evolutionary extravaganza. It makes sense to take risks that may lead to inordinate evolutionary success – or at least to avoid evolutionary oblivion.\nOne of my favourite examples of this comes from research by Richard Ronay and Bill von Hippel. They got some young male skateboarders to perform tricks, including a difficult trick that they could complete only half the time. Halfway through filming, a woman rated as highly attractive (corroborated by “many informal comments and phone number requests from the skateboarders”) walked onto the scene. Once she appeared, they took more risks and were less likely to bail a trick halfway through, instead riding all the way through to the crash landing (a story on ABC’s Catalyst demonstrates this effect).\nFirst, this risk taking should be seen in the context of what they are trying to achieve – attracting the female. So much of economics – and behavioural economics – is looking at the wrong objective.\nSecond, this change in risk preference in the presence of a women points to one of the most important findings in evolutionary psychology – our decision-making changes with the immediate context. We might be considered to be different personalities. Evolution has not shaped an all-knowing computer, but rather a modular computer for making different decisions based on different contexts.\nAs an example of this, show one group of people the movie The Shining, the other half a romantic movie starring Ethan Hawke. Then manipulate the ads they see during the movies to either accentuate the uniqueness of the product, or its popularity.\nThose watching The Shining are more likely to prefer popular products – safety in numbers as their danger avoidance personality is triggered. For those watching the romantic movie, they wanted unique products so that they would stand out from the crowd. Their mating motives have been triggered. You effectively get a change in preferences based on which movie they are watching and which self is answering the questions about the products. The effectiveness of social proof varied with context.\nPresent bias\nLet’s look at a traditional behavioural economics problem – present bias, which is the strong preference for present rewards over those in the future. The largest discount for the initial delay.\nIf I ask you the following question, some of you will choose A, and some B.\nChoose between:\n\nOne apple today\nTwo apples tomorrow\n\nBut if I ask you the following question, almost no-one will choose A:\nChoose between:\n\nOne apple in one year\nTwo apples in one year and one day\n\nThis change in preference shouldn’t be seen if we discount the future consistently. And if I asked you to revise your choice in the second question at the one year mark, I am effectively asking you the first question and some of you might change your mind.\nOn the one hand this seems irrational. But what if the immediate objective isn’t maximising lifetime consumption of apples?\nIn an experiment by Margo Wilson and Martin Daly – two of the pioneers of evolutionary psychology, and I recommend you read their book Homicide if you haven’t – they exposed men and women to either pictures of attractive faces or pictures of cars before undergoing tests of their degree of present bias.\nThe men who had seen the attractive faces became more severe discounters than those who had seen the cars. They became focused on the present – the mating opportunity.  The women did not become increasingly severe discounters in this experiment – although there may be a smaller effect that the experiment did not have the power to detect.\nSo here, what might be called a very strong present bias has a degree of rationality to it in that the objective of the participants is mating. Obviously, they didn’t have a chance to mate with these pictures – so there we have the issue of mismatch – but you can see the evolutionary foundation of their decision. If they did manage to capitalise on that moment and manage to mate, their evolutionary future is set.\nAn extreme example of this is seen in other species. A male black widow or preying mantis would allow themselves to be eaten at the moment of mating – this picture is of a male preying mantis getting lucky but losing his head as a consequence – massive present bias in terms of the typical measures an economist might use, highly rational from an evolutionary perspective.\nCostly signalling\nNow I want to move to what I believe is the most important idea I will communicate today.\nShortly after publishing The Origin of Species, Charles Darwin wrote “The sight of a feather in a peacock’s tail, whenever I gaze at it, makes me sick!”. He wrote this because, to him, the tail simply did not make any sense. It harmed the peacocks chance of survival. Why would a female mate with a long-tailed male and subject her long-tailed son to the same dilemma.\nBut in the mid-1970’s an evolutionary biologist, Amotz Zahavi, proposed that signals such as peacock tails can be a trusted as they handicap the bearer. Only a high quality peacock can bear the cost. If a sickly peacock tried to carry such a large tail, they’d be toast. In evolutionary lingo, the peacock’s tail is an excellent fitness indicator.\nBiologists argued about whether signals could be honest because they create a handicap for fifteen or so years after Zahavi espoused this theory. But in the early 1990s it was agreed that the maths checked out, and the idea is now broadly accepted by biologists.\nThis handicap principle also applies to human signalling. When humans are seeking a mate, you want to know as much as you can about them. You want to know their intelligence, their health, the level of conscientiousness, their kindness, the resources at their disposal and so on. You can’t just see this straight away – so people seek to signal these traits. And the products they buy are a major part of that signal.\nConspicuous consumption\nThe most obvious example of this type of signalling is conspicuous consumption. Conspicuous consumption is a signal of resources and the traits required to acquire those resources.\nOne of the most expensive watches in the world is the Patek Philippe Calibre 89. I first heard of this watch when I read Robert Franks Luxury Fever. Only four were made, with the first selling for $2.5 million and the last auction price I can find was over $5 million.  The watch has 1728 components, gives you the date of Easter each year, and unlike most mechanical watches, will not record the years 2100, 2200 and 2300 as leap years, while still recording 2400 as one (as per the order of Pope Gregory XIII in 1582). It has 28 hands and there are 2800 stars on the star chart.\nSince it is mechanical, it includes a tourbillon, a mechanism to improve accuracy by accounting for the earth’s rotation. But the funny thing is that my cheap quartz watch does not require such a mechanism, as gravity does not affect the vibrations of the crystal. The Calibre 89 also weighs over a kilo and is the size of a hockey puck. For several million dollars less, I have scored a more accurate watch that I can wear.\nBut it is the waste inherent in the Calibre 89 that makes it a reliable signal of resources – and the qualities required to accumulate those resources. All that extra expenditure is effectively waste that a man with low resources cannot bear. Think of all the most expensive consumer goods – super yachts, high quality sports cars, gold Apple watches. In terms of transport or timekeeping there are much cheaper and in fact much more reliable methods, but the waste inherent in these goods makes them an excellent signal of resources.\nSo, does this conspicuous consumption actually work as a signal?\nThere’s a decent size literature on this topic, so let’s look at two typical experiments – one on the desire of men to conspicuously consume, a second on the effect of that consumption on women.\nTake a group of men and show them pictures of attractive women and then ask them what they will do with their money. The mating prime makes men more likely to engage in conspicuous consumption or conspicuous charitable donation, but has no effect on inconspicuous consumption.\nWomen can also be affected by mating primes, although in that particular experiment their change in behaviour in response to pictures of attractive men was an desired increase in volunteering in a public way (but no increase in private benevolence).\nThe difference reflects the different traits each are communicating – men are communicating resources and the traits required to accumulate them, women their conscientiousness.\n\n\nDunn et al\n\n\n\nOn the effect of the signal, in one study men and women were shown pictures of members of the opposite sex in either a red Ford Fiesta or a silver Bentley. Unfortunately the photos in the paper are provided in black and white – as shown in this slide – but these indicate the types of images the experimental subjects were shown.\nThe result – the expensive car made the male more attractive to the females, whereas there was no effect on male perception of the female drivers. The increase in male’s attractiveness was equivalent to around 1 point on a scale of 1 to 10.\nSignalling other traits\nOf course, signalling involves far more than conspicuous consumption. We don’t only signal resources, but want to signal intelligence, conscientiousness, agreeableness or other features.\nWe buy a Cassini 1100mm reflecting telescope to signal our intelligence. We subject ourselves to years of post-secondary education to signal intelligence and conscientiousness. We buy hybrid cars to signal our agreeableness. And we don’t only signal to potential mates. We also signal to friends, relatives and rivals.\nImportantly, good signals are difficult to fake. It is difficult to exploit many products if you don’t have the right personality traits – faking education below certain levels of intelligence or conscientious is too difficult, faking wealth will run a poor person dry, faking appreciation of jazz if you have low openness will drive you nuts – the handicap is what makes the signal reliable.\nUltimately, this approach indicates that there is an important question to be asked when marketing a product. How does your product or brand allow the consumer to signal their traits to potential mates, their spouse, allies or rivals?\nUnfortunately, it’s not as simple as communicating this point directly to a potential consumer in your advertisements. A sports car ad for young males does not directly inform them that it will attract more females.\nThat is, unless you are Lynx, or Axe as it seems to be called in most countries. Lynx states the strategy overtly – “Lynx gives guys the edge in the mating game”.\nBut is this actually the strategy for most products? It is just a question of how many times removed the product is from mating outcomes. The product will increase your status, giving you an edge in the mating game. This product will intimidate rivals, giving you an edge in the mating game. This product will indicate your wealth, giving you an edge in the mating game. This product will allow you to get a high paying job to buy a sports car to indicate your wealth to give you an edge in the mating game.\nSo when a man sees a billboard with an attractive woman on a billboard, it gets attention. And from an evolutionary perspective, this is exactly the sort of thing that would draw attention. In our evolutionary past, an attractive woman would have been right there – you might think you are in with a shot.\nBut there is another more important, subtle message. This product will help you in the mating game. The girl on the car gets attention, but the more important implicit message is that this car can get you the girl. I understand there is the saying “sex sells”, and then the rebuttal, “sex sells, but only if you are selling sex”. Well, far more of you are selling sex than you realise.\nPersonally, I’d like to see more research in this area. Survey the buyers of different cars for number of sexual encounters too see if there is a difference. Of course, we have selection bias issues with those who buy the cars – so maybe we need some random allocation of sports cars to get some reliable results.\nA reading list\nNow, I have only scratched the surface over the last half hour or so, but if you are interested in this area, here are a few books to get you started – and I should say that these books heavily influenced what I have talked about today.\nThe Red Queen: Sex and the Evolution of Human Nature] by Matt Ridley was the first book that made me realise that evolutionary biology was at the core of understanding human behaviour. The first half gives a great synopsis of the origins of sex – that is, why we have sex as opposed to budding off clones – and the second asks what this means for human interactions.\nIn Spent: Sex, Evolution and Consumer Behavior, Geoffrey Miller asks whether the signalling we engage in in a mass-consumerist society does a good job of signalling the traits of interest. A consumer culture has a degree of self-deception – that above average products can compensate for below average traits. We get to know each other in minutes and are quite good at judging other people’s qualities from our interactions – that is, our intelligence, conscientiousness and so on. We can see through the product haze, and most products do a crap job of signalling the traits we think we are.\nNext, Gad Saad is the pioneer of examining consumption through an evolutionary lens. The Evolutionary Bases of Consumption is a more technical book, while The Consuming Instinct: What Juicy Burgers, Ferraris, Pornography, and Gift Giving Reveal About Human Nature is an easier read. By the end of those two books the idea that evolutionary theory is important for understanding consumption decisions will have been well and truly hammered into you.\nI spoke a lot about signalling, and Amotz Zahavi was the person in the mid-1970’s who first saw how important this is in biology. The Handicap Principle: A Missing Piece of Darwin’s Puzzle is his popular book on the topic. Robert Frank’s Luxury Fever extends the examination of signalling to conspicuous consumption.\nGerd Gigerenzer’s Rationality for Mortals: How People Cope with Uncertainty is from where I stole the first four stages of human decision making. If you do start reading Gigerenzer’s books, I suggest you don’t stop there.\nThe Rational Animal: How Evolution Made Us Smarter Than We Think by Douglas Kendrick and Vlad Griskevicus in some ways does what I did in the early part of the presentation – they show that many apparently irrational actions are actually quite rational from an evolutionary perspective. They are behind a lot of the studies I have referred to.\nAnd then there are some related articles that I also recommend reading – particularly by Owen Jones who writes a lot about the need to interface behavioural economics and evolutionary biology.\nI have a longer reading list on my blog, where I have reviews of many of these books and links to interesting papers.\nThree thoughts to chew on\nSo, having said all this, here are three ideas for you to walk away from this presentation with.\nObviously, to understand humans you need to understand our evolutionary past. An evolutionary lens provides a guide as to what people are looking for in a product. As Gad Saad points out in The Consuming Instinct, try selling Harlequin-type romance novels to men and see where that takes you – some strategies will be doomed to failure because they do not align with our evolved preferences.\nSecond, a large part of our evolved behaviour involves our desire to signal important traits and qualities to potential mates, allies and rivals. When buying a product, what traits does the consumer believe they will be signalling?\nAnd third, our evolved minds are sometimes out-of-sync with our modern environments. Use Gigerenzer’s framework (or Herbert Simon’s scissors) – what are the decision making tools we have evolved to use, what is the environment we intend to use them in, and what is the resulting decision? Biases, purchases and a large range of human behaviour will make much more sense when look at them under this lens."
  },
  {
    "objectID": "posts/pinker-takes-on-group-selection.html",
    "href": "posts/pinker-takes-on-group-selection.html",
    "title": "Pinker takes on group selection",
    "section": "",
    "text": "I was surprised at the easy run that group selection has recently had in social science circles, so I am pleased to see that Steven Pinker has waded into the fray with an essay in Edge. Pinker’s whole essay is worth a read, but there were a couple of parts of it that I particularly liked.\nThe first was Pinker’s highlighting that when many social scientists talk of group selection, they are talking of cultural group selection. Pinker writes:\n\n[M]ost of the groupwide traits that group selectionists try to explain are cultural rather than genetic. The trait does not arise from some gene whose effects propagate upward to affect the group as a whole, such as a genetic tendency of individuals to disperse which leads the group to have a widespread geographic distribution, or an ability of individuals to withstand stressful environments which leads the species to survive mass extinction events. Instead, they are traits that are propagated culturally, such as religious beliefs, social norms, and forms of political organization. Modern group selectionists are often explicit that it is cultural traits they are talking about, or even that they are agnostic about whether the traits they are referring to are genetic or cultural.\nWhat all this means is that so-called group selection, as it is invoked by many of its advocates, is not a precise implementation of the theory of natural selection, as it is, say, in genetic algorithms or artificial life simulations.\n\nCultural group selection is less prone than “biological group selection” to the criticism that migration and gene transfer between groups prevents genetic differentiation from emerging. It may be possible to argue that when someone joins the group, they absorb the culture or that it is the culture of the whole population that matters. However, you then run into Pinker’s broader question of whether the concept of cultural group selection adds anything to “history”.\nAnother interesting point Pinker makes is that apparently altruistic behaviour may be more a case of manipulation than evidence of a generally selfless inclinations.\n\nWhat we don’t expect to see is the evolution of an innate tendency among individuals to predictably sacrifice their expected interests for the interests of the group—to cheerfully volunteer to serve as a galley slave, a human shield, or cannon fodder. … What could evolve, instead, is a tendency to manipulate others to become suicide attackers, and more generally, to promulgate norms of morality and self-sacrifice that one intends to apply in full force to everyone in the group but oneself. If one is the unlucky victim of such manipulation or coercion by others, there’s no need to call it altruism and search for an evolutionary explanation, any more than we need to explain the “altruism” of a prey animal who benefits a predator by blundering into its sights.\n\nThis manipulation extends into the manner in which we treat non-kin as kin.\n\nThe cognitive twist is that the recognition of kin among humans depends on environmental cues that other humans can manipulate. Thus people are also altruistic toward their adoptive relatives, and toward a variety of fictive kin such as brothers in arms, fraternities and sororities, occupational and religious brotherhoods, crime families, fatherlands, and mother countries. These faux-families may be created by metaphors, simulacra of family experiences, myths of common descent or common flesh, and other illusions of kinship.\n\nPinker makes some other good points, but one element I alluded to above is missing from Pinker’s argument - the critique of group selection on the basis of migration and gene flow between groups. While advocates of group selection in humans often spend much effort on showing how human groups experienced regular conflicts and wars, little is focused on the likely transfer of people between groups through capturing women from the losers in war or through exogamy. It takes little inter-group migration to prevent genetic differentiation between groups. If you were to ask what it would take for me to believe that group selection was a significant force in shaping human traits, it is on this point that you would need to change my mind.\n*I make some comments on the responses to Pinker in this later post."
  },
  {
    "objectID": "posts/philip-tetlock-on-messing-with-the-algorithm.html",
    "href": "posts/philip-tetlock-on-messing-with-the-algorithm.html",
    "title": "Philip Tetlock on messing with the algorithm",
    "section": "",
    "text": "From an 80,000 hours podcast episode:\n\nRobert Wiblin: Are you a super forecaster yourself?\nPhilip Tetlock: No. I could tell you a story about that. I actually thought I could be, I would be. So in the second year of the forecasting tournament, by which time I should’ve known enough to know this was a bad idea. I decided I would enter into the forecasting competition and make my own forecasts. If I had simply done what the research literature tells me would’ve been the right thing and looked at the best algorithm that distills the most recent forecast or the best forecast and then extremises as a function of the diversity of the views within, if I had simply followed that, I would’ve been the second best forecaster out of all the super forecasters. I would have been like a super, super forecaster.\nHowever, I insisted … What I did is I struck a kind of compromise. I didn’t have as much time as I needed to research all the questions, so I deferred to the algorithms with moderate frequency. I often tweaked them. I often said they’re not right about that, I’m going tweak this here, I’m going to tweak this here. The net effect of all my tweaking effort, which was to move me from being in second place which I would’ve been if I’d mindlessly adopted the algorithmic prediction, to about 35th place. So that was … I fell 33 positions thanks to the cognitive effort I devoted there.\n\nTetlock was tweaking an algorithm that is built on human inputs (forecasts), so this isn’t a lesson that we can leave decision-making to an algorithm. The humans are integral to the process. But it is yet another story of humans taking algorithmic outputs and making them worse.\nThe question of where we should simply hand over forecasting decisions to algorithms is being explored in a new IARPA tournament involving human, machine, and human-machine hybrid forecasters. It will create some interesting data on the boundaries of where each performs best - although the algorithm described by Tetlock above and used by the Good Judgment team suggests that even a largely human system will likely need statistical combination of forecasts to succeed.\n\nRobert Wiblin: [F]irst, you have a new crowdsourcing tournament going on now, don’t you, called Hybrid Mind?\nPhilip Tetlock: Well, I wouldn’t claim that it belongs to me. It belongs to IARPA, the Intelligence Advanced Research Projects Activity, which is the same operation and US intelligence community that ran the earlier forecasting tournament. The new one is called Hybrid Forecasting Competition, and it, I think, represents a very important new development in forecasting technology. It pits humans against machines against human-machine hybrids, and they’re looking actively for human volunteers.\nSo hybridforecasting.com is the place to go if you want to volunteer.\n…\nWell, there are a lot of unknowns. It may seem obvious that machines will have an advantage when you’re dealing with complex quantitative problems. It would be very hard for humans to do better than machines when you’re trying to forecast, say, patterns of economic growth in OECD countries where you have very rich, pre-quantified time series, cross-sectional data sets, correlation matrices, lots of macro models. It’s hard to imagine people doing much better than that, but it’s not impossible because the models often over fit.\nSo far, as the better forecasters are aware of turbulence on the horizon and appropriately adjust their forecasts, they could even have an advantage on turf where we might assume machines would be able to do better.\nSo there’s a domain, I think, of questions where there’s kind of a presumption among many people observe these things that the machines have an advantage. Then there are questions where people sort of scratch their heads and say how could the machines possibly do questions like this? Here, they have in mind the sorts of questions that were posed, many of the questions that were posed anyway, on the earlier IARPA forecasting tournament, the one that lead to the discovery of super forecasters.\nThese are really hard questions about how long is the Syrian civil war going to last in 2012? Is the war going to last another six months or another 12 months? When the Swiss and French medical authorities do an autopsy on Yasser Arafat, will they discover polonium? It’s hard to imagine machines getting a lot of traction on many of these quite idiosyncratic context-specific questions where it’s very difficult to conjure any kind of meaningful statistical model.\nAlthough, when I say it’s hard to construct those things, it doesn’t mean it’s impossible.\n\nFinally, Robert Wiblin is a great interviewer. I recommend subscribing to the 80,000 hours podcast."
  },
  {
    "objectID": "posts/perrows-normal-accidents-living-with-high-risk-technologies.html",
    "href": "posts/perrows-normal-accidents-living-with-high-risk-technologies.html",
    "title": "Charles Perrow’s Normal Accidents: Living with High-Risk Technologies",
    "section": "",
    "text": "A typical story in Charles Perrow’s Normal Accidents: Living with High-Risk Technologies runs like this.\n\nWe start with a plant, airplane, ship, biology laboratory, or other setting with a lot of components (parts, procedures, operators). Then we need two or more failures among components that interact in some unexpected way. No one dreamed that when X failed, Y would also be out of order and the two failures would interact so as to both start a fire and silence the fire alarm. Furthermore, no one can figure out the interaction at the time and thus know what to do. The problem is just something that never occurred to the designers. Next time they will put in an extra alarm system and a fire suppressor, but who knows, that might just allow three more unexpected interactions among inevitable failures. This interacting tendency is a characteristic of a system, not of a part or an operator; we will call it the “interactive complexity” of the system.\nFor some systems that have this kind of complexity, … the accident will not spread and be serious because there is a lot of slack available, and time to spare, and other ways to get things done. But suppose the system is also “tightly coupled,” that is, processes happen very fast and can’t be turned off, the failed parts cannot be isolated from other parts, or there is no other way to keep the production going safely. Then recovery from the initial disturbance is not possible; it will spread quickly and irretrievably for at least some time. Indeed, operator action or the safety systems may make it worse, since for a time it is not known what the problem really is.\n\nTake this example:\n\nA commercial airplane … was flying at 35,000 feet over Iowa at night when a cabin fire broke out. It was caused by chafing on a bundle of wire. Normally this would cause nothing worse than a short between two wires whose insulations rubbed off, and there are fuses to take care of that. But it just so happened that the chafing took place where the wire bundle passed behind a coffee maker, in the service area in which the attendants have meals and drinks stored. One of the wires shorted to the coffee maker, introducing a much larger current into the system, enough to burn the material that wrapped the whole bundle of wires, burning the insulation off several of the wires. Multiple shorts occurred in the wires. This should have triggered a remote-control circuit breaker in the aft luggage compartment, where some of these wires terminated. However, the circuit breaker inexplicably did not operate, even though in subsequent tests it was found to be functional. … The wiring contained communication wiring and “accessory distribution wiring” that went to the cockpit.\n\nAs a result:\n\nWarning lights did not come on, and no circuit breaker opened. The fire was extinguished but reignited twice during the descent and landing. Because fuel could not be dumped, an overweight (21,000 pounds), night, emergency landing was accomplished. Landing flaps and thrust reversing were unavailable, the antiskid was inoperative, and because heavy breaking was used, the brakes caught fire and subsequently failed. As a result, the aircraft overran the runway and stopped beyond the end where the passengers and crew disembarked.\n\nAs Perrow notes, there is nothing complicated in putting a coffee maker on a commercial aircraft. But in a complex interactive system, simple additions can have large consequences.\nAccidents of this type in complex, tightly coupled systems are what Perrow calls a “normal accident”. When Perrow uses the word “normal”, he does not mean these accidents are expected or predictable. Many of these accidents are baffling. Rather, it is an inherent property of the system to experience an interaction of this kind from time to time.\nWhile it is fashionable to talk of culture as a solution to organisational failures, in complex and tightly coupled systems even the best culture is not enough. There is no improvement to culture, organisation or management that will eliminate the risk. That we continue to have accidents in industries with mature processes, good management and decent incentives not to blow up suggests there might be something intrinsic about the system behind these accidents.\nPerrow’s message on how we should deal with systems prone to normal accidents is that we should stop trying to fix them in ways that only make them riskier. Adding more complexity is unlikely to work. We should focus instead on reducing the potential for catastrophe when there is failure.\nIn some cases, Perrow argues that the potential scale of the catastrophe is such that the systems should be banned. He argues nuclear weapons and nuclear energy are both out on this count. In other systems, the benefit is such that we should continue tinkering to reduce the chance of accidents, but accept they will occur despite our best efforts.\nOne possible approach to complex, tightly coupled systems is to reduce the coupling, although Perrow does not dwell deeply on this. He suggests that the aviation industry has done this to an extent through measures such as corridors that exclude certain types of flights. But in most of the systems he examines, decoupling appears difficult.\nDespite Perrow’s thesis being that accidents are normal in some systems, and that no organisational improvement will eliminate them, he dedicates a considerable effort to critiquing management error, production pressures and general incompetence. The book could have been half the length with a more focused approach, but it does suggest that despite the inability to eliminate normal accidents, many complex, tightly coupled systems could be made safer through better incentives, competent management and the like.\nOther interesting threads:\n\nNormal Accidents was published in 1984, but the edition I read had an afterword written in 1999 in which Perrow examined new domains to which normal accident theory might be applied. Foreshadowing how I first came across the concept, he points to financial markets as a new domain for application. I first heard of “normal accidents” in Tim Harford’s discussion financial markets in Adapt. Perrow’s analysis of the upcoming Y2K bug under his framework seems slightly overblown in hindsight.\nThe maritime accident chapter introduced (to me) the concepts of radar assisted collisions and non collision course collisions. Radar assisted collisions are a great example of the Peltzman effect, whereby vessels that would have once remained stationary or crawled through fog now speed through. The first vessels with radar were comforted that they could see all the stationary or slow-moving obstacles as dots on their radar screen. But as the number of vessels with radars increased and those other dots also start moving with speed, we have more radar assisted collisions. On non collision course collisions, Perrow notes that most collisions involve two (or more) ships that were not on a collision course, but on becoming aware of each other managed to change course to effect a collision. Coordination failures are rife.\nPerrow argues that nuclear weapon systems are so complex and prone to failure that there is inherent protection against catastrophic accident. Not enough pieces are likely to work to give us the catastrophe. Of course, this gives reason for concern about whether they will work when we actually need them (again, maybe a positive). Perrow even asks if complexity and coupling can be so problematic that the system ceases to exist.\nPerrow spends some time critiquing hindsight bias in assessing accidents. He gives one example of a Union Carbide plant that received a glowing report from a US government department. Following an accidental gas release some months later, that same government department described the plant as accident waiting to happen. I recommend Phil Rosenzweig’s The Halo Effect for a great analysis of this problem in assessing the factors behind business performance after the fact."
  },
  {
    "objectID": "posts/payment-for-winning-the-genetic-lottery.html",
    "href": "posts/payment-for-winning-the-genetic-lottery.html",
    "title": "Payment for winning the genetic lottery",
    "section": "",
    "text": "One of the more interesting issues in the inequality debate is how we should treat the genetic lottery that contributes to unequal outcomes.\nIn a recent Econtalk podcast, Mike Munger and Russ Roberts touched on this issue in their discussion of profits and entrepreneurship (the quotes below are from the Econtalk website, so are not exact):\n\nMunger: [T]he question that John Rawls raised in the Theory of Justice and elsewhere was: Why would any of us think that we deserve any of the character or effort or intelligence that lead us to perform well? … Rawls says you got those from your parents. You won a genetic lottery. Those are morally arbitrary. We may decide to keep some of it, but only what is necessary to motivate you to do what’s good for society. The things that you know, that you do, your character–those are collectively held. They are not privately owned, according to that theory of desert. … If you start from the premise that all our talents and character are collectively owned because they are morally arbitrary, then the only reason that anyone gets to keep profits is to motivate them to do what we want them to do for the public good. …\nRoberts: … [M]y first thought is, once you give the power to the state to allocate the rewards you are not going to be in the world of justice like you hope. You are going to be in the world of rent seeking and power. My second thought is, so okay, I’m willing to admit that people would still be hedge fund managers–assuming they do something valuable, which I think many of them do. Not all financial folks are doing productive things these days, but many do. And you’d say: I concede the point that a billion is too much in the sense that less than that would still motivate them to do a good job. How would you pick the number that you think was the right number, and how would you enforce it?\nMunger: It is really interesting that most of us think, if we are defenders of capitalism–no one that I know would defend the greed claim. What we defend instead is that what the price system does is provide information about the scarcity of resources so that it’s easier for us to correct mistakes.\n\nI appreciated that Roberts and Munger do not argue that people receive exactly what they deserve, which is equal to their value to society. Instead, Roberts and Munger hit on the two of the points that I usually make about how we should respond to the genetic lottery. First, government does not equalise outcomes. More often than not, it entrenches the status quo, with redistribution to powerful interest groups (for example, home owners or agribusiness) and not the economically weak.\nThe second point is the more interesting. How do you decide how much a person should receive to motivate them to use their genetic talents wisely and allocate them to their most valuable use? As Roberts and Munger ask, how much should Steve Jobs have received for all that he did? At what point would you say, “Steve, you’ve received a lot of money, and as you received your talent in a lottery, you can’t keep any more”? How would this have affected his effort in delivering the raft of Apple consumer goods that so many of us use? I don’t know, but neither does the person determining what is someone else’s just deserts.\n(And as for most Econtalk podcasts, the rest of the discussion is worth a listen.)"
  },
  {
    "objectID": "posts/patience-and-iq.html",
    "href": "posts/patience-and-iq.html",
    "title": "Patience and IQ",
    "section": "",
    "text": "Following from my recent post on marshmallows and impatience, I have come across the following article by Dohmen et al. It was found that lower cognitive ability was associated with risk aversion and impatience."
  },
  {
    "objectID": "posts/paleo-hypotheses.html",
    "href": "posts/paleo-hypotheses.html",
    "title": "Paleo-hypotheses",
    "section": "",
    "text": "In my post on Marlene Zuk’s Paleofantasy, I referred to a review by John Hawks. Hawks suggested that Zuk’s fantasies should be thought of as hypotheses to be tested. I was not convinced that Zuk used this approach, but Hawks’s comment triggered me to write a list of what are the most interesting questions about the paleo lifestyle that I would like to see more evidence on. The list is below.\nThe common thought that runs through them, beyond Zuk’s points about recent evolution, is that adaptation to a particular diet doesn’t mean it can’t be improved. Also, humans faced broadly varied diets in our past, so I expect that humans have much flexibility in what we can eat.\nI should also note that I am posing these questions not as a challenge to the central tenets of the paleo diet, which I expect will largely hold. However, if the answers to these questions fall a certain way, the resulting dietary recommendations probably won’t be called paleo.\nFinally, while there is already evidence on the likely answer for some of them, the questions are far from closed.\n\nHow does the performance of our mental hardware vary across different diets? I ask this for two reasons. First, we haven’t only evolved physically since the dawn of agriculture, but also mentally. Second, while the paleo-diet may be a good starting point, there must almost certainly be some ways it can be improved, particularly in the context of specific domains such as intelligence. [For me, this is the question I’d most like to know more about.]\nHow does the paleo-diet compare to the Mediterranean or the Okinawan diet in terms of longevity? Or other health measures? Evolution doesn’t shape humans to live as long as possible. It shapes us to have viable offspring. I should also throw in the growing evidence that the costs of carrying some extra fat are not as high as some people claim.\nFollowing from this, what are the trade-offs? If a diet full of red meat improves health and reproductive success at some age points, does it increase cancer risk in old age? Is there a trade-off between physical and mental output? I am skeptical of claims of a world without trade-offs.\nOn the flip side to the above two points, what of Michael Rose’s argument that traits at different ages may not be directly linked, and selective forces act more strongly when we are young? Is a paleo diet more beneficial during old age, as the selective forces associated with agriculture have had less opportunity to shape traits that express when we are old? If we switch over to a paleo-diet at age 40, what costs of our pre-40 behaviour persist?\nWhile there is plenty of evidence about the potential for human evolution since the dawn of agriculture, the potential for evolution of our microbiome is orders of magnitude larger. What is the effect of the changes in our microbiome? If someone eats a paleo diet, their microbiome is considerably different from when they were eating a diet full of sugar and grain. But how much does a paleo-diet microbiome today resemble the human microbiome of tens of thousands of years ago?\nGrass seeds tend to be poisonous. But how much has the level of poison changed since humans commenced farming grains relative to our ability to digest those poisons? There is likely to have been significant evolution of grains if less poisonous varieties were selected for (and communities that farmed them would likely have had an advantage). Is rice really that bad?\nHow much of the benefit of the paleo-diet comes from simply excluding sugars and highly processed flour? If you read the paleo testimonials on sites such as Mark’s Daily Apple, I’m guessing most of the benefits came from cutting out the junk and getting some exercise. I’m sounding like Marlene Zuk here, but I’d like to see this tested.\nHow much of the benefit of the paleo-diet is due to calorific restriction? When the muffins or donuts get passed around at work, you say no. Not only are you excluding simple carbs, but you are also consuming less calories than you might have otherwise.\nIf we remember that most people hanging out on paleo-websites are the people for whom the diet works, what is the actual rate of attrition of people who chose a paleo diet as opposed to other diets? I know that I find the paleo-diet to be an easy to use heuristic, but is this the case in general?\nHow do the answers to these questions change as we look at people of different ethnicity with different agricultural histories? How much variation is there within these groups?\nAnd an exercise question - how much variation is there in feet type between people and populations? Is barefoot running better for some people’s feet than others?\n\nI’m sure I can come up with more, but these will do for the moment. Explorations of some of these questions would probably make a nice blog post, so I’ll revisit some of them soon."
  },
  {
    "objectID": "posts/overcoming-implicit-bias.html",
    "href": "posts/overcoming-implicit-bias.html",
    "title": "Overcoming implicit bias",
    "section": "",
    "text": "I have been working through The Behavioral Foundations of Public Policy, edited by Eldar Shafir, and have mixed views so far. As I go through, I will note some interesting points.\nThe opening substantive chapter by Curtis Hardin and Mahzarin Banaji is on bias - and particularly implicit bias. Implicit biases are unconscious negative (or positive) attitudes towards a person or group. Most people who claim (and believe) they are not biased because they don’t show explicit bias will nevertheless have implicit bias that affects their actions.\nThere is no shortage of tests out there on implicit bias (here’s one set, although you have to fill out a set of surveys before you get to play) and they consistently show that implicit bias exists. Even when you know it is occurring, it’s hard to overcome. Playing with the tests when writing this post, I came up with a strong automatic preference for thin over fat people.\nAs the chapter is in a book on public policy, it turns to how policy makers should deal with implicit bias. It has a generally optimistic tone about the potential to reduce implicit bias - one that I don’t necessarily share from a public policy perspective - so the paragraphs that stood out for me indicated how complicated any plans to intervene would be.\n\nResearch also suggests that the interpersonal regulation of implicit prejudice is due in part to a motivation to affiliate with others who are presumed to hold specific values related to prejudice, as implied by shared reality theory (e.g., Hardin and Conley, 2001). For example, participants exhibited less implicit racial prejudice in the presence of an experimenter wearing a T-shirt with an antiracism message than a blank T-shirt, but only when the experimenter was likeable (Sinclair et al., 2005). When the experimenter was not likeable, implicit prejudice was actually greater in the presence of the ostensibly egalitarian experimenter. In addition, social tuning in these experiments was mediated by the degree to which participants liked the experimenter, providing converging evidence that interpersonal dynamics play a role in the modulation of implicit prejudice, as they do in other dimensions of social cognition (Hardin and Conley, 2001; Hardin and Higgins, 1996).\nAs regards public and personal policy, these findings suggest that a public stance for egalitarian values is a double-edged sword, and a sharp one at that. Although it may reduce implicit prejudice among others when espoused by someone who is likeable and high in status, it may backfire when espoused by someone who is not likeable or otherwise of marginal status. This finding suggests one mechanism by which common forms of “sensitivity training” in service of the reduction of workplace sexism and racism may be subverted by interpersonal dynamics, however laudable the goals.\n\nI’m guessing that in many scenarios government and its agents would fall into the “not likeable or otherwise of marginal status” category."
  },
  {
    "objectID": "posts/ormerods-why-most-things-fail.html",
    "href": "posts/ormerods-why-most-things-fail.html",
    "title": "Ormerod’s Why Most Things Fail",
    "section": "",
    "text": "After sitting on my reading list for a few years, I have finally read Paul Ormerod’s Why Most Things Fail. Ormerod’s basic argument is that failure is all around us and given the complexity of the world, there are limits to how much corporations can control their fate or governments can control the success of their policies. Governments, firms and households lack complete information. They do not have the cognitive power to process the available information to determine the optimal choice. As a result, when you look at their success, the outcomes look more like the result of chance than of rational strategic decisions.\nOrmerod’s argument is built upon some interesting work done by himself and others in which he examined the extinction rate of United States firms (and ultimately a wider suite of global firms). Firm death tends to follow a power law distribution, and when mapped against the historical extinction of species, which we know is built upon chance events, the pattern looks similar. In models of firm extinction involving networks of interconnected firms, if firms are given much more than 10 per cent of the available information about their relationships with other firms and are able to affect those relationships, the patterns of firm death cease to mirror those which we see. This suggests that firms act with little control over their success or failure.\nWhile this is an interesting and important observation, is the mapping of firms to species the right mapping for the analogy? For example, a species is defined (roughly) as a group of organisms that are capable of interbreeding and producing viable offspring. Thus, the units of selection, the genes, are limited to within that species. In the case of a firm, if we consider the unit of selection to be a strategy, these are able to spread to any firm. All firms are capable of interbreeding and producing fertile offspring. So are firms more akin to members of a species than to each being a species on their own? And if so, what implications does this have for the model? If individual organisms within species have similar patterns of death without reproduction to that observed for the extinction of species (which I expect is roughly the case), then the implications may be small. However, without exploring these types of questions, Ormerod has not convinced that his comparison is the right one.\nOrmerod takes some time to build to his exploration of firm extinction and some detours of varying interest along the way. One of his building blocks is an exploration of Schelling’s models of segregation, which Ormerod uses to show that simple rules can result in surprising and complex phenomena. This example forms ones of the pillars of Ormerod’s case about the complexity of the world, but I wondered at times if this was the most convincing example available. Despite the complex behaviour in Schelling’s models and the difficulty of predicting which person will end up living where, the model does allow some prediction at the macro level. It is also the case for other models explored in this area, whether that being the first-order difference equations investigated by Robert May or Brian Arthur’s El Farol bar. Predicting specific results is near impossible, but picking the pattern and the effect of parameter changes on that pattern is possible.\nThe detours also includes some bashing of the neoclassical economics straw man. Ormerod’s choice of supporting evidence is interesting, but the omissions are often obvious. Take his quoting of Vernon Smith on the flaws with existing models of the operation of markets, but no mention of Smith’s experimental work which suggests how well markets seem to find an equilibrium despite the knowledge shortfalls and bounded rationality. Similarly, when discussing bounded rationality, Ormerod does not explore the success or failure of heuristics (Also strange was the crediting of bounded rationality to Akerlof and Stiglitz with no mention of Herbert Simon). Ormerod could still have made his case with a more in-depth discussion, and then it might have felt more convincing.\nOnce Ormerod has established that companies have little control over their fate, and that the world is too complicated for governments to make decisions (both arguments I am sympathetic to), he dedicates little space to ask what this means. In the company case, it comes with a call to innovation and flexibility. But given that strategic choice has little to no effect on the probability of firm survival, why will that particular approach work?\nWhen it comes to government, again the questions left unanswered are more interesting than those addressed. If governments are likely to achieve success only by chance and cannot possibly achieve success through detailed planning, what should they do? We have a host of government interventions ranging from legislation to enable joint stock companies to protection of property rights, each arguably important for our wellbeing. How would these be facilitated in a world where we otherwise throw up our hands in despair? Ormerod’s hints at some ideas but instead of exploring them, he sticks to denouncements of governments acting as though Soviet Russia was a success. Fair enough, but I sense the book sells Ormerod’s thoughts on this question short."
  },
  {
    "objectID": "posts/only-economists-are-rational.html",
    "href": "posts/only-economists-are-rational.html",
    "title": "Only economists are rational",
    "section": "",
    "text": "Andrew Gelman makes the following observation:\n\nPop economists (or, at least, pop micro-economists) are often making one of two arguments:\n\nPeople are rational and respond to incentives. Behavior that looks irrational is actually completely rational once you think like an economist.\nPeople are irrational and they need economists, with their open minds, to show them how to be rational and efficient.\n\n….\nEconomists are different from everybody else, because . . . economists “assume everyone is fundamentally alike”! But if everyone is fundamentally alike, how is it that economists are different “from almost anyone else in society”?\n\nGelman notes that it is OK to argue one or the other, just not at the same time. The difficulty is how to distinguish the proper argument for a particular case. There is a degree of arbitrariness in the choice.\nIn my pop economics moments, I tend to use a mix of the two - people are boundedly rational, but no-one, be that government, economists or me are likely to be able to make them more efficient and rational than they already are. An economist or government might be able to give them more information than they already have, but we’re less likely than them to know what they should do with it.\nThen there is the pop-evolutionary approach. You could consider there to be two similar arguments:\n\nPeople are the product of evolutionary processes, act to maximise their fitness and rationally do so.\nPeople are not adapted to modern environments, but rather to our ancestral environment of the Pleistocene. As a result, people irrationally take actions that reduce their fitness.\n\nIt is somewhat easier (though not always so) to decide between these two alternatives as there is a clear objective against which the person’s actions can be assessed - maximising fitness. In the rationality case for the economists, it is not always clear what the objective is."
  },
  {
    "objectID": "posts/oneils-weapons-of-math-destruction-how-big-data-increases-inequality-and-threatens-democracy.html",
    "href": "posts/oneils-weapons-of-math-destruction-how-big-data-increases-inequality-and-threatens-democracy.html",
    "title": "Cathy O’Neil’s Weapons of Math Destruction: How Big Data Increases Inequality and Threatens Democracy",
    "section": "",
    "text": "In her interesting Weapons of Math Destruction: How Big Data Increases Inequality and Threatens Democracy, Cathy O’Neil defines Weapons of Math Destruction based on three criteria - opacity, unfairness and scale.\nOpacity makes it hard to assess the fairness of mathematical models (I’ll use the term algorithms through most of this post), and it facilitates (or might even be a key component of) an algorithm’s effectiveness if it relies on naive subjects. “These bonds have been rated by maths geniuses - buy them.” Unfairness relates to whether the algorithm operates in the interest of the modelled subject. Scale is not just that algorithms can affect large numbers of people. Scale can also lead to the establishment of norms that do not allow anyone to escape the operation of the algorithm.\nThese three factors are common across most of the problematic algorithms O’Neil discusses, and she makes a strong and persuasive case that many algorithms could be developed or used better. But the way she combines many of her points, together with her politics, often makes it unclear what exactly the problem is or what potential solutions could (should) be.\nA distinction that might have made this clearer (or at least that I found useful) is between algorithms that don’t do what the developer intends, algorithms working as intended but that have poor consequences for those on the wrong side of their application, and algorithms that have unintended consequences once released into the wild. The first is botched math, the second is math done well to the detriment of others, while the third is good or bad math with naive application.\nFor this post I am going to break O’Neil’s case into these three categories.\nMath done poorly\nWhen it comes to the botched math, O’Neil is at her best. Her tale of teacher scoring algorithms in Washington DC is a case where the model is not helping anyone. Teachers were scored based on the deviations of student test scores from those predicted by models of the students. The bottom 2% to 5% of teachers were fired. But the combination of modelled target student scores and small classrooms made the scoring of teachers little better than random. There was almost no correlation in a teacher’s scores from one year to the next.\nHer critique of the way many models are developed is also important. Are we checking the model is working, rather than just assuming that the teachers we fired are the right ones? She contrasts the effort typically spent testing a recidivism model (for use in determining prison sentences) to the way Amazon learns about its customers. Amazon doesn’t simply develop a single “recidivism score” equivalent and take that as determinative. Instead they continue to test and learn as much as they can about their interactions with customers to make the best models they can.\nThe solutions to the botched math are simpler (at least in theory) than many of the other problems she highlights. The teacher scoring models simply require someone with competence to consider what it is they might want to care about and measure, and if it can be done, work out whether it can be done in a statistically meaningful way. If it can’t, so be it. The willingness to concede that they can’t develop a meaningful model is important if that is the case, particularly if it is designed to inform high-stakes decisions. Similarly, recidivism scoring algorithms should be subject to constant scrutiny.\nBut this raises the question of how you assess an algorithm. What is the appropriate benchmark? Perfection? Or the system it is replacing? At times O’Neil places a heavy focus on the errors of the algorithm, with little focus on the errors of the alternative - the humans it replaced. Many of O’Neil’s stories involve false positives, leading to a focus on the obvious algorithm errors, with the algorithm’s greater accuracy and the human errors unseen. A better approach might be to simply compare alternative approaches and see which is better, rather than having the human as the default. Once the superior alternative is selected, we also need to remain cognisant that the best option still might not be very good.\nAs O’Neil argues, some of the poor models would also be less harmful if they were transparent. People could pull the models apart and see whether they were working as intended. A still cleaner version might be to just release the data and let people navigate it themselves (e.g. education data), although this is not without problems. Whatever is the most salient way of sorting and ranking will become the new defacto model. If we don’t do it ourselves, someone will take that data and give us the ranking we crave.\nMath done well (for the user anyhow)\nWhen comes to math done well, O’Neil’s three limbs of the WMD definition - opacity, unfairness and scale - are a good description of the problems she sees. O’Neil’s critique is usually not so much about the maths, but the unfair use of the models for purposes such as targeting of the poor (think predatory advertising by private colleges or payday lenders) or treating workers as cogs in the machine through the use of scheduling software.\nIn these cases, it is common that the person being modelled does not even know about the model (opacity). And if they could see the model, it may be hard to understand what characteristics are driving the outcome (although this is not so different to the opacity of human decision-making). The outcome then determines how we are treated, the ads we see, the prices we see, and so on.\nOne of O’Neil’s major concerns about fairness is that the models discriminate. She suggests they discriminate against the poor, African-Americans and those with mental illness. This is generally not through a direct intention to discriminate against these groups, although O’Neil gives the example of a medical school algorithm rejecting applicants based on birthplace due to biased training data. Rather, the models use proxies for the variables of interest, and those proxies also happen to correlate with certain group features.\nThis points to the tension in the use of many of these algorithms. Their very purpose is to discriminate. They are developed to identify the features that, say, employers or lenders want. Given there is almost always a correlation between those features and some groups, you will inevitably “discriminate” against them.\nSo what is appropriate discrimination? O’Neil objects to tarring someone with group features. If you live in a certain postcode, is it fair to be categorised with everyone else in that postcode? Possibly not. But if you have an IQ that is judged likely to result in poor job performance or creditworthiness based on the past performance of other people with that IQ, is that acceptable? What of having a degree?\nThe use of features such as postcodes, IQ or degrees come from the need to identify proxies for the traits people want to identify, such as whether they will pay back the loan or deliver good work performance. Each proxy varies in the strength of prediction, so the obvious solution seems to be to get more data and better proxies. Which of these is going to give us the best prediction of what we actually care about?\nBut O’Neil often balks at this step. She tells the story of a chap who can’t get minimum wage job due to his results on a five-factor model personality test, despite his “near perfect SAT”. The scale of the use of this test means he runs into this barrier with most employers. When O’Neil points out that personality is only one-third as predictive as cognitive tests, she doesn’t make the argument that employers should be allowed to use cognitive tests. She even suggests that employers are rightfully barred from using IQ tests in recruitment (as per a 1971 Supreme Court case). But absent the cognitive tests, do employers simply turn to the next best thing?\nSimilarly, when O’Neil complains about the use of “e-scores” (proxies for credit scores) in domains where entities are not legally allowed to use credit scores to discriminate, she complains that they are using a “sloppy substitute”. But again she does not complain about the ban on using the more direct measures.\nThere are also two sides to the use of these proxies. While the use of the proxies may result in some people being denied a job or a loan, it may allow someone else to get that job or loan, or to pay a better price, when a cruder measure might have seen that person being rejected.\nO’Neil gives the example of ZestFinance, a payday lender that typically charges 60% lower than the industry standard. ZestFinance does this by finding every proxy for creditworthiness it can, picking out proxies such as correct use of capitalisation on the application form, and whether the applicant read the terms and conditions. O’Neil complains about those who are accepted for a loan but have to pay higher fees because of, say, poor spelling. This is something the poor and uneducated are more likely to incur. But her focus is on one type of outcome, those with more expensive loans (although probably still cheaper than from other payday lenders), leaving those people receiving the cheapest loans unseen. Should we deny this class of people the access to the cheaper finance these algorithms allow?\nOne interesting case in the book concerns the pricing of car insurance. An insurer wants to know who is the better driver, so they develop algorithms to price the risk appropriately. Credit scores are predictive of driving performance, so those with worse credit scores end up paying more for this.\nBut insurers also want to price discriminate to the extent that they can. That is, they want to charge each individual the highest price they will tolerate. Price discrimination can be positive for the poor. Price discrimination allows many airlines to offer cheap seats in the back of the plane when the business crowd insists on paying extra for a few inches of leg room. I benefited from the academic pricing of software for years, and we regularly see discounted pricing for students and seniors. But price discrimination can also allow the uninformed, lazy and those without options to be stripped of a few extra dollars. In the case of the insurer pricing algorithms, they are designed to price discriminate in addition to price the policy based on risk.\nIt turns out that credit score is not just predictive of driving performance, but also of buyer response to price changes. The resultant insurance pricing is an interaction of these two dimensions. O’Neil gives an example from Florida, where adults with clean driving records but poor credit scores paid $1,552 more (on average) than drivers with excellent credit but a drunk driving conviction, although it is unclear how much of this reflects risk and how much price discrimination.\nNaive math\nOne of O’Neil’s examples of a what I will call naive math are those algorithms that create a self-reinforcing feedback loop. The model does what it is supposed to do - say, predict an event - but once used in a system, the model’s classification of a certain cohort becomes self-fulfilling or self-reinforcing.\nFor example, if longer prison sentences make someone more likely to offend on their release, any indicator that results in longer sentences will in effect become more strongly correlated with re-offending. Even if the model is updated to disentangle this problem, allowing the effect of the longer sentences to be isolated, the person who received a longer sentence is doomed the next time they are scored.\nIn a sense, the model does exactly what it should, predicting who will re-offend or not, and there is ample evidence that they do better than humans. But the application of the model does more than simply predicting recidivism. It might ultimately affirm itself.\nAnother example of a feedback loop is a person flagged as a poor credit risk. As they can’t get access to cheap credit, they then go to an expensive payday lender and ultimately run into trouble. That trouble is flagged in the credit scoring system, making it even harder for them to access financial services. If the algorithm made an error in the first instance - the person was actually a good credit risk - that person might then become a poor risk because the model effectively pushed them into more expensive products.\nThe solutions to these feedback loops are difficult. On the one hand, vigilant investigation and updating the models will help ameliorate the problems. O’Neil persuasively argues that we don’t do this enough. Entities such as ZestFinance that use a richer set of data can also break the cycle for some people.\nBut it is hard to solve the case for individual mis-classification. Any model will have false positives and false negatives. The model development process can only try to limit them, often with a trade-off between the two.\nIn assessing this problem we also need to focus on the alternative. Before these algorithms were developed, people would be denied credit, parole and jobs for all sorts of whimsical decisions on the part of the human decision makers. Those decisions would then result in feedback loops as their failures are reflected in future outcomes. The algorithms might be imperfect, but can be an improvement.\nThis is where O’Neil’s scale point becomes interesting. In a world of diverse credit scoring mechanisms, a good credit risk who is falsely identified as a poor risk under one measure might by accurately classified under another. The false positive is not universal, allowing them to shop around for the right deal. But if every credit provider uses the same scoring system, someone could be universally barred. The pre-algorithm world, for all its flaws, possibly provided more opportunities for someone to find the place where they are not incorrectly classified.\nA final point on naive models (although O’Neil has more) is that models reflect goals and ideology. Sometimes this is uncontroversial - we want to keep dangerous criminals off the street. Sometimes this is more complicated - what risk of false positives are we willing to tolerate in keeping those criminals off the street? In many ways the influence of O’Neil’s politics on her critique provide the case in support of this point.\nSolutions\nBefore reading the book, I listened to O’Neil on an Econtalk episode with Russ Roberts. There she makes the point that where we run into flawed algorithms, we shouldn’t always be going back to the old way of doing things (she made that comment in the context of judges). We should be making the algorithms better.\nThat angle was generally absent from the book. O’Neil takes the occasional moment to acknowledge that many algorithms are not disrupting perfect decision-making systems, but are replacing biased judges, bank managers who favoured their friends, and unstructured job interviews with no predictive power. But through the book she seems quite willing to rip those gains down in the name of fairness.\nMore explicitly, O’Neil asks whether we should sacrifice efficiency for fairness. For instance, should we leave some data out? In many cases we already do this, by not including factors such as race. But should this extend to factors such as who someone knows, their job or their credit score.\nO’Neil’s choice of factors in this instance is telling. She asks whether someone’s connections, job or credit score should be used in a recidivism model, and suggests no as they would be inadmissible in court. But this is a misunderstanding of the court process. Those factors are inadmissible in determining guilt or innocence, but form a central part of sentencing decisions. Look at the use of referees or stories about someone’s tough upbringing. So is O’Neil’s complaint about the algorithm, or the way we dispense criminal justice in general? This reflects a feeling I had many times in the book that O’Neil’s concerns are much deeper than the effect of algorithms and extend to the nature of the systems themselves.\nPossibly the point on which I disagree with O’Neil most is her suggestion that human decision-making has a benefit in that it can evolve and adapt. In contrast, a biased algorithm does not adapt until someone fixes it. The simple question I ask is where is the evidence of human adaptation? You just need to look at all the programs to eliminate workplace bias with no evidence of effectiveness for a taste of how hard it is to deliberately change people. We continue to be prone to seeing spurious correlations, and making inconsistent and unreliable decisions. For many human decisions there is simply no feedback loop as to whether we made the right decision. How will a human lender ever know they rejected a good credit risk?\nWhile automated systems are stuck until someone fixes them, someone can fix them. And that is often what happens. Recently several people forwarded to me an article on the inability of some facial recognition systems to recognise non-Caucasian faces. But beyond the point that humans also have this problem (yes, “they all look alike”), the problem with facial recognition algorithms has been identified and, even though it is a tough problem, there are major efforts to fix it. (Considering some of the major customers of this technology are police and security services, there is an obvious interest in solving it.) In the meantime, those of us raised in a largely homogeneous population are stuck with our cross-racial face blindness."
  },
  {
    "objectID": "posts/obesity-is-not-a-public-health-problem.html",
    "href": "posts/obesity-is-not-a-public-health-problem.html",
    "title": "Obesity is not a public health problem",
    "section": "",
    "text": "It has taken a while for this month’s Cato Unbound, “Can Public Policy Stop Obesity?”, to warm up. But Christopher Snowdon’s latest post is full of good material. He takes on the question of whether obesity is a drain on the public purse, whether we consumer high sugar soda because we have no no choice, and the burden of sugar taxation.\nThe opening is particularly pointed. Is obesity a public health problem? And, can parents solve the childhood obesity crisis?\n\nFirstly, I should say that I do not hold some of the opinions that Harris and Saunders consider to be truisms. Saunders says ”that obesity is a major U.S. public health problem is not a subject of much dispute” while Harris says that “All agree that parents cannot solve the childhood obesity crisis on their own.” In fact, I do dispute that obesity is a “public health problem.” I don’t share the currently fashionable view that a public health problem is merely the aggregate of a nation’s private health problems. Obesity differs from genuine public health issues, such as unclean drinking water, pollution, and tuberculosis, in that it is not infectious and it can be prevented and cured without government intervention. It a private health problem – or, more correctly, it is a risk factor for private health problems.\nI am loath to start a sentence with the words “as a parent,” but as a parent I reject Harris’s assertion that “parents can’t compete with the overwhelmingly unhealthy food environment surrounding their children as soon as they step outside the front door.” Parents can prevent their children becoming obese, particularly when they are young. Collectively, therefore, parents can “solve the childhood obesity crisis,” if it must be put in those terms. Similarly, parents can prevent themselves from becoming obese. The evidence is all around us. Even in the United States, with its supposedly “obesogenic” environment, two-thirds of adults and 83 percent of children are not obese. Obesity is not rare enough to be called deviant, but nor is it normal enough to be viewed as an unavoidable consequence of forces that are beyond the individual’s control.\n\nRead the rest.\nIn response, Russell Saunders partially runs up the white flag. Jennifer Harris gives a more spirited defence, particularly on advertising."
  },
  {
    "objectID": "posts/nudging-for-freedom.html",
    "href": "posts/nudging-for-freedom.html",
    "title": "Nudging for freedom",
    "section": "",
    "text": "“Nudges” change the decision environment so that people make “better” decisions, while retaining freedom of choice. Fitting within what Cass Sunstein and Richard Thaler call “libertarian paternalism”, nudges are often framed as alternatives to coercive measures. If you can nudge most people toward the “right” decision through the way you frame the choice, the coercive measure is not required.\nA recent example is the introduction of default retirement savings in Illinois. A default three per cent of income will be directed to a retirement savings account, with freedom to opt out or increase the contribution. Another is where the Australian Financial System Inquiry recommended offering a default retirement income product (with certain income and risk management characteristics) to people when they retire, with people otherwise free to choose another product or blow their retirement savings on a sports car.\nOf course, plenty of coercive measures get branded as nudges, such as proposed bans on large sugary drinks. And after extolling the benefits of retaining choice, choice restricting measures are often praised (such as in this speech by Andrew Leigh, where he praises compulsory superannuation and then defends behavioural economics against claims it is paternalistic).\nBut, to the point of this post - Are there are any examples of coercive government requirements being wound back explicitly because a nudge was considered effective? Has anyone stated “We have some coercive measures in place, but we have realised that by framing decision in the right way, most of you will make a good decision. Let’s remove these coercive requirements and replace them with a nudge.”?\nFor example, have there been any compulsory savings programs replaced by default programs on the basis that the default program could be just as effective? (In fact, a default program with a higher contribution rate could result in more savings than a compulsory program.)\nIf you know of any examples, please help me out. At the moment, my example basket is empty.\n*Bryan Caplan has previously proposed some measures of this nature, none of which have been adopted."
  },
  {
    "objectID": "posts/nudging-and-the-problem-of-context-dependent-preferences.html",
    "href": "posts/nudging-and-the-problem-of-context-dependent-preferences.html",
    "title": "Nudging and the problem of context dependent preferences",
    "section": "",
    "text": "In my recent post on Robert Sugden’s The Community of Advantage: A Behavioural Economist’s Defence of the Market, I noted a couple of papers in which Sugden and Cass Sunstein debated how to make people better off “as judged by themselves” if they have context dependent preferences.\nBelow is one point and counterpoint that I found useful.\nIn a reply to Sugden’s paper, Cass Sunstein writes:\n\n\n\nMary is automatically enrolled in a Bronze Health Care Plan – it is less expensive than Silver and Gold, but it is also less comprehensive in its coverage, and it has a higher deductible. Mary prefers Bronze and has no interest in switching. In a parallel world (a lot like ours, but not quite identical, Wolf 1990), Mary is automatically enrolled in a Gold Health Care Plan – it is more expensive than Silver and Bronze, but it is also more comprehensive in its coverage, and it has a lower deductible. Mary prefers Gold and has no interest in switching.\nThomas has a serious illness. The question is whether he should have an operation, which is accompanied with potential benefits and potential risks. Reading about the operation online, Thomas is not sure whether he should go ahead with it. Thomas’ doctor advises him to have the operation, emphasizing how much he has to lose if he does not. He decides to follow the advice. In a parallel world (a lot like ours, but not quite identical), Thomas’s doctor advises him not to have the operation, emphasizing how much he has to lose if he does. He decides to follow the advice.\n\n\nIn the latter two cases, Mary and Thomas appear to lack an antecedent preference; what they prefer is an artifact of the default rule (in the case of Mary) or the framing (in the case of Thomas). …\nThese are the situations on which I am now focusing: People lack an antecedent preference, and what they like is a product of the nudge. Their preference is constructed by it. After being nudged, they will be happy and possibly grateful. We have also seen that even if people have an antecedent preference, the nudge might change it, so that they will be happy and possibly grateful even if they did not want to be nudged in advance.\nIn all of these cases, application of the AJBT [as judged by themselves] criterion is less simple. Choice architects cannot contend that they are merely vindicating choosers’ ex ante preferences. If we look ex post, people do think that they are better off, and in that sense the criterion is met. For use of the AJBT criterion, the challenge is that however Mary and Thomas are nudged, they will agree that they are better off. In my view, there is no escaping at least some kind of welfarist analysis in choosing between the two worlds in the cases of Mary and Thomas. There is a large question about which nudge to choose in such cases (for relevant discussion, see Dolan 2014). Nonetheless, the AJBT criterion remains relevant in the sense that it constrains what choice architects can do, even if it does not specify a unique outcome (as it does in cases in which people have clear ex ante preferences and in which the nudge does not alter them).\n\nSugden responds:\n\nIn Sunstein’s example, Thomas’s preference between having and not having an operation varies according to whether his attention is directed towards the potential benefits of the operation or towards its potential risks. A choice architect can affect Thomas’s choice by choosing how to present given information about benefits and risks. The problem for the AJBT criterion is that Thomas’s judgement about what makes him better off is itself context-dependent, and so cannot be used to determine the context in which he should choose.\nIn response to the question of what the choice architect ought to do in such cases, Sunstein concludes that ‘there is no escaping at least some kind of welfarist analysis’—that is, an analysis that makes ‘direct inquiries into people’s welfare’. In his comment, Sunstein does not say much about how a person’s welfare is defined or assessed, but many of the arguments in Nudge imply that the method of enquiry is to try to reconstruct the (assumedly context-independent) latent preferences that fully informed choosers would reveal in the absence of psychologically induced error. Sunstein seems to endorse this methodology when he says: ‘It is psychologically fine to think that choosers have antecedent preferences, but that because of a lack of information or a behavioural bias, their choices will not satisfy them’. Here I disagree. In the first part of my paper, which summarises a fuller analysis presented by Infante et al. (2016), I argued that it is not psychologically fine to assume that human choices result from interactions between context-independent latent preferences and behavioural biases. I maintain that the concept of latent preference is psychologically ungrounded.\nI have interpreted AJBT, as applied to category (3) cases, as referring to the judgements implicit in choosers’ latent preferences. In his comment, Sunstein offers a different interpretation—that the relevant judgements are implicit in choosers’ actual posterior preferences. Take the case of Thomas and the operation. We are told that, in whichever direction Thomas is nudged, he will be ‘happy’ with his decision, judging himself to be better off than if he had chosen differently. In other words, any nudge that causes Thomas to change his decision can be said to make him better off, as judged by himself. I think Sunstein is going astray here by thinking of nudges as causing changes in preference. Suppose the doctor directs Thomas’s attention towards the benefits of the operation and advises him to have it. Thomas accepts this advice. At the moment of choice, Thomas is thinking about the options in the frame provided by the doctor, and so he thinks he is making the right decision. But suppose that, shortly before he is wheeled into the operating theatre, he looks at some medical website that uses the opposite frame. If his preferences are context-dependent, he may now wish he had chosen differently. Sunstein is not entitled to assume that, after choosers have been nudged, their judgements become context-independent. If the AJBT criterion is to have bite—if, as Sunstein says, it is to ‘discipline the content of paternalistic interventions’—it must adjudicate between the judgements that the chooser makes in different contexts. That is why Thaler and Sunstein need the concept of latent preference—with all its problems."
  },
  {
    "objectID": "posts/not-the-jam-study-again.html",
    "href": "posts/not-the-jam-study-again.html",
    "title": "Not the jam study again",
    "section": "",
    "text": "Go to any behavioural science conference, event or presentation, and there is a high probability you will hear about “the jam study”. Last week’s excellent MSiX was no exception, with at least three references I can recall. The story is wonderfully simple and I have, at times, been mildly sympathetic to the idea. However, it is time for this story to be retired or heavily qualified with the research that has occurred in the intervening years.\nAs a start, what is the jam study? In 2000, Mark Lepper and Sheena Iyengar published their findings (ungated pdf) about the response of consumers to displays of jam in an upmarket grocery store. Their paper also contained similar experiments involving choice of chocolate and essay questions, but those experiments have not gained the same reputation.\nOn two Saturdays, they set up tasting displays of either six or 24 jars of jam. Consumers could taste as many jams as they wished, and if they approached the tasting table, also received a $1 discount coupon to buy the jam. For attracting initial interest, the large display of 24 jams did a better job, with 60 per cent of people who passed the display stopping. Forty per cent stopped at the six jam display. But only three per cent of those who stopped at the 24 jam display purchased any of the jam, compared with almost 30 per cent who stopped at the six jam display.\nThis result has been one of the centrepieces of the argument that more choice is not necessarily good. The larger display seemed to reduce consumer motivation to buy the product. The theories around this concept and the associated idea that more choice does not make us happy are often labelled the choice overload hypothesis or the paradox of choice.\nFast-forward 10 years to another paper, this one by Benjamin Scheibehenne, Rainer Greifeneder and Peter Todd. They surveyed the literature on the choice overload hypothesis - there is plenty. And across the basket of studies, evidence of choice overload does not emerge so clearly. In some cases, choice increases purchases. In others it reduces them. Scheibehenne and friends determined that the mean effect size of changing the number of choices across the studies was effectively zero.\nMore pointedly, the reviewed studies included a few attempts to replicate the jam study results. An experiment using jam in an upscale German supermarket found no effect. Other experiments found no effect of choice size using chocolates or jelly beans. There were small differences in study design between these and the original jam study (as original authors are always quick to point out when replications fail), but if studies are so sensitive to study design and hard to replicate, it seems foolhardy to extrapolate the results of the original study too far.\nThat is not to say that there is not something interesting going on here. Scheibehenne and friends suggest that there may be a set of restrictive conditions under which choice overload occurs. These conditions might involve the complexity (and not the size) of the choice, the lack of dominant alternatives, assortment of options, time pressure or the distribution of product quality. These considerations are not issues of the size of the choice itself but the way the choice is undertaken. And since the jam study appears tough to replicate, these conditions might be particularly narrow. Still, the common refrain of making it easy for customers - as recommended for dealing with choice overload issues - holds for most of them. But they suggest different and more subtle solutions than simply reducing choice.\nThere are a lot of interesting studies floating around on choice overload - from decisions about turning off life-support (ungated pdf) to retirement savings (ungated pdf) - and the message is not always the same. But reading through them makes it clear that the jam study is just the tip of an iceberg and not necessarily representative of what lies beneath.\nFinally, when Tim Harford wrote about these studies several years ago, he pointed out another often neglected argument about the importance of choice. It is only because we have choice that we are offered any good products at all, with companies incentivised to compete for us as customers. Even if choice has negative consequences, a world without choice might be worse."
  },
  {
    "objectID": "posts/not-quite-paleo.html",
    "href": "posts/not-quite-paleo.html",
    "title": "Not quite paleo",
    "section": "",
    "text": "Peter Turchin, advocate of Cliodynamics, has posted on his recent success in adopting the “paleo diet”. The diet is based on the food presumably eaten by our evolutionary ancestors in the Paleolithic era, which is before the dawn of agriculture. Lots of meat, fruit, vegetables and nuts, but no grains.\nAlthough I have much sympathy for the paleo diet for its health benefits (its largely how I eat), I’ve always thought the paleo label was not quite right. Turchin raises this point:\n\nWhen I explain to friends that I don’t eat any cereals or grains, legumes, or dairy, a frequent reply is – “what’s left?!” Actually, a lot. All kinds of meat, any seafood, eggs, all kinds of fresh vegetables (salad type – lettuce, tomatoes, cukes, radishes, green scallions, cilantro, peppers), other vegetables (all varieties of cabbages, numerous kinds of squash, avocado, olives, asparagus, onions and leaks, spinach), root vegetables (potatoes, yams, carrots, root parsley, yucca, and a number of others I haven’t explored yet), fruits and berries and nuts. No caveman ate the kind of varied diet that we can obtain by an easy trip to the supermarket. So the ‘paleo diet’ is a complete misnomer.\n\nA bigger issue, however, is the evolutionary interpretation of the paleo diet. Evolution is about reproduction, not health. That is why agriculture came to dominate the world despite the initial hit to health that resulted. And even if the paleo diet is the healthiest diet and we are well adapted to it, there is no rule of evolution that says it cannot be improved on. Turchin writes:\n\nAdditionally, there is no particular virtue in eating an undomesticated variety, compared to a domesticated one. In particular, I suspect that wild rice is probably worse for you than white rice. Both are grass seeds, and so poisonous by definition. But with the domesticated rice there is at least hope that the most poisonous varieties have been selected out (although it is not a certainty). Interesting how an evolutionary approach makes you look at things from a very different angle.\n\nAnd this is before we consider recent human evolution - compare the ability to digest grains and alcohol between groups with varying histories of agriculture, and we get very different results. Again, steering clear of rice or grains may be the better health option, but evolution has changed the equation from what it once was.\nI prefer the example of tomatoes - sourced from South America, not consumed by out ancestors on the African plains, but they are a core element of many paleo diets. I would suggest the tomato eaters are better off for it. When our hominid ancestors started to eat meat, did a group of them refuse to join in as they preferred to eat the “Pliocene diet”?"
  },
  {
    "objectID": "posts/nobel-prizes-and-marriage-markets.html",
    "href": "posts/nobel-prizes-and-marriage-markets.html",
    "title": "Nobel prizes and marriage markets",
    "section": "",
    "text": "The committee for selecting the 2012 winners of the Sveriges Riksbank Prize in Economic Sciences in Memory of Alfred Nobel (remember, it is not an original Nobel Prize) seems to have done a better job than the Peace Prize Committee. Alvin Roth and Lloyd Shapely have been awarded the prize “for the theory of stable allocations and the practice of market design”, and they are worthy winners.\nThere is plenty of commentary about their contribution in the media and blogosphere, so I’ll draw attention to just one element of Shapely’s work - the 1962 development of the Gale-Shapely algorithm with David Gale, and its application to marriage markets. The Information for the Public provided by the Royal Swedish Academy of Sciences gives a good summary of this work:\n\nHow should ten women and ten men be matched, while respecting their individual preferences? The main challenge involved designing a simple mechanism that would lead to a stable matching, where no couples would break up and form new matches which would make them better off. The solution – the Gale-Shapley “deferred acceptance” algorithm – was a set of simple rules that always led straight to a stable matching.\nThe Gale-Shapley algorithm can be set up in two alternative ways: either men propose to women, or women propose to men. In the latter case, the process begins with each woman proposing to the man she likes the best. Each man then looks at the different proposals he has received (if any), retains what he regards as the most attractive proposal (but defers from accepting it) and rejects the others. The women who were rejected in the first round then propose to their second-best choices, while the men again keep their best offer and reject the rest. This continues until no women want to make any further proposals. As each of the men then accepts the proposal he holds, the process comes to an end. Gale and Shapley proved mathematically that this algorithm always leads to a stable matching.\n\nIf we suppose that the men are making the offers, and the women considering them, the woman should reject all the offers except for her favourite, and keep stringing them along for the possibility that someone even better will come along. However, Gale and Shapely also pointed out a more important consideration in achieving the optimal result:\n\nThe specific setup of the algorithm turned out to have important distributional consequences; it matters a great deal whether the right to propose is given to the women – as in our example – or to the men. If the women propose, the outcome is better for them than if the men propose, because some women wind up with men they like better, and no woman is worse off than if the men had been given the right to propose. Indeed, the resulting matching is better for the women than any other stable matching. Conversely, the reverse algorithm – where the men propose – leads to the worst outcome from the women’s perspective.\n\nIt’s not an entirely intuitive result, but you are better off if you are the one making the offers. No prizes for shrinking violets! However, if the relative rankings of the men and women are consistent between different men and women, which party is making the offers becomes less important.\nThis is, of course, a simplified model and can fall apart under all sorts of conditions (such as offers flying in both directions, search costs, time constraints and the entry of new potential partners), which provided Alvin Roth with plenty of scope to further develop the work.\nI should also note that despite the prize being for economics, Shapely prefers the label of mathematician to economist."
  },
  {
    "objectID": "posts/newports-so-good-they-cant-ignore-you-why-skills-trump-passion-in-the-quest-for-work-you-love.html",
    "href": "posts/newports-so-good-they-cant-ignore-you-why-skills-trump-passion-in-the-quest-for-work-you-love.html",
    "title": "Newport’s So Good They Can’t Ignore You: Why Skills Trump Passion in the Quest for Work You Love",
    "section": "",
    "text": "I suspect I would have enjoyed Cal Newport’s So Good They Can’t Ignore You more if it had been written by a grumpy armchair economist. Newport’s advice is just what you would expect that economist to give:\n\nGet good at what you do (build human capital), then someone might be willing to pay you for it. If you simply follow your passion but you don’t offer anything of value, you likely won’t succeed.\nIf you become valuable, you might be able to leverage that value into control of your career and a mission. Control without value is dangerous - ask anyone who tries to set up their own business or passive income website without having something that people are willing to pay for.\n\nSince we have Newport’s version and not the grumpy economist’s, the advice is framed somewhat less bluntly and Newport tells us a series of stories about people who became good (through deliberate practice) and leveraged that skill into a great career. It’s not easy to connect with many of the examples - TV hosts, script writers, organic farmers, a programmer working at the boundary of programming and music - but I suppose they are more interesting than stories of those in dreary jobs who simply bite the bullet, skill up and get promoted.\nIn TED / self-help style, Newport introduces us to a new set of buzzwords (“career capital”, “craftsman mindset” etc.) and “laws”. I’m glad Newport independently discovered of the “law of financial viability” -  do what people are willing to pay for - but at many points of the book we are left witnessing a battle between common sense and “conventional wisdom” rather than the discovery of new deep insights.\nOne piece of advice that the economist might not have given was how to find a mission. Newport’s advice is that you should become so skilled that you are the frontier of your field. You then might be able to see new possibilities in the “adjacent possible” that you can turn into a mission. And not only does the approach need to be cutting edge, it should also be “remarkable”, defined as being so compelling that people remark on it and it can be launched in a venue that compels remarking (luckily Newport has the venue of peer review….). I suspect this might be interesting advice for a few people, but I suspect not a lot of help for the average person stuck behind a desk.\nDespite entering the book with a high degree of mood affiliation - I believe the basic argument is right - there was little in the book that convinced me either way. The storytelling and buzzwords were accompanied by little data. Threads such as those on the 10,000 hours rule and the unimportance of innate ability were somewhat off-putting.\nThat said, some points were usefully framed. Entering the workplace expecting to follow your passion will likely lead to chronic unhappiness and job shifting. Instead, suck it up and get good. There are a lot of books and blogs encouraging you to follow your passion, and most of them are garbage. So if more people follow Newport’s fluffed up way of giving some basic economic advice, that seems like a good thing."
  },
  {
    "objectID": "posts/neoclassical-theory-won-because-it-backed-the-right-horse.html",
    "href": "posts/neoclassical-theory-won-because-it-backed-the-right-horse.html",
    "title": "Neoclassical theory won because it backed the right horse",
    "section": "",
    "text": "An interesting idea in Herb Gintis’s review of The Origin of Wealth (pdf):\n\nOne of the ironies of history is that if the Walrasian model were plausible, there would be no need for real markets, real competition, or even capitalism itself. Socialism, consisting of a bureau of technocrats implementing the Walrasian auctioneer, could harness the general equilibrium system to a system of public ownership of wealth. This aspect of the general equilibrium model was clearly understood by Oskar Lange, F. M. Taylor, and Enrico Barone in their famous defense of market socialism (Barone 1935, Lange and Taylor 1938). This defense was so successful that it induced Josef Schumpeter to predict the imminent demise of capitalism (Schumpeter 1942), and led Friedrich von Hayek to rethink, and finally abandon, his commitment to neoclassical theory (Hayek 1945).\nIronically, however, Neoclassical theory has been an unrelenting defender of capitalism, and by casting its lot with real-world “competition” and real-world “markets”, it has thereby made a strategic choice that ensured its victory over the Socialists, the Syndicalists, the Institutionalists, the Populists, the Anarchists, the Communalists, and the other various movements that proposed alternatives to capitalism. Nevertheless, neoclassical theory is quite incapable of explaining what role “competition” and “markets” in fact play in a successful economy, since the terms refer to completely different concepts in Walrasian theory and in economic reality."
  },
  {
    "objectID": "posts/natural-selection-and-the-collapse-of-economic-growth.html",
    "href": "posts/natural-selection-and-the-collapse-of-economic-growth.html",
    "title": "Natural selection and the collapse of economic growth",
    "section": "",
    "text": "In my last post, I discussed Oded Galor and Omer Moav’s paper Natural Selection and the Origin of Economic Growth. As I noted then, my PhD supervisors, Juerg Weber and Boris Baer, and I have written a discussion paper that describes a simulation of the model.\nIn the discussion paper we discuss an extension of the model in which we consider the entry of people into the population that have a low preference for child quality - i.e. they weight child quantity more highly. Entry could be through migration or mutation. We show that if people with a low enough preference for quality enter the population, their higher fitness in the modern growth state can drive the economy back into Malthusian conditions.\nTo show this, we simulated a version of the model which had present at a low level in the initial population a genotype with a very low preference for educating their children (I refer to them as the strongly quantity-preferring genotype). This strongly quantity-preferring genotype has a similar fitness to other genotypes that do not educate in the Malthusian state, and declines in prevalence while the quality-preferring genotype increases.\nOnce the economy takes off into the modern growth state, the strongly quantity-preferring genotype has the highest fitness as it dedicates the lowest proportion of its resources to educating its children. The strongly quantity-preferring genotype increases in prevalence until, eventually, the average level of education plummets, undermining technological progress. The world returns to a Malthusian state, with high population growth eroding the income benefits of all earlier technological progress.\nThe following chart shows the rate of growth of population, technological progress and income per person. The first 70 to 80 generations look like the base model simulation I described in my earlier post. However, after that point, technological progress plummets to zero. For the next 150 or so generations, population growth is positive, which can occur as per person income is above subsistence. Eventually, population growth drives income down to subsistence levels.\n\nIn the next figure, you can see that the strongly quantity-preferring genotype, genotype c, grows from being a negligible part of the population to being over 90 per cent . It is this change in population composition that drives the return to Malthusian conditions (you can also see the small peak in quality-preferring types around generation 48 that kicks off the Industrial Revolution). The strongly quantity-preferring genotypes educate their children far less than the other genotypes, depressing technological progress.\n\nThere is no escape from the returned Malthusian conditions. The quality-preferring genotype will have a fitness advantage in this new Malthusian state and will increase in prevalence. Whereas that caused a take-off in economic growth the first time, this time there is no take-off. The strongly quantity-preferring types, which now dominate the population, cannot be induced to educate their children. They simply breed faster to take advantage of any technological progress spurred by the small part of the population that is educating their children.\nThis result could also be achieved by introducing the strongly quantity-preferring genotype into the simulation at other points in time. If it occurs after the Industrial Revolution, the timing of the return to Malthusian conditions will occur later. However, short of restricting the range of potential quality-quantity preferences, there is no way to avoid the return to Malthusian conditions in this version of model. The strongly quantity-preferring genotypes will always have a fitness advantage when income is above subsistence and their population growth will drive income back down to subsistence levels.\nThere are, of course, a few possible interpretations of this result. The model or assumptions may be missing an important element (or at the extreme are wrong). Humans may only have quality-quantity preferences in the growth promoting range. Or possibly, modern levels of economic growth are only transient."
  },
  {
    "objectID": "posts/natural-selection-and-economic-growth.html",
    "href": "posts/natural-selection-and-economic-growth.html",
    "title": "Natural selection and economic growth",
    "section": "",
    "text": "Natural Selection and the Origin of Economic Growth by Oded Galor and Omer Moav is somewhat of an outlier. I’m not aware of any other paper that models the Industrial Revolution as a result of natural selection, apart from a similar paper by Galor and Michalopoulos. Zak and Park wrote a paper that examines population genetics and economic growth but they do not directly tackle the Industrial Revolution. In A Farewell to Alms, Greg Clark notes that Galor and Moav’s paper reignited his interest in this topic, but Clark does not model his hypothesis.\nGalor and Moav’s paper is based on a model that has two types of people in the population. Each of these types has a genetically inherited preference for quality or quantity of children. The quality-preferring genotype wants their children to have higher human capital, so they invest more in their education, while the quantity-preferring genotype is more interested in raw numbers.\nDuring the long Malthusian era in which both genotypes struggle to earn enough to subsist (i.e. during the thousands of years leading up the Industrial Revolution), the quality-preferring genotypes have a fitness advantage. As the quality-preferring genotypes are of higher quality, they earn higher wages. These higher wages are more than enough to cover education expenses, so they are also able to have more children than the quantity-preferring genotypes.\nThis fitness advantage leads the quality-preferring genotypes to increase in prevalence. As this occurs, technological progress increases, as the average level of education in the population drives technological progress. This in turn increases the incentive to invest in education, creating a feedback loop between technology and education.\nAs this goes on, the population grows. Per capita income does not increase as any technological progress is balanced out by population growth, which is the central problem of the Malthusian world.\nEventually, the rate of technological progress gets high enough to induce the quantity-preferring genotypes to invest in education. When this happens, the average level of education jumps, boosting technological progress and causing the Industrial Revolution.\nDuring this process, the population growth rate changes. Up to the time of the Industrial Revolution, population growth increases with technological progress. However, when the level of technology leaps with the Industrial Revolution, the level of education becomes so high that population growth drops dramatically. Everyone is investing more into education than raw numbers of children.\nFrom an evolutionary perspective, the Industrial Revolution also changes the selection pressure in the model. After the Industrial Revolution, the quality-preferring genotypes invest so much into education that they have lower fertility than the quantity-preferring genotypes. They then reduce in prevalence, their fitness advantage erased.\nGalor and Moav paper work through the dynamics of the model using phase diagrams. It is not particularly easy or intuitive to see the processes working together in their paper, so my two PhD supervisors and I have just put out a discussion paper that describes simulations of the model - and shows the dynamics in a form that is easier to visually comprehend. In the chart below, you can see the dramatic jump in technological progress around generation 45 of the simulation, with per capita income growth also jumping at that time. Meanwhile, population growth drops to zero.\n\nThis second chart shows the change population composition. The quality-preferring genotype (genotype a) steadily increases in prevalence through to the Industrial Revolution, peaking at just under 5 per cent of the population. Afterwards, it is selected against.\n\nThis change in selection pressure has an interesting implication. While natural selection is the trigger of the Industrial Revolution, the population composition before and after the transition is the same. There is no difference in population composition between developed and undeveloped countries. The only time there is a difference in population composition is during the transition, when the quality-preferring genotypes peak.\nIn some ways, the natural selection occurring in Galor and Moav’s model is a sideshow to the main event, the quality-quantity trade-off. In a similar model by Galor and Weil, a scale effect triggered the Industrial Revolution - that is, the concept that more people leads to more ideas, so technological progress increases with population growth. I am sure that other triggers could be substituted.\nThat highlights the point where I am not convinced that the model is true (to the extent that a model can be). As far as human evolution relates to economic growth, I expect that inherent quality is more important (and by quality, I mean economically useful qualities) than the quality-quantity trade-off. The Industrial Revolution was possible because higher quality people were selected for in the lead-up (and the lead up encompasses thousands of years).\nIf quality is inherent, a high-quality person should have as many children as possible and this would have little effect on quality. For a man of low resources, his larger problem is convincing a woman to mate with him and not deciding on the right quantity-quantity mix.\nThe other thing that I should note is that, like most economic models, Galor and Moav’s model includes consumption with no clear evolutionary rationale (an issue I have discussed in an earlier post). Why do people in the model consume more than subsistence? If some people chose to focus all excess consumption into raising children they would come to dominate the population. This might be justified as being something to which the population has not yet adapted, but that explanation does not satisfy me.\nHaving made these quibbles, the model is still an impressive feat. It would not have been an easy task to create a model with technological progress, population and per capita income all following a path that resembles the last few thousand years of economic growth. There are some further issues and extensions to the model that we explore in the discussion paper I referred to above, but I’ll talk about them in my next post."
  },
  {
    "objectID": "posts/my-podcast-appearances.html",
    "href": "posts/my-podcast-appearances.html",
    "title": "My podcast appearances",
    "section": "",
    "text": "Over the last few years I have appeared on several podcasts, the most recent being a discussion with Phil Agnew on the Nudge podcast. I am definitely more a writer than a speaker, but if you prefer audio to the written, check out the below.\nNudge podcast - Beware of Behaviour Science BS\n42courses - Behavioural Science & Evolutionary Biology\nTodd Nief - Loss Aversion and Ergodicity Economics: This was a long and pretty technical conversation on the back of a primer I wrote on ergodicity economics. Todd’s audience spans crossfit, death metal and the rationalist community. He assured me that my collection of early Sepultura records from my youth classifies as proto-death metal, so I managed to have some connection to all three.\nA Bunch of BS - Knowing Our Limits, Expanding Our Knowledge\nRationally Speaking with Julia Galef - A skeptical take on behavioral economics: I wrote a post afterward offering some additional thoughts. (Rationally Speaking is one of the few podcasts for which I listen to at least the beginning of every episode.)"
  },
  {
    "objectID": "posts/my-latest-article-at-behavioral-scientist-principles-for-the-application-of-human-intelligence.html",
    "href": "posts/my-latest-article-at-behavioral-scientist-principles-for-the-application-of-human-intelligence.html",
    "title": "My latest article at Behavioral Scientist: Principles for the Application of Human Intelligence",
    "section": "",
    "text": "I am somewhat slow in posting this - the article has been up more than a week - but my latest article is up at Behavioral Scientist.\nThe article is basically an argument that the scrutiny we are applying to algorithmic decision making should also be applied to human decision making systems. Our objective should be good decisions, whatever the source of the decision.\nThe introduction to the article is below.\n\nPrinciples for the Application of Human Intelligence\nRecognition of the powerful pattern matching ability of humans is growing. As a result, humans are increasingly being deployed to make decisions that affect the well-being of other humans. We are starting to see the use of human decision makers in courts, in university admissions offices, in loan application departments, and in recruitment. Soon humans will be the primary gateway to many core services.\nThe use of humans undoubtedly comes with benefits relative to the data-derived algorithms that we have used in the past. The human ability to spot anomalies that are missed by our rigid algorithms is unparalleled. A human decision maker also allows us to hold someone directly accountable for the decisions.\nHowever, the replacement of algorithms with a powerful technology in the form of the human brain is not without risks. Before humans become the standard way in which we make decisions, we need to consider the risks and ensure implementation of human decision-making systems does not cause widespread harm. To this end, we need to develop principles for the application for the human intelligence to decision making.\nRead the rest of the article here."
  },
  {
    "objectID": "posts/my-blogroll.html",
    "href": "posts/my-blogroll.html",
    "title": "My blogroll",
    "section": "",
    "text": "After my recent post on how I focus, I received a couple of requests for the blogs I follow. Here are my current subscriptions in Feedly, with occasional comments.\nSome of these blogs have been in my reader for years, others I am trialling. I am usually trialling a few at any time, and tend to have a “one in, one out” pattern of subscription. It normally takes me about 10 minutes once every day or two to scan the new entries and decide which are worth reading. This set of blogs generates more posts for my read later pile than I can get through.\nAskblog (Arnold Kling has been one of my main influences in thinking about causation in social science and economics )\nBehavioral Public Policy Blog\nBehavioral Scientist (For which I am a founding columnist. You can find my contributions here.)\nBehavioural Insights Team\nThe BE Hub\nBryan Caplan at Econlog (too much politics in the other Econlog bloggers for my taste)\nCal Newport (Author of Deep Work, for which I will I will post a review at some point. My review of So Good They Can’t Ignore You is here.)\nCentre for Advanced Hindsight\nDecision Science News\nDominic Cumming’s blog\nThe Enlightened Economist (For the book recommendations)\nEvonomics\nErgodicity Economics (Started subscribing after seeing the video posted at the bottom of this post)\nFarnam Street\nFresh Economic Thinking\nGene Expression (I’m subscribed to the full Razib Khan firehose, but am there for the gnxp material)\nideas42\nInformation Processing (Keeps me on top of the latest on genomic prediction)\nJason Collins blog (As a check that my feed is working)\nJohn Kay (Most of my day job is in financial services and markets)\nMarginal Revolution\nMatt Ridley\nMegan McArdle\nOffsetting Behaviour\nO’Reilly Media\nSlate Star Codex\nStatistical Modelling, Causal Inference, and Social Science (Andrew Gelman’s blog. In terms of what I have learnt, the most valuable blog on the list)\nTim Harford"
  },
  {
    "objectID": "posts/morriss-why-the-west-rules-for-now.html",
    "href": "posts/morriss-why-the-west-rules-for-now.html",
    "title": "Morris’s Why the West Rules For Now",
    "section": "",
    "text": "Over the Easter break, I read Ian Morris’s Why the West Rules- for Now. Morris seeks to develop what might be called a “unified theory of history” that can shed light on why the West rules the world and not the East. He covers from the emergence of the first members of the genus homo in Africa, through the development of agriculture and the Industrial Revolution to modern times.\nMorris looks at his question through the lens of biology, sociology and geography. In this post, I’ll focus on Morris’s treatment of the biological factors, as his conclusions on biology make it obsolete for his central claims. I’ll offer my thoughts on the rest of the book in another post later this week (which I should note are more positive than what I am have written below).\nIn Chapter 1, Morris describes the history of human development. Starting from the emergence of homo habilis in Africa, Morris walks the reader through the various migrations of early humans out of Africa and their spread across the globe, the discovery of “Peking man” and finally, the Out of Africa migration that occurred 60,000 to 70,000 years ago. For Morris, the final migration from Africa and the fact that this migration generally swept away humans from earlier migrations is the nail in the coffin for any biological theory of why there is a difference between East and West. He states that:\n\nIf modern humans replaced Neanderthals in the Western Old World and Homo erectus in the Eastern regions without interbreeding, racist theories tracing contemporary Western rule back to prehistoric biological differences must be wrong.\n\nMorris then looks at some of the evidence for interbreeding. While DNA evidence shows some interbreeding with Neanderthals, the similar, low proportion of Neanderthal genome in modern Easterners and Westerners suggests that this could not be a reason for the difference. He concludes that:\n\nRacist theories grounding Western rule in biology have no basis in fact. People, in large groups, are much the same wherever we find them, and we have all inherited the same restless, inventive minds from our African ancestors. Biology by itself cannot explain why the West rules.\n\nI find it odd that Morris tackles this 1930s argument, which can now only be described as a straw man. Morris should have addressed the modern argument that biology matters, which tends to focus on evolution in the last 60,000 years - that is, since modern humans left Africa. Differing selection pressures in the last 60,000 years and in particular, since the dawn of agriculture, has shaped human traits. This was the argument that Morris needed to discuss before he could ignore biology as it relates to his question.\nFurther, Morris’s statement that while people are different, you take large groups of these people and the mean traits of the group will be similar, is a sound statistical concept, but to hold it relies on you drawing the groups from the same sample. If you consider that different groups of humans have faced differing selection pressures, then measuring the mean traits of each group won’t bridge the difference.\nSome of Morris’s other references to biology were also unsatisfactory. Through the book, Morris uses an index of social development as a framework for discussing development. His index suggests that between (about) 541 and 1773, Eastern development was higher than that in the West. On this basis, he states that:\n\n[I]f Westerners really were genetically superior to everyone else, the graphs of social development that fill Chapters 4–10 would look very different. After taking an early lead, the West would have stayed ahead.\n\nA biological explanation requires nothing of the sort. Through the book, Morris talks of “the advantage of backwardness”. This refers to the idea that in a more backward area, the pressures faced by that population may give them incentive to develop solutions to their particular problems that may lead to that area becoming more developed. In some ways it is an evolutionary idea, with different ideas working better in different times and places and the successful ideas being shaped by the relevant environment. Sometimes harsher environments are better for developing these ideas.\nWe can apply this same concept to biological explanations. Whether one group’s biological traits lead to a higher state of development than another depends heavily on the environment the group is in. The shifts between violence, disorder and peace that Morris describes through the book would change whether a violent disposition, health, patience or intelligence were more beneficial traits for an individual to have. Based on this, it is possible to argue that there are biological factors relevant to development without requiring linear growth in development. Was it between 541 and 1773 that certain traits in the West, which were particularly conducive to economic growth and the reproductive success, spread (as Gregory Clark suggests was the case for England in his book A Farewell to Alms), leading Europe to an Industrial Revolution before the rest of the world? Unfortunately, Morris does not address this point.\nUpdate: Part II of my review can be found here."
  },
  {
    "objectID": "posts/more-praise-of-mathematics.html",
    "href": "posts/more-praise-of-mathematics.html",
    "title": "More praise of mathematics",
    "section": "",
    "text": "Following my post last week on the need for more complicated models in economics, a new paper in PLOS Biology argues for the importance of mathematical models in showing ‘proof of concept’ (HT: Santa Fe Institute News). The authors write:\n\nProof-of-concept models, used in many fields, test the validity of verbal chains of logic by laying out the specific assumptions mathematically. The results that follow from these assumptions emerge through the principles of mathematics, which reduces the possibility of logical errors at this step of the process. The appropriateness of the assumptions is critical, but once they are established, the mathematical analysis provides a precise mapping to their consequences.\n\nThey point to lack of trust many people have in mathematical models, but argue that once the theoretician fulfils their duty of making the robustness of the assumptions transparent, readers should take the results seriously.\n\nMuch of the doubt about the applicability of models may stem from a mistrust of the effects of logistical assumptions. It is the responsibility of the theoretician to make his or her knowledge of the robustness of these assumptions transparent to the reader; it may not always be obvious which assumptions are critical versus logistical, and whether the effects of the latter are known. It is likewise the responsibility of the empirically-minded reader to approach models with the same open mind that he or she would an experiment in an artificial setting, rather than immediately dismiss them because of the presence of logistical assumptions.\n\nSeveral examples are provided in the paper, but my favourite example of models as ‘proof of concept’ relates to the handicap principle. I have posted about this model before (that time in the context of economists solving the problem 17 years before the biologists figured it out), so I will use some of my previous words.\n\n[In 1975], Amotz Zahavi had a paper published titled Mate selection - a selection for a handicap. This paper spelt out Zahavi’s handicap principle, which described how honest signals of quality between animals could evolve. The signals are honest because they impose a handicap on the signaller that only a high quality signaller can bear.\nThe handicap principle was not accepted at first. Richard Dawkins wrote in an early edition of The Selfish Gene:\n\nI do not believe this theory, although I am not quite so confident in my scepticism as I was when I first heard it. I pointed out then that the logical conclusion to it should be the evolution of males with only one leg and only one eye. Zahavi, who comes from Israel, instantly retorted: ‘Some of our best generals have only one eye!’ Nevertheless, the problem remains that the handicap theory seems to contain a basic contradiction. If the handicap is a genuine one-and it is of the essence of the theory that it has to be a genuine one-then the handicap itself will penalize the offspring just as surely as it may attract females. It is, in any case, important that the handicap must not be passed on to daughters.\n\nJohn Maynard Smith published papers (such as this) suggesting that no model could be found in which the handicap principle could hold (although he did not rule out someone else finding one).\nFinally, in 1990, Alan Grafen published two papers in which he established the population genetic and game theoretic foundations to the handicap principle. Mathematically, it could work. It convinced people such as Dawkins that the handicap principle could be right. … While Grafen’s papers are quite technical, the following diagram by Rufus Johnstone provides a simple illustration of how it works - and how similar it is to the work of Michael Spence. If two different quality individuals face differential costs and the same benefits (or differential benefits and the same costs), they will signal at different levels, making their signal a reliable indicator of their quality. The high-quality individual maximises costs relative to benefits at s*_{high}, while the low-quality individual maximises their benefits relative to costs at slow.\n\n\nI like this example for two reasons. First, a mathematical model effectively settled a dispute in biology. Most biologists now agree the evolution of handicaps as signals is plausible - it is now a question of how prevalent. But second, once the complicated model was developed, a quick intuitive mathematical explanation that is relatively easy to convert back into English followed."
  },
  {
    "objectID": "posts/more-people-means-more-ideas-and-mutations.html",
    "href": "posts/more-people-means-more-ideas-and-mutations.html",
    "title": "More people means more ideas AND mutations",
    "section": "",
    "text": "A core ideas in economics is that more people means more ideas. To take an extreme case, you would expect a population of one person to generate fewer ideas that a population of one million people. The precise relationship between population and ideas depends on factors such as the fishing-out of ideas, network effects, the composition of the population and the like, but it would seem to be strongly positive.\nWhen you combine this assumption with the Malthusian concept that the level of technology constrains population, a larger population grows faster than a smaller population as a larger population generates more ideas to ease this Malthusian constraint. Michael Kremer used this argument to explain the greater than exponential population growth of the last million or so years (although that pattern has broken down since 1950).\nThis argument has a counterpart in evolutionary biology. More people means more mutations. From R.A. Fisher:\n\nThe great contrast between abundant and rare species lies in the number of individuals available in each generation as possible mutants. The actual number of mutations in each generation must therefore be proportional to the population of the species. With mutations having appreciable mutation rates, this makes no difference, for these will reach an equilibrium with counterselection at the same proportional incidence. The importance of the contrast lies with the extremely rare mutations, in which the number of new mutations occurring must increase proportionately to the number of individuals available. It is to this class, as has been shown, that the beneficial mutations must be confined, and the advantage of the more abundant species in this respect is especially conspicuous.\n\nThe greater number of mutations then provides more variation on which natural selection can act. Larger groups will, other things being equal, experience faster evolutionary change. Fisher again:\n\nThe theoretical deduction that the actual number of a species is an important factor in determining the amount of variance which it displays, thus seems to be justified by such observations as are at present available. Its principal consequence for evolutionary theory seems to be that already inferred by Darwin, that abundant species will, ceteris paribus, make the most rapid evolutionary progress, and will tend to supplant less abundant groups with which they come into competition. We may infer that in the ordinary condition of the earth’s inhabitants a large number of less abundant species will be decreasing in numbers, while a smaller number of more abundant species will be increasing …\n\nCombining these two concepts - more people means more ideas and more mutations - gives larger human populations a double advantage over a long-term horizon. The higher level of production of ideas and beneficial mutations provides two avenues from which large populations can continue to grow."
  },
  {
    "objectID": "posts/monkeys-respond-malthusian-limit.html",
    "href": "posts/monkeys-respond-malthusian-limit.html",
    "title": "Monkeys respond to the Malthusian limit",
    "section": "",
    "text": "From Smithsonian magazine (HT: John Hawks):\n\nThough northern muriquis are critically endangered, the population in Strier’s study site, which is protected from further deforestation and hunting, has increased. There are now 335 individuals in four groups, a sixfold increase since Strier started her study.\nThat’s a development worth celebrating, but it’s not without consequences. The monkeys appear to be outgrowing the reserve and, in response to this population pressure, altering millennia of arboreal behavior. These tree-dwellers, these born aerialists, are spending more and more time on the ground. At first the behavior was surprising. Over time, though, Strier made some sense of it. “They’re on an island, with no place to go but up or down. When humans didn’t have enough food, they invented intensive agriculture. Monkeys come to the ground. It makes me think of how hominids had to eke out an existence in a hostile environment. Our ancestors would have brought to that challenge the plasticity we’re seeing here.”\nInitially the muriquis descended only briefly and only for necessities, Strier says. Now they’re staying down for up to four hours—playing, resting and even mating. …\nStrier wonders about the potential for other changes. What will peaceful, egalitarian primates do if crowding becomes more severe and resources run short? “I predict a cascade of effects and demographic changes,” she says. Will the monkeys become more aggressive and start to compete for food and other essentials the way chimps and baboons do? Will the clubby camaraderie between males fall apart? Will the social fabric tear, or will the muriquis find new ways to preserve it? Strier has learned that there is no fixed behavior; instead, it’s driven by circumstances and environmental conditions. Context matters.\n\nFor humans, the stagnation in income in the Malthusian world hid an underlying dynamism as people competed for scarce resources. Each innovation that increased resources allowed population density to increase. In a similar way, the muriquis are able to increase their density through a new innovation, moving to the ground.\nIn the human case, the innovation in the Malthusian state was ultimately the seed for the Industrial Revolution. The muriquis are some way from that, but it is possible to see it on the same spectrum of change that humans have undergone in the past."
  },
  {
    "objectID": "posts/models-without-data.html",
    "href": "posts/models-without-data.html",
    "title": "Models without data",
    "section": "",
    "text": "A new paper in PNAS suggests that the similarity between European and Neanderthal genomes is due to population structure in Africa (500,000 odd years ago), not recent interbreeding (50,000 odd years ago). It has been getting a decent bashing, much of it before it was even released. The problem is that the model underlying the theory does not match recent data, which has overtaken the model since the idea behind it was first conceived.\nJohn Hawks writes:\n\nPaleoanthropology is a field where data are rare and precious, and we do a lot of arguing about the validity of models. …\nGenomics is not such a field. We have abundant data today to compare with Neandertal genomes. Yet puzzlingly, the idea of Neandertal ancestry has been challenged by several papers that haven’t performed any new empirical comparisons at all. I’m struggling to figure this out. We have an unparalleled ability to explore the genomes of humans and Neandertals, and we should believe a computer model with no empirical data?\nModeling is a lot of work. We’re trying to avoid putting a lot of investment into modeling that will be easily refuted by the next piece of genomic data. Data are flowing now so rapidly that we can afford to be naive empiricists. …\nDavid Reich dismissed the new paper by Eriksson and Manica as “obsolete”. I agree. The paper describes a model without carrying out any new empirical comparisons, and so has fallen behind where the science has gone.\n\nIf we set up a continuum between the rare data of paleoanthropology and the abundant data of genomics, economics is closer to the genomics end of the continuum. Yet papers with models and no reference to the empirical evidence abound, even where the data is plentiful. I suspect that this is at least partly due to the culture in economics. As I wrote earlier this year, a beautiful model in economics is often appreciated, even where it is in direct conflict with empirical evidence. And the pile of economic models that have been discarded as they are inconsistent with empirical observation is very small."
  },
  {
    "objectID": "posts/modelling-populations.html",
    "href": "posts/modelling-populations.html",
    "title": "Modelling populations",
    "section": "",
    "text": "In my previous two posts, I described the model contained in Galor and Moav’s paper Natural Selection and the Origin of Economic Growth and an extension in which we introduced genotypes with a low preference for educating their children.\nHaving been through the process of parametrising and simulating a complex economic model, I would recommend it as a method of increasing understanding of the basic model mechanics. More importantly, it can also highlight issues that are not clear from the mathematical consideration that is traditionally given to models.\nWhen Nils-Petter Lagerlof simulated a similar model by Galor and Weil, he found that the population cycled between generations. If population was low, incomes would jump causing a population boom. In the next generation, that excess population would cause populations to crash. We found a similar result in Galor and Moav’s natural selection model. For the parameter values used in our paper, the population jumped or shrank by up to 30 per cent per generation. Many parameter values drove the population extinct. This made it difficult to use parameter values that generated realistic outcomes. The base level of investment per child used in the simulation is probably higher than I would have otherwise chosen, but low values made extinction more likely.\nAs Robert May discussed in his important 1976 paper, this reflects the fact that simple mathematical models can have very complicated dynamics. May showed that in a simple population model where population growth depends on the potential rate of population growth and the carrying capacity of the environment, the population trend can vary from a stable equilibrium to apparently chaotic population perturbations.\nThis observation is an important consideration with many economic models. Since simulating the Galor and Moav model, I have played with a few other economic models and have found that many have chaotic behaviour for certain parameter values and functional forms. I have been unable to simulate some models at all without the population crashing into extinction."
  },
  {
    "objectID": "posts/mike-walsh-interviews-me-on-algorithm-aversion.html",
    "href": "posts/mike-walsh-interviews-me-on-algorithm-aversion.html",
    "title": "Mike Walsh interviews me on algorithm aversion",
    "section": "",
    "text": "Mike Walsh recently interviewed me for his Between Worlds Podcast, and here is the result. I largely talk about the material in two of my Behavioral Scientist articles on algorithm aversion (1 and 2).\nhttps://soundcloud.com/mikewalsh/jason-collins\nI listened to some of the Between Worlds back catalogue, and I recommend the following with David Epstein, author of The Sports Gene.\nhttps://soundcloud.com/mikewalsh/david-epstein-on-genetics-kenyan-marathon-runners-and-the-art-of-finding-fighter-pilots"
  },
  {
    "objectID": "posts/michael-mauboussins-think-twice-harnessing-the-power-of-counterintuition.html",
    "href": "posts/michael-mauboussins-think-twice-harnessing-the-power-of-counterintuition.html",
    "title": "Michael Mauboussin’s Think Twice: Harnessing the Power of Counterintuition",
    "section": "",
    "text": "Michael Mauboussin’s Think Twice: Harnessing the Power of Counterintuition is a multi-disciplinary book on how to improve your decision making. Framed around eight common decision-making mistakes, Mauboussin draws on disciplines including psychology, complexity theory and statistics.\nGiven the scope of the book, it does not reach great depth for most of its subject areas. But the interdisciplinary nature of the book means that most people are likely to find something new. I gained pointers to a lot of interesting reading, plus some new ways of thinking about familiar material. Below are a few interesting parts.\nOne early chapter contrasts the inside and outside views when making a judgement or prediction, a perspective I have often found helpful. The inside view uses the specific information about the problem at hand. The outside view looks at whether there are similar situations - a reference class - that can provide a statistical basis for the judgement. The simplest statistical basis is the “base rate” for that event - the probability of it generally occurring. The outside view, even a simple base rate, is typically a better indicator of the outcome than an estimate derived from the inside view.\nMauboussin points out that ignorance of the outside view is not the sole obstacle to its use. People will often ignore base rate information even when it is right in front of them. Mauboussin discusses an experiment by Freymuth and Ronan (pdf) where the experimental participants selected treatment for a fictitious disease. When the participants were able to choose a treatment with a 90% success rate that was paired with a positive anecdote, they chose it 90% of the time (choosing a control treatment with 50% efficacy the remaining 10% of the time). But when paired with a negative anecdote, only 39% chose the 90% efficacy treatment. Similarly, a treatment with 30% efficacy paired with a negative anecdote was chosen only 7% of the time, but this increased to 78% when it was paired with a positive anecdote. The stories drowned out the base rate information.\nTo elicit an outside view, Mauboussin suggests the simple trick of pretending you are predicting for someone else. Think about how the event will turn out for others. This will abstract you from the distracting inside view information and bring you closer to the more reliable outside view.\nMauboussin is at his most interesting, and differs from most standard examinations of decision making, when he considers decision making in complex systems (which happens to be the environment of many of our decisions).\nOne of his themes is it is nearly impossible to manage a complex system. Understanding any individual part may be of limited use in understanding the whole, and interfering with that part may have many unintended consequences. The century of bungling in Yellowstone National Park (via Alston Chase’s book Playing God in Yellowstone provides an example. In an increasingly connected world, more of our decisions are going to be in these types of systems.\nOne barrier to understanding a complex system is that the agents in an apparently intelligent system may not be that intelligent themselves. Mauboussin quotes biologist Deborah Gordon:\n\nIf you watch an ant try to accomplish something, you’ll be impressed by how inept it is. Ants aren’t smart, ant colonies are.\n\nComplex systems often perform well at a system level despite the dumb agents. No single ant understands what the colony is doing, yet the colony does well.\nMauboussin turns this point into a critique of behavioural finance, suggesting it is a mistake to look at individuals rather than the market:\n\nRegrettably, this mistake also shows up in behavioral finance, a field that considers the role of psychology in economic decision making. Behavioral finance enthusiasts believe that since individuals are irrational—counter to classical economic theory—and markets are made up of individuals, then markets must be irrational. This is like saying, “We have studied ants and can show that they are bumbling and inept. Therefore, we can reason that ant colonies are bumbling and inept.” But that conclusion doesn’t hold if more is different—and it is. Market irrationality does not follow from individual irrationality. You and I both might be irrationally overconfident, for example, but if you are an overconfident buyer and I am an overconfident seller, our biases may cancel out. In dealing with systems, the collective behavior matters more. You must carefully consider the unit of analysis to make a proper decision.\n\nMauboussin’s discussion of the often misunderstood concept of reversion (regression) to the mean is also useful. Here are some snippets:\n\n“Mediocrity tends to prevail in the conduct of competitive business,” wrote Horace Secrist, an economist at Northwestern University, in his 1933 book, The Triumph of Mediocrity in Business. With that stroke of the pen, Secrist became a lasting example of the second mistake associated with reversion to the mean—a misinterpretation of what the data says. Secrist’s book is truly impressive. Its four hundred-plus pages show mean-reversion in series after series in an apparent affirmation of the tendency toward mediocrity.\n…\nIn contrast to Secrist’s suggestion, there is no tendency for all companies to migrate toward the average or for the variance to shrink. Indeed, a different but equally valid presentation of the data shows a “movement away from mediocrity and [toward] increasing variation.” A more accurate view of the data is that over time, luck reshuffles the same companies and places them in different spots on the distribution. Naturally, companies that had enjoyed extreme good or bad luck will likely revert to the mean, but the overall system looks very similar through time. …\nA counterintuitive implication of mean reversion is that you get the same result whether you run the data forward or backward. So the parents of tall children tend to be tall, but not as tall as their children. Companies with high returns today had high returns in the past, but not as high as the present. …\nHere’s how to think about it. Say results are part persistent skill and part transitory luck. Extreme results in any given period, reflecting really good or bad luck, will tend to be less extreme either before or after that period as the contribution of luck is less significant. …\n\nOn this last point, a simple test of whether your activity involves skill is whether you can lose on purpose. For example, try to build a stock portfolio that will do worse than the benchmark.\nMauboussin links reversion of the mean to the “halo effect” (I recommend reading Phil Rosenzweig’s book of that name). The halo effect is the tendency of impressions from one area to influence impressions of another. In business, if people see a company with good profits, they will tend to assess the CEO’s management style, communications, organisational structure, strategic direction as all being positive.\nWhen the company’s performance later reverts to the mean, people then interpret all of these things as going bad, when it is quite possible nothing has changed. The result is that great results tend to be followed by glowing stories in the media followed by the fall:\n\nTom Arnold, John Earl, and David North, finance professors at the University of Richmond, reviewed the cover stories that Business-Week, Forbes, and Fortune had published over a period of twenty years. They categorized the articles about companies from most bullish to most bearish. Their analysis revealed that in the two years before the cover stories were published, the stocks of the companies featured in the bullish articles had generated abnormal positive returns of more than 42 percentage points, while companies in the bearish articles underperformed by nearly 35 percentage points, consistent with what you would expect. But for the two years following the articles, the stocks of the companies that the magazines criticized outperformed the companies they praised by a margin of nearly three to one.\n\nAnd to close, Mauboussin provides a great example of bureaucratic kludge preventing the use of a checklist in medical treatment:\n\nToward the end of 2007, a federal agency called the Office for Human Research Protections charged that the Michigan program violated federal regulations. Its baffling rationale was that the checklist represented an alteration in medical care similar to an experimental drug and should continue only with federal monitoring and the explicit written approval of the patient. While the agency eventually allowed the work to continue, concerns about federal regulations needlessly delayed the program’s progress elsewhere in the United States. Bureaucratic inertia triumphed over a better approach."
  },
  {
    "objectID": "posts/michael-lewiss-the-undoing-project-a-friendship-that-changed-the-world.html",
    "href": "posts/michael-lewiss-the-undoing-project-a-friendship-that-changed-the-world.html",
    "title": "Michael Lewis’s The Undoing Project: A Friendship That Changed The World",
    "section": "",
    "text": "My journey into understanding human decision making started when I read Michael Lewis’s Moneyball in 2005. The punchline - which, as it turns out, has been known across numerous domains since at least the 1950s - is that “expert” judgement is often outperformed by simple statistical analysis.\nA couple of years later I read Malcolm Gladwell’s Blink and was diverted into the world of  Gary Klein, which then led me to Kahneman and Tversky among others. It was only then that I started to think about the what it is that causes the experts to under-perform (For all Gladwell’s flaws, Gladwell is a great gateway to new ideas).\nIn the opening to The Undoing Project: A Friendship That Changed The World, Lewis tells of a similar intellectual journey (although obviously with a somewhat closer connection to Moneyball):\n\n[O]nce the dust had settled on the responses to my book [Moneyball], one of them remained more alive and relevant than the others: a review by a pair of academics, then both at the University of Chicago—an economist named Richard Thaler and a law professor named Cass Sunstein. Thaler and Sunstein’s piece, which appeared on August 31, 2003, in the New Republic, managed to be at once both generous and damning. The reviewers agreed that it was interesting that any market for professional athletes might be so screwed-up that a poor team like the Oakland A’s could beat most rich teams simply by exploiting the inefficiencies. But—they went on to say—the author of Moneyball did not seem to realize the deeper reason for the inefficiencies in the market for baseball players: They sprang directly from the inner workings of the human mind. The ways in which some baseball expert might misjudge baseball players—the ways in which any expert’s judgments might be warped by the expert’s own mind—had been described, years ago, by a pair of Israeli psychologists, Daniel Kahneman and Amos Tversky. My book wasn’t original. It was simply an illustration of ideas that had been floating around for decades and had yet to be fully appreciated by, among others, me.\n\nLewis realised that there was a deeper story to tell, with The Undoing Project the result.\nI am increasingly of the view that a biography or autobiography is one of the more effective (although not always balanced) ways to lay out a set of ideas. Between The Undoing Project and Richard Thaler’s Misbehaving, a layperson would struggle to find a more accessible and interesting introduction to behavioural science and behavioural economics.\nThe first substantive chapter of the Undoing Project focuses on Daryl Morey, General Manager of the Houston Rockets. It felt like a Moneyball style essay for which Lewis hadn’t been able to find another use (although you can read this chapter on Slate). However, it was an interesting illustration of the idea that once you have the statistics in hand, it’s still hard to eliminate the involvement of the human mind. For instance:\n\nIf he could never completely remove the human mind from his decision-making process, Daryl Morey had at least to be alive to its vulnerabilities. He now saw these everywhere he turned. One example: Before the draft, the Rockets would bring a player in with other players and put him through his paces on the court. How could you deny yourself the chance to watch him play? But while it was interesting for his talent evaluators to see a player in action, it was also, Morey began to realize, risky. A great shooter might have an off day; a great rebounder might get pushed around. If you were going to let everyone watch and judge, you also had to teach them not to place too much weight on what they were seeing. (Then why were they watching in the first place?) If a guy was a 90 percent free-throw shooter in college, for instance, it really didn’t matter if he missed six free throws in a row during the private workout.\nMorey leaned on his staff to pay attention to the workouts but not allow whatever they saw to replace what they knew to be true. Still, a lot of people found it very hard to ignore the evidence of their own eyes. A few found the effort almost painful, as if they were being strapped to the mast to listen to the Sirens’ song. One day a scout came to Morey and said, “Daryl, I’ve done this long enough. I think we should stop having these workouts. Please, just stop doing them.” Morey said, Just try to keep what you are seeing in perspective. Just weight it really low. “And he says, ‘Daryl, I just can’t do it.’ It’s like a guy addicted to crack,” Morey said. “He can’t even get near it without it hurting him.”\n\nI tend to have little interest in personal histories, so I found the following chapters leading up to Kahneman and Tversky’s collaboration less interesting. In part, this is because any attempt to understand someone’s achievements in the context of their upbringing is little more than storytelling.\nBut once the book hits the development of Kahneman and Tversky’s ideas - their work on the basic heuristics (availability, representativeness, anchoring ), the development of prospect theory, their work on happiness - the sequential discussion of how these ideas were developed added some real understanding (for me). You can also see the care that went into developing their work, with a desire to create something that would stand the test of time rather than create a headline through a cute result.\nOne of the more interesting parts of the book near the close relates to Kahneman and Tversky’s interaction with Gerd Gigerenzer (who I have written about a fair bit). While Lewis’s characterisation of Gigerenzer as an “evolutionary psychologist” is wide of the mark, Lewis captures well the frustration that I imagine Kahneman and Tversky must have felt during some of the exchanges. Lewis writes:\n\n[I]n Danny and Amos’s view he’d ignored the usual rules of intellectual warfare, distorting their work to make them sound even more fatalistic about their fellow man than they were. He also downplayed or ignored most of their evidence, and all of their strongest evidence. He did what critics sometimes do: He described the object of his scorn as he wished it to be rather than as it was. Then he debunked his description.\n\nThis debate is interesting enough that I’ll explore it in more detail in a future post. (You can now read that post here.)"
  },
  {
    "objectID": "posts/measurement-nihilism.html",
    "href": "posts/measurement-nihilism.html",
    "title": "Measurement nihilism",
    "section": "",
    "text": "Following from my recent post on Scott Barry Kaufman’s heritability measurement nihilism, Jonah Lehrer has gone a step further and taken a swipe at measurement in general, and in particular, at short-term tests. Lehrer argues that:\n\nThe larger lesson is that we’ve built our society around tests of performance that fail to predict what really matters: what happens once the test is over.\n\nI’m not averse to arguments that some people use measurements in inappropriate ways. However, Lehrer overstates his case in this article as he misses a crucial element of his argument - that people are actually mis-using the performance measures in the way he suggests.\nLehrer pulls out three examples in support of his position. First, Lehrer notes that tests of short-term cashier speed had a surprisingly weak correlation with longer term speed as measured by the electronic cashier system. There was a gap between maximum performance when they were being tested and typical performance when they weren’t. All I can say on this example is that I am sure that grocery store workers don’t get tested once for 30 items and then get left alone for the rest of their scanning careers.\nSecond, Lehrer points out that while SAT scores can predict around 12 per cent of the variation in freshman grade point average, they are less effective in predicting post-graduation achievement. Similarly, the LSAT had almost no relationship with career success. (I do not know which studies Lehrer is referring to here, so I can’t comment on the specific results.) On this example, I might be concerned if SAT scores were the sole entry measurement and they had such low predictive power, but SAT scores are not used on their own. College admission departments combine them with grade point averages, interviews and examination of the applicant’s CV. The SAT score may give context to the grade point average by indicating the mix of talent and hard work that led to their high school performance.\nThird, Lehrer calls the NFL Scouting Combine (a week-long event showcasing around 300 NFL aspirants during which they undergo a series of tests) “a big waste of time” as according to a recent study there is no consistent statistical relationship between the results of the Combine and NFL performance. Lehrer draws this conclusion from astudy by Kuzmits and Adams. Looking at the original paper, one of the performance indicators used in the study was draft position. The authors found that there was no consistent statistical relationship between Combine performance and draft position, except for the speed of running backs. The lack of a correlation between Combine performance and draft position suggests that for the teams making the draft decision, they already know the lack of predictive ability of many of the Combine’s tests.\nThe study authors also noted that a range of other activities take place at the Combine, such as team interviews, injury evaluations and urine tests. If we consider the advertising and other fan interest generated by the event, there are a number of plausible benefits. Instead, Lehrer has taken an indicator that the teams do not use and argues that as that measurement does not matter, the Combine is a waste.\nWhile my perspective is that these measurements aren’t being abused in the way Lehrer implies, Lehrer identifies the problem with these short-term tests as the failure to identify “grit”. Lehrer states:\n\nThe problem, of course, is that students don’t reveal their levels of grit while taking a brief test. Grit can only be assessed by tracking typical performance for an extended period. Do people persevere, even in the face of difficulty? How do they act when no one else is watching? Such traits often matter more than raw talent. We hear about them in letters of recommendation, but hard numbers take priority.\n\nIt is interesting that Lehrer does not consider that there are any short-term tests which might indicate how people act in the face of difficulty or when people are not watching. Lehrer is certainly aware of Walter Mischel’s marshmallow test and the large predictive power this test (given to four-year old children) had for future life success. Lehrer also seems to ignore that when colleges make decisions using SAT scores or Combine results, the decision makers use long-term performance data in the form of high school grades and college football performance.\nI wonder what Lehrer would recommend be done about these tests. Would he simply abolish the Combine and SAT? My perspective is that, if anything, we are not measuring enough. Like Michael Lewis argued in Moneyball was the case for baseball drafting, I am sympathetic to the view that we pay too little attention to measurement and too much to gut instinct. For Lehrer to convincingly argue that short-term measurement is playing too much of a role, I’d like to see some evidence that there are alternative measurements that outperform those indicators that are actually being used."
  },
  {
    "objectID": "posts/me-on-rationally-speaking-plus-some-additional-thoughts.html",
    "href": "posts/me-on-rationally-speaking-plus-some-additional-thoughts.html",
    "title": "Me on Rationally Speaking, plus some additional thoughts",
    "section": "",
    "text": "My conversation with Julia Galef on Rationally Speaking is out, exploring territory on how behavioural economics and its applications could be better.\nThere are links to a lot of the academic articles we discuss on the Rationally Speakingsite. We also talk about several of my own articles, including:\nPlease not another bias! An evolutionary take on behavioural economics (This article is my presentation script for a marketing conference. I’ve been meaning to rewrite it as an article for some time to remove the marketing flavour and to replace the evolutionary discussion with something more robust. Much of the evolutionary psychology experimental literature relies on priming, and I’m not confident the particular experiments I reference will replicate.)\nRationalizing the “Irrational”\nWhen Everything Looks Like a Nail: Building Better “Behavioral Economics” Teams\nThe difference between knowing the name of something and knowing something\nThere were a couple of questions for which I could have given different (better) answers, so here are some additional thoughts.\nAn example of evolutionary biology “explaining” a bias: I gave an example of the availability heuristic, but one for which more work has been done explicitly on the evolutionary angle is loss aversion. Let me quote from an article by Owen Jones, who has worked directly on this:\n\nTo test the idea that a variety of departures from rational choice predictions might reflect evolved adaptations, I had the good fortune to team up with primatologist Sarah Brosnan.\n…\nThe perspective from behavioral biology on the endowment effect is simple: in environments that lacked today’s novel features (such as reliable property rights, third-party enforcement mechanisms, and the like) it is inherently risky to give up what you have for something that might be slightly better. Nothing guarantees that your trading partner will perform. So in giving up one thing for another you might wind up with neither.\nSo the hypothesis is that natural selection would historically have favored a tendency to discount what you might acquire or trade for, compared to what you already have in hand, even if that tendency leads to irrational outcomes in the current (demonstrably different) environment. The basis of the hypothesis is a variation of the maxim that a bird in the hand has been, historically, worth two in the bush.\nFirst, we predicted that if the endowment effect were in fact an example of Time-Shifted Rationality then the endowment effect would likely be observable in at least some other species. Here’s why, in a nutshell. … If the endowment effect tends to lead on average to behaviors that were adaptive when there are asymmetric risks of keeping versus exchanging, then this isn’t likely to be true only for humans. It should at a minimum be observable in some or all of our closest primate relatives, i.e. the other 4 of the 5 great apes: chimpanzees, orangutans, gorillas, and bonobos.\nSecond, we predicted that if the endowment effect were in fact an example of Time-Shifted Rationality, the prevalence of the endowment effect in other species is likely to vary across categories of items. This follows because selection pressures can, and very often do, narrowly tailor behavioral predispositions that vary as a function of the evolutionary salience (i.e., significance) of the circumstance. Put another way, evolved behavioral adaptations can be “facultative,” consisting of a hierarchical set of “if-then” predispositions, which lead to alternate behaviors in alternate circumstances. Because no animal evolved to treat all objects the same, there’s no reason to expect that they, or humans, would exhibit the endowment effect equally for all objects, or equally in all circumstances. Some classes of items are obviously more evolutionarily significant than others – simply because value is not distributed evenly across all the items a primate encounters.\nThird (and as a logical consequence of prediction (2)), we predicted that the prevalence of the endowment effect will correlate – increasing or decreasing, respectively – with the increasing or decreasing evolutionary salience of the item in question. Evolutionary salience refers to the extent to which the item, under historical conditions, would contribute positively to the survival, thriving, and reproducing of the organism acquiring it.\nTo test these predictions, we conducted a series of experiments with chimpanzee subjects. No other extant theory generated this set of three predictions. And the results of our experiments corroborated all three.\n…\nOur results provided the first evidence of an endowment effect in another species. Specifically, and as predicted, chimpanzees exhibit an endowment effect consonant with many of the human studies that find the effect. As predicted, the prevalence of the effect varies considerably according to the class of item. And, as predicted most specifically, the prevalence was far greater (fourteen times greater, in fact) within a class of trading evolutionarily salient items – here, food items – for each other than it was when trading within a class of non- evolutionarily salient items – here toys. Put another way, our subjects were fourteen times more likely to hang onto their less-preferred evolutionarily salient item, when they could trade it for their more-preferred evolutionarily salient item, than they were to hang onto their less-preferred item with corresponding, but not evolutionarily salient, items.\n\nOn the role of signalling: Costly signalling theory is the idea that for a signal of quality to be honest, it should impose a cost on the signaller that someone without that quality would not be able to bear. For instance, peacocks have large unwieldy tails that consume a lot of resources, with only the highest quality males able to incur this cost.\nTo understand how signalling might affect our understanding of human behaviour, I tend to categorise the possible failures to understand their behaviour in the following three ways.\nFirst, we can simply fail to understand the objective. If a person’s objective is status, and we try to understand their actions as attempts to maximise income, we might mistakenly characterise their decisions as errors.\nSecond, we might understand the proximate objective, but fail to realise that there is an underlying ultimate objective. Someone might care about, say, income, but if it is in the context of achieving another objective, such as getting enough income to make a specific purchase, we might similarly fail to properly assess the “rationality” of their decisions. For example, there might be a minimum threshold that leads us to take “risky” actions in relation to our income.\nSignalling sometimes falls into this second basket, as the proximate objective is the signal for the ultimate objective. For instance, if we use education as a signal of our cognitive and social skills to get a job, viewing the objective as getting a good education misses the point.\nThird, even if we understand the proximate and ultimate objectives, we might fail to understand the mechanism by which the objective is achieved. Signalling can lead to complicated mechanisms that are often overlooked.\nTo illustrate, even if we know that someone is only seeking further education to increase their employment prospects, you would expect different behaviour if education was a signal and not a source of skills for use on the job. If education is purely a signal, people may only care about getting the credential, not what they learn. If education serves a more direct purpose, we would expect students to invest much more in learning.\nI discuss a couple of these points in my Behavioral Scientist article Rationalizing the”Irrational”. Evolutionary biology is a great source of material on signalling, although as I have written about before, economists did haveat least onecrucial insight earlier.\nFinally, I’ve plugged Rationally Speaking before, and here is a list of some of the episodes I enjoyed the most (there are transcripts if you prefer to read):\nTom Griffiths on how our biases might be signs of rationality\nDaniel Lakens on p-hacking\nPaul Bloom on empathy\nBryan Caplan on parenting\nPhil Tetlock on forecasting\nTom Griffiths and Brian Christian on Algorithms to Live By\nDon Moore on overconfidence\nJason Brennan on “Against Democracy”\nJessica Flanigan on self-medication\nAlison Gopnik on parenting\nChristopher Chabris on collective intelligence\nI limited myself to eleven here - there are a lot of other great episodes worth listening to."
  },
  {
    "objectID": "posts/masels-bypass-wall-street-a-biologists-guide-to-the-rat-race.html",
    "href": "posts/masels-bypass-wall-street-a-biologists-guide-to-the-rat-race.html",
    "title": "Masel’s Bypass Wall Street: A Biologist’s Guide to the Rat Race",
    "section": "",
    "text": "Tyler Cowen described Joanna Masel’s Bypass Wall Street: A Biologist’s Guide to the Rat Race as “Darwin plus Fred Hirsch on positional goods as applied to finance and portfolios. Unorthodox, interesting.”\nI agree with Cowen’s description of the book as unorthodox and interesting, although I was looking forward to more Darwin and more of a biological lens. As the title of the book implies, it provides a biologist’s view on savings and investment, and Masel’s background as a biologist - she is Associate Professor of Ecology & Evolutionary Biology at the University of Arizona - has likely guided her as to what arguments she is sympathetic to.\nBut the examination is not on the face of it from a biological perspective. Only two biological arguments directly referenced. The first is the distinction between absolute and relative competition. Relative competition can lead to wasteful arms races that are, on net, destructive of value. The second is a brief pointer to the competition between siblings for their parents’ finite attention and resources. If you asked someone to read Masel’s book and Robert Frank’s The Darwin Economy and guess who is the economist and who is the biologist, they’d likely guess their occupations the wrong way around.\nA stronger influence has been some of Masel’s reading in economics. In the preface, she points to two books to which she owes an intellectual debt - Keynes’s The General Theory of Employment, Interest and Money, and Fred Hirsch’s Social Limits to Growth. Her analysis of savings and investment rests heavily on Keynes, and Hirsch’s views on positional goods provides a hook for her biological intuition that competition can be wasteful and zero sum.\nThe main thread of the book is the journey of “Jen” (a thinly disguised Masel?) as she decides how she should invest for her retirement. Masel builds up the analysis from near first principles and works through a set of possible investment options. She asks whether Jen should invest in stocks? Which stocks? Index funds? What are the future prospects of the stock market? If returns are unlikely to be strong, what are the other options? Is there a way to tap into areas traditionally the domain of public investment, such as health and infrastructure? What of more unorthodox options? And so on.\nI won’t go into detail about where Masel lands - in some ways the most compelling part of the book is wondering just where Jen will end up - except to say that I doubt many people are going to find much guidance relevant to themselves. There are some points along Jen’s journey where I’m not convinced I agree, but they mostly relate to the finer points of what exactly savings and investment are, how it flows, and the like.\nThere are many moments in the book where Masel channels arguments argued in detail elsewhere - even though there is no sign that Masel has read these other sources. She shares Tyler Cowen and Robert Gordon’s view that many of the big innovations are over as part of her view that the stock market may be overvalued (although she is closer to Gordon’s pessimism). There are also many times where I could hear Robert Frank talking out of the pages, with her views on relative competition and public investment reflecting those of Frank.\nOn that point, the book is quite reference light - something Masel admits was deliberately done to avoid it becoming a heavily footnoted academic tome. I have some sympathy for that, but there are occasions in the book where I was longing for Masel to put up complements or counterpoints to her thinking and to discuss them.\nDespite the different paths to get there, Masel often lands on conclusions that I have a lot of sympathy for. For example, she points out the crudeness of regulation defining “sophisticated” investors based on income or assets - which limits investment options for those who don’t meet the threshold. A university lecturer, who has likely forsaken material income in their career choice, does not meet the threshold despite likely being much more sophisticated than others who do.\nShe also mounts a strong argument for setting retirement accounts free. Today’s poor need the money now. There are many vested interests keen to keep people’s money locked in retirement accounts because of the fees they can charge. (As an aside, in Australia you can self manage your compulsory retirement savings - you can’t access them before retirement, but you have effective control on the asset allocation and who takes a cut.)\nOne other argument I have sympathy for is the role of education as a signal. Education can become susceptible to arms races, leading to over-investment compared to that which would optimally be obtained absent the relative competition.\nTo close, I will suggest a short reading list for Masel. Maybe she has already read some of these, but I expect she will find a lot of material of interest.\n\nRobert Frank’s The Darwin Economy and Luxury Fever: A more explicit incorporation of the biological insight that competition can be wasteful. Frank’s views on public investment will also appeal to Masel.\nThorstein Veblen’s The Theory of the Leisure Class: Not an easy read, but if you can make it through Keynes’s General Theory…\nTyler Cowen’s The Great Stagnation and Robert Gordon’s The Rise and Fall of American Growth: Further arguments that many of the big innovations have already occurred (and for a counterpoint, Erik Brynjolfsson and Andrew McAfee’s The Second Machine Age.\nBryan Caplan’s The Case Against Education (picked up by Princeton University Press and due out in 2017): Still to come out. What does a biological framework add to the analysis of a specific issue such as this?\nRichard Nelson and Sidney Winter’s An Evolutionary Theory of Economic Change: Masel might find this interesting, and I’d like to know what she thinks of it."
  },
  {
    "objectID": "posts/markets-and-family-values.html",
    "href": "posts/markets-and-family-values.html",
    "title": "Markets and family values",
    "section": "",
    "text": "Larry Arnhart’s recent post at Darwinian Conservatism makes a couple of interesting points on family values and classical liberalism. The piece is largely a response to Geoffrey Hodgson’s claim that a market individualist cannot support family values:\n\n“Generally, if contract and trade are always the best way of organising matters, then many functions that are traditionally organised in a different manner should become commercialized . . . Pushed to the limit, market individualism implies the commercialization of sex and the abolition of the family. A consistent market individualist cannot be a devotee of ‘family values’ . . . They cannot in one breath argue that the market is the best way of ordering all socio-economic activities, and then deny it in another. If they cherish family values, then they have to recognise the practical and moral limits of the market imperatives and pecuniary exchange”\n\nI am perplexed when claims such as these are made. Where are the people claiming that the market is the best way to organise all socio-economic activities? The more usual claim is that people should be able to do as they choose – with the expectation that the innate tendency to form families will dominate in some spheres, the spontaneous order of the market in others.\nArnhart quotes Hayek to point out that the tension between markets and family has not been missed by classical liberals:\n\nIt is important, then, Hayek explains, that we neither apply the rules of the market to family life nor apply the rules of family life to the market. “If we were to apply the unmodified, uncurbed, rules of the micro-cosmos (i.e., of the small band or troop, or of, say, our families) to the macro-cosmos (our wider civilization), as our instincts and sentimental yearnings often make us wish to do, we woud destroy it. Yet if we were always to apply the rules of the extended order to our more intimate groupings, we would crush them. So we must learn to live in two sorts of world at once”\n\nArnhart takes Hayeks’ argument further, and suggests that the existence of these two worlds points to the need for the family as an institution:\n\nAs Horwitz indicates, Hayek’s idea of “living in two worlds at once” points to the need for the family as an institution in which children can learn the moral rules for both the micro world of face-to-face interactions and the macro world of anonymous interactions in the extended spontaneous order of society.\nThe Hayekian insight is that families are best situated to do this because of their advantage in knowledge and incentives. The intimacy of the family allows parents to have an intimate knowledge of each child’s individual character and situation that allow parents to teach them their social lessons–by both explicit instruction and implicit example–in a manner that is suitable for the individual child. … No extended order of spontaneous cooperation could provide either the knowledge or the incentives that arise within the intimate experience of families.\n\nI would use a weaker word than “need”. The lack of influence that parents have on child outcomes outside of their genetic contribution suggests that children are relatively robust to the arrangement in which they are raised. However, the innate tendency to form and operate in families suggests that families are valued – and will continue to form.\nArnhart’s closing paragraph is interesting.\n\nWe might also notice that this special role of the family in transmitting social learning about how best to succeed in society could explain the great transformation that came with the Industrial Revolution. If we accept Gregory Clark’s argument about the importance of an evolutionary process of “survival of the richest” by which families that taught their children the bourgeois virtues were more successful in England in the 18th century, which led to the Industrial Revolution, then we could explain this great transition into Hayek’s Great Society as a product of an evolutionary transformation in family life.\n\nHow much of the transmission was by families and how much by genes? Clark seems to lean towards the latter."
  },
  {
    "objectID": "posts/manzis-uncontrolled.html",
    "href": "posts/manzis-uncontrolled.html",
    "title": "Manzi’s Uncontrolled",
    "section": "",
    "text": "In social science, a myriad of factors can affect outcomes. Think of all the factors claimed to affect school achievement - student characteristics such as intelligence, conscientiousness, patience and willingness to work hard, parental characteristics such as income and education, and then there is genetics, socioeconomic status, school peers, teacher quality, class size, local crime and so on.\nIn assessing the effect of any policy or program, researchers typically attempt to control for these confounding factors. But as James Manzi forcefully argues in Uncontrolled: The Surprising Payoff of Trial-and-Error for Business, Politics, and Society, the high “causal density” in these settings nearly always results in the possibility that there is an important factor you have missed or do not understand.\nAs a result, Manzi advocates the use of randomised field trials (RFTs) to attempt to tease out whether interventions are having the desired effect. If control and treatment groups are randomised, any unidentified factors affecting the outcome should affect each group equally.\nThe ubiquity of uncontrolled factors and the ability of RFTs to do a better job of capturing them was demonstrated by John Ioannidis in a 2005 paper evaluating the reliability of forty-nine studies. As Manzi reports, 90 per cent of the large randomized experiments had produced results that could be replicated, compared to only 20 per cent of the non-randomized studies.\nManzi notes that RFTs have critics and limitations, and people such as James Heckman have argued that it is possible to achieve the same results as RFTs using non-experimental mathematical techniques. However, as Manzi points out, Heckman and friends’ demonstration that RFT results can be replicated using improved econometric methods after the fact is not the same as defining a set of procedures that can produce the same effect as _future _RFTs.\nAlthough Manzi is a strong advocate of RFTs, he is clear that RFTs will not lead to a new era where we will understand everything. High causal density will always place limits on the ability to generalise experimental results. Manzi writes:\n\n[I]ncreasing complexity has another pernicious effect: it becomes far harder to generalize the results of experiments. We can run a clinical trial in Norfolk, Virginia, and conclude with tolerable reliability that “Vaccine X prevents disease Y.” We can’t conclude that if literacy program X works in Norfolk, then it will work everywhere. The real predictive rule is usually closer to something like “Literacy program X is effective for children in urban areas, and who have the following range of incomes and prior test scores, when the following alternatives are not available in the school district, and the teachers have the following qualifications, and overall economic conditions in the district are within the following range.” And by the way, even this predictive rule stops working ten years from now, when different background conditions obtain in the society.\n\nManzi’s critique of the famous jam study is indicative. Can you truly generalise from 10 hours in one store with shoppers randomised into one hour chunks? Taken literally, the result implies that eliminating 75 per cent of products could increase sales by 900 per cent. However, that hasn’t stopped popularisers telescoping “the conclusions derived from one coupon-plus-display promotion in one store on two Saturdays, up through assertions about the impact of product selection for jam for this store, to the impact of product selection for jam for all grocery stores in America, to claims about the impact of product selection for all retail products of any kind in every store, ultimately to fairly grandiose claims about the benefits of choice to society.”\nIt’s not hard to come up other studies that are generalised in this matter. The Perry Pre-School project that found benefits for disadvantaged African American children in public pre-schools in the 1960s is generalised to promote more intensive early childhood education for everyone, regardless of country, race, socioeconomic status or era. A single Kenyan case study of deworming leads to a plan to deworm the world. And so on.\nAs a result, succeeding or failing in a single trial doesn’t usually constitute adequate evaluation of a program. Rather, promising ideas need to be subject to iterative evaluation in the relevant contexts.\nManzi’s reluctance to suggest RFTs will lead us to a new era also stems from the results of the few RFTs conducted in social science. Most programs fail replicated, independent, well-designed RFTs, so we should be sceptical of claims about the effectiveness of new programs. As Manzi states, innovative ideas rarely work.\nIn his review of RFTs in the social sciences, he does suggest one pattern emerges. Programs targeted at improving behaviour or raising skills or consciousness are more likely to fail than changes in incentives or environment. This might be considered a nod to both standard and behavioural economic tools.\nAt the end of the book, Manzi provides some guidance on how government should consider programs in an environment of high causal density.\nFirst, he recommends that government build strong experimental capability. To keep the foxes out of the henhouse and avoid program advocates influencing results, he recommends a separate organisational entity be established to evaluate programs.\nSecond, there should be experimentation at the state level, or at the smallest possible competent authority. This might involve state by state deviation from Federal laws or programs on a trial basis.\nManzi recommends a broader scope for experimentation than you might normally hear advocated, with his suggestion that experimentation extend to examining different levels of coercion:\n\nThe characteristic error of the contemporary Right and Left in this is enforcing too many social norms on a national basis. All that has varied has been which norms predominate. The characteristic error of liberty-as-goal libertarians has been more subtle but no less severe: the parallel failure to appreciate that a national rule of “no restrictions on non-coercive behavior” (which, admittedly, is something of a cartoon) contravenes a primary rationale for liberty. What if social conservatives are right and the wheels really will come off society in the long run if we don’t legally restrict various sexual behaviors? What if some left-wing economists are right and it is better to have aggressive zoning laws that prohibit big-box retailers? I think both are mistaken, but I might be wrong. What if I’m right for some people at this moment in time but wrong for others, or what if I’m wrong for the same people ten years from now?\nThe freedom to experiment needs to include freedom to experiment with different governmental (i.e., coercive) rules. So here we have the paradox: a liberty-as-means libertarian ought to support, in many cases, local autonomy to restrict at least some personal freedoms.\n\nTo enable experimentation, Manzi uses an evolutionary framing and notes there is a need to encourage variation, cross-pollination of ideas and selection pressure. Encouraging variation requires a willingness to allow failure and deviation from whatever vision of social organisation we believe is best.\n\nOur ignorance demands that we let social evolution operate as the least bad of the alternatives for determining what works. Subsocieties that behave differently on many dimensions are both the raw materials for an evolutionary process that sifts through and hybridizes alternative institutions, and also are analogous to the kind of evolutionary “reserves” of variation that may not be adaptive now but might be in some future changed environment. We want variation in human social arrangements for some of the same reasons that biodiversity can be useful in genetic evolution. This is the standard libertarian insight that the open society is well suited to developing knowledge in the face of a complex and changing environment. As per the first two parts of this book, it remains valid. But if we take our ignorance seriously, the implications of this insight significantly diverge from much of what the modern libertarian movement espouses.\n\nManzi highlights the importance of selection pressure is his discussion of school vouchers. He considers that “giving choice” to parents does not necessarily provide an environment in which trial-and-error improvement will occur as there may not be alternatives to status quo, the right incentives for market participants or adequate information for parents. Manzi is also sceptical as to whether taxpayer funded vouchers will come with so many controls to render the experiment useless.\nManzi’s proposals to provide selection pressure are not without problems. He suggests a comprehensive national exam for all schools receiving government funding, with those results published. But is the need to do well in this test is a form of control that kills off much of the experimentation, turning the education system into a group of organisations competing for high test scores?\nOne of Manzi’s more interesting ideas relates to immigration. Manzi supports programs to attract highly skilled immigrants, such as skills-based immigration programs, or offering entry to foreign students upon completing certain degrees. He proposes testing this idea by using a subset of the visas granted through lotteries to run a RFT. Immigrant outcomes could then be tracked.\nUltimately, however, Manzi’s message is one of humility. No matter what our worldview, we should be prepared to allow experimentation with alternatives, as we may well be wrong. And that favourite program you have been promoting? Feel free to experiment, but don’t expect success. And if it works in that context, test and test again, as it may not work somewhere else."
  },
  {
    "objectID": "posts/mandelbrot-and-hudsons-the-misbehaviour-of-markets-a-fractal-view-of-risk-ruin-and-reward.html",
    "href": "posts/mandelbrot-and-hudsons-the-misbehaviour-of-markets-a-fractal-view-of-risk-ruin-and-reward.html",
    "title": "Mandelbrot (and Hudson’s) The (mis)Behaviour of Markets: A Fractal View of Risk, Ruin, and Reward",
    "section": "",
    "text": "If you have read Nassim Taleb’s The Black Swan you will have come across some of Benoit Mandelbrot’s ideas. However, Mandelbrot and Hudson’s The (mis)Behaviour of Markets: A Fractal View of Risk, Ruin, and Reward offers a much clearer critique of the underpinnings of modern financial theory (there are many parts of The Black Swan where I’m still not sure I understand what Taleb is saying). Mandelbrot describes and pulls apart the contributions of Markowitz, Sharpe, Black, Scholes and friends in a way likely understandable to the intelligent lay reader. I expect that might flow from science journalist Richard Hudson’s involvement in writing the book.\nMandelbrot’s critique rests on two main pillars. The first is that - seemingly stating the obvious - markets are risky. Less obviously, Mandelbrot’s point is that market changes are more violent than often assumed. Second, trouble runs in streaks.\nWhile Mandelbrot’s critique is compelling, it’s much harder to construct plausible alternatives. Mandelbrot offers two new metrics - α (a measure of how wildly prices vary) and H (a measure of the dependence of price changes upon past changes) - but as he notes, the method used to calculate each can result in wild variation in those measures themselves. On H, he states that “If you look across all the studies to date, you find a perplexing range of H values and no clear pattern among them.”\nI’ll close this short note with a brief excerpt from near the end of the book painting a picture of what it is like to live in the world Mandelbrot describes (which just happens to be our world):\n\nWhat does it feel like, to live through a fractal market? To explain, I like to put it in terms of a parable:\n\nOnce upon a time, there was a country called the Land of Ten Thousand Lakes. Its first and largest lake was a veritable sea 1,600 miles wide. The next biggest lake was 919 miles across; the third, 614; and so on down to the last and smallest at one mile across. An esteemed mathematician for the government, the Kingdom of Inference and Probable Value, noticed that the diameters scaled downwards according to a tidy, power-law formula.\nNow, just beyond this peculiar land lay the Foggy Bottoms, a largely uninhabited country shrouded in dense, confusing mists and fogs through which one could barely see a mile. The Kingdom resolved to chart its neighbour; and so the surveyors and cartographers set out. Soon, they arrived at a lake. The mists barred their sight of the far shore. How broad was it? Before embarking on it, should they provision for a day or a month? Like most people, they worked out what they knew: They assumed this new land was much like their own and that the size of lakes followed the same distribution. So, as they set off blindly in their boats, they assumed they had at least a mile to go and, on average, five miles.\nBut they rowed and rowed and found no shore. Five miles passed, and they recalculated the odds of how far they had to travel. Again, the probability suggested: five miles to go. So they rowed further - and still no shore in sight. they despaired. Had they embarked upon a sea, without enough provisions for the journey? Had the spirits of these fogs moved the shore?\nAn odd story, but one with a familiar ring, perhaps, to a professional stock trader. Consider: The lake diameters vary according to a power law, from largest to smallest. Once you have crossed five miles of water, odds are you have another five to go. If you are still afloat after ten miles, the odds remain the same: another ten miles to go. And so on. Of course, you will hit shore at some point; yet at any moment, the probability is stretched but otherwise unchanged."
  },
  {
    "objectID": "posts/male-income-and-reproductive-success.html",
    "href": "posts/male-income-and-reproductive-success.html",
    "title": "Male income and reproductive success",
    "section": "",
    "text": "As happens occasionally, I have just come across an article that I should have seen years ago. In an article titled Natural Selection on Male Wealth in Humans, Daniel Nettle and Thomas Pollet look at data from a variety of societies, ranging from subsistence groups to industrialised societies, and show that the link between male income and reproductive success is strong and ubiquitous.\nIn their main analysis, they use longitudinal data for a group of British men born in a single week in March 1958. These men were tracked through to age 46. With around 96 per cent of reproduction for men occurring before this age, the number of children at this age provides a good guide to reproductive success.\nAmong the men in the sample, the effect of male income on reproductive success was positive, with a selection gradient of 0.10. This means that for every standard deviation increase in income, there is a 0.1 standard deviation increase in reproductive success. This may not sound like a lot, but it is strong selection compared to that observed in many other species. This effect was mostly driven by whether people had any children or not, rather than through variation in family size.\nThe authors also controlled for education by splitting men into three separate education groups (which has a negative effect on fertility for both men and women), which tended to strengthen the observed effect of income.\nWhere we see even stronger selection, however, is in the historical data. Nettle and Pollet took their results and compared them to selection gradients calculated from a range of other papers. They found that for contemporary United States and Sweden, English testators in 1540 to 1850, Norwegian farmers in 1700 to 1900, and a range of pastoral and hunter-gatherer groups, there was a positive selection gradient based on male wealth or hunting ability.\nFurther, while the strength of selection in the three contemporary societies was similar to each other, it was lower than in the pre-Industrial societies. The authors suggested that this weakened pressure was largely due to an increase in the level of monogamy and reduced variation in effective family size, rather than changing preferences for income. As the authors note however, while weakened, the selection in modern societies is still significant.\nAn underlying question to these results is whether the selection of phenotypic features (income) has any relationship to underlying genetic qualities. Both income, and many of the factors underlying income, such as IQ, have been found to be heritable, so there is likely to be evolutionary changes to the composition of the population.\nThe other question is why is there this link. The most obvious explanation is that women prefer men with more resources, both for the resources themselves and for the fact that they signal quality. That the effect of income was largely related to whether someone had zero or more children, rather than size of family, might be suggestive of this. To have more than zero children, you need to attract a mate. However, while doubting the idea, the authors did not rule out an explanation that men who have children earn more to support them.\nEconomists might try to pin this relationship down to the decisions of the male with higher income. If children are normal goods, a man should want to buy more as their income goes up. There is a range of issues with this argument, and I am going to address them in a post next week."
  },
  {
    "objectID": "posts/maladaptive-ideas.html",
    "href": "posts/maladaptive-ideas.html",
    "title": "Maladaptive ideas",
    "section": "",
    "text": "Following the Consilience Conference and some suggestions for additions to my reading list, I have been convinced to read some more work by Robert Boyd and Peter Richerson. I’ve started with The Origin and Evolution of Cultures.\nOne quote in the introduction caught my eye:\n\n[A]cquiring adaptive information from others also opens a portal into people’s brains through which maladaptive ideas can enter—ideas whose content makes them more likely to spread, but do not increase the genetic fitness of their bearers. Such ideas can spread because they are not transmitted as genes are. For example, in the modern world, beliefs that increase the chance of becoming an educated professional can spread even if they limit reproductive success because educated professionals have high status and thus may likely be emulated. Professionals who are childless can succeed culturally as long as they have an important influence on the beliefs and goals of their students, employees, or subordinates. The spread of such maladaptive ideas is a predictable by-product of cultural transmission.\n\nWhile I might characterise the maladaptation differently - it is not being a professional in itself that limits reproductive success - this approach contrasts with the economic explanation. Economists usually frame the fertility decision as a rational quality-quantity trade-off. In addition to the usual economic assumption of rationality concerning the pursuit of an objective, there is also an assumption that the objective itself (fitness in a biological sense) is rationally chosen. There is no maladaptation, and I suspect this is the cause of some of the difficulty the economic approach to the problem has faced."
  },
  {
    "objectID": "posts/long-term-social-mobility-is-low.html",
    "href": "posts/long-term-social-mobility-is-low.html",
    "title": "Long-term social mobility is low",
    "section": "",
    "text": "There have been a few recent pointers to Gregory Clark and Neil Cummin’s work on long-term social mobility using surnames (papers here, here and here). The basic method used in these studies is to examine the share of rare surnames in high or low status occupations and compare it to the overall prevalence of that surname in the population. By tracking the relative status of the rare surname through time (effectively treating those with the same surname as a large family), the change in status through the generations can be measured.\nThe abstract of the paper presented by Clark at a Becker-Friedman Institute conference on intergenerational mobility earlier this year gives a good summary of the general results:\n\nWhat is the true rate of social mobility? Modern one-generation studies suggest considerable regression to the mean for all measures of status – wealth, income, occupation and education across a variety of societies. The β that links status across generations is in the order of 0.2-0.5. In that case inherited surnames will quickly lose any information about social status. Using surnames this paper looks at social mobility rates across many generations in England 1086-2011, Sweden, 1700-2011, the USA 1650-2011, India, 1870-2011, Japan, 1870-2011, and China and Taiwan 1700-2011. The underlying β for long-run social mobility is around 0.75, and is remarkably similar across societies and epochs. This implies that compete regression to the mean for elites takes 15 or more generations.\n\nThe lack of social mobility is consistent across cultures, social systems and times. Clark’s conclusion from this (although he does not actively discuss the basis for his conclusion) is that “Social status is likely mainly of genetic origin.”\nThis contrasts with Dylan Matthews’s interpretation at the Washington Post:\n\n[G]enetics likely has little to do with those results. Clark and Cummins studied surnames across eight generations. So, two people with the same surname in 1800 and 2011 would only share 0.58 = 0.4 percent of their DNA.\n\nWhat Matthews misses, however, is the reason that Clark attributes to the very high value of β - assortative mating. Thus, while a person may contribute only 0.4 per cent of the genome of a descendant 8 generations down the track (assuming no intermarriage between relations in that time), the descendant’s genome will largely consist of DNA contributed by other high-socioeconomic status people."
  },
  {
    "objectID": "posts/libertarians-and-fertility.html",
    "href": "posts/libertarians-and-fertility.html",
    "title": "Libertarians and fertility",
    "section": "",
    "text": "As I noted in yesterday’s post, Bryan Caplan has written the lead essay for this month’s Cato Unbound on The Politics of Family Size. Caplan argues that as there are strong benefits to increasing population, libertarians should support “libertarian policies” to increase population, educate and persuade people to have more children and while they are at it, have more children themselves.\nCaplan suggests that people underestimate both the social and private benefits of having children. In the case of the private benefits, Caplan uses twin research (among other things) to build the case that as a parent’s contribution to the child is largely in the form of their genes, children require less work than people think. This is a major focus of his new book Selfish Reasons to Have New Kids, which I am currently reading. I find these arguments convincing, and I will address them in more detail when I review Caplan’s book next week (the review can be found here).\nThe social benefits referenced by Caplan rest in part on the work by Julian Simon. A larger population has benefits as more people leads to more ideas. Caplan writes:\n\nEconomists’ central discovery about economic growth is that new ideas are more important than labor or capital. The main reason we’re richer than we used to be is that we know more than we used to know. We know how one man can grow food for hundreds. We know how to build flying machines. We know how to build iPhones. Best of all: Once one person discovers a new idea, billions can cheaply adopt it.\n\nCaplan also notes that we tend to seek areas of higher population. Despite the problems with congestion and crowds, people choose to live in cities - and in fact, more than half of the world’s population now does so.\nIn the first response to Caplan’s essay, Gregory Clark questions for how long increased population will continue to deliver net benefits. Clark notes that there is a balance between the costs and benefits of population growth:\n\nPopulation growth always generates gains and losses. More population drives up the cost of limited resources: land, minerals, and fossil energy. But larger populations reduce production costs through scale economies. … There is thus a race between resource costs and scale economies as population grows.\n\nI have some sympathy with Clark’s argument. We have had a 200 year period, in some parts of the world, where the benefits of population have outweighed the costs. As Clark points out, the costs of population are evident in the time after the Black Death in Europe, with the lower population delivering incomes that were not exceeded again until 100 years after the Industrial Revolution. Today, there are many parts of the world where population growth has resulted in problems. Jared Diamond’s discussion of Rwanda in Collapse comes to mind. Some parts of the world are still in an effective Malthusian trap.\nEven for the developed world, I am reluctant to extrapolate such a small slice of history too far forward. Eventually, population could catch up with and possibly overtake economic growth, as those who have a heritable inclination to have more children do so. Having stated this, I agree with Caplan’s argument that in the short-term for developed countries such as the United States, if resources are priced appropriately, people would be better off with the increased range of ideas that comes with a larger population.\nOn the fiscal (government finance) side, Caplan argues that despite concerns about the weight of new people on government programs, new children have a large, positive fiscal externality. Citing work by Wolf and colleagues (which I am going to have to read in detail - published here), Caplan notes that each child born in the United States has a $217,000 positive externality (2009 dollars). After noting the significant fertility effect of a small baby bonus in Quebec, Caplan suggests that anyone who wants to improve the government’s fiscal health should support natalist tax credits to boost fertility (I have some doubts about the strength of these effects for reasons I will expound in a later post).\nThis is one point where I depart from Caplan. While a tax credit is clearly better than a subsidy, this type of social engineering by government makes me nervous, particularly where it seems to be based on a fiscal cost-benefit analysis. In Australia, the social engineering attempts of successive governments have resulted in a horrible mess of tax credits for childcare, women in the workforce and stay at home mothers. While Caplan’s argument might be clean, the actual implementation never is.\nThe claim of a positive fiscal externality is also interesting. Given that the United States government runs a large deficit, it would seem that government spends more than it receives. Wolf and colleagues achieve their result of a net positive fiscal benefit to children by excluding pure public goods - that is goods that are non-rivalrous. An extra child should theoretically result in no extra expenditure for these pure public goods. Defence is one example.\nThe problem with this argument, however, is that government does not work in this way. As most governments’ persistent deficits suggest, governments tend to spend all that they receive. On average, people are close to fiscally neutral or mildly negative. On that basis, an extra child is likely to be fiscally neutral or mildly negative once you consider how a government will actually act and spend the surplus generated during the child’s life. In that case, I suggest that Caplan is on stronger ground when he argues of the positive social benefits received by other people, in the form of the ideas, goods and services produced by that child, rather than the child’s lifetime fiscal contribution."
  },
  {
    "objectID": "posts/lehrer-on-measurement.html",
    "href": "posts/lehrer-on-measurement.html",
    "title": "Lehrer on measurement",
    "section": "",
    "text": "Jonah Lehrer has expanded his recent focus on measurement and grit (on which I recently posted) in an article on the usefulness of the Wonderlic test, a quasi-IQ test, in predicting quarterback performance. Lehrer cites a paper by David Berri and Rob Simmons which suggests that some metrics, including the Wonderlic test, are influencing draft position even though they are not predictive of performance. Lehrer writes:\n\nWhile they found that Wonderlic scores play a large role in determining when QBs are selected in the draft – the only equally important variables are height and the 40-yard dash – the metric proved all but useless in predicting performance. The only correlation the researchers could find suggested that higher Wonderlic scores actually led to slightly worse QB performance, at least during rookie years.\n\nUnlike Lehrer’s piece on which I previously posted, I’m sympathetic to the argument that this suggests that some people are acting on some not particularly useful measurements. However, I’m not as convinced when Lehrer (again) moves into the idea that the missing element is grit. Lehrer closes the article with the following:\n\nSo where is all this heading? How will grit become a bigger part of the scouting equation? The first step is to finally acknowledge that maximal tests aren’t effective. “I really see the Wonderlic as a reading test,” says former NFL executive Michael Lombardi, now with the NFL Network. “Until we get a better test, teams are just going to have to evaluate players the old-fashioned way, by watching them play in actual games. It takes good instincts to be a QB. Maybe it takes good instincts to find one, too.”\nHasselbeck suggests that teams pay more attention to the fundamentals of college quarterbacks, since their passing mechanics are often a window into how much grit they possess. “You know these guys have been coached for years,” he says. “So if you see a QB with flawed fundamentals, you gotta wonder what’s wrong. Is he coachable? Will he work to improve? Because that’s important. You can teach a kid to throw the ball, but only if he wants to learn.”\nAfter all, deliberate practice makes perfect.\n\nThis first paragraph is almost the opposite of Michael Lewis’s Moneyball, where he suggested that there was too much faith in instinct and not enough in measured performance. Having years of college performance at hand, I’d be sceptical if some measures of “grit”, assuming it was the important missing variable, do not already exist.\nOn that note, the second paragraph suggests an opportunity. Maybe we should see some predictions in the lead-up to the next few drafts, where some of these grit loving experts could assess “passing mechanics” as a measure of grit, state who the teams should draft where and see if their performance measure is a better indicator of future success than actual draft position. That is what was impressive about Moneyball. Rather than being a story about someone complaining that teams should do a difficult task better,  it was a story about someone taking their belief and acting on it (and with the presence of Michael Lewis, putting one season’s draft selections on the record for an assessment of those beliefs)."
  },
  {
    "objectID": "posts/lasts-what-to-expect-when-no-ones-expecting-americas-coming-demographic-disaster.html",
    "href": "posts/lasts-what-to-expect-when-no-ones-expecting-americas-coming-demographic-disaster.html",
    "title": "Last’s What to Expect When No One’s Expecting: America’s Coming Demographic Disaster",
    "section": "",
    "text": "I’ve recently read a couple of books on demographic trends, and there don’t seem to be a lot of silver linings in current fertility patterns in the developed world. The demographic boat takes a long time to turn around, so many short-term outcomes are already baked in.\nDespite the less than uplifting subject, Jonathan Last’s What to Expect When No One’s Expecting: America’s Coming Demographic Disaster is entertaining - in some ways it is a data filled rant.\nLast doesn’t see much upside to the low fertility in most of the developed world. Depopulation is generally associated with economic decline. He sees China’s One Child Policy - rather than saving them - as leading them down the path to demographic disaster. Poland needs a 300% increase in fertility just to hold population stable to 2100. The Russians are driving toward demographic suicide. In Germany they are converting prostitutes into elderly care nurses. Parts of Japan are now depopulated marginal land.\nAnd Last sees little hope of a future increase (I have some views on that). He rightly lampoons the United Nations as having no idea. At the time of writing the book, the United Nations optimistically assumed all developed countries would have their fertility rate increase to the replacement level of 2.1 children per woman (although the United Nations has somewhat - but not completely - tempered this optimism via its latest methodology). There was no basis for this assumption, and the United Nations is effectively forecasting blind.\nSo why the decline? Last is careful to point out that the world is so complicated that it is not clear what happens if you try to change one factor. But he points to several causes.\nFirst, children used to be an insurance policy. If you wanted care in your old age, your children provided it. With government now doing the caring, having children is consumption. Last points to one estimate that social security and medicare in the United States suppresses the fertility rate by 0.5 children per woman (following the citation trail, here’s one source for that claim).\nThen there is the pill, which Last classifies as a major backfire for Margaret Sanger. She willed it into existence to stop the middle classes shouldering the burden of the poor, but the middle class have used it more.\nNext is government policy. As one example, Last goes on a rant about child car seat requirements (which I feel acutely). It is impossible to fit more than 2 car seats in a car, meaning that transporting a family of five requires an upgrade. This is one of many subtle but real barriers to large family size.\nFinally (at least of those factors I’ll mention), there is the cost of children today. Last considers that poorer families are poorer because they chose to have more children, or as Last puts it, “Children have gone from being a marker of economic success to a barrier to economic success.” Talk about maladaptation. (In the preface to the version I read, Last asked why feminists were expending so much effort demanding right to be child free and not railing against the free market for failing women who want children.)\nThe fertility decline isn’t just a case of people wanting fewer children, as - on average - people fall short of their ideal number of kids. In the UK, the ideal is 2.5, expected is 2.3, actual 1.9. If people could just realise their target number of children, fertility would be higher.\nBut this average hides some skew - less educated people end up with more than is ideal, educated people end up with way less. By helping the more educated reach their ideal, the dividend could be large.\nSo what should government do? Last dedicates a good part of the book to the massive catalogue of failures of government policy to boost birth rates. The Soviet Union’s motherhood medals and lump sum payments didn’t stop the decline. Japan’s monthly per child subsidies, daycare centres and paternal leave (plus another half dozen pro-natalist policies Last lists) had little effect. Singapore initially encouraged the decline, but when they changed their minds and started offering tax breaks and other perks for larger families, fertility kept on declining.\nThis suggests that you cannot bribe people into having babies. As Last points out, having kids is no fun and people aren’t stupid.\nThen there is the impossibility of using migration to fill the gap. To keep the United States support ratio (retirees per worker) where it currently is (assuming you wanted to do this), the US would need to add 45 million immigrants between 2025 and 2035. The US would need 10.8 million a year until 2050 to get the ratio somewhere near what it was in 1960. Immigration is not as good for demographic profile as baby making and comes with other problems. Plus the sources of immigrants are going through own transition, so at some point that supply of young immigrants will dry up.\nSo, if government can’t make people have children they don’t want and can’t simply ship them in, Last asks if they could help people get the children they do want. As children go on to be taxpayers, Last argues government could cut social security taxes for those with more children and make people without children pay for what they’re not supporting. (Although you’d want to make sure there was no net burden of those children across their lives, as they’ll be old people one day too. There are limits to how far you could take that Ponzi scheme.)\nLast also suggests eliminating the need for college, one of the major expenses of children. Allowing IQ testing for jobs would be one small step toward this.\nPut together, I’m not optimistic much can be done, but Last is right in that there should be some exploration of removing unnecessary barriers (let’s start with those car seat rules).\nI’ll close this post where Last closes the book. In a world where the goal is taken to be pleasure, children will never be attractive. So how much of the fertility decline is because modernity has turned us into unserious people?"
  },
  {
    "objectID": "posts/krugman-on-gould-and-maynard-smith.html",
    "href": "posts/krugman-on-gould-and-maynard-smith.html",
    "title": "Krugman on Gould and Maynard Smith",
    "section": "",
    "text": "I’ve posted before about Paul Krugman’s dislike of the work of Stephen Jay Gould, but I have come across another old essay in which Krugman weighs in on the question of Gould’s role in communicating evolutionary biology. Krugman argues that Gould was attractive to readers because he made no attempt to explain the mathematical logic of evolutionary theory. Krugman writes:\n\nAsk a working biologist who is the greatest living evolutionary thinker, and he or she will probably answer John Maynard Smith (with nods to George Williams and William Hamilton). Maynard Smith not only has a name that should have made him an economist; he writes and thinks like an economist, representing evolutionary issues with stylized mathematical models that are sometimes confronted with data, sometimes simulated on the computer, but always serve as the true structure informing the verbal argument. A textbook like his Evolutionary Genetics (1989) feels remarkably comfortable for an academic economist: the style is familiar, and even a good bit of the content looks like things economists do too. But ask intellectuals in general for a great evolutionary thinker and they will surely name Stephen Jay Gould – who receives one brief, dismissive reference in Maynard Smith (1989). …\nWhat does Gould have that Maynard Smith does not? He is a more accessible writer – but evolutionary theory is, to a far greater extent than economics, blessed with excellent popularizers: writers like Dawkins (1989) or Ridley (1993), who provide beautifully written expositions of what researchers have learned. (Writers like Gould or Reich are not, in the proper sense, popularizers: a popularizer reports on the work of a community of scholars, whereas these writers argue for their own, heterodox points of view). No, what makes Gould so popular with intellectuals is not merely the quality of his writing but the fact that, unlike Dawkins or Ridley, he is not trying to explain the essentially mathematical logic of modern evolutionary theory. It’s not just that there are no equations or simulations in his books; he doesn’t even think in terms of the mathematical models that inform the work of writers like Dawkins. That is what makes his work so appealing. The problem, of course, is that evolutionary theory – the real thing – is _based _on mathematical models; indeed, increasingly it is based on computer simulation. And so the very aversion to mathematics that makes Gould so appealing to his audience means that his books, while they may seem to his readers to contain deep ideas, seem to people who actually know the field to be mere literary confections with little serious intellectual content, and much of that simply wrong. In particular, readers whose ideas of evolution are formed by reading Gould’s work get no sense of the power and reach of the theory of natural selection – if anything, they come away with a sense that modern thought has shown that theory to be inadequate.\n\nKrugman’s larger question in the essay is why the concept of comparative advantage is so hard to communicate. One reason is that comparative advantage, like evolutionary theory, has at its base a mathematical model. The whole essay is worth the read (as is most Krugman from that era)."
  },
  {
    "objectID": "posts/kling-on-patterns-of-sustainable-specialisation-and-trade.html",
    "href": "posts/kling-on-patterns-of-sustainable-specialisation-and-trade.html",
    "title": "Kling on patterns of sustainable specialisation and trade",
    "section": "",
    "text": "I have just listened to the recent Econtalk podcast with Arnold Kling on his new “paradigm”, Patterns of Sustainable Specialisation and Trade (PSST). On first thoughts, I am not convinced about the idea. If anything, the paradigm appears to need a lot more development - although reading Kling’s blog posts, he may agree. I felt that many of the stories involved too much hand-waving and not enough empirical backbone to be convincing.\nI won’t go into the details of Kling’s paradigm - I suggest listening to the podcast or tracking through Kling’s posts at Econlog for background. His most recent one on PSST is here. But, I had a couple of initial thoughts.\nFirst, this paradigm has many similarities to evolutionary economics. Nelson and Winter’s An Evolutionary Theory of Economic Change contains a lot of material on search, competition and organisation is along the same lines as that discussed by Kling. Naturally flowing from this, would agent based modelling or other evolutionary economic modelling techniques be useful in developing working models of Kling’s theory?\nSecond, and given that I am far from convinced as to whether this paradigm is correct (for example, can PSST explain the current high levels of unemployment), I was wondering what empirical evidence would sway me towards it. If we track workers who have become unemployed during this recession, the PSST paradigm would predict that a sizeable chunk of this group will go to new jobs created by entrepreneurs looking to take advantage of this cheap resource. Will this be the case? How many construction or manufacturing workers will end up in jobs in which they have a new comparative advantage, and how many will get employment doing almost the same thing they were doing before? We could apply a similar test to the employer side. Do firms hire back workers for positions that they previously dumped workers from, or is the hiring in new positions in new firms?\nUpdate: A quick additional thought - what does this paradigm say about immigration? If the gates are opened and immigration levels jump, how long is the period of adjustment for entrepreneurs to be able to take advantage of this huge resource of presumably low-skilled labour?"
  },
  {
    "objectID": "posts/kenrick-and-griskeviciuss-the-rational-animal.html",
    "href": "posts/kenrick-and-griskeviciuss-the-rational-animal.html",
    "title": "Kenrick and Griskevicius’s The Rational Animal",
    "section": "",
    "text": "I am in two minds about Doug Kenrick and Vlad Griskevicius’s The Rational Animal: How Evolution Made Us Smarter Than We Think. As an introduction to evolutionary psychology and the idea that evolutionary psychology could add a lot of value to economics - and behavioural economics in particular - it does a pretty good job.\nOn the other hand, the occasional straw man discussion of economics and the forced attempt to sex up the book kept distracting me from the central argument, so I never found myself really enjoying the reading experience. Then there is the heavy reliance on priming research - more on that later.\nThe basic argument of the book is that people are deeply rational. Today’s choices “reflect a deep-seated evolutionary wisdom”. That wisdom sometimes works well, but we can have an impression that behaviour is irrational because we do not understand what people are trying to achieve. And sometimes this wisdom backfires when the environment is different from the one in which we evolved.\nImportantly - and I struggle with this point - they argue that humans pursue several different evolutionary goals and the evolutionary goal that is on someone’s mind at a particular moment will affect the decisions they make. Someone will make a different decision if thinking about acquiring a mate as opposed to responding to a threat to their safety.\nKenrick and Griskevicius identify seven sub-selves that relate to specific evolutionary goals - self-protection, disease avoidance, alliance building, status building, mate acquisition, mate retention and care of kin. When thinking of mate acquisition, we will be interested in demonstrating our value over others. When self protection becomes the focus, we will be happier mixing in with the crowd. Most of the book is an examination of how these sub-selves affect our decision making, including how they vary between the sexes and change over our lifespan.\nAs a neat example (although note my comments below about priming), people watched a clip from one of two movies, The Shining and the romantic Before Sunrise. They then saw a set of commercials in which products were promoted as being popular (e.g. “visited by over a million people a year”) or unique (e.g. “limited edition”). Those who saw the ads after seeing the clip about The Shining preferred popular products (safety in numbers), while those who saw the romantic film preferred unique products (to attract a mate you need to stand out from the crowd). Different films triggered different sub-selves and accordingly, different decisions.\nThrough the book, here are a few of the random snippets that I bookmarked:\n\nA classic behavioural science problem involves framing a choice between two disease treatments. One group has a choice between saving 200 of 600 people, or having a 33% chance of saving all 600. A second group has a choice between 400 of 600 people dying or a 33% chance that no-one will die. Those who hear the first (positive) framing tend to choose the certain treatment, but most choose the uncertain treatment in the second (negative frame). However, those numbers differ from the typical group size of our evolutionary past - around one hundred people. X. T. Wang found that when the same problem was framed with numbers similar to an ancestral band - i.e. 20 of 60 will be saved - the framing effect disappears.\nWhen the prisoner’s dilemma is played between brothers, the payoffs from an inclusive fitness perspective encourage cooperation, and that is what we see.\nWhen something is coming toward us - say a rock at our head - our brain tells us it will hit sooner than actually will. It’s an error, but making a predictable error in this direction is not a bad thing. There are asymmetric costs to an error in either direction. The propensity to sense that an approaching object will arrive sooner than it will is called auditory looming.\nJust seeing diseased people can trigger an immune system response.\n\nNow to the main issue that gnawed at me through the book. The arguments heavily draw on research in priming, which is not faring particularly well through the failure to replicate many priming studies and evidence of publication bias. I’ve been willing to give some benefit of the doubt to priming research in evolutionary psychology, as there seems to be some basis for it. It feels reasonable that seeing picture of an attractive woman - even if it is just a picture - could result in more mating-related behaviour (well, certainly more of a basis for that than reading words related to the elderly and walking more slowly).\nAlas, even the work in this space seems to be falling apart. I’ve cited that work in my published papers, and believe that many of the underlying phenomena are there (for instance, men taking more risk in the presence of attractive women), but it looks like priming is not the way to show this."
  },
  {
    "objectID": "posts/keeping-economists-honest.html",
    "href": "posts/keeping-economists-honest.html",
    "title": "Keeping economists honest",
    "section": "",
    "text": "Paul Frijters has written an interesting review of Daniel Kahneman’s Thinking Fast and Slow over at Club Troppo. In the review, Frijters suggests that Kahneman’s main contribution to economics is keeping economists honest:\n\nIn terms of the whole rationality debate, the main contribution that Kahneman makes with this book, and that he in my opinion has made throughout his career, is to keep economists honest. …\nKahneman’s value derives from the great temptation amongst economists, particularly those of a strong theoretical bent, to fall in love with their abstractions and to pretend they are accurate descriptions of how things really work. Whereas the early economists were explicit about how their view of ‘rational economic man’ was merely an abstraction, later generations have far too often taken that abstraction and other closely associated ones (such as the whole notion of stable preferences, discount rates, loss-aversion, risk-aversion, etc.) literally. The number of economists I know who pretend to their students and themselves that things ‘derived’ from these abstractions, like the Welfare theorems, ‘prove’ things about the real world is astoundingly high. That pretence cannot be confronted and belittled often enough. Our models are derivative of an understanding of the real world, not the ultimate source of that understanding.\n\nEach time an economist develops a model incorporating individual choice, the economist should ask whether changing the underlying assumptions to incorporate loss aversion, the planning fallacy or any of the raft of other biases and heuristics identified by Kahneman and colleagues is likely to change the model conclusions. It is worth knowing what is built on sand.\nOtherwise, most of Frijters’s post is focused on the usefulness of the System 1 and System 2 device that Kahneman uses through the book to describe the two modes of human thinking - fast, intuitive use of heuristics and slow rational calculation. Frijters doubts that the System 1 and System 2 device will be broadly adopted in the mainstream economics canon as, among other reasons, the distinction between System 1 and System 2 is unclear and there are more intuitive alternative labels available.\nIn part, I feel that Frijters was looking for something different to what Kahneman sought to provide. Kahneman was using the labels to frame the body of work and to offer a way of thinking about it, and not seeking to sell System 1 and System 2 as the all-encompassing framework for economic analysis. Kahneman chose the System 1 and System 2 labels over others for working memory reasons. Further, the adoption of the dual process account of reasoning under the System 1 and System 2 and other labels throughout psychology (and even popular discourse) suggests that it has some utility. Whether economists find it useful and take it up is another question."
  },
  {
    "objectID": "posts/kaufmanns-shall-the-religious-inherit-the-earth-demography-and-politics-in-the-twenty-first-century.html",
    "href": "posts/kaufmanns-shall-the-religious-inherit-the-earth-demography-and-politics-in-the-twenty-first-century.html",
    "title": "Kaufmann’s Shall the Religious Inherit the Earth?: Demography and Politics in the Twenty-First Century",
    "section": "",
    "text": "While I suggested in my post on Jonathan Last’s What to Expect When No One’s Expecting that reading about demographics in developed countries was not uplifting, the consequences described by Last could be considered pretty minor.\nA slight tightening of government budgets could be dealt with by raising pension ages by a few years. Incomes may be lower than otherwise, but as Last states, “A decline in lifestyle for a middle-class American retiree might mean canceling cable, moving to a smaller apartment, and not eating out.” Not exactly disastrous - although of more consequence than the subject of almost every other economic debate.\nI found it harder to generate the same blasé reaction to Eric Kaufmann’s Shall the Religious Inherit the Earth?: Demography and Politics in the Twenty-First Century. I don’t have a lot of confidence in most long-term projections of fertility, population, religious retention and social opinions, but even if the world described by Kaufmann has only a 10 per cent chance of occurring, it is worth thinking about.\nKaufmann’s basic argument is that the higher fertility of fundamentalist religious groups, together with their high rates of retention, is going to shift in the make up of the populations in the West over the next century, profoundly affecting our politics and freedoms.\nThe important word in that above sentence is fundamentalist. This is not a case of religious groups breeding faster than the irreligious. Fertility levels for many groups are rapidly converging in the West. Muslim family sizes are shrinking. Catholic families are no larger than those of Protestants.\nWhere the action lies is within each faith. There the fundamentalists have markedly higher fertility than both the moderates and seculars. And, if anything, that gap is widening.\nTo give a sense of the power of this higher fertility, the Old Order Amish in the United States have increased from 5,000 people in 1900 to almost a quarter of a million members. In the United Kingdom, Orthodox Jews make up 17 per cent of the Jewish population but three-quarters of Jewish births.\nAt one point Kaufmann likens the process to the development by insects of resistance to DDT (although he spends little time on the heritability of religiosity). The growth of secularism has produced new resistant strains of religion, with the middle ground between fundamentalism and irreligion hemorrhaging people, revealing a fundamentalist core.\nKaufmann labels these high fertility religious groups as endogenous growth sects. They grow their own rather than convert - mainstream fundamentalists recognise this is where their advantage lies - and they have high rates of retention for their home-grown. As an example, three-quarters of the relative growth in conservative Protestant denominations in the United States in the 20th century was due to fertility differences, not conversion.\nSo what does this change mean? Kaufmann argues that we may have reached the peak of secular liberalism. The growth of these fundamentalist religious groups is going to start influencing policy and leading to less liberal outcomes.\nAs a start, fundamentalist Christian, Muslim, Jewish groups have elevated the most illiberal aspects of their traditions to the status of sacred symbols - be that outlandish dress requirements (often of quite recent origin) and positions on women’s roles and family size. This has helped inoculate them against secular trends.\nFor the United States, those who believe homosexuality or abortion is always wrong have a growing fertility advantage and they are becoming a larger part of the population. Combined with the tendency of children to adopt the positions of their parents, Kaufmann projects a slight increase in those who oppose abortion by mid-century, whereas opposition to homosexuality will decline only marginally. By the end of the century, however, opposition to abortion could increase from 60 to 75 per cent, and increases in opposition to homosexuality will reverse changes in opinion of the last few decades.\nKaufmann projects similar trends will occur in Europe, and he argues that you can’t speak of secular Europe and religious immigrant minorities. In the future the children of the religious minorities will be Europe. Most large European countries will have between 10 and 15 per cent Muslim population in 2050 (From mid-single digits today. Sweden will be more like 20 to 25 per cent). Depending on whether fertility converges, that proportion will grow through to 2100. And importantly for Kaufmann’s thesis, this growth will largely relate to the fundamentalist core.\nKaufmann goes on to suggest that the growth of these fundamentalist groups points to a contradiction in liberalism. The combination of tolerance of fundamentalism with a choice not to reproduce may well be the agent that destroys it.  To do other than tolerate would be against liberalism principles.\nKaufmann also discusses the implications for world politics. One starting point - hard to perceive in the West - is that the world is becoming more religious and is projected to become more so. While rich nations are still tending more secular (for the moment), poorer religious regions are growing faster.\nWith nation states boundaries generally well-defined, demographic changes within states are the main cause of change in relative size - and superpowers tend to be demographic heavyweights (although to what extent this holds through the 21st century will be interesting to see). Kaufmann quotes Jackson and Howe that it is “[D]ifficult to find any major instance of a state whose regional or global stature has risen while its share of the regional or global population has declined.”\nThus, if you are someone who worries about international geopolitics, trends aren’t going in the right direction - although China and Russia are running into a demographic wall. Kaufmann asks whether the short-term choice is inter-ethnic migration to increase population or accepting a decline in international power?\nPut together, Kaufmann’s case worries me more than tales of government deficits due to demographic change. Even if you assign a low probability to Kaufmann’s projections, it provides another strand to the case that low fertility in the secular West is not without costs."
  },
  {
    "objectID": "posts/kahnemans-thinking-fast-and-slow.html",
    "href": "posts/kahnemans-thinking-fast-and-slow.html",
    "title": "Kahneman’s Thinking, Fast and Slow",
    "section": "",
    "text": "[See my 2016 update]\nOn glancing down the chapter list of Daniel Kahneman’s Thinking, Fast and Slow, I saw a list of heuristics and biases that behavioural scientists have discovered over the last few decades. I am often critical of behavioural science in that it is often presented as a list of biases with no linking framework, so the chapter list played to my fears. I was not confident that the book would present me with many new ideas.\nThankfully, the chapter list was deceiving. Thinking, Fast and Slow is a magnificent book. Kahneman has such a clear writing style and ability to draw simple examples that my grasp of many of the heuristics and biases has increased. I am sure that I will be using many of Kahneman’s examples when trying to explain these biases to others in the future. Thinking, Fast and Slow is my first recommendation to anyone wanting to come to grips with behavioural economics, regardless of their background or level of technical skill.\nMore importantly (for me), was Kahneman’s ability to link many of these heuristics and biases. Kahneman’s use of the dual process model of the brain allows him to thread a coherent story through each bias and to give them something resembling a framework. The basic concept is that the brain has two modules: System 1 which is fast and applies quick, efficient but occasionally wrong intuitive decisions; and System 2, which is more analytical, but lazy and prone to its own biases.\nWhen placed into an evolutionary context, the dual model framing makes intuitive sense. System 1 take over in emergencies. That quick, intuitive reaction allowed our ancestors to survive. System 2 is only called upon when there is time to decide and the energy used in deploying System 2 is worthwhile. This model allows Kahneman’s story to be less about “irrationality” and more about the costs and benefits of different forms of decision-making, particularly when the human brain, which evolution has shaped over many generations in an environment vastly different from today’s, is placed in the modern world.\nKahneman’s discussion of prospect theory adds to his story’s coherence. Under prospect theory, people evaluate losses and gains from their current reference point, and not as a calculation of their total wealth. People weight losses more than gains, and as such, are risk seeking in the domain of losses and risk averse when it comes to gains. This leads to the fourfold pattern of risk preferences: risk-averse behaviour towards a high probability of gains (fear of disappointment) and towards a small probability of losses (hope to avoid loss - insurance); and risk-seeking behaviour towards a high probability of losses (hope to avoid loss) and towards a small probability of gains (lotteries). Many of the biases discussed sit within this framework.\nThere is not much to dislike about the book, but Kahneman’s endorsement of libertarian paternalism to nudge people away from their behavioural biases was one area in which I would tread more carefully. As other chapters in the book show, groups of experts are also prone to biases. As Kahneman described his experience on a curriculum committee, which saw vast underestimates of the time involved and the probability of a negative result, I kept picturing government committees committing the same planning fallacy.\nSimilarly, Kahneman describes a debate between himself and Gary Klein over the usefulness of expert prediction. Part of their disagreement stemmed from the types of experts they were considering. Kahneman saw little usefulness for experts in fields where the outcome is inherently predictable. Klein saw more utility in fields where consistent feedback and practice allow a real degree of expertise to be achieved. Government largely sits in the former category. However, that small complaint should not detract from one of the best books I have read."
  },
  {
    "objectID": "posts/kahneman-on-the-price-of-freedom.html",
    "href": "posts/kahneman-on-the-price-of-freedom.html",
    "title": "Kahneman on the price of freedom",
    "section": "",
    "text": "From Daniel Kahneman’s Thinking, Fast and Slow:\n\nMuch is therefore at stake in the debate between the Chicago school and the behavioral economists, who reject the extreme form of the rational-agent model. Freedom is not a contested value; all the participants in the debate are in favor of it. But life is more complex for behavioral economists than for true believers in human rationality. No behavioral economist favors a state that will force its citizens to eat a balanced diet and to watch only television programs that are good for the soul. For behavioral economists, however, freedom has a cost, which is borne by individuals who make bad choices, and by a society that feels obligated to help them. The decision of whether or not to protect individuals against their mistakes therefore presents a dilemma for behavioral economists. The economists of the Chicago school do not face that problem, because rational agents do not make mistakes. For adherents of this school, freedom is free of charge.\n\nKahneman’s comment could also be applied to the belief that markets will always deliver an optimal outcome. Obviously, there are further layers of debate beyond the basic question of whether people are rational or markets fail. Kahneman is only addressing the first stage of libertarian denial."
  },
  {
    "objectID": "posts/joness-hive-mind-how-your-nations-iq-matters-so-much-more-than-your-own.html",
    "href": "posts/joness-hive-mind-how-your-nations-iq-matters-so-much-more-than-your-own.html",
    "title": "Jones’s Hive Mind: How Your Nation’s IQ Matters So Much More Than Your Own",
    "section": "",
    "text": "Garett Jones has built much of his excellent Hive Mind: How Your Nation’s IQ Matters So Much More Than Your Own on foundations that, while relatively well established, are likely surprising (or even uncomfortable) for some people. Here’s a quick list off the top of my head:\n\nHigh scores in one area of IQ tests tends to show up in others - be that visual, maths, vocabulary etc. The “g factor” can capture almost half of the variation in performance across the different tests.\nIQ is as good as the best types of interviews at predicting employee performance (and most interviews aren’t the “best type”) .\nIQ is the best single predictor of executive performance, and for performance in the middle to high-end range of the workforce.\nIQ predicts practical social skills. If you know someone’s IQ and are trying to predict job or school performance, there is little benefit in learning their EQ score. Conversely, if you know their EQ score, their IQ score has valuable information.\nIQ scores in poor countries predict earning power, just as they do in developed countries.\nTest scores such as the PISA test are better predictors of a country’s economic performance than years of education.\nCorruption correlates strongly (negatively) with IQ.\nIQ scores are good predictors of cooperative behaviour.\n\nAnd so on.\nOn that last point, there was one element that I had not fully appreciated. Jones reports an experiment in which players were paired in a cooperative game. High-IQ pairs were five times more cooperative than high-IQ individuals. The link between IQ and cooperation came from smart pairs of players, not smart individual players\nOnce you put all those pieces together, you reach the punchline of the book, which is an attempt to understand why the link between income and IQ, while positive both across and within countries, is of a larger magnitude across countries.\nJones’s argument builds on that of Michael Kremer’s classic paper, The O-Ring Theory of Economic Development. Kremer’s insight was that if production in an economy consists of many discrete tasks and failure in any one of those tasks can ruin the final output (such as an O-ring failure on a space shuttle), small differences in skills can drive large differences in output between firms. This can lead to high levels of inequality as the high-skilled work together in the same firm, leading them to be disproportionately more productive.\nJones extended Kremer’s argument this by contemplating what the world would look like if it comprised a combination of what he calls an O-ring sector and a foolproof sector. Here’s what I wrote about Jones’s argument previously based on an article he wrote:\n\nThe foolproof sector is not as fragile as the more complex O-ring sector and includes jobs such as cleaning, gardening and other low-skill occupations. The key feature of the foolproof sector is that being of low skill (which Jones suggests relates more to IQ than quantity of education) does not necessarily destroy the final product. It only reduces the efficiency with which it is produced. A couple of low-skill workers can substitute for a high-skill worker in the foolproof sector, but they cannot effectively fill the place of a high-skill O-ring sector worker, no matter how many low-skill workers are supplied.\nIn this economy, low-skill workers will work in the foolproof sector as these firms will pay them more than an O-ring sector firm. High-skill workers are found in both sectors, with their level of participation in each sector such that high-skill workers are paid the same regardless of which sector they work in (the law of one price).\nThus, within a country, firms will pay high-skill workers more than their low-skill counterparts, but not dramatically so. Their wage differential is determined by the difference in their outputs in the foolproof sector.\nAcross countries, however, things are considerably different. The highest skill workers in a country provide labour for the O-Ring sector. If they are low skilled relative to the high-skilled in other countries, their output in that fragile sector will be much lower. This occurs even for relatively small skill differences. Their income will reflect their low output, with wages also lower in the foolproof sector as high-skill workers apportion themselves between sectors such that the law of one price holds. The net result is much lower wages for workers in comparison to another country with a higher-skill elite.\n\nThe picture is a bit more subtle than that, depending on the mix of skills in the economy (which Jones describes in more detail in both the paper and book). But the basic pattern of large income gaps between countries and small gaps within is relatively robust.\nOne thing I would have liked to have seen more of in the book - although I suspect this might have somewhat been counter to Jones’s objective - would have been for Jones to challenge some of the research. At times it feels like Jones is tiptoeing through a minefield - the book is peppered with distracting qualifications that you feel he has to make to broaden the audience of the book.\nBut that said, I’m likely not the target audience. And I like the thought of that new audience hearing what Jones has to say."
  },
  {
    "objectID": "posts/jones-on-iq-and-immigration.html",
    "href": "posts/jones-on-iq-and-immigration.html",
    "title": "Jones on IQ and immigration",
    "section": "",
    "text": "David Henderson has posted on a recent presentation by Garett Jones of George Mason University in which Jones discussed IQ and cooperation.\nAs Jones notes, higher IQ people cooperate more in repeated prisoner’s dilemma games, are more trusting, have lower levels of divorce and engage more in activities such as voting and organ and cash donation. Jones suggests that if this link between trust and IQ improves a country’s institutions, countries should seek to raise national IQ, and one way of achieving this is by boosting immigration of high IQ populations.\nHenderson states that:\n\nIn Q&A, I asked him if he was suggesting something like Canada’s immigration rules that seem to put a higher weight on IQ indirectly or whether he would be happy with a Bryan Caplan solution that would allow pretty much anyone in who wanted to come. …. Garett seemed to lean to the former and said that one thing the government could so is, when an immigrant got a Ph.D., “staple a green card to it.”\n\nIt is interesting that Jones might suggest such an indirect method of achieving his objective. Would a simple IQ test be more effective? Or does Jones hope to capture people with other traits that a PhD might be indicative of, such as a propensity to work? Another question is whether a country should adopt such a passive method of giving residency to those who have already come to a country (which as Henderson notes, probably has some selection effects in itself)? Could a country be more aggressive and set up IQ testing stations in countries around the world and encourage immigration by all those who pass?\nA natural implication of this policy is that it will lower the average IQ of the donor country, and following Jones’s logic, lead to poorer institutions in that country. The net effect for the high IQ person is likely to be overwhelmingly positive, but the long-run dynamic effects on the donor country could be negative."
  },
  {
    "objectID": "posts/james-crow-on-the-quality-of-people.html",
    "href": "posts/james-crow-on-the-quality-of-people.html",
    "title": "James Crow on the quality of people",
    "section": "",
    "text": "Working through my reading pile, I finally read this great 1966 article by James Crow - The Quality of People: Human Evolutionary Changes. For those unfamiliar with Crow’s work, it’s worth watching this piece from Wisconsin Public Television (HT: Steve Hsu). The introduction captures some of his achievements.\nIn the paper, Crow opens by discussing the prediction of future evolutionary trends for humans:\n\nPrediction of future evolutionary trends is difficult because man himself plays such a decisive role. The largest influence in man’s future is man himself - the things that the individual and society do, intentionally or unwittingly. Yet, some trends are clear. Bacterial and protozoon diseases have been drastically reduced in many parts of the world. A few decades ago a gene producing a decreased susceptibility to smallpox would have had a great selective advantage. Now, in much of the world such a gene is of little value. We can expect that throughout the world selection for resistance to infection will become less and less important. …\nThe greater mobility of contemporary populations will also have genetic consequences. There is certain to be less inbreeding as persons tend to find mates away from their home environs. This should decrease the incidence of rare recessive diseases and cause some increase in general health and vigor - although the latter may not be measurable directly.\nA second consequence of mobility may be enhanced degree of assortive marriage. The greater participation in higher education, the stratification of students by aptitude, the growth of communities with similar interests and attainments all can lead to increased correlations between husband and wife. Added to this is the greater range of choice created by affluence and mobility so that any inherent preferences for assortative marriage are more easily realized.\nThe effect of assortative marriage is to increase the population variability. There is already a high correlation in IQ between husband and wife, and this may well increase. To the extent that this trait is heritable there will be greater variability next generation than would otherwise be the case. This means more geniuses as well as more at the other end of the scale.\n\nAfter discussing the rate of human evolution (in many dimensions slow) and the potential for selection in human societies as death rates decline (large enough that considerable selection can still occur), Crow moves to the question of eugenics. One of his more interesting points concerns the purpose of eugenics.\n\nAn immediate difficulty is to avoid the bias of our own society. What constitutes a good phenotype is not likely to be thought to be the same in Africa, China, and Greenland. …\n\nCrow also foreshadows the challenges that the development of genetic technologies will present in the future.\n\nIt is clear that biological and chemical possibilities for influencing human evolution and development are certain to come, probably before we have thought them through. Eugenics could be a far more potent force in the future than previously. In the past it has been tolerated partly because it was not likely to make an appreciable genetic change. The early eugenics was genetically naive and was connected with various dubious and even tragic political movements. I think the time is here when the subject should be reopened and discussed by everyone - not just biologists - with a serious consideration of the consequences of misjudgments as well as the possibilities for good.\n\nOver 40 years later, those possibilities are starting to crystallise (particularly with the rise of positive eugenics). The serious consideration is still to come."
  },
  {
    "objectID": "posts/is-there-an-ai-workslop-problem.html",
    "href": "posts/is-there-an-ai-workslop-problem.html",
    "title": "Is there an AI workslop problem?",
    "section": "",
    "text": "The debate about whether large language models are transformative or overhyped occasionally produces real-world data. These inevitably arrive with eye-catching headlines (e.g. MIT report: 95% of generative AI pilots at companies are failing) built on shaky methodology (based on 52 structured interviews at conferences, analysis of 300+ public AI initiatives and surveys with 153 leaders). There’s something to be learned here, but the headline isn’t it.\nThe latest example is a Harvard Business Review article AI-Generated “Workslop” is Destroying Productivity. Workslop is “AI generated work content that masquerades as good work, but lacks the substance to meaningfully advance a given task”.\nThe authors offer some striking numbers. Forty percent of survey respondents had received workslop in the last month. Fifteen percent of the work they receive is workslop. Add in the nearly two hours each respondent spends dealing it and you get a cost of $186 per employee per month, or $9 million annually for a 10,000 worker organisation. (That 10% of respondents said receiving workslop made colleagues seem more creative, capable, reliable, trustworthy and intelligent also suggests some survey respondent slop.)\nSo how did they reach these conclusions? The survey (available online when I wrote this) drew responses from 1,150 US-based full-time employees.\nThe first substantive question asks:\n\nHave you received work content that you believe is AI-generated that looks like it completes a task at work, but is actually unhelpful, low quality, and/or seems like the sender didn’t put in enough effort?\nIt could have appeared in many different forms, including documents, slide decks, emails, and code. It may have looked good, but was overly long, hard to read, fancy, or sounded different than normal.\nIn the last month, in general, how much of the work you received from colleagues fits this description?\n\nRespondents then report who sent the workslop and how much time they spent dealing with it in the past month. From this, the authors estimate workslop’s frequency and cost.\nThis is a problem. To estimate cost we need the counterfactual. What would the respondents have received without AI? I have received plenty of human-generated workslop over the years and spent considerable time fixing it. I would certainly clock more than two hours per month on that.\nThe AI workslop might actually be an improvement. People using AI to generate slop would likely generate slop anyway.\nThe methodology has another gap in that it only captures poor AI work. The study measures “bad things I noticed” whilst ignoring “good things I benefited from”. Without both, productivity estimates are meaningless. What about asking “Have you received AI-generated work content that is helpful and high-quality?” Or “How much time have you saved in the past month through using AI?”\nInclude both estimates and the story might flip to AI’s productivity benefits. Identifying AI-generated workslop shows there’s room for improvement (as does seeing human-generated workslop). But you need to measure both costs and benefits to measure whether generative AI delivers net value. Without the counterfactual, the study hasn’t measured productivity destruction at all."
  },
  {
    "objectID": "posts/is-poverty-in-our-genes.html",
    "href": "posts/is-poverty-in-our-genes.html",
    "title": "Is poverty in our genes?",
    "section": "",
    "text": "Is Poverty in Our Genes? is the title of a new extended critique of Ashraf and Galor’s forthcoming American Economic Review paper on genetic diversity and economic development. Published in Current Anthropology, the critique is an extension of an earlier piece by a group of academics (mainly from Harvard) who argue that Ashraf and Galor’s work is false and undesirable.\nThe critique spends some time focusing on the data underlying Ashraf and Galor’s work, which provides a good reminder of the complexity of human migratory history. For example, the authors write:\n\nHistorical flaws also exist in Ashraf and Galor’s treatment of concepts of innovation in table A3. Here the achievements of the diverse populations at Cordoba are taken to stand for measures of “European” innovation at 1000 CE. It is misleading to use Cordoba as a measure of European success, given that it was ruled by North African Moors until 1236 CE. Likewise, it seems inconsistent to classify Constantinople as part of Europe in 1000 CE but part of Asia in 1500 CE (Ashraf and Galor 2013, table A3). It should also be remembered that Europe’s role in innovation is a very recent phenomenon. Indeed, if we are to look for traces of “innovation” according to Ashraf and Galor’s standards in Europe, archaeology has made it clear that agriculture was not independently invented in Europe, but rather spread there from the Near East (Bellwood 2006). One can also show that Renaissance Europe was heavily influenced by Greek and Arab thought (Lewis 2009; Saliba 2007). Clearly, there is a great deal of multicontinental interaction in the circum-Mediterranean region. If one excluded these data coming from the heavily African- and Middle Eastern–influenced Mediterranean region, population levels (and hence innovation levels, according to Ashraf and Galor) in Europe would be low compared to other areas of the world until the late medieval period (after 1470).\n\nThese are interesting arguments, but I’m not convinced that shifting a few data points will materially change the general findings. The more fruitful area of criticism is the causative mechanism. In that area, the authors make some interesting points about evidence from other species.\n\nAshraf and Galor’s theoretical model argues that genetic diversity can play a positive role in the expansion of a society’s “production possibility frontier” or its ability to innovate. In their appendix H, they use animal studies to justify this claim. They describe studies on insects that link genetic diversity to disease resistance and to several aspects of hive performance in honeybees (Seeley and Tarpy 2007; Tarpy 2003). The two bee studies cited by Ashraf and Galor correlate genetic diversity with bee foraging rates and hive temperature and indicate that disease susceptibility relates to inbreeding. Another cited insect study on fruit flies (Drosphila species) shows that genetic diversity helps increase resistance to environmental changes (Frankham et al. 1999). It is unclear how either of these relates to an ability to innovate. Perhaps Ashraf and Galor were inspired to use these data because there is no research demonstrating that genetic heterozygosity at the population level is associated with capacity to innovate.\nIn addition, these cross-species comparisons of genetic diversity seem to not take into account how genetic diversity varies widely among species. Humans are noted for having extremely low levels of genetic diversity compared to other animals, including our closest cousins, chimpanzees. In fact, some chimpanzee breeding groups, such as those in the Taï forests of West Africa, are estimated to have greater nucleotide diversity than the entire human species (Gagneux et al. 1999). It is important to put into perspective that the total amount of human genetic diversity is actually quite small compared to that found in other model organisms.\n\nMy posts on Ashraf and Galor’s paper on genetic diversity and economic growth are as follows:\n\nA summary of the paper methodology and findings\nDoes genetic diversity increase innovation?\nDoes genetic diversity increase conflict?\nIs genetic diversity a proxy for phenotypic diversity?\nIs population density a good measure of technological progress?\nWhat are the policy implications of the effects of genetic diversity on economic development?\nShould this paper have been published?\n\nOther debate on this paper can also be found here, here, here and here."
  },
  {
    "objectID": "posts/is-loss-aversion-a-bias.html",
    "href": "posts/is-loss-aversion-a-bias.html",
    "title": "Is loss aversion a bias?",
    "section": "",
    "text": "From the Journal of Personality and Social Psychology:\n\nMuch research shows that people are loss averse, meaning that they weigh losses more heavily than gains. Drawing on an evolutionary perspective, we propose that although loss aversion might have been adaptive for solving challenges in the domain of self-protection, this may not be true for men in the domain of mating. Three experiments examine how loss aversion is influenced by mating and self-protection motives. Findings reveal that mating motives selectively erased loss aversion in men. In contrast, self-protective motives led both men and women to become more loss averse. Overall, loss aversion appears to be sensitive to evolutionarily important motives, suggesting that it may be a domain-specific bias operating according to an adaptive logic of recurring threats and opportunities in different evolutionary domains.\n\nUnfortunately, I cannot access the article beyond the abstract, but one of the article’s authors, Douglas Kenrick, has blogged about the paper at Psychology Today (HT: Åse). Kenrick writes:\n\n[R]esearch by several members of our team shows that loss aversion waxes and wanes in flexible ways, depending of whether or not the person is experiencing different fundamental motivational states (such as self-protection or mating motivation). Research participants were asked how happy or unhappy it would make them to gain or lose $100, for example, or to experience a 30-percentile boost in their financial assets. As in the previous research, losses typically loomed slightly larger than gains. But all that changed for participants who answered the questions in a mating frame of mind (after imagining themselves having a romantic encounter with someone they found highly attractive). …\nAccording to Jessica Li, who was the first author of the study: “For men in a mating frame of mind, loss aversion completely disappeared, so that they became more focused on wins than losses. For women, on the other hand, mating motivation led them to be even more loss averse.”\n\nIn another evolutionary scenario, where losses were clearly more costly than gains, participants become even more loss averse.\n\nWhen we put participants in a self-protective frame of mind (by having them imagine being alone in the house on a dark night and hearing an intruder breaking in), both men and women became more loss averse in their judgments.\n\nThe way I prefer to think about loss aversion is to consider what the objective of the person is - and that is normally an evolutionary objective. Once the gains and losses are framed in evolutionary terms, what is loss aversion in one dimension is not loss aversion in the dimension that matters. For example, if a small cash win will not increase the number of mates but a small cash loss might cost them the mate that they have, a loss averse response to the potential cash pay-offs can be contrasted to the zero-gain potential in the mating dimension. It is not loss aversion to reject a bet with no upside."
  },
  {
    "objectID": "posts/is-it-human-nature-to-riot.html",
    "href": "posts/is-it-human-nature-to-riot.html",
    "title": "Is it human nature to riot?",
    "section": "",
    "text": "In a post earlier this month, Eric Johnson put together an interesting argument on the evolution of collective violence  (I recommend reading his whole post).\nJohnson opens with some of the arguments that group violence is a consequence of our evolutionary history. One of this arguments is the called the Elaborated Social Identity Model of crowd behaviour:\n\nEach individual remains a rational actor, but has been primed by natural selection to identify with the group during a period of crisis. This well developed ingroup/outgroup bias is what has allowed our species to be the most cooperative of the primates, but certain conditions have the potential to turn us against our own community. …\n“Collective violence,” wrote Harvard primatologist Richard Wrangham in the Annals of the New York Academy of Sciences, shows “a common human pattern evident in societies lacking effective central authority, manifested in ethnic riots, blood feuds, lethal raiding, and warfare.” Such aggression, he says, is directly related to that of nonhuman primates and demonstrates a common evolutionary history.\n\nOne piece of evidence in support of this theory of inherent group aggression were a series of attacks by baboons in London zoo, with two-thirds of the 140 baboons dying during the violence. However, subsequent observations of baboons in captivity show how strong the environmental influences are on the actions of the groups:\n\nWhat Kummer found was that captive baboons showed many more aggressive acts than their free-ranging counterparts (nine times more for females and seventeen and a half times more for males). The massacre of Monkey Hill therefore represents a kind of controlled experiment on the potential dangers of social engineering, one that demonstrates the lethal consequences of flawed assumptions. …\nSince the events of Monkey Hill, hundreds of studies with captive primates have shown that impoverished environments result in heightened aggression and antisocial behavior. Such behavior has been shown to significantly increase under conditions of overcrowding, when there’s a lack of novelty in food, entertainment, or social opportunities, when the population increases and the number of strangers in a colony grows, or, most crucially, when food is limited and/or fluctuates dramatically.\n\nUsing these observations, Johnson draws some conclusions about the London riots:\n\n [T]he riot outbreaks were clustered in the most economically deprived regions of the city. It was these regions that would have been most aversely affected by the austerity measures and, with a peak in both food and energy prices occurring at the same time, the environmental conditions were ideal for a triggering event that would push an already stressed population over into social discord…..\nFor London and the cities throughout North Africa and the Middle East, it appears there was a free choice to riot after all. But the choice didn’t come from the rioters alone, it rose from leaders and policymakers and the larger society as a whole. Riots reveal a colony in discord. Many of us have acknowledged the widening inequality and economic decline of our most impoverished citizens–but we chose to ignore it\n\nThese conclusions might seem to flow from the observations of stressed primates, but we have an extra piece of data on the London rioters. Over 70 per cent of those convicted of rioting had prior convictions – 15 on average. While society may influence the chance of a riot, it seems that some people are much more susceptible than others."
  },
  {
    "objectID": "posts/is-genetic-diversity-a-proxy-for-phenotypic-diversity.html",
    "href": "posts/is-genetic-diversity-a-proxy-for-phenotypic-diversity.html",
    "title": "Genetic diversity, phenotypic diversity and the founder effect",
    "section": "",
    "text": "In two recent posts I examined the causative mechanisms underlying Ashraf and Galor’s hypothesis linking genetic diversity to economic growth (innovation and conflict). In those posts, I avoided examining whether genetic diversity could be considered a proxy of phenotypic diversity unrelated to that genetic diversity (such as language).\nPart of the reason for this is that Ashraf and Galor do not indicate in their paper or web appendix that they intended to use genetic diversity as a proxy in this way. As I have posted about before, the language of the paper is focused on genetic diversity and the phenotypic expression of that genetic diversity. In dissecting the paper, I wanted to focus on what the paper states.\nHowever, given that Ashraf and Galor have now made an argument [Update: the response is no longer online] that genetic diversity is a proxy in their response to a critique of their paper, the argument is worth assessing. Ashraf and Galor write:\n\nThe key is that the measure of intra-population genetic diversity that we employ should be interpreted as a proxy (i.e., a correlated summary measure) for diversity amongst individuals in a myriad of observable and unobservable personal traits that may be physiological, behavioral, socially-constructed, or otherwise. …\nThe fact that the measure of genetic diversity we use is based on variation across individuals in non-protein- coding regions of the genome (and, thus, in genomic characteristics that are not necessarily phenotypically expressed so as to be subject to the forces of natural selection) is clear reason why our findings should be interpreted through the lens of our measure serving as a proxy for diversity more broadly defined.\nThe more relevant question to ask therefore is to what extent the measure we use can reasonably be considered a proxy for diversity in unobserved phenotypic or socially-constructed characteristics. There is indeed an emerging body of scientific evidence that establishes remarkable correlations in this regard.\n\nAshraf and Galor refer to two articles on this point - one on diversity in head shape, which I noted in my post on innovation, and a second diversity in language. On the second, they cite a Science paper from 2011 in which Atkinson reports a finding that diversity in phonemes - perceptually distinct units of sound that differentiate words - declines with distance from Africa. This pattern reflects that found for genetic diversity, and Atkinson suggests that similar forces were acting on each. There are still significant hurdles to show a causative link between diversity in phonemes and innovation and conflict, but the persistence of the phenotypic diversity leaves opens this possibility. The task is to identify what forms of phenotypic diversity might be relevant.\nAshraf and Galor build the case further in a new paper that will be published in the American Economic Review Proceedings and Papers in May. They propose that:\n\nBuilding on the role of deeply-rooted biogeographical forces in comparative development, this research empirically demonstrates that genetic diversity, predominantly determined during the prehistoric “out of Africa” migration of humans, is an underlying cause of various existing manifestations of ethnolinguistic heterogeneity.\n\nRather than proposing that genetic diversity is a proxy for other diversity shaped by the Out of Africa event, they propose that genetic diversity is a cause of ethnolinguistic heterogeneity. Depending on whether they use a modern global or old world sample, they find that genetic diversity is responsible for between 7 and 11 per cent of ethnolinguistic heterogeneity.\nAn alternative approach would have been to attribute genetic and ethnolinguistic diversity to a common cause in the founder effect. I asked Ashraf and Galor by email why they preferred an explanation of genetic diversity causing ethnolinguistic heterogeneity and they replied as follows:\n\n[T]he ethnicities located at greater migratory distances from East Africa (especially those outside of the African continent) DO NOT represent a subset of the ethnicities in Africa. Had the serial founder model (as applicable in generating the global distribution of genetic diversity) been equally applicable in explaining global spatial variation in ethnic diversity, we should observe that ethnic groups extant outside of Africa are also present within Africa. This is clearly NOT the case.\nThus, our hypothesis is that of quasi-random migrant selection from the origin in each step of the “out of Africa” demic diffusion process, with the migrants engaging in endogenous group selection (or endogenous sorting) upon reaching their destination in that step of the diffusion. Moreover, this group selection process would take into account the trade-off associated with intragroup diversity (i.e., diversity across individuals WITHIN the new group), and possibly, also reflect the interaction of intragroup diversity with location-specific geographical factors.\n\nThey further expand on the mechanism in their paper:\n\nFollowing the “out of Africa” migration, the initial level of genetic diversity in indigenous settlements presumably facilitated the formation of distinct ethnic groups through a process of endogenous group selection, based on the tradeoff between the costs and benefits associated with heterogeneity and scale. While heterogeneity raised the likelihood of disarray and mistrust, reducing cooperation and thus adversely affecting group-specific productivity, complementarities across diverse productive traits and preferences stimulated productivity. Since in a given environment, diminishing marginal returns to diversity and homogeneity entail an optimal size for each group, higher initial genetic diversity would have positively contributed to the number of groups, and thus to the degree of fractionalization. Further, to the extent that higher initial diversity did not lead to an excessively large number of groups, it would have positively contributed to the degree of polarization as well.\n\nThe causative argument relies in part on the conflict between genetically dissimilar individuals limiting group size, which would in turn affect ethnic fractionalisation, which would then affect inter-group conflict. I need to think about this argument more, but one possible implication of this group-sorting argument is that groups with less genetic diversity would become larger. Part of the benefit to lower diversity would be to enable a scale effect - that is, more people leading to more ideas. A larger group would be more innovative simply through having more innovators.\nSo, to answer the question of whether genetic diversity can be a proxy for phenotypic diversity beyond phenotypic expression of that genetic diversity, yes. And as a plausible causative link may exist between ethnolinguistic heterogeneity and conflict, this builds the case for the link between genetic diversity and conflict. But, as their new paper suggests, Ashraf and Galor propose a more direct relationship than through a common cause in the founder effect, at least as it relates to ethnolinguistic diversity. Rather than pursuing the proxy argument, their new paper builds the case that the relationship between genetic diversity and conflict is more direct.\nMy posts on Ashraf and Galor’s paper on genetic diversity and economic growth are as follows:\n\nA summary of the paper methodology and findings\nDoes genetic diversity increase innovation?\nDoes genetic diversity increase conflict?\nIs genetic diversity a proxy for phenotypic diversity? (this post)\nIs population density a good measure of technological progress?\nWhat are the policy implications of the effects of genetic diversity on economic development?\nShould this paper have been published?\n\nEarlier debate on this paper can also be found here, here and here."
  },
  {
    "objectID": "posts/is-darwin-or-smith-the-father-of-economics.html",
    "href": "posts/is-darwin-or-smith-the-father-of-economics.html",
    "title": "Is Darwin or Smith the father of economics?",
    "section": "",
    "text": "In his new book, The Darwin Economy: Liberty, Competition, and the Common Good, Robert Frank argues that within the next century, Charles Darwin will become known as the intellectual founder of economics, displacing Adam Smith from that role. Frank’s prediction rests on the contrasting perspectives on competition provided by the two. Smith had the counterintuitive insight that selfish actions could increase the common good, while Darwin recognised that competition could be wasteful as individuals compete for survival and mates. Frank argues that the Darwinian picture is a better representation of the economy.\nHaving approached the book more than ready to be convinced by Frank, I actually ended it with a reinforced appreciation of Smith. Smith might not have given as clear perspective on the conflict between group and individual interests as did Darwin, but his insights about specialisation, trade and the benefit to others in the absence of benevolent intentions remain among the core understandings of economics. One look around us shows that competition in the developed world has delivered enormous wealth. That this benefit to the group occurred despite the selfish actions of individuals shows us that Smith’s ideas have been the dominant force. We should not ignore the negative consequences of competition, and policy decisions should acknowledge them, but the dominant trend is increased group welfare.\nFrank might argue in response that it is the constraint and institutions provided by government that has allowed this wealth to be created, and on that point, he would be right (although precisely which constraints and institutions is debatable). But it is the understanding of Smith that shows us what can happen within the right institutional framework. If people can reap the benefits of their own efforts, they will specialise, trade, and take advantage of the growing size of the market, which on average will benefit others too.\nAs for Darwin, he deserves a central role in economics, but largely through the fact that humans evolved through natural selection. This could considerably change economics, but it does not displace Smith’s core insights. My hope is that instead of Charles Darwin being understood as the founder of economics, economics is considered as a branch of ecology. It is from that perspective that Charles Darwin can rightfully stand at the top."
  },
  {
    "objectID": "posts/is-aid-really-so-complex.html",
    "href": "posts/is-aid-really-so-complex.html",
    "title": "Is aid really so complex?",
    "section": "",
    "text": "Since Bill Easterly stuck his head above the parapet last week and referred to complex systems in response to Paul Collier, the “complexity” community has been up in arms. In a quick reference to complexity, Easterly wrote:\n\nA popular topic in the aid blogosphere this week was not about Haiti or Ivory Coast or south Sudan but about complex systems, i.e. systems that cannot be reduced to a simple mathematical or statistical model, where actions often have unintended effects.\n\nHe then (rightly in my opinion) questioned Collier’s ability to predict the consequences of supporting a coup, including Collier’s chain of predictions about the actions of various levels of Ivory Coast army officers. With what confidence can the course of a civil conflict be predicted? Not much if previous conflicts are any guide.\nAlthough I agree with Easterly’s analysis, his use of complexity is out-of-place. First, complexity can be simple. As Philip Auerswald said:\n\n[T]he core insight of the study of complexity …. is this: systems that are not just reduced to, but actually defined by, simple mathematical models, have the potential to generate extremely…well, complex behaviors.\n\nFor examples of this, I recommend Thomas Schelling’s Micromotives and Macrobehavior.\nSecond, and to me the most interesting point, comes from Auerswald’s likening of Easterly’s use of complexity to previous attempts by economists to tie complexity theory into Hayek’s concept of spontaneous order. While Hayek’s work provides some early thinking on complexity, Auerswald suggested that bringing these ideas together is a failed undertaking from the outset, with Hayek’s concern being about calculation in the presence of randomness, not the emergence of complexity in the absence of randomness. That distinction is important, and the problem identified by Hayek was fundamentally one of calculation (although the calculation problem was about more than just randomness).\nHowever, there is a more important distinguishing feature. Hayek saw a benevolent force (or invisible hand) creating a more efficient system than humans could create by central planning. A complexity theorist might argue that there is no such benevolent force and that human interference may be required for an efficient outcome (this distinction was the central conclusion of a paper by Kilpatrick). If we look at Easterly’s argument under this distinction, it is much more Hayekian than of modern complexity theory.\nThe third point, and flowing directly from this distinction, is that if a system is complex, that (in itself) does not mean that we should not touch the system. We can see this in some of the work that has come out of the Santa Fe Institute. With concepts such as path dependence, increasing returns and out-of-equilibrium dynamics, one can argue that the current state of affairs is not ideal and a few “tweaks” might help. I don’t generally agree with that argument, but you need something more than “it’s too complex” to respond.\nFourth and finally, I don’t think this third point matters in the current debate. Complexity is not the major problem. As stated by David Ellerman in response to Easterly’s update on where the complexity debate is at:\n\nThe mistake in applying complexity theory to human relationships such as the education, management, development aid, and helping in general is that the basic problem is NOT that the human “systems” are complex, “messy,” nonlinear, etc. The basic problem …. is that success lies in achieving more autonomy on the part of the doers, and autonomy is precisely the sort of thing that cannot be externally supplied or provided by the would-be helpers. This is the fundamental conundrum of all human helping relations, and it is the basic reason, not complexity, why engineering approaches and the like don’t work."
  },
  {
    "objectID": "posts/iq-as-a-necessary-but-not-sufficient-condition-for-genius.html",
    "href": "posts/iq-as-a-necessary-but-not-sufficient-condition-for-genius.html",
    "title": "IQ as a necessary but not sufficient condition for genius",
    "section": "",
    "text": "A quote from Arthur Jensen (From Steve Hsu. A fuller version of the interview can be found here):\n\n[T]he outstanding feature of any famous and accomplished person, especially a reputed genius, such as Feynman, is never their level of g (or their IQ), but some special talent and some other traits (e.g., zeal, persistence). Outstanding achievements(s) depend on these other qualities besides high intelligence. The special talents, such as mathematical musical, artistic, literary, or any other of the various “multiple intelligences” that have been mentioned by Howard Gardner and others are more salient in the achievements of geniuses than is their typically high level of g. Most very high-IQ people, of course, are not recognized as geniuses, because they haven’t any very outstanding creative achievements to their credit. However, there is a threshold property of IQ, or g, below which few if any individuals are even able to develop high-level complex talents or become known for socially significant intellectual or artistic achievements. This bare minimum threshold is probably somewhere between about +1.5 sigma and +2 sigma from the population mean on highly g-loaded tests.\n\nBut IQ is not meant to capture everything of interest:\n\nSo-called intelligence tests, or IQ, are not intended to assess these special abilities unrelated to IQ or any other traits involved in outstanding achievement. It would be undesirable for IQ tests to attempt to do so, as it would be undesirable for a clinical thermometer to measure not just temperature but some combination of temperature, blood count, metabolic rate, etc. A good IQ test attempts to estimate the g factor, which isn’t a mixture, but a distillate of the one factor (i.e., a unitary source of individual differences variance) that is common to all cognitive tests, however diverse."
  },
  {
    "objectID": "posts/introducing-evonomics.html",
    "href": "posts/introducing-evonomics.html",
    "title": "Introducing Evonomics",
    "section": "",
    "text": "What is Evonomics?\n\nEvonomics is an online magazine and intellectual movement built on the pillars of complexity science and evolutionary principles, and includes key insights from the synthesis that has slowly been growing across disciplines in areas like behavioral, experimental, institutional, and ecological economics. The magazine showcases the new scientific foundations for human nature and society and demonstrates its relevance to contemporary economic and political issues.\n\nSign up on the Evonomics site for updates as it gears up for launch. You can also follow Evonomics on twitter."
  },
  {
    "objectID": "posts/intelligence-and-assortive-mating.html",
    "href": "posts/intelligence-and-assortive-mating.html",
    "title": "Intelligence and assortive mating",
    "section": "",
    "text": "Arnold Kling writes:\n\nThe story I tell for bimodalism is mating behavior. When high earners marry high earners, class divisions will emerge. But this has implications for the IQ distribution. One would expect bimodalism to appear in the IQ distribution, with the children of high-IQ parents tending centered around one mode and the children of low-IQ parents centered around another.\n\nKling’s expectation depends on our assumptions about the nature of the assortive mating.\nTake an extreme example, where mating is perfectly assortive and everyone mates with someone of the same income and intelligence as themselves. If intelligence is perfectly heritable, their children’s intelligence will be the same. Intelligence in the next generation will only vary to the extent that people of different levels of intelligence have different levels of fertility. If there is no difference in fertility, the distribution of intelligence will be the same one generation to the next.\nSo how could we generate a bimodal distribution? One way would be if there is some threshold level of intelligence that acted as a barrier to mating. If, for example, those of below average intelligence only mated with people of below-average intelligence (although not necessarily the same intelligence as themselves), while those with above-average intelligence only mate with others in their group, the two populations’ intelligence will cluster around different means.\nFor this threshold to exist, there must be some non-linear returns to intelligence. There might be competition to live in certain suburbs or attend certain schools. Those that scrape into the high intelligence college get access to much larger rewards and access to significantly more intelligent mates than those that narrowly miss out.\nAs for what is actually occurring, I do not expect that the existing mating patterns in developed countries such as the United States will result in a bimodal distribution of intelligence. The focus on assortive mating masks the huge level of mixing that currently occurs. Consider the assortive mating that occurred as humans spread themselves across the globe and mated within their small bands. Populations were separated for millennia. Even in recent centuries, people largely mated within their small communities and class. Today’s population has a level of dynamism and mixture far beyond most of human history, regardless of what class divisions there now are."
  },
  {
    "objectID": "posts/inequality-persistence-circa-5000-bce.html",
    "href": "posts/inequality-persistence-circa-5000-bce.html",
    "title": "Inequality persistence circa 5000 BCE",
    "section": "",
    "text": "An article by Bentley and colleagues published in PNAS last month points to some very early evidence of persistent inequality. The study headline is the uncovering of the earliest (statistically significant) evidence of status and wealth differences among the first farmers of Neolithic Europe and the existence of a patrilocal kinship system. However, the analysis also suggests that the healthiest farmers when young were also the richest when buried. Early advantage persisted until death.\nIt takes some reading between the lines in the PNAS article to see this result, but a Guardian piece containing interviews with the study authors is more direct:\n\nSome of the male skeletons were buried with stone adzes – cutting and chopping tools – which were often beautifully polished and made from carefully selected stone, and so were probably also symbols of status and wealth. An analysis of the strontium isotopes in their tooth enamel showed these individuals had lived on food grown in “loess”, the most fertile and productive soil.\nBecause strontium markers are laid down in tooth enamel in childhood, it seems they hadn’t earned but inherited this richer diet, and the fact that they were buried with the adzes suggests that they died as they had lived: privileged to the end.\n“This strongly suggests that access to the best soils was being passed on between generations,” Bickle said. “Thus, while I think it’s not news that status differences and subsistence specialisms date to the Neolithic, this is perhaps the first time we’ve been able to show that inheritance was a large part of this.”\n\nThe usual question arises at this point - what mix of genetic endowments, productivity effects related to upbringing (such as the poor health) and resource transfers delivered this result?"
  },
  {
    "objectID": "posts/income-and-iq.html",
    "href": "posts/income-and-iq.html",
    "title": "Income and IQ",
    "section": "",
    "text": "As I noted in my recent post on Malcolm Gladwell’s Outliers, Gladwell ignored the possibility that traits with a genetic component, other than IQ, might play a role in determining success. His approach reminded me of a useful paper by Samuel Bowles and Herbert Gintis from 2002 on the inheritance of inequality. Bowles and Gintis sought to explain the observed correlation between parental and child income (a correlation of around 0.4) by examining IQ, other genetic factors, environment, race and schooling.\nAs an example of the consequences of the transmission of income. Bowles and Gintis cited a paper by Hertz which showed that a son born to someone in the top decile of income had a 22.9 per cent chance of attaining that decile himself, compared to a 1.3 per cent chance for someone born to parents in the bottom decile. Conversely, a child born to parents in the top decile had only a 2.4 per cent chance of finishing in the lowest decile compared to over 31.2 per cent for those born to bottom decile parents.\nAs Gladwell did, Bowles and Gintis started their examination with IQ. To calculate  the inheritance of income through genetically inherited IQ, Bowles and Gintis considered the correlation between parent IQ and income, the heritability of IQ from parent to child and the correlation between IQ and income for the child. Breaking this down, Bowles and Gintis used the following steps and estimates:\n\nThe correlation between parental income and IQ is 0.266.\nIf the parents’ genotypes are uncorrelated, the genetic correlation between the genotype of the parents and of the child is 0.5. This can be increased with assortive mating (people pairing with people more like themselves) to a maximum of one (clones mating). Bowles and Gintis use 0.6.\nThe heritability of IQ is 0.5.\nThe correlation between child income and IQ is 0.266.\n\nMultiplying these four numbers together gives the intergenerational correlation of income due to genetically based transmission of IQ. I think there is a mistake in the calculations used by Bowles and Gintis, as they find an intergenerational correlation of 0.01, where I calculated 0.02. This leads to genetically inherited IQ variation explaining 5.3 per cent of the observed intergenerational correlation in income. Regardless of the error, this is a  low proportion of the income heritability. (After I wrote this post I did a google search to find if someone had spotted this error before - and they had - on a earlier Gene Expression post on this same paper.)\nI would have used some slightly higher numbers, but pushing the numbers to the edges of feasible estimates, such as increasing the correlation between income and IQ to 0.4, the genetically based correlation between parent and child IQ to 0.8 and the degree of assortive mating so that parent-child genotype correlation is 0.8 only yields an intergenerational correlation of 0.10. Genetically inherited IQ would account for approximately 26 per cent of the observed intergenerational correlation.\nUnlike Gladwell, Bowles and Gintis then asked what role other genetic factors may play. By using twin studies, which provide an estimate of the degree of heritability of income (using the difference in correlation between fraternal and identical twins) and the degree of common environments of each type of twin, Bowles and Gintis estimated that genetic factors explain almost a third (0.12) of the 0.4 correlation between parent and child income. Loosening their assumptions on the degree of shared environments by identical twins compared to fraternal twins (i.e. assuming near identical environments for both identical and fraternal twins) can generate a higher estimate of the genetic basis of almost three-quarters of the variability in income.\nFrom this, it seems that genetic inheritance plays an important role income transmission between generations. The obvious question is what these factors might be. I expect that patience or ability to delay gratification must play a role, although I would expect that there would be a broad suite of relevant personality traits. I would also expect that appearance and physical features would be relevant. Bowles and Gintis do not take their analysis to this point.\nThe authors finished their analysis with some consideration of other factors, and conclude that race, wealth and schooling are more important than IQ as a transmission mechanism of income across generations (although as the authors noted, they may have overestimated the importance of race by not including a measure of cognitive performance in the regression). That conclusion may be fair, but as they had already noted, there is a substantial unexplained genetic component.\nThis highlights the paper’s limitation, as once the specific idea that heritability of IQ is a substantial cause of intergenerational income inequality has been dented,  the identification of other (but unknown) genetic factors leaves open a raft of questions about income heritability. Using Bowles and Gintis’s conservative estimates, we still have 25 per cent of income heritability being put down to genetic factors without any understanding of what these traits are and the extent of the role they play.\nIn their conclusion, Bowles and Gintis touch on whether policy interventions might be based on these results. They are somewhat vague in their recommendations, but suggest that rather than seeking zero intergenerational correlation, interventions should target correlations that are considered unfair. They suggest, as examples, that there are large majorities supporting compensation for inherited disabilities while intervention for good looks is not appropriate.\nOne thing I find interesting in an analysis of heritability such as this is that over a long enough time horizon, to the extent that someone with a trait has a fitness advantage (or disadvantage), the gene(s) behind the trait will move to fixation (or be eliminated) as long as heritability is not zero. The degree of heritability is relevant only to the rate at which this occurs and only in a short-term context. The obvious question then becomes (which is besides the point of this post) whether IQ (through income or not) currently yields a fitness advantage. Over a long enough time period, variation will tend to eliminate itself and Bowles and Gintis would be unable to find any evidence of IQ heritability affecting income across generations."
  },
  {
    "objectID": "posts/in-praise-of-malcolm-gladwell.html",
    "href": "posts/in-praise-of-malcolm-gladwell.html",
    "title": "In praise of Malcolm Gladwell",
    "section": "",
    "text": "While Malcolm Gladwell bashing season is still in full swing and before the mob burns the Gladwell effigy, I want to record a few thoughts that I feel are under-appreciated (or under-emphasised) by some of Gladwell’s critics. (For those not up to speed, Chabris opens, Gladwell responds, Chabris has another go, Gelman comments, twitter goes nuts, and if you google, you’ll find plenty of other contributions).\nFirst, I am not a fan of, for want of a better way of describing it, the conformist strain that underlies some of the critiques. “There’s our way or the highway.” There is a role in science for speculation, for advocacy, for taking a position and seeing how far you can push it (even if you don’t believe it). Gladwell’s stories make you think about issues in ways you haven’t before. It’s through confronting and dismantling those stories that you get to understand the question better. Of those who have read Outliers, who hasn’t thought more about the mix of training and talent behind elite performers. Daniel Kahneman’s Thinking, Fast and Slow was a richer read (for me) having previously read Blink and thought about the two systems that Kahneman dissects (in fact, Blink triggered a path of reading that continues to this day). I largely agree with the “stone soup” section of Gelman’s post, which captures this point better than I do here.\nHaving said this, there are times where I would prefer more evidence to underlie Gladwell’s claims. In fact, I often wish that about whatever I read. I recently wished the same thing about Dan Ariely’s The Upside of Irrationality. But Gladwell does draw on a lot of peer-reviewed science (just not always the papers preferred by his critics). The 10,000 hour rule did not come from nowhere. Of course, the flip-side of this demand is a world there every word is vetted, every claim pinned to a peer-reviewed paper, and no-one wants to read.\nI suspect part of my discomfort with the critiques of Gladwell is that so much of what is published as peer-reviewed science won’t stand the test of time (particularly in economics, psychology and other human focused fields). It won’t (or can’t) be replicated. It relies on tortured statistics. Those who don’t want to believe it won’t, or they’ll stick to their preferred piece of the literature while ignoring the rest. And among that mess, we pick on Gladwell for being too neat or not logical enough or making general claims without evidence (and why aren’t more of the criticisms of Gladwell being directed at the authors of the studies he is referring to and not Gladwell himself).\nIn that context, another part of my discomfort comes from the expectation that despite my “superior logic”, Gladwell might well be right on some points where I (and others) disagree with him. A small degree of humility does not go astray.\nGladwell is also not as neat in his story telling as he is often accused. You can see why he laments that people only read the first half of Blink, when the latter chapters are filled with examples of where snap judgments fail. If anything, parts of Blink seem inconsistent and by the end of the book, un-reconciled. His recent story on whether drugs may be a fair way to overcome genetic differences in sporting ability is a great example of his ability to raise a murky question and let it linger in the air (in my opinion, Gladwell is a better essayist than book author).\nFinally, although Gladwell doesn’t seem keen to engage in a back and forth debate, when he does, he doesn’t do too badly. Or, as I’d rather put it, his opponents don’t deliver the slam dunks you might expect. Andrew Gelman suggested that “Gladwell’s credibility has been weakened over the years by fights with bigshots such as Steven Pinker”. But if you read the exchangebetween Pinker and Gladwell, Pinker’s major sources were blogs and petitions. Maybe the bloggers and petitioners were right, as Pinker suggested (I think they were). But it’s hardly the dismemberment that someone of Pinker’s stature might be expected to deliver. Of course, over the next couple of years, some of those critiques strengthened. Thanks to Malcolm Gladwell, people thought about it more and nailed their arguments down.\nHaving said the above, I disagree with a lot of Gladwell’s arguments, including many of the central themes of Outliers. I am with Steven Pinker on the lonely ice floe of I.Q. fundamentalism (although threads started by Gladwell played a small part in convincing me that traits such as time preference and conscientiousness are also important). But I’d like to hear less criticism of Malcolm Gladwell as a package and more of “That’s interesting, but I think it’s wrong. Here’s why.” Because its when people grapple with Gladwell’s ideas that his true value has been realised. Those areas touched by Gladwell are better for it.\n*If you want more Malcolm Gladwell love, try this piece by Ian Leslie."
  },
  {
    "objectID": "posts/improving-behavioural-economics.html",
    "href": "posts/improving-behavioural-economics.html",
    "title": "Improving behavioural economics",
    "section": "",
    "text": "A neat new paper has appeared on SSRN from Owen Jones - Why Behavioral Economics Isn’t Better, and How it Could Be (HT: Emanuel Derman via Dennis Dittrich). My favourite part is below. As I have said many times before, giving a bias a name is not theory.\n\n[S]aying that the endowment effect is caused by Loss Aversion, as a function of Prospect Theory, is like saying that human sexual behavior is caused by Abstinence Aversion, as a function of Lust Theory. The latter provides no intellectual or analytic purchase, none, on why sexual behavior exists. Similarly, Prospect Theory and Loss Aversion – as valuable as they may be in describing the endowment effect phenomena and their interrelationship to one another – provide no intellectual or analytic purchase, none at all, on why the endowment effect exists. …\n[Y]ou can’t provide a satisfying causal explanation for a behavior by merely positing that it is caused by some psychological force that operates to cause it. That’s like saying that the orbits of planets around the sun are caused by the “orbit-causing force.” …\n[L]oss aversion rests on no theoretical foundation. Nothing in it explains why, when people behave irrationally with respect to exchanges, they would deviate in a pattern, rather than randomly. Nor does it explain why, if any pattern emerges, it should have been loss aversion rather than gain aversion. Were those two outcomes equally likely? If not, why not?\n\nPart of the solution provided by Jones, as reflected in much of his past work, rests in evolutionary theory."
  },
  {
    "objectID": "posts/ignore-the-sunk-costs.html",
    "href": "posts/ignore-the-sunk-costs.html",
    "title": "Ignore the sunk costs",
    "section": "",
    "text": "Edge has a great set of short notes by various authors on how Daniel Kahneman has influenced them. It is worth flicking through them all, but excerpts from my two favourites are below.\nFirst, some excellent advice via Jason Zweig:\n\nAnyone who has ever collaborated with him tells a version of this story: You go to sleep feeling that Danny and you had done important and incontestably good work that day. You wake up at a normal human hour, grab breakfast, and open your email. To your consternation, you see a string of emails from Danny, beginning around 2:30 a.m. The subject lines commence in worry, turn darker, and end around 5 a.m. expressing complete doubt about the previous day’s work.\nYou send an email asking when he can talk; you assume Danny must be asleep after staying up all night trashing the chapter. Your cellphone rings a few seconds later. “I think I figured out the problem,” says Danny, sounding remarkably chipper. “What do you think of this approach instead?”\nThe next thing you know, he sends a version so utterly transformed that it is unrecognizable: It begins differently, it ends differently, it incorporates anecdotes and evidence you never would have thought of, it draws on research that you’ve never heard of. If the earlier version was close to gold, this one is hewn out of something like diamond: The raw materials have all changed, but the same ideas are somehow illuminated with a sharper shift of brilliance.\nThe first time this happened, I was thunderstruck. _How did he do that? How could _anybody _do that? _When I asked Danny how he could start again as if we had never written an earlier draft, he said the words I’ve never forgotten: “I have no sunk costs.”\n\nSecond, Eric Kandel (an 84 year-old Nobel laureate):\n\nDaniel Kahneman has not yet influenced my work on snails and mice, but I am only in an early point in my career and I still look forward to exploring his ideas in a molecular biological context in the future."
  },
  {
    "objectID": "posts/hypotheticals-versus-the-real-world-the-trolley-problem.html",
    "href": "posts/hypotheticals-versus-the-real-world-the-trolley-problem.html",
    "title": "Hypotheticals versus the real world: The trolley problem",
    "section": "",
    "text": "Daniel Engber writes:\n\nPicture the following situation: You are taking a freshman-level philosophy class in college, and your professor has just asked you to imagine a runaway trolley barreling down a track toward a group of five people. The only way to save them from being killed, the professor says, is to hit a switch that will turn the trolley onto an alternate set of tracks where it will kill one person instead of five. Now you must decide: Would the mulling over of this dilemma enlighten you in any way?\nI ask because the trolley-problem thought experiment described above—and its standard culminating question, Would it be morally permissible for you to hit the switch?—has in recent years become a mainstay of research in a subfield of psychology. …\nFor all this method’s enduring popularity, few have bothered to examine how it might relate to real-life moral judgments. Would your answers to a set of trolley hypotheticals correspond with what you’d do if, say, a deadly train were really coming down the tracks, and you really did have the means to change its course? In November 2016, though, Dries Bostyn, a graduate student in social psychology at the University of Ghent, ran what may have been the first-ever real-life version of a trolley-problem study in the lab. In place of railroad tracks and human victims, he used an electroschock machine and a colony of mice—and the question was no longer hypothetical: Would students press a button to zap a living, breathing mouse, so as to spare five other living, breathing mice from feeling pain?\n“I think almost everyone within this field has considered running this experiment in real life, but for some reason no one ever got around to it,” Bostyn says. He published his own results last month: People’s thoughts about imaginary trolleys and other sacrificial hypotheticals did not predict their actions with the mice, he found.\n\nOn what this finding means for the trolley problem:\n\nIf people’s answers to a trolley-type dilemma don’t match up exactly with their behaviors in a real-life (or realistic) version of the same, does that mean trolleyology itself has been derailed? The answer to that question depends on how you understood the purpose of those hypotheticals to begin with. Sure, they might not predict real-world actions. But perhaps they’re still useful for understanding real-world reactions. After all, the laboratory game mirrors a common experience: one in which we hear or read about a thing that someone did—a policy that she enacted, perhaps, or a crime that she committed—and then decide whether her behavior was ethical. If trolley problems can illuminate the mental process behind reading a narrative and then making a moral judgment then perhaps we shouldn’t care so much about what happened when this guy in Belgium pretended to be electrocuting mice.\n…\n[Joshua Greene] says, Bostyn’s data aren’t grounds for saying that responses to trolley hypotheticals are useless or inane. After all, the mouse study did find that people’s answers to the hypotheticals predicted their actual levels of discomfort. Even if someone’s feeling of discomfort may not always translate to real-world behavior, that doesn’t mean that it’s irrelevant to moral judgment. “The more sensible conclusion,” Greene added over email, “is that we are looking at several weakly connected dots in a complex chain with multiple factors at work.”\n…\nBostyn’s mice aside, there are other reasons to wary of the trolley hypotheticals. For one thing, a recent international project to reproduce 40 major studies in the field of experimental philosophy included stabs at two of Greene’s highly cited trolley-problem studies. Both failed to replicate.\n\nI recommend reading the whole article."
  },
  {
    "objectID": "posts/hunting-gathering-and-comparative-advantage.html",
    "href": "posts/hunting-gathering-and-comparative-advantage.html",
    "title": "Hunting, gathering and comparative advantage",
    "section": "",
    "text": "From an article by Gijsbert Stoet in the latest issue of Evolution and Human Behaviour:\n\nThe hunter-gatherer theory of sex differences states that female cognition has evolutionarily adapted to gathering and male cognition to hunting. Existing studies corroborate that men excel in hunting-related skills, but there is only indirect support for women excelling in gathering tasks. This study tested if women would outperform men in laboratory-based computer tests of search and gathering skills. In Experiment 1, men found target objects faster and made fewer mistakes than women in a classic visual search study. In Experiment 2, participants gathered items (fruits or letters presented on screen), and again, men performed significantly better. In Experiment 3, participants’ incidental learning of object locations in a search experiment was studied, but no statistically significant sex differences were observed. These findings found the opposite of what was expected based on the hypothesis that female cognition has adapted to gathering.\n\nThe expectation that women would be superior to men in gathering tasks is misplaced. The observed division of labour is indicative that women have a comparative advantage, not an absolute advantage, in gathering. We should expect that the relative efficiency of production by men and women differs. If relative efficiency does vary, women and men both benefit from specialisation, even if one or the other is more productive in both activities. Without hitting the nail on the head, Stoet hints at this:\n\n\nThere can be different reasons for a division of labor in a society, and it is not necessarily the case that both genders need to be optimized for the tasks they are doing. It could simply have been the case that a division of labor was driven solely by the fact that men were good at hunting. Women might have chosen to do the gathering, not because they were adapted to it, but because it was the task that remained to be doing. Given that there is no apparent evidence for women being excellent gatherers, this must be considered a plausible scenario. Indeed, empirical research supports the idea that women doing the gathering might often be the best arrangement for a group of hunter–gatherers as a whole, who need to reckon with multiple constraints (Gurven & Hill, 2009; Wood & Eagly).\n\n\nWhile I normally lament the lack of evolutionary biology in economics, this is one example where economics can lend a genuine (200-year-old) insight in the other direction."
  },
  {
    "objectID": "posts/hungry-judges.html",
    "href": "posts/hungry-judges.html",
    "title": "Hungry judges",
    "section": "",
    "text": "The media and blogosphere has dedicated plenty of column and blog inches to a recently published study by Danziger and colleagues on how parole rates by Israeli judges vary through the day. From the abstract:\n\nWe record the judges’ two daily food breaks, which result in segmenting the deliberations of the day into three distinct “decision sessions.” We find that the percentage of favorable rulings drops gradually from ≈65% to nearly zero within each decision session and returns abruptly to ≈65% after a break. Our findings suggest that judicial rulings can be swayed by extraneous variables that should have no bearing on legal decisions.\n\nThe following chart provides a good illustration.\n\nOf other factors to influence the judges’ decisions, only history of re-offending and the presence of a rehabilitation programme were found to have a statistically significant effect. The crime committed, time served, ethnicity or sex did not affect parole probability.\nThe cases were presented to the judges in effectively random order and they did not know which case was coming next. The time of the hearing was generally dependent on the time of arrival of the prisoner’s lawyer. As a result, the pattern of declining parole rates during a session was not due to easier cases being heard first.\nThe response in the media and blogosphere is that this is another example of people being irrational and that judges are subject to the same biases as everyone else. The authors suggest even wider implications:\n\n[W]e suspect the presence of other forms of decision simplification strategies for experts in other important sequential decisions or judgments, such as legislative decisions, medical decisions, financial decisions, and university admissions decisions.\n\nHaving now identified the effect of this bias, what do we do? Jonah Lehrer suggested that “it’s imperative that we make judges aware of these tendencies, so that they can take steps to reduce their effects.” Ed Yong quotes Nita Farahany of Vanderbilt University as saying that “improvements in the justice system may likewise require that society acknowledge the effects of biological contributions to legal decision-making.”\nHaving stewed on it for a month, I am wondering whether we should simply remove the judges altogether - and replace them with a set of decision rules. This would have two effects - the first of which is that it would end the problem created by the meal breaks and hungry or tired judges.\nMore importantly, it could result in the right decision being made more often. In discipline after discipline, evidence is emerging that simple decision rules or algorithms can outperform expert decisions.  Take the examples in Ian Ayres’s Supercrunchers, where he talks of how decision rules have outperformed wine experts in predicting wine quality, legal experts in predicting Supreme Court decisions and doctors in predicting heart attacks. Most relevantly, Ayres discusses the increasing use of sentencing guidelines in parole decisions. As people can’t seem to let completely go of judicial discretion, there is always some space left for it. But as Ayres notes:\n\nParole and Civil Commitment boards that make exceptions to the statistical algorithm and release inmates who are predicted to have a high probability of violence tend time and again to find that the high probability parolees have higher recidivism rates than those predicted to have a low probability.\n\nAs an extra thought, the next time a politician seeks to legislate specific sentences instead of relying on judicial discretion, they should pull this study out and suggest that judges are not the rational people we hope they are."
  },
  {
    "objectID": "posts/humans-vs-algorithms.html",
    "href": "posts/humans-vs-algorithms.html",
    "title": "Humans vs algorithms",
    "section": "",
    "text": "My first column over at the Behavioral Scientist is live.\nThe column is an attempt to bring together two potentially conflicting stories.\nThe first is that the best decisions result from humans and machines working together. This is encapsulated in the story of freestyle chess, whereby the best software is trumped by a human-computer team.\nThe other is the deep literature on whether humans or algorithms make better decisions, starting with Paul Meehl’s classic Clinical Versus Statistical Prediction. The common story in this literature is that there are few domains where humans outperform statistical or algorithmic approaches (even relatively simple ones). There is also an admittedly thinner literature on what happens when humans can have the result of the algorithm and decide whether to use or overrule it, and the story there is that people should generally leave the algorithm alone.\nIf you take the latter to be the usual case, the world will not be so much like freestyle chess, but more a case of steady replacement of humans decision by decision. The humans will remain relevant not because they can improve the algorithm’s decisions, but because there are inputs we need to provide, there are domains the algorithms cannot go yet, or we just don’t want to hand over control.\nYou can read the column here."
  },
  {
    "objectID": "posts/human-nature-and-libertarianism.html",
    "href": "posts/human-nature-and-libertarianism.html",
    "title": "Human nature and libertarianism",
    "section": "",
    "text": "There is another interesting topic in this month’s Cato Unbound, with Michael Shermer arguing in the lead essay that human nature is best represented by the libertarian political philosophy.\nShermer (rightly) spends most of the essay shooting down the blank slate vision of humans that underpins many policies on the left, and suggests that moderates on both the left and right should accept a “Realistic vision” of human nature. He then simply states that the libertarian philosophy best represents this vision. Unfortunately, Shermer provides no explanation about why that might be the case, and in particular, does not detail why libertarianism might better reflect human nature than conservatism.\nIn the first response to Shermer’s essay, Eliezer Yudkowsky puts Shermer’s argument as such:\n\n[B]ecause variance in IQ seems to be around 50% genetic and 50% environmental, the Soviets were half right. And that this, in turn, makes libertarianism the wise, mature compromise path between liberalism and conservatism.\n\nYudkowsky’s response to this argument is spot on:\n\nIn every known culture, humans experience joy, sadness, disgust, anger, fear, and surprise. In every known culture, these emotions are indicated by the same facial expressions. …\nComplex adaptations like “being a little selfish” and “not being willing to work without reward” are human universals. The strength might vary a bit from person to person, but everyone’s got the same machinery under the hood, we’re just painted different colors.\nWhich means that trying to raise perfect unselfish communists isn’t like reading Childcraft books to your kid, it’s like trying to read Childcraft books to your puppy.\nThe Soviets were not 50% right, they were entirely wrong. They weren’t quantitatively wrong about the amount of variance due to the environment, they were qualitatively wrong about what environmental manipulations could do in the face of built-in universal human machinery.\n\nShermer’s argument was a change from the line of reasoning that I have heard from him before, which is that if the left understood that capitalism is an emergent system like evolution, they would be more accepting of it. I find that argument even less convincing. My understanding of evolution provides one of the strongest challenges to my libertarian leanings - evolution is full of wasteful competition for relative status and what is good for the individual is often not good for the group.\nThe weakness of these arguments is probably reflected in the deeper rationale for Shermer’s libertarianism. As Yudkowsky questions, is human nature the real reason for Shermer’s libertarianism?\n\nWould Michael Shermer change his mind and become a liberal, if these traits were shown to be 10% hereditary?\n… Before you stake your argument on a point, ask yourself in advance what you would say if that point were decisively refuted. Would you relinquish your previous conclusion? Would you actually change your mind? If not, maybe that point isn’t really the key issue.\n\nYudkowsky’s answer to the question of why he is a libertarian is similar to mine:\n\nWhen I ask myself this question, I think my actual political views would change primarily with my beliefs about how likely government interventions are in practice to do more harm than good. I think my libertarianism rests chiefly on the empirical proposition—a factual belief which is either false or true, depending on how the universe actually works—that 90% of the time you have a bright idea like “offer government mortgage guarantees so that more people can own houses,”someone will somehow manage to screw it up, or there’ll be side effects you didn’t think about, and most of the time you’ll end up doing more harm than good, and the next time won’t be much different from the last time.\n\nA human nature thread could underlie some of this explanation, with the nature of individuals in government and bureaucracy shaping the outcomes from government intervention. However, an understanding of human nature, in itself, does not settle the case for libertarianism. It may provide some support, but it provides just as many challenges."
  },
  {
    "objectID": "posts/how-likely-is-likely.html",
    "href": "posts/how-likely-is-likely.html",
    "title": "How likely is “likely”?",
    "section": "",
    "text": "From Andrew Mauboussin and Michael Mauboussin:\n\nIn a famous example (at least, it’s famous if you’re into this kind of thing), in March 1951, the CIA’s Office of National Estimates published a document suggesting that a Soviet attack on Yugoslavia within the year was a “serious possibility.” Sherman Kent, a professor of history at Yale who was called to Washington, D.C. to co-run the Office of National Estimates, was puzzled about what, exactly, “serious possibility” meant. He interpreted it as meaning that the chance of attack was around 65%. But when he asked members of the Board of National Estimates what they thought, he heard figures from 20% to 80%. Such a wide range was clearly a problem, as the policy implications of those extremes were markedly different. Kent recognized that the solution was to use numbers, noting ruefully, “We did not use numbers…and it appeared that we were misusing the words.”\nNot much has changed since then. Today people in the worlds of business, investing, and politics continue to use vague words to describe possible outcomes.\n\nTo examine this problem in more depth, team Mauboussin asked 1700 people to attach probabilities to a range of words or phrases. For instance, if a future event is likely to happen, what percentage of the time would you estimate it ends up happening? Or what if the future event has a real possibility of happening?\nUnsurprisingly, the answers are all over the place. The HBR article has a nice chart of the distribution of responses, and you see more detailed results here. (You can also take the survey there too).\nWhat is the range of answers for an event that is “likely”? The 90% probability range for “likely” - that is the range that 90% of the answers fell within (and 5% of the answers were above, and 5% below) was 55% to 90%. “Real possibility” had a probability range of between 20% and 80% - the phrase in near meaningless. Even “always” is ambiguous, with a probability range of 90% to 100%.\nAn interesting finding of the survey was that men and women differ in their interpretations. Women are more likely to take a phrase as indicating a higher probability.\nSo what does team Mauboussin suggest we should do? Use numbers. Pin down those subjective probabilities using objective benchmarks. Practice.\nAnd to close with another piece of Sherman Kent wisdom:\n\nSaid R. Jack Smith:  Sherm, I don’t like what I see in our recent papers. A 2-to-1 chance of this; 50-50 odds on that. You are turning us into the biggest bookie shop in town.\nReplied Kent:  R.J., I’d rather be a bookie than a [blank-blank] poet."
  },
  {
    "objectID": "posts/how-big-is-the-effect-of-a-nudge.html",
    "href": "posts/how-big-is-the-effect-of-a-nudge.html",
    "title": "How big is the effect of a nudge?",
    "section": "",
    "text": "Last month a new meta-analysis of ‘nudges’ by Stephanie Mertens and friends was published, with a headline finding that:\n\nchoice architecture interventions overall promote behavior change with a small to medium effect size of Cohen’s d = 0.45 (95% CI [0.39, 0.52]).\n\nThe criticism came fast. Andrew Gelman jumped in to “broadcast the problems with this article right away”. He wrote:\n\nWha . . .? An effect size of 0.45 is not “small to medium”; it’s huge. Huge as in implausible that these little interventions would shift people, on average, by half a standard deviation. I mean, sure, if the data really show this, then it would be notable — it would be big news—because it’s a huge effect.\n\nI agree with the tenor of this statement, but want to note that some of these interventions are not “little”. I believe some do have “huge” effect sizes, but they’re typically results that are so unsurprising that I’m surprised they made the benchmark for publication.\nSome of the food choice studies in the meta-analysis are like this. The field is tarred by a bunch of Brian Wansink studies that should have been excluded from the meta-analysis, but let me briefly come to the defence of some of the others.\nOne of the meta-analysis studies with the largest effect size is a study by Diliberti and friends. They served restaurant customers two sizes of entrée. One entrée was 50% larger than the other. Those served the bigger entrée ate 43% more entrée than those served the smaller one. Most people just ate whatever size entrée they were served. The larger entrée also translated into an increase of calories across the whole meal by 25%. I find this unsurprising (this is no Brian Wansink magic bowl of soup) and a reasonable effect size (although it also wouldn’t surprise me if it were smaller in a larger replication). There are several other studies in the meta-analysis of this nature.\nDiliberti and friends, of course, don’t just stop with the meal. They go on to claim that “These results support the suggestion that large restaurant portions may be contributing to the obesity epidemic.” That’s a stretch - the effect size on obesity may well be zero - but this claim doesn’t undermine the reported effect sizes for the far less ambitious measure of calorie consumption during a meal. This is one of the common features of interventions with large and realistic effect sizes: the measure is close to the intervention. Unfortunately, these are often not the outcomes that someone interested in, say, the policy implications would care about.\nDespite my brief defence of some of the reported effect sizes, I agree with Andrew Gelman that, on net, there are too many studies in the meta-analysis that likely have overestimated effect sizes. As he writes:\n\nI’m concerned about selection bias within each of the other 200 or so papers cited in that meta-analysis. This is a literature with selection bias to publish “statistically significant” results, and it’s a literature full of noisy studies. If you have a big standard error and you’re publishing comparisons you find that are statistically significant, then by necessity you’ll be estimating large effects. This point is well known in the science reform literature (for example see the example on pages 17-18 here).\nDo a meta-analysis of 200 studies, many of which are subject to this sort of selection bias, and you’ll end up with a wildly biased and overconfident effect size estimate. It’s just what happens! Garbage in, garbage out.\n\nThe headline finding of the meta-analysis also raises an interesting question: is it sensible to report an aggregate effect size involving any study that sits under the banner of “choice architecture” or a “nudge”?\nTake the following two examples as points on a spectrum.\nFirst, consider Eric Johnson and Dan Goldstein’s famous (and typically misinterpreted) study into organ donation defaults (pdf). The lab experiment in that paper is used in the meta-analysis (I think an over-estimated effect size), but let’s consider the more famous intervention discussed in that paper.\nIn many European countries, people are presumed to consent to organ donation. They are never asked. Depending on the country, they typically need to do something like find and fill out a form to change that presumption. This gives us 99.98% of Austrians who are presumed to consent, versus the 12% who register as organ donors in Germany. Huge effect size via a very rigid mechanism. (Note also here that the outcome measure is registration for organ donation, not organ donation itself. Go downstream and the effect size rapidly dwindles.)\nNow, let’s pick another study in the meta-analysis relating to organ donation, one by Sallis and friends on the use of persuasive messages on organ donation registrations (the link points to the published study, but the numbers in the meta-analysis were pulled from an earlier preliminary report). When registering for their driver’s licence, applicants were shown either a control message or one of a series of “theoretically informed persuasive messages”. The best message substantially outperformed the control, getting about 30% more people to sign up. But that difference was less than 0.1% of people who saw the message. The Cohen’s d: 0.05.\nWhat is the unifying element between a default “presumed consent” and a web pop-up asking someone to register? It’s only this label of “nudge”. I’m not convinced that averaging the two tells us anything.\nBut this brings us to another ambition of the authors:\n\nPrevious studies have mostly been restricted to the analysis of a single choice architecture technique or a specific behavioral domain, leaving important questions unanswered, including how effective choice architecture interventions overall are in changing behavior and whether there are systematic differences across choice architecture techniques and behavioral domains that so far may have remained undetected and that may offer new insights into the psychological mechanisms that drive choice architecture interventions.\n\nI like this ambition to compare across domains and techniques, but am unconvinced the field is ready for a quantitative answer of the fashion proposed. There is too much garbage. Some fields are a mess. Comparing the studies is interesting - I’ve already learnt a lot from the paper and associated data - but to declare certain fields having larger effect sizes is premature.\nAlso, relating to one of my points above, the choice of outcome also makes a substantial difference. If you’re aiming to change calorie consumption in a meal, you could obtain a big effect. If you’re aiming to reduce obesity, not so much. If you’re looking to increase numbers “consenting” to organ donation, potentially big effect. Trying to change organ donation itself, much harder. Are defaults highly effective in organ donation if they increase the number registered? Or are they weak if they hardly change the number of actual donations?\nWhat would I have done myself? No idea. But on reflection, I wonder whether a less formal qualitative approach would have been more enlightening. Sure, calculate some of the numbers, but don’t take them too seriously. Pull apart why some are larger than others. Try to understand how study quality and choice of outcome variable might affect reported effects. This meta-analysis provides a useful starting point for that analysis.\n\nOutside of this question of what the meta-analysis covers, there are a couple of other points worth noting.\nAs hinted above, the meta-analysis included some studies by Brian Wansink. It also included the the “sign at the top” honesty experiments that had previously been subject to failed replications, with the paper later retracted for fraud. (Why didn’t the replications make the meta-analysis?) There is a discussion about the inclusion of the retracted paper in the meta-analysis in Retraction Watch. (And note the positive, constructive engagement by one of the meta-analysis authors.)\nI feel for the authors here. How do you set clear guidelines in your pre-analysis plan that enable you to say: “we’re not trusting anything by this guy, whether the particular study is retracted or not”. I think this comes back to my question above about whether you have to be a bit more flexible or qualitative at this point in time.\nThe authors are also far from Robinson Crusoe in having a meta-analysis contaminated by Brian Wansink studies. One meta-analysis on choice overload that I have referred to many times found an average effect of zero. But look at the plot of the effect sizes and the huge effects that increasing range of choice had in some some studies. And then look at the authors of the three outliers at the bottom….This choice overload meta-analysis suffers the same problems as the paper the subject of this post.\n\nAnother point worth noting is that the meta-analysis authors considered the possibility of publication bias.\n\nAssuming a moderate one-tailed publication bias in the literature attenuated the overall effect size of choice architecture interventions by 26.79% from Cohen’s d = 0.42, 95% CI [0.37, 0.46], and τ2=0.20 (SE=0.02) to d=0.31 and τ2=0.23. Assuming a severe one-tailed publication bias attenuated the overall effect size even further to d=0.03 and τ2=0.34; however, this assumption was only partially supported by the funnel plot.\n\nAssuming the worst, the aggregate effect is pretty small.\n\nTo close with a random story, one of my previous employers announced that they were giving all employees a $300 bonus. But, they weren’t going to just pay it to us. We had to claim it against an expense we had incurred. It could be any expense - our grocery shopping, a celebratory dinner, new shoes, whatever - but you had to go through the painful expense system to claim it. The tight-arse partners were hoping they’d get the goodwill of giving the bonus but save save money if not everyone claims (although given who they were, they might also have been thinking about tax advantages…). A lot of people didn’t claim - from memory around 30% missed out.\nAnyhow, we ran an internal campaign of “behaviourally informed” emails to get more people to claim the bonus. The reminders worked (about a 5 percentage point bump) but none of the different messages distinguished from each other. That’s pretty typical of this type of study - the obvious part works (people tend to benefit from reminders), the more sexy part doesn’t (yet another failure for loss framing). But just think if instead of text messages we had simply changed the default to automatic payment of $300. Simple “nudge”. Huge effect."
  },
  {
    "objectID": "posts/heritability-of-religion-and-fertility.html",
    "href": "posts/heritability-of-religion-and-fertility.html",
    "title": "Heritability of religion and fertility",
    "section": "",
    "text": "The United States is one of the few developed countries in the world with a fertility rate close to the replacement rate - that is, the rate of fertility required to maintain existing population levels. The two reasons most often cited for this is are high levels of fertility in the Hispanic immigrant population and the high level of fertility of religious people. Even when you control for income and education, religious people have more children than non-religious people (on average).\nThe higher fertility among religious people raises a couple of questions. As religion is heritable (that is, the predisposition to be religious and not the specific religion itself), will religion spread through society and what will the consequences of this be? What will happen to the underlying alleles (alleles are the different variants of a particular gene)?\nIf we had a situation where the religious have higher fertility and a religious genotype is determinative, it is clear that they will eventually form the largest group in the population and the religiosity allele will dominate the gene pool. However, what if there are more complicated dynamics such as defections between groups, with the religiosity allele(s) giving a predisposition as opposed to being determinative?\nRobert Rowthorn addressed this question in a paper published earlier this year in which he explored the dynamic consequences of heritable religiosity and the higher fertility of religious people. Razib at Gene Expression wrote a great post on this paper when it was first released, but as is often the case, writing a blog post myself is the best way to get my head around it.\nRowthorn noted that a natural result of defections from a high-fertility religious group is that the religious group would be smaller than it would otherwise be. However, he also points out that defection from the religious group allows the religious allele(s) to spread to and within the non-religious group. If this religiosity allele affects the number of people from the non-religious group who become religious and as a result, boosts their fertility, the religious allele may come to dominate the population regardless.\nTo explore this issue, Rowthorn constructed two models (one haploid, in which predisposition is determined by a single gene, one diploid, in which predisposition is determined by two genes, one from each parent). In these models, culture (religion) determined fertility, while religious predisposition has a genetic component. On becoming an adult, one can defect from their religious or non-religious group, with their underlying genotype giving the probability of switching.\nSome of the outcomes of the model are unsurprising. Firstly, the pace of evolution is heavily dependent on the fertility differential between religious and non-religious types. With a differential of two or three to one, large changes in population structure occur within five to 10 generations. Where the fertility differential is 1.3 to one, it might take several hundred generations for the share of the religious gene in the population to dominate from a low base.\nWith defection added, there are two main results. First, the speed of the spread of the religiosity allele is reduced, although it still spreads through the population. Second, the total number of religious people in the population is reduced, possibly significantly. Defection changes the expression of religiosity, but not the eventual spread of the gene.\nFor high fertility sects such as the Amish, with fertility rates two to three times above the national average, this would imply that they would come to dominate the population in five to ten generations, unless there is a particularly high level of defection. If they do have a high defection rate, while reducing the size of the Amish population, the religious gene will still spread. Even with a 50 per cent defection rate, the religious gene still spread to most of the population within 20 generations in Rowthorn’s model. From this, Rowthorn notes that while secularisation might reduce the growth of high-fertility sects, the importing of the religious gene into the non-religious population results in the religious allele ultimately spreading through the entire population.\nRowthorn does not venture far into the implications of his findings beyond the spread of religion. In the last paragraph, he makes the following observation:\n\nIt is interesting to speculate how such a predisposition might manifest itself in a secular context. The findings of Koenig & Bouchard suggest that a genetic predisposition towards religion is associated with obedience to authority and conservatism. If this is correct, then the diffusion of religiosity genes into the rest of society should see an increase in the number of secular people who are genetically inclined towards such values. The implications of such a development are beyond the scope of this paper to consider.\n\nIs the growing conservatism among youth, which seems to be a common media topic in recent years, an early reflection this effect?\nRowthorn does not specifically explore the question of overall population size. If the religious allele(s) spread through society, fertility would be expected to be higher and overall population higher. This could have significant implications for the debate on population size I have posted about in the last week (such as here, here and here).\nFurther, Rowthorn’s findings show the general result that any genetically based predisposition that increases fertility can be expected to spread through the population. Whether that predisposition is religion, dislike of contraception, urge to have a larger family or some other trait, the effect is the same. The alleles responsible for higher fertility spread and, barring further environmental shocks, population growth increases. The only question is how long this will take.\n*As an extension to this idea, I have a working paper that examines the consequences of heritability of fertility more generally."
  },
  {
    "objectID": "posts/henrich-on-markets-trust-and-monogamy.html",
    "href": "posts/henrich-on-markets-trust-and-monogamy.html",
    "title": "Henrich on markets, trust and monogamy",
    "section": "",
    "text": "The Edge has put up video and transcript of a great interview with Joe Henrich (the Canada Research Chair in Culture, Cognition and Evolution at UBC). The whole interview is worth watching or reading.\nA couple of the more interesting snippets are below. First, on the division of labour:\n\nOne of the interesting things about the division of labor is that you’re not going to specialize in a particular trade—maybe you make steel plows—unless you know that there are other people who are specializing in other kinds of trades which you need—say food or say materials for making housing, and you have to be confident that you can trade with them or exchange with them and get the other things you need. There’s a lot of risk in developing specialization because you have to be confident that there’s a market there that you can engage with. Whereas if you’re a generalist and you do a little bit of farming, a little bit of manufacturing, then you’re much less reliant on the market. Markets require a great deal of trust and a great deal of cooperation to work. Sometimes you get the impression from economics that markets are for self-interested individuals. They’re actually the opposite. Self-interested individuals don’t specialize, and they don’t take it [to market], because there’s all this trust and fairness that are required to make markets run with impersonal others.\n\nI don’t agree with Heinrich’s use of the word self-interested in the last sentence, as being trusting, specialising and trading has large individual benefits. However, the importance of trust is rarely emphasised enough.\nSecond, on monogamy:\n\nSocieties that have this are better able to maintain a harmonious population, increase trade and exchange, and have economic growth more than societies that allow polygamy, especially if you have a society with widely varying amounts of wealth, especially among males. Then you’re going to have a situation that would normally promote high levels of polygyny. The absolute levels of wealth difference of, say, between Bill Gates and Donald Trump and the billionaires of the world, and the men at the bottom end of the spectrum is much larger than it’s ever been in human history, and that includes kings and emperors and things like that in terms of total control of absolute wealth. Males will be males in the sense that they’ll try to obtain extra matings, but the billionaires are completely curbed in terms of what they would do if they could do what emperors have done throughout the ages. They have harems and stuff like that. Norms of modern society prevent that.\nOtherwise, there would be massive male-male competition, and even to get into the mating and marriage market you would have to have a high level of wealth if we were to let nature take it’s course as it did in the earliest empires. It depends on what your views are about freedom versus societal level benefits.\n\nThe nature of the causative link between monogamy and economic growth is an interesting question. Monogamy promotes stability, but I suspect that populations that implement monogamy are the same populations likely to implement a range of other growth promoting institutions.\nI also tend to see the tradeoff between the freedom of polygamy and the “societal level benefits” of monogamy as being an indirect tradeoff. If a few men monopolised all the women, they would quickly find their freedom curtailed by the other men.\nThe interview has plenty of other interesting food for thought."
  },
  {
    "objectID": "posts/health-trade-offs.html",
    "href": "posts/health-trade-offs.html",
    "title": "Health trade-offs",
    "section": "",
    "text": "There are always trade-offs. From the British Medical Journal in June:\n\nWe think these results have important implications. They show that overweight and obesity were already common among women who had never smoked in this population more than 35 years ago, its true extent concealed by the high smoking rates in the population as a whole. They suggest the decline in smoking rates in recent decades may have contributed to the increase in overweight and obesity. Although lifelong smoking is clearly responsible for much higher mortality rates, obesity, and especially severe obesity, is an important contributor to premature mortality.\n\nTo what extent is smoking used by low-status women to stay thin? Beyond the health question, it would be interesting to explore some other trade-offs. Does smoking or obesity have the greater effect on fertility - both in terms of likelihood of attracting a partner and the physiological effects? And what of satisfaction with or quality of life?\nWhile the authors are clear that smoking is a larger contributor to mortality than obesity, studies like this are a sound reminder that policy decisions are full of trade-offs and unintended consequences."
  },
  {
    "objectID": "posts/has-the-behavioural-economics-pendulum-swung-too-far.html",
    "href": "posts/has-the-behavioural-economics-pendulum-swung-too-far.html",
    "title": "Has the behavioural economics pendulum swung too far?",
    "section": "",
    "text": "Over at Behavioral Scientist, as part of their “Nudge Turns 10” special issue, is my latest article When Everything Looks Like a Nail: Building Better “Behavioral Economics” Teams. Here’s the opening:\n\nAs someone who became an economist via a brief career as a lawyer, I did notice that my kind had privileged access to the halls of government and business. Whether this was because economics can speak the language of dollars, or that we simply claimed that we had all the answers, the economists were often the first consulted (though not necessarily listened to) on how we priced, regulated, and designed our policies, services, and products.\nWhat I lacked, however, was a privileged understanding of behavior. So about a decade ago, with the shortcomings of economics as an academic discipline top of mind, I commenced a Ph.D. to develop that understanding. It was fortuitous timing. Decades of research by psychologists and “misbehaving” economists was creating a new wave of ideas that would wash out of academia and into the public and corporate spheres. Famously encapsulated in Richard Thaler and Cass Sunstein’s Nudge, there was now a recognized need to design our world for humans, not “econs.”\nFollowing Nudge, a second wave found many organizations creating their own “nudge units.” The Behavioural Insights Team (BIT) within 10 Downing Street was the precursor to government behavioral teams around the world. Although the first dedicated corporate behavioral units predated the BIT, a similar, albeit less visible, pattern of growth can be seen in the private sphere. These teams are now tackling problems in areas as broad as tax policy, retail sales, app design, and social and environmental policy.\nOn net, these teams have been a positive and resulted in some excellent outcomes. But my experience working in and alongside nudge units has me asking: Has the pendulum swung too far? My education and experience have proven to me that economics and the study of human behavior are complements rather than substitutes. But I worry that in many government departments and businesses, behavioral teams have replaced rather than complemented economics teams. Policymakers and corporate executives, their minds rushing to highly available examples of “nudge team” successes, often turn first to behavioral units when they have a problem.\nA world in which we take advice only from economists risks missing the richness of human behavior, designing for people who don’t exist. But a world in which policymakers and corporate executives turn first to behavioral units has not been without costs. A major source of these costs comes from how we have been building behavioral teams.\nWe have been building narrow teams. We have been building teams with only a subset of the skills required to solve the problems at hand. When you form a team with a single skillset, there is the risk that everything will start to look like a nail.\nIt’s now time for a third wave. We need to build multidisciplinary behavioral units. Otherwise we may have results such as those reflected in the observations below. Some of the observations relate to my own experiences and errors, some are observations by others. To protect identities, confidential projects, and egos (including my own), I have tweaked the stories. However, the lessons remain the same.\n\nYou can read the rest here.\nI considered a few alternative angles for the special issue article. One was around the question of whether behavioural interventions that look impressive in isolation are less so if we consider the system-wide effects. Another angle I considered, hinted at in the published piece, is around replicability and publication bias in the public sphere. Maybe they can be topics for future articles.\nI also considered an alternative introduction, but changed my approach on feedback from a friend who reviewed the first draft. Here’s the old introduction, which takes too long to get to the point and is too narrow for the ultimate thread of the article, but which makes the point about narrow approaches in a stronger way:\n\nEconomists have never been shy about applying the economic toolkit to what are normally considered the non-economic aspects of life. They have tackled discrimination, the family, crime, culture, religion, altruism, sports and war, to name a few.\nThis “economics imperialism” has often been controversial, but (at least in this author’s opinion) left many subjects better off. Some of the subjects benefited from a different approach, with the effort to repel the imperialists creating more robust disciplines.\nBut at times the economics imperialists simply missed the mark. Often this was because they lacked domain knowledge. Complexities invalidated their underlying assumptions or created a dynamic that they simply didn’t foresee.\nSome economists also have a habit of leaving the complexities of their own body of work behind when they wander into new domains. A rich understanding of moral hazard, adverse selection, information asymmetries and principle-agent problems often becomes a simple declaration to let the price mechanism do its job.\nOne (almost caricatured) illustration of this occurred when Freakonomics authors Steven Levitt and Stephen Dubner met with Prime Minister David Cameron to discuss increasing health expenditure in the United Kingdom’s free healthcare system. As described in Think Like A Freak, they posed a thought experiment:\n\nWhat if, for instance, every Briton were also entitled to a free, unlimited, lifetime supply of transportation? That is, what if everyone were allowed to go down to the car dealership whenever they wanted and pick out any new model, free of charge, and drive it home?\nWe expected him to light up and say, “Well, yes, that’d be patently absurd—there’d be no reason to maintain your old car, and everyone’s incentives would be skewed. I see your point about all this free health care we’re doling out!”\nInstead, Cameron said nothing, offered a quick handshake and disappeared to “find a less-ridiculous set of people with whom to meet.”\n\nCan Levitt and Dubner have expected a different response? Even if Levitt had a more serious proposal up his sleeve, Levitt and Dubner’s failure to engage seriously with the particular features of the healthcare market rendered the message useless. They had effectively ignored the complexities of the problem and hammered away in the hope they had found a nail.\nA few years before the visit by the Freakonomics team, David Cameron’s Conservative Government established the Behavioural Insights Team, or “Nudge unit” within 10 Downing Street. The team was tasked with realising the Government’s intention to find “intelligent ways to encourage, support and enable people to make better choices for themselves”.\nNow spun out of the Cabinet Office, the Behavioural Insights Team was the precursor to government based behavioural teams around the world. Although the first dedicated corporate behavioural units pre-dated the Behavioural Insights Team, a similar, albeit slower pattern of growth can be seen in the private sphere. These teams are now tackling issues as broad as tax evasion, customer conversion, domestic violence and climate change.\nWhile the development of these teams has been a positive and resulted in some excellent outcomes, these teams have not been without weaknesses – in fact, some of the same weaknesses suffered by the economics imperialists. The primary one is that when you form a team around a central idea, there is the risk that everything will start to look like a nail."
  },
  {
    "objectID": "posts/harfords-adapt-why-success-always-starts-with-failure.html",
    "href": "posts/harfords-adapt-why-success-always-starts-with-failure.html",
    "title": "Harford’s Adapt: Why Success Always Starts with Failure",
    "section": "",
    "text": "Natural selection operates through heritable variation in traits and differential reproductive success due to those traits. Many combinations of genes and mutations are failures, but the variation in traits creates a natural experiment in which highly evolved solutions to the environment can develop.\nIn his excellent book Adapt: Why Success Always Starts with Failure, Tim Harford applies this evolutionary concept to business, war, accidents and other human pursuits. How did on-the-ground experimentation lead to a better outcome in Iraq? How does Google or W.L. Gore & Associates develop new ideas? Harford’s argument is that by allowing low-level experimentation, solutions to highly complex problems are more likely to be found than through top-down decree.\nUnlike much of the work in areas such as evolutionary economics, which use an evolutionary analogy to describe business activities or other social phenomena, Harford moves beyond the descriptive and asks how these processes can improve policy, reduce accidents and improve business outcomes.\nFor example, Harford’s encourages more government experimentation. Politicians tend towards large, sweeping plans, which can have unintended consequences and allow little opportunity for alternative approaches to be examined. If governments were more tolerant of failure (the lack of tolerance a function of the electorate as much as politicians), they could allow many options to be tried, with the best and most successful then applied on a broader scale. Harford also questions whether government should offer prizes or alternative incentive mechanisms to encourage private sector solutions where existing incentives such as patents have credibility issues.\nOne of my favourite sections of the book was Harford’s discussion of accidents. Most of the problems Harford examines in the book are complex and “loosely coupled”, which allows experimentation with failure. But what if the system is tightly coupled, meaning that failures threaten the survival of the entire system? This concept reminded me of work by Robert May, which undermined the belief that increased network complexity led to stability.\nThe concept of “normal accidents”, taken from a book of that title by Charles Perrow, is compelling. If a system is complex, things will go wrong. Safety measures that increase complexity can increase the potential for problems. As such, the question changes from “how do we stop accidents” to how do we mitigate their damage when they inevitably occur? This takes us to the concept of decoupling. When applied to the financial system, can financial institutions be decoupled from the broader system so that we can let them fail?\nClimate change is also addressed, as Harford takes on the soft target of the well-meaning environmentalist. Decisions as to which options are most “environmentally friendly” are inevitably problematic as it is impossible for someone to understand the full network of cause and effect underlying their decision. In deciding which type of coffee is most environmentally friendly, how do you consider the inputs to the coffee, the cup, the building in which it you purchased it and the manner in which the barista got to work? As Harford points out, it is only through the decentralised price system that this information can be reliably provided to the consumer, while also providing incentives for the less well-meaning to change their behaviour.\nNormally I am indifferent to criticisms of the well-meaning environmentalist, as the people who mock Harford’s environmentalist are often those who oppose measures to introduce a carbon price. Thankfully, Harford takes the relatively rare option of pointing out the flaws of a piecemeal approach in a complex world but providing an option to address the problem.\nHarford also demonstrated his strong understanding of evolution by including the concept of survivability in his recommendations for how to implement his evolutionary approach to problems. While some people talk about evolution being for the good of the species, it is actually only good for those individuals that survive. In future generations, the survivors are the species. Drawing on experiments involving the adaptation of guppies in response to predation, Harford writes:\n\nAdapting is not necessarily something we do. It may well be something that is done to us. We may think of ourselves as Professor Endler, but we’re actually the guppies. No individual guppy adapted, but some guppies avoided being eaten and some did not. …\nAs the pike cichlid closes in for a meal, it’s little consolation to the polka-dotted guppy that its failure is helping clear space for a thriving population of pebble-coloured nieces and nephews. A struggling entrepreneur is just as unlikely to be comforted by the thought that the failure of her start-up is part of a wealth-generating process of creative destruction. …\n[U]nlike Amazon, or geniuses like Mitchell or Capecchi, or a pebble-coloured guppy, we don’t all get it right first time. Fortunately we have something that guppies do not: the ability to adapt as we go along.\n\nMost of the individuals are toast. As a result, to apply the evolutionary ideas to your own life, you should experiment more, but you want to undertake experiments that you can survive. You are only one guppy."
  },
  {
    "objectID": "posts/happiness-adjusts.html",
    "href": "posts/happiness-adjusts.html",
    "title": "Happiness adjusts",
    "section": "",
    "text": "Robert Frank has written a piece for the New York Times on why worrying is good. He writes of the well-known phenomena that after large life changes, people’s level of happiness tends to drift back to where it was before the event. Humans are also particularly bad at predicting this effect, placing far more importance on events before they occur than the later effect on happiness would warrant.\nThe point of Frank’s post is how much sense this makes from an evolutionary perspective. It is the miscalculation of how happy we will be if, say, we get the new job, that leads us to strive to achieve it. As Frank states:\n\nThe human brain was formed by relentless competition in the natural world, so it should be no surprise that we adapt quickly to changes in circumstances. Much of life, after all, is graded on the curve. Someone who remained permanently elated about her first promotion, for example, might find it hard to muster the drive to compete for her next one.\nEmotional pain is fleeting, too. Behavioral economists often note that while people who become physically paralyzed experience the expected emotional devastation immediately after their accidents, they generally bounce back surprisingly quickly. Within six months, many have a daily mix of moods similar to their pre-accident experience.\n\nFrank does temper the observation that large events do not change our long-term mix of emotions by noting that people can still feel regret for things that did not work out.\nThe reset of the level of happiness after major events presents an interesting problem for attempts to measure happiness in society. It will tend to flatten any relationship between success and happiness, with the strongest relationship being found immediately after a successful event. As humans have not evolved to be happy, and being unhappy could lead to greater success, I’ve always found this to be a gap in any studies of happiness.\nOne of the things I like about Frank’s writing is that when he writes of evolution, he recognises that these evolutionary drivers are individual drivers and not for the benefit of society. At the end of the article, Frank notes that people are in competition for jobs and resources, and as income grows, the acceptable level of income rises, leading to further competition for higher paying jobs. From a societal perspective, the competition driven by peoples’ worrying might be a significant waste of resources. As a result, if economists want to develop arguments that the market often delivers the best results for society (as I usually do), evolutionary theory provides a counterpoint that we should not ignore."
  },
  {
    "objectID": "posts/haidts-the-righteous-mind.html",
    "href": "posts/haidts-the-righteous-mind.html",
    "title": "Haidt’s The Righteous Mind",
    "section": "",
    "text": "I am going to give my thoughts on Jonathan Haidt’s The Righteous Mind: Why Good People Are Divided by Politics and Religion over two posts as I want to split the good from the bad (second post here). The first two-thirds and the conclusion of the book are excellent. However, slotted in the last third is Haidt’s take on group selection. His group selection argument deserves attention, but I don’t want to derail this post with a group selection critique, particularly when Haidt’s broader arguments do not rest on it.\nHaidt’s goal is to explain why people are divided by politics and religion. He has three major explanations for this division: we are primarily guided by our intuitions (not reason); there’s more to morality than harm or fairness; and morality binds and blinds.\nPart 1 of the book is based on the concept that intuition comes first, strategic reasoning comes second. When presented with a new situation, we tend not to reason to our moral response. Rather, our instincts offer a moral response, and we then use our power of reasoning to justify it. Haidt asks us to picture our reasoning as a rider on an intuitive elephant. The elephant leans in response to a situation, and the rider rationalises why they are going in that direction. It takes a real effort to turn the elephant.\nMuch of the material through Part 1 is the fodder of popular accounts of decision-making, ranging from material on confirmation bias to Philip Tetlock’s work on expert political judgment. Haidt’s elephant and the rider also draws comparisons with Daniel Kahneman’s System 1 and System 2. However, the application of this framework to moral psychology is interesting, particularly as the nature of the elephant changes between people - as Haidt highlights in Part 2.\nHaidt starts Part 2 with a story about an experiment in which he exposes subjects to a novel moral dilemma and makes them justify their moral judgement. One dilemma involves a man who buys a chicken from the supermarket (already dead) and has sex with it before he cooks and eats it. As no-one is harmed, someone rationalizing the story under the scrutiny of an interviewer might ultimately decide that there was no moral transgression.\nWhen Haidt moved beyond his usual WEIRD (Western, educated, industrialised, rich and democratic) experimental subjects to people entering a suburban McDonald’s, he found that there was astonishment at the interviewer’s questions as to whether the action was wrong. Why do you even need to ask? From this picture, Haidt argues that there is more to morality than fairness and harm, the staples of liberal morality (liberal in the sense the term is used in the United States - and how I will use it for the rest of this post). Instead, there are six foundations to morality - care/harm, liberty/oppression, fairness/cheating, loyalty/betrayal, authority/subversion, and sanctity/degradation.  An extra wrinkle is that fairness contains equality and proportionality elements.\nLiberal morality tends to rest on the care/harm and to a lesser extent on the fairness/cheating (equality) and liberty/oppression dimensions. Conservative morality tends to rely on all six, with an emphasis on proportionality for the fairness/cheating dimension.  The libertarian moral framework rests almost entirely on the liberty/oppression dimension (with a small dose of fairness/cheating thrown in). Haidt suggests that the broader moral foundation of conservatives gives them an edge in understanding the concerns of the full political spectrum. It is not that conservatives don’t care about harm. They simply weight it differently. When conservatives and liberals undertake an ideological Turing test, where they had to answer questions as though they were the other, conservatives and moderates do better than liberals. Haidt does not delve into the consequences of the narrow libertarian moral foundations in detail, but it raises the question of libertarian’s ability to understand and communicate with other audiences.\nThe moral framework test at YourMorals.org that informs much of Haidt’s book suggests that I have a liberal framework, although with more weighting to proportionality than equality in the fairness/cheating dimension and a stronger tendency towards liberty (my scores are the green bars). I lean libertarian, but this is more due to my beliefs about how to reduce harm than a foundation built on freedom from interference, so the assessment may make sense given my bleeding heart libertarian tendencies.\n\n\nHaidt applies little substantive judgement to the merit of these moral foundations. In the conclusion, he supports moral pluralism, not relativism - but you get little of that flavour in the rest of the book. In part, Haidt’s swing towards conservatism makes him disinclined to critique any of the conservative foundations. However, as a description of moral frameworks, the discussion is excellent.\nIn Part 3, Haidt notes the grouping instincts of humans. In times of crisis, such as after 9/11, people act less selfish and pull together as a group. This groupish behaviour can act as a barrier to understanding others and is parochial, but Haidt argues that there are ways to increase group cohesiveness in ways that are not necessarily harmful to outgroups. We should be looking for ways to trigger this cohesiveness.\nTo illustrate this, Haidt  dedicates a chapter to religion, the ultimate in groupish behaviour. He argues that religion is an evolved cultural trait, not a maladaptive meme, as religion binds people into groups, suppresses freeriding and supports cooperation (he even goes as far as putting religion into the group selection basket, but I will also save that issue for my second post).  It is not an argument that will win fans among the new atheists.\nHaidt closes the book with some suggestions to answer the opening question of the book: “Can we all get along?” Haidt is slightly naive in his hope that understanding someone else’s moral foundation will reduce conflict, but some of his other throw away ideas, such as having the families of legislators live in the same neighbourhoods to build civility, are interesting - although as Haidt suggests, we might be too far gone for that. If nothing else, his framework might help meet Haidt’s initial goal of understanding conservative morality and allow the Democrats write some better speeches with broad appeal.\nMy second post on Haidt’s book is here."
  },
  {
    "objectID": "posts/groups-kin-and-self-interest.html",
    "href": "posts/groups-kin-and-self-interest.html",
    "title": "Groups, kin and self interest",
    "section": "",
    "text": "In my last post on group selection, I described how multilevel selection differed from more traditional (and popular) concepts of group selection. One difference is that the multilevel selection framework defines groups as any subset of interacting individuals, such as a cooperating pair or family unit, rather than restricting the definition to population size groups.\nThere are few tangible examples available on how a multilevel selection framework works, so below is an attempt to offer an illustration of how the definition of group in a multilevel selection framework is used. It also serves as a test of how well I understand the concept myself. This numerical example also illustrates why it is generally the less intuitive approach, which is also the reason I consider that inclusive fitness - the sum of direct and indirect (kin) fitness - has proven to be the more fruitful approach in evolutionary biology. At the end, I place the discussion in an economic context to draw out my point.\nThis numerical example is loosely based on the approach David Sloan Wilson used in his 1975 and 1977 papers, which might be seen as the beginning of modern multilevel selection theory. The maths in the multilevel selection debate has moved on since this time, but this illustration works for the point I want to make.\nSuppose there are 200 agents in a population, of which half are cooperators and half are defectors. Cooperators always seek to cooperate and engage in a mutual trade (say, making an alert sound or entering into a transaction), while the defector will always shirk.\nAgents live for one generation during which they are randomly paired with another agent. From a multilevel selection perspective, we will describe these pairs as groups. This gives us 100 groups, each comprising two agents. From random pairing, we expect that 50 of the cooperators will be paired with other cooperators, and the other 50 will be paired with defectors. Similarly, 50 of the defectors are paired with other defectors, and 50 with cooperators.\nA cooperator will seek to cooperate with whoever they are paired, generating a benefit of one fitness unit for themselves, but donating two fitness units to whoever they cooperate with. Thus, if a cooperator meets another cooperator, they both cooperate and generate a surplus, from which they each get a pay-off of three fitness units (one from their own action and two from their partner). If a cooperator pairs with a defector, the group still generates a surplus through the efforts of the cooperator, but the cooperator only receives one fitness unit while the defector receives two. Finally, if a defector is paired with another defector, there is no cooperation or surplus generated, so both defectors receive zero.\nWithin the groups of all cooperators and all defectors, both agents get the same pay-off (three or zero), so there is no individual level selection. Within mixed groups of cooperators and defectors, the defectors get double the fitness units of the cooperator, so there is individual level selection against the cooperators. Therefore, on average, there is individual selection against cooperators within groups. Within the group, the cooperator’s action appears to be an altruistic act. David Sloan Wilson has called this situation where an agent’s absolute fitness increases but their relative fitness is decreased within a group “weak altruism”.\nNow for the competition between groups. The groups of cooperators get a total pay-off of six, mixed groups both get a total pay-off of three, while the groups of all defectors receive a pay-off of zero. There is selection for groups comprising solely of cooperators relative to the other two groups, and selection for mixed groups relative to groups of defectors. Group success increases with the proportion of cooperators.\nGroup and individual selection are operating in different directions – individual selection favours defectors while group level selection favours cooperators. Which one wins? Across all cooperators, they receive an average of two fitness units each, while defectors receive an average of one fitness unit each. Competition between groups is the dominant force and cooperators increase in prevalence despite being selected against within groups. Wilson showed in his papers that all it requires in this case of random assortment is that the cooperator have positive absolute fitness - then the group selection will overcome the relative fitness disadvantage within groups.\nNow, let’s reframe this from an inclusive fitness perspective. An cooperator’s action gives them a pay-off of one, and a pay-off of two to whomever they are paired with. If we ignore kin for a moment, that pay-off of two to their partner represents an average fitness increase of 0.01 for the rest of the population (two fitness units across a population of 199). One is more than 0.01, so the cooperator’s relative fitness in the population is increased due to the transaction (the transaction also increases cooperators fitness relative to 198 of the 199 others in the population). It is in the cooperators self-interest to conduct a transaction with their partner, no matter who the partner is.\nFactoring in kin, the random assortment means that the two donated fitness units will on average increase the fitness of receiving defectors (non-kin) or cooperators (their kin) by an equal amount, so the effect of that donation nets out to zero instead of 0.01. Thus, the mere fact that the cooperator receives a positive pay-off is sufficient for them to increase in prevalence. Further, if there is any assortment by type, the cooperators’ pay-off can even be negative as their kin are even more likely to benefit from their cooperative acts.\nThe benefit of the inclusive fitness approach is that we are not left asking why someone enters a transaction when their partner obtains a fitness advantage relative to them. The reason is that this partner is not the relevant benchmark. Rather, it is the broader population. When looked at from the population level, the situation described above involves no altruism in the ordinary sense that we define it – it is pure self-interest or benefit to kin. So what if your particular partner does well from dealing with you? The deal still makes sense. The label of weak altruism appears out of place.\nIf we frame this example in an economic context, the inclusive fitness approach appears even more intuitive. In economics, there is a concept known as consumer and producer surplus, which is the benefit one receives from a transaction. In the case of a consumer, if you value a good at $2 but only have to pay $1, then your consumer surplus is $1. Similarly, if a producer is willing to sell a good for $1 but receives $2 for it, there is $1 of producer surplus. Every economic transaction involves a distribution of surplus between the two parties.\nNow, imagine we have a population of economic agents, some of whom are cooperators and others are defectors. When two cooperators get together, a transaction occurs and each receives $3 of consumer or producer surplus. If a cooperator meets a defector, the defector rips them off, but not so much that the transaction does not occur. A defecting producer might use sub-standard materials, while a defecting consumer might try to shortchange the purchaser. The net result is that the defector walks away from the transaction with $2 of surplus, while the cooperator receives $1. If two defectors meet, their mutual attempts to get the better of the transaction results in it collapsing and no surplus is gained by either party.\nObviously, this is just a slightly different framing of my earlier example. If we treat each consumer-producer pair as a group, there is within group selection against cooperators, but group selection for cooperators. The net effect is that cooperators prosper. Similarly, if looked at from an inclusive fitness perspective, the cooperators will end up better off as their fitness gain is higher than that for the rest of the population.\nNow, an economist looking at these exchanges would say they are obviously beneficial, regardless of any group framing. This is partly a consequence of the economic focus on absolute and not relative gains, but it also reflects the general fact that the majority of transactions do not have a perfectly equal division of the surplus. If you limit your group to the two people conducting the transaction, there is almost always “weak altruism” within the group. But is the cooperator being altruistic in any ordinary sense? No. Of course the altruist would enter into the transaction, even if the relative share of the benefits is not perfectly equal. We enter into transactions of this type every day because we benefit from the exchange. Ask yourself how often you consider yourself to be altruistic when you enter into an economic exchange. The only time we would not agree to enter such an exchange is spite, which Alan Grafen noted when he said that “a self interested refusal to be spiteful” was a far better description than “weak altruism” of what is occurring when we do transact.\nThe above is a simple example, but it captures a fundamental issue with the multilevel selection approach. The groupings are often less intuitive and, in my opinion (and I suspect most biologists’ opinion), less insightful than simply looking at the issue from an inclusive fitness angle to begin with. Group selection tends to be a more intuitive concept when the groups are population size groups. But then we find ourselves back in the old group selection debate and discussing factors such as the degree of migration between groups and whether intergroup competition can override the spread of cheaters within them. But to be realistic, that is where much of the popular debate about human altruism is anyhow."
  },
  {
    "objectID": "posts/grandparents-affect-social-mobility.html",
    "href": "posts/grandparents-affect-social-mobility.html",
    "title": "Grandparents affect social mobility",
    "section": "",
    "text": "In his research on social mobility using surnames, Gregory Clark has found lower levels of social mobility than many other studies. Clark has defended this finding on the basis that analysis of social mobility across a single generation or using a single variable will overestimate it. Clark writes:\n\nConventional estimates of social mobility, which look at just single aspects of social status such as income, are contaminated by noise. If we measure mobility on one aspect of status such as income, it will seem rapid.\nBut this is because income is a very noisy measure of the underlying status of families. The status of families is a combination of their education, occupation, income, wealth, health, and residence. They will often trade off income for some other aspect of status such as occupation. A child can be as socially successful as a low paid philosophy professor as a high paid car salesman. Thus if we measure just one aspect of status such as income we are going to confuse the random fluctuations of income across generations, influenced by such things as career choices between business and philosophy, with true generalised social mobility.\n\nA new paper published in the American Sociological Review by Chan and Boliver provides some evidence in favour of Clark’s argument. Chan and Boliver examined three British birth cohort studies comprising three generations of family members. They found that even after controlling for parental characteristics, grandparents still have a significant effect on their grandchild’s social position. From the abstract:\n\nNet of parents’ social class, the odds of grandchildren entering the professional-managerial class rather than the unskilled manual class are at least two and a half times better if the grandparents were themselves in professional-managerial rather than unskilled manual-class positions. This grandparents effect in social mobility persists even when parents’ education, income, and wealth are taken into account.\n\nAlthough I am referring to this work in support of Clark’s argument, which Clark suggests likely has a genetic (or something very like genetic) transmission component, Chan and Boliver do not reference inherited characteristics. They point to social factors such as grandparental resources and involvement in childrearing, and the possibility that those who have experienced a single generation of downward mobility may be better positioned or more motivated to bounce back.\nWe could take “better positioned” to implicitly refer to Clark’s argument, but we can state it more clearly. At least part of what Chan and Boliver are observing is social mobility minus the single-generation noise. Grandchildren of lower status parents but higher status grandparents are simply moving back toward the level of status that reflects their underlying characteristics. They are “better positioned” due to these characteristics and their higher motivation may well be one of those characteristics.\nChan and Boliver’s work also received some coverage from the BBC."
  },
  {
    "objectID": "posts/gottschalls-the-storytelling-animal.html",
    "href": "posts/gottschalls-the-storytelling-animal.html",
    "title": "Gottschall’s The Storytelling Animal",
    "section": "",
    "text": "In The Storytelling Animal: How Stories Make Us Human, Jonathan Gottschall asks why we live and breathe stories. We are prolific storytellers. We consume movies, novels and plays. We even create stories in our sleep.\nGottschall’s argument is that our propensity to storytelling is an evolved trait that helps us navigate problems. He likens stories to flight simulators that prepare us for problems when they arise.\nHere are snippets from two chapters. First, the idea that the mind is a storyteller - an idea common in Nassim Taleb’s writings:\n\n[W]hile Sherlock Holmes stories are good fun, it pays to notice that Holmes’s method is ridiculous.\nTake the risk story Holmes concocts after glancing at Watson in the lab [at the beginning of A Study in Scarlet]. Watson is dressed in ordinary civilian clothes. What gives him “the air of a military man”? Watson is not carrying his medical bag or wearing a stethoscope around his neck. What identifies him as “a gentleman of a medical type”? And why is Holmes so sure that Watson had just returned from Afghanistan rather than from one of many other dangerous tropical garrison where Britain, at the height of its empire, stationed troops? (Let’s ignore the fact that Afghanistan is not actually in the tropical band.) …\nIn short, Sherlock Holmes’s usual method is to fabricate the most confident and complete explanatory stories from the most ambiguous clues. Holmes seizes on one of a hundred different interpretations of a clue and arbitrarily insists that the interpretation is correct. This then becomes the basis for a multitude of similarly improbable interpretations that all add up to a neat, ingenious, and vanishingly improbable explanatory story. …\nWe each have a little Sherlock Holmes in our brain. His job is to “reason backwards” from what we can observe in the present and show what orderly series of causes led to particular effects. Evolution has given us an “inner Holmes” because the world really is full of stories (intrigues, plots, alliances, relationships of cause and effect), and it pays to detect them. …\nBut the storytelling mind is imperfect. … The storytelling mind is allergic to uncertainty, randomness and coincidence. It is addicted to meaning. If the storytelling mind cannot find meaningful patterns in the world, it will try to impose them. In short, the storytelling mind is a factory that churns out true stories when it can, but will manufacture lies when it can’t.\n\nThe second snippet relates to the fallibility of our memories in telling stories. Memories are open to contamination, and are fictionalisations of past events rather than perfectly recollections.\n\nIn a classic experiment, Elizabeth Loftus and her colleagues gathered information from independent sources about undergraduate students’ childhoods. The psychologists then brought students into the lab and went over lists of actual events in their lives. The lists were Trojan horses that hid a single lie: When the student was five years old, the psychologists claimed, he wandered away from his parents in a mall. His parents were frightened, and so was he. Eventually an old man reunited him with his parents. At first, the students had no memory of this fictional event. But when they were later called back into the lab and asked about the mall episode, 25 percent of them said they remembered it. These students not only recalled the bare events that the researchers had supplied, but they also added many vivid details of their own.\nThe study was among the first of many to show how shockingly vulnerable the memory system is to contamination by suggestion.\n\nI have several “clear” childhood memories that I suspect did not occur. That doesn’t overly worry me, but what does is my recollection of papers and books that I regularly refer to in conversation. Each time I recall the paper or book, I affect my memory of it. More than once I have gone back to the original after several years to re-read it, and realised that, even if not wrong in fact, my recollection of the tone, nuance and strength of the argument was well off.\nHaving pulled out two snippets of storytelling gone wrong, the book is positive about the effect of storytelling on the world. Gottschall argues that storytelling is often deeply moral, normally deals with problems of great (evolutionary) relevance to us and is a major cohering force in society. And I tend to agree."
  },
  {
    "objectID": "posts/gigerenzer-versus-nudge.html",
    "href": "posts/gigerenzer-versus-nudge.html",
    "title": "Gigerenzer versus nudge",
    "section": "",
    "text": "Since I first came across it, I have been a fan of Gerd Gigerenzer’s work. But I have always been slightly perplexed by the effort he expends framing his work in opposition to behavioural science and “nudges”. Most behavioural science aficionados who are aware of Gigerenzer’s work are fans of it, and you can appreciate behavioural science and Gigerenzer without suffering from two conflicting ideas in your mind.\nIn a recent LSE lecture about his new book Risk Savvy: How to Make Good Decisions (which sits unread in my reading pile), Gigerenzer again has a few swipes at Daniel Kahneman and friends. The blurb for the podcast gives a taste. A set of coercive government interventions are listed, none of which are nudges, and it is suggested that we need risk savvy citizens who won’t be scared into surrendering their freedom. Slotted between these is the suggestion that some people see a need for “nudging”.\nGigerenzer does provide a different angle to the behavioural science agenda. His work has provided strong evidence for the accuracy of heuristics and shown that many of our so-called irrational decisions make sense from the perspective of the environment where they were designed (evolved). But his work doesn’t undermine the fact that many decisions are made outside of the environment where they originated – those fast, frugal and well-shaped heuristics have not stopped us getting fat, spending huge amounts on unused gym memberships and failing to save for retirement. Gigerenzer’s work provides depth to the behavioural analysis, rather than undermining it, and points to a richer set of potential solutions.\nWhen Gigerenzer starts throwing around solutions, the difference between his approach and nudging becomes even hazier. In the LSE lecture he suggests that doctors be trained to present risks to patients in a certain way. That doesn’t seem much different from a typical nudge, although here it is the previously statistically-illiterate doctors presenting the information that nudges the patient behaviour.\nOne other interesting point in the lecture is when Gigerenzer speaks about the failure of breast cancer screening to cut deaths, and the presentation of results in deceptive ways designed to increase screening rates. He proposes presenting information by showing natural frequencies, which would likely reduce the rate of screening. But what of screening that doesn’t have deleterious side-effects for false positives of the same scale as breast cancer? Should they presented as Gigerenzer proposes, or in alternative ways more likely to induce screening? There has been no shortage of work in behavioural science designed to increase screening rates, particularly given the other biases and barriers that need to be overcome. I prefer Gigerenzer’s approach, but can see the arguments that would be mounted for the other side.\nOtherwise, Gigerenzer’s speech channels Nassim Taleb on financial markets, before hinting at some of the very interesting work he is doing with the Bank of England. It’s generally worth a listen."
  },
  {
    "objectID": "posts/gigerenzer-on-system-one-and-system-two.html",
    "href": "posts/gigerenzer-on-system-one-and-system-two.html",
    "title": "Gigerenzer on system one and system two",
    "section": "",
    "text": "If you have read Daniel Kahneman’s Thinking, Fast and Slow, you will be familiar with the concepts of System One and System Two. Gerd Gigerenzer is not a fan of the dichotomy, with the below passage from an interesting interview by Justin Fox (the one over N heuristic Gigerenzer refers to is the heuristic to invest your money equally across your N options):\n\nWhat is system one and system two? It’s a list of dichotomies. Heuristic versus calculated rationality, unconscious versus conscious, error-prone versus always right, and so on. Usually, science starts with these vague dichotomies and works out a precise model. This is the only case I know where one progresses in the other direction. We have had, and still have, precise models of heuristics, like one over N. And at the same time, we have precise models for so-called rational decision making, which are quite different: Bayesian, Neyman-Pearson, and so on. What the system one, system two story does, it lumps all of these things into two black boxes, and it’s happy just saying it’s system one, it’s system two. It can predict nothing. It can explain after the fact almost everything. I do not consider this progress.\nThe alignment of heuristic and unconscious is not true. Every heuristic can be used consciously or unconsciously. The alignment between heuristic and error-prone is also not true. So, what we need is to go back to precise models and ask ourselves, when is one over N a good idea, and when not? System one, system two doesn’t even ask this. It assumes that heuristics are always bad, or always second best."
  },
  {
    "objectID": "posts/gerd-gigerenzers-rationality-for-mortals-how-people-cope-with-uncertainty.html",
    "href": "posts/gerd-gigerenzers-rationality-for-mortals-how-people-cope-with-uncertainty.html",
    "title": "Gerd Gigerenzer’s Rationality for Mortals: How People Cope with Uncertainty",
    "section": "",
    "text": "Gerd Gigerenzer’s collection of essays Rationality for Mortals: How People Cope with Uncertainty covers most of Gigerenzer’s typical turf: ecological rationality, heuristics that make us smart, understanding risk and so on.\nBelow are observations on three of the more interesting essays: the first on different approaches to decision making, the second on the power of simple heuristics, and the third on how biologists treat decision making.\nFour ways to analyse decision making\nIn the first essay, Gigerenzer provides four approaches to decision making – unbounded rationality, optimisation under constraints, cognitive illusions (heuristics and biases) and ecological rationality.\n1. Unbounded rationality\nUnbounded rationality is the territory of neoclassical economics. Omniscient and omnipotent people optimise. They are omniscient in that they can see the future – or at least live in a world of risk where they can assign probabilities. They are omnipotent in that they have all the calculating power they need to make perfect decisions. With that foresight and power, they make optimal decisions.\nPossibly the most important point about this model is that it is not designed to describe precisely how people make decisions, but rather to predict behaviour. And in many dimensions, it does quite well.\n2. Optimisation under constraints\nUnder this approach, people are no longer omniscient. They need to search for information. As Gigerenzer points out, however, this attempt to inject realism creates another problem. Optimisation with constraints can be even harder to solve than optimisation with unbounded rationality. As a result, the cognitive power required is even greater.\nGigerenzer is adamant that optimisation under constraints is not bounded rationality – and if we use Herbert Simon’s definition of the term, I would agree – but analysis of this type commonly attracts the “boundedly rational” label.\n3. Cognitive illusions – logical irrationality\nThe next category is the approach in much of behavioural science and behavioural economics. It is often labelled as the “heuristics and biases” program. This program looks to understand the processes under which people make judgments, and in many cases, seeks to show errors of judgment or cognitive illusions.\nGigerenzer picks two main shortcomings of this approach. First, although the program successfully shows failures of logic, it does not look at the underlying norms. Second, it tends not to produce testable theories of heuristics. As Gigerenzer states, “mere verbal labels for heuristics can be used post hoc to “explain” almost everything.”\nAn example is analysis of overconfidence bias. People are asked a question such as “Which city is farther north – New York or Rome?”, and asked to give their confidence that their answer is correct. When participants are 100 per cent certain of the answer, less than 100 per cent tend to be correct. That pattern of apparent overconfidence continues through lower probabilities.\nThere are several critiques of this analysis, but one of the common suggestions is that people are presented with questions that are unrepresentative of a typical sample. People typically use alternative cues to answer a question such as the above. In the case of latitude, temperature is a plausible cue. The overconfidence bias occurs because the selected cities are a biased sample where the cue fails more often than expected. If the cities are randomly sampled from the real world, the overconfidence disappears. The net result is that what appears to be a bias may be better explained by the nature of the environment in which the decision is made. (Kahneman and Tversky contest this point, suggesting that even when you take a representative sample, the problem remains.)\n4. Ecological rationality\nEcological rationality departs from the heuristics and biases program by examining the relationship between mind and environment, rather than the mind and logic. Human behaviour is shaped by scissors with two blades – the cognitive capabilities of the actor, and the environment. You cannot understand human behaviour without understanding both the capabilities of the decision maker and the environment in which those capabilities are exercised. Gigerenzer would apply the bounded rationality label to this work.\nThere are three goals to the ecological rationality program. The first is to understand the adaptive toolbox – the heuristics of the decision maker and their building blocks. The second is to understand the environmental structures in which different heuristics are successful. The third is to use this analysis to improve decision making through designing better heuristics or changing the environment. This can only be done once you understand the adaptive toolbox and the environments in which different tools are successful.\nGigerenzer provides a neat example of how the ecological rationality departs from the heuristics and biases program in its analysis of a problem – in this case, optimal asset allocation. Harry Markowitz, who received a Nobel Memorial Prize in Economics for his work on optimal asset allocation, did not use the results of his analysis in his own investing. Instead, he invested his money using the 1/N rule – spread your assets equally across N assets.\nThe heuristics and biases program might look at this behaviour and note Markowitz is not following the optimal behaviour determined by himself. He is making important decisions without using all the available information. Perhaps it is due to cognitive limitations?\nAs Gigerenzer notes, optimisation is not always the best solution. Where the problem is computationally intractable or the optimisation solution lacks robustness due to estimation errors, heuristics may outperform. In the case of asset allocation, Gigerenzer notes work showing that 500 years of data would have been required for Markowitz’s optimisation rule to outperform his practice of 1/N. In a world of uncertainty, it can be beneficial to leave information on the table. Markowitz was using a simple heuristic for an important decision, but rightfully so as it is superior for the environment in which he is making the decision.\nSimple heuristics make us smart\nGerd Gigerenzer is a strong advocate of the idea that simple heuristics can make us smart. We don’t need complex models of the world to make good decisions.\nThe classic example is the gaze heuristic. Rather than solving a complex equation to catch a ball, which requires us to know the ball’s speed and trajectory and the effect of the wind, a catcher can simply run to keep the ball at a constant angle in the air, leading them to the point where it will land.\nGigerenzer’s faith in heuristics is often taken to be based on the idea that people have limited processing capacity and are unable to solve the complex optimisation problems that would be needed in the absence of these rules. However, Gigerenzer points out this is perhaps the weakest argument for heuristics:\n\n[W]e will start off by mentioning the weakest reason. With simple heuristics we can be more confident that our brains are capable of performing the necessary calculations. The weakness of this argument is that it is hard to judge what complexity of calculation or memory a brain might achieve. At the lower levels of processing, some human capabilities apparently involve calculations that seem surprisingly difficult (e.g., Bayesian estimation in a sensorimotor context: Körding & Wolpert, 2004). So if we can perform these calculations at that level in the hierarchy (abilities), why should we not be able to evolve similar complex strategies to replace simple heuristics?\n\nRather, the advantage of heuristics lies in their low information requirements, their speed and, importantly, their accuracy:\n\nOne answer is that simple heuristics often need access to less information (i.e. they are frugal) and can thus make a decision faster, at least if information search is external. Another answer – and a more important argument for simple heuristics – is the high accuracy they exhibit in our simulations. This accuracy may be because of, not just in spite of, their simplicity. In particular, because they have few parameters they avoid overfitting data in a learning sample and, consequently, generalize better across other samples. The extra parameters of more complex models often fit the noise rather than the signal. Of course, we are not saying that all simple heuristics are good; only some simple heuristics will perform well in any given environment.\n\nAs the last sentence indicates, Gigerenzer is careful not to make any claims that heuristics generally outperform. A statement that a heuristic is “good” is ill-conceived without considering the environment in which it will be used. This is the major departure of Gigerenzer’s ecological rationality from the standard approach in the behavioural sciences, where the failure of a heuristic to perform in an environment is taken as evidence of bias or irrationality.\nOnce you have noted what heuristic is being used in what environment, you can have more predictive power than in a well-solved optimisation model. For example. an optimisation model to catch a ball will simply predict that the catcher will be at the place and time where the ball lands. Once you understand that they use the gaze heuristic to catch the ball, you can also predict the path that they will take to get to the ball – including that they won’t simply run in a straight line to catch it. If a baseball or cricket coach took the optimisation model too seriously, they would tell the catcher that they are running inefficiently by not going straight to where it will land. Instructions telling them to run is a straight line will likely make their performance worse.\nBiologists and decision making\nBiologists are usually among the first to tell me that economists rely on unrealistic assumptions about human decision making. They laugh at the idea that people are rational optimisers who care only about maximising consumption.\nBut the funny thing is, biologists often do the same. Biologists tend to treat their subjects as optimisers.\nGigerenzer has a great chapter considering how biologists treat decision making, and in particular, to what extent biologists consider that animals use simple decision-making tools such as heuristics. Gigerenzer provides a few examples where biologists have examined heuristics, but much of the chapter asks whether biologists are missing something with their typical approach.\nAs a start, Gigerenzer notes that biologists are seeking to make predictions rather than accurate descriptions of decision making. However, Gigerenzer questions whether this “gambit” is successful.\n\nBehavioral ecologists do believe that animals are using simple rules of thumb that achieve only an approximation of the optimal policy, but most often rules of thumb are not their interest. Nevertheless, it could be that the limitations of such rules of thumb would often constrain behavior enough to interfere with the fit with predictions. The optimality modeler’s gambit is that evolved rules of thumb can mimic optimal behavior well enough not to disrupt the fit by much, so that they can be left as a black box. It turns out that the power of natural selection is such that the gambit usually works to the level of accuracy that satisfies behavioral ecologists. Given that their models are often deliberately schematic, behavioral ecologists are usually satisfied that they understand the selective value of a behavior if they successfully predict merely the rough qualitative form of the policy or of the resultant patterns of behavior.\n\nYou could write a similar paragraph about economists. If you were to give the people in an economic model objectives shaped by evolution, it would be almost the same.\nBut Gigerenzer has another issue with the optimisation approach in biology. As for most analysis of human decision making, “missing from biology is the idea that simple heuristics may be superior to more complex methods, not just a necessary evil because of the simplicity of animal nervous systems.” Gigerenzer writes:\n\nThere are a number of situations where the optimal solution to a real-world problem cannot be determined. One problem is computational intractability, such as the notorious traveling salesman problem (Lawler et al., 1985). Another problem is if there are multiple criteria to optimize and we do not know the appropriate way to convert them into a common currency (such as fitness). Thirdly, in many real-world problems it is impossible to put probabilities on the various possible outcomes or even to recognize what all those outcomes might be. Think about optimizing the choice of a partner who will bear you many children; it is uncertain what partners are available, whether each one would be faithful, how long each will live, etc. This is true about many animal decisions too, of course, and biologists do not imagine their animals even attempting such optimality calculations.\n…\nInstead the behavioral ecologist’s solution is to find optima in deliberately simplified model environments. We note that this introduces much scope for misunderstanding, inconsistency, and loose thinking over whether “optimal policy” refers to a claim of optimality in the real world or just in a model. Calculating the optima even in the simplified model environments may still be beyond the capabilities of an animal, but the hope is that the optimal policy that emerges from the calculations may be generated instead, to a lesser level of accuracy, by a rule that is simple enough for an animal to follow. The animal might be hardwired with such a rule following its evolution through natural selection, or the animal might learn it through trial and error. There remains an interesting logical gap in the procedure: There is no guarantee that optimal solutions to simplified model environments will be good solutions to the original complex environments. The biologist might reply that often this does turn out to be the case; otherwise natural selection would not have allowed the good fit between the predictions and observations. Success with this approach undoubtedly depends on the modeler’s skill in simplifying the environment in a way that fairly represents the information available to the animal.\n\nAgain, Gigerenzer could equally be writing about economics. I think we should be thankful, however, that biologists don’t take their results and develop policy prescriptions on how to get the animals to behave in ways we believe they should.\nOne interesting question Gigerenzer asks is whether humans and animals use similar heuristics. Consideration of this question might uncover evidence of the parallel evolution of heuristics in other lineages facing similar environmental structures, or even indicate a common evolutionary history. This could form part of the evidence as to whether these human heuristics are evolved adaptations.\nBut are animals more likely to use heuristics than humans? Gigerenzer suggests the answer is not clear:\n\nIt is tempting to propose that since other animals have simpler brains than humans they are more likely to use simple heuristics. But a contrary argument is that humans are much more generalist than most animals and that animals may be able to devote more cognitive resources to tasks of particular importance. For instance, the memory capabilities of small food-storing birds seem astounding by the standards of how we expect ourselves to perform at the same task. Some better-examined biological examples suggest unexpected complexity. For instance, pigeons seem able to use a surprising diversity of methods to navigate, especially considering that they are not long-distance migrants. The greater specialism of other animals may also mean that the environments they deal with are more predictable and thus that the robustness of simple heuristics may not be such as advantage.\n\nAnother interesting question is whether animals are also predisposed to the “biases” of humans. Is it possible that “animals in their natural environments do not commit various fallacies because they do not need to generalize their rules of thumb to novel circumstances.” The equivalent for humans is mismatch theory, which proposes that a lot of modern behaviour (and likely the “biases” we exhibit) is due to a mismatch between the environment in which our decision making tools evolved and the environments we exercise them in today."
  },
  {
    "objectID": "posts/genome-wide-association-studies-and-socioeconomic-outcomes.html",
    "href": "posts/genome-wide-association-studies-and-socioeconomic-outcomes.html",
    "title": "Genome Wide Association Studies and socioeconomic outcomes",
    "section": "",
    "text": "A few months back, I posted about a Conference on Genetics and Behaviour held by the Human Capital and Economic Opportunity Global Working Group at the University of Chicago. In that post, I linked to a series of videos from the first session on the effect of genes on socioeconomic aggregates.\nOver the last couple of days, I watched the videos from the session on Genome Wide Association Studies (GWAS). As for the first set of videos, they are technical (as you might expect for a bunch of academics) - particularly the questions - but cover some important points.\nIn early studies linking genetic factors to behaviour and socioeconomic outcomes, candidate gene studies were the dominant method. In a candidate gene study, a gene is hypothesised to have an effect, and that hypothesis is tested directly. However, there are some major problems with candidate gene studies, with the literature littered with claims of the “gene for X” that simply can’t be replicated.\nDavid Cesarini opened the session by pointing to this low level of replication of candidate gene studies. He suggests three problems might be causing this failure to replicate. These are multiple hypothesis testing coupled with publication bias, population stratification, and the low power of the small samples typically used.\nMultiple hypothesis testing in candidate gene studies arises because more than one gene tends to be tested. In that case, the significance level of the tests should be adjusted to account for the multiple tests. But the reality is that the many negative tests never see the light of day, with the successful ones presented as successfully meeting a threshold appropriate for a single test. Publication bias exacerbates that problem as negative results tend not the be published and you don’t know how many tests have been conducted.\nIn contrast, GWAS is a hypothesis free approach. All SNPs in a sample (single nucleotide polymorphisms - DNA sequence variations in which a single nucleotide varies in the population) are tested for association with a trait. As there are as many hypotheses being tested as there are SNPs, very high significance thresholds are applied to avoid false positives. But as the number of SNPs in an array is known from the start, there is no doubt about the appropriate threshold.\nCesarini’s talk focused on the second problem, population stratification. This occurs where allele (variants of a gene) frequencies correlate with confounding variables. A classic example is analysing a mixed population of Asians and Caucasians and discovering the chopsticks gene. This can be overcome in GWAS by a technique called principal components analysis, which can be used to model the ancestry of the population and correct for stratification before conducting the analysis.\nhttp://youtu.be/mbRItXENYkE\nThe next speaker, Daniel Benjamin, spoke on the third problem - the low power of candidate gene studies. Power is the ability to statistically demonstrate an association when that association exists. A test with low power will miss the associations most of the time.\nThe low power of candidate gene studies is partly due to their typically low sample size, usually between 50 and 3,000 people. Benjamin points out that there may not be any genes in social science with effects large enough to be detected in samples of this size.\nThe low power of a study has an important implication beyond the inability to find any effects that exist. If real results are rare, they will be swamped by the false positives, which would occur for 1 in 20 tests using the typical significance level. Benjamin runs through some numerical examples and shows that given the expected effect sizes of genes on social science outcomes, you simply shouldn’t trust most candidate gene study results. False positives will drown the real findings. This contrasts with GWAS. Once you get to decent sample sizes in the order of 100,000, you can be relatively confident that what you do find (even though you miss a lot) will be true.\nBenjamin also talks about the Social Science Genetic Association Consortium (SSGAC), which is an attempt to build datasets large enough to apply GWAS to social outcomes such as IQ and risk aversion. The proof of concept was on educational attainment, which the next speaker covers in more detail.\nhttp://youtu.be/vPDqFB2GMUk\nPhilipp Koellinger opens by asking why there are so many null results in the search for genetic influences. Is it because the effects are small? Because they are non-linear? Or there are gene-environment interactions? Maybe the results of twin studies showing most social outcomes are heritable are wrong?\nPart of the answer was given by a study of educational attainment in which Koellinger and the previous two speakers were involved. They used a GWAS to search for SNPs that affected educational attainment in an initial sample of 100,000 people. They then replicated the result in another sample of 25,000 people. All three SNPs found in the discovery stage were replicated.\nImportantly, the effect sizes were smaller than expected, with those three SNPs explaining 0.02% of the variation in educational attainment. If you added up the effects of all the SNPs in their sample, you could explain around 2 to 2.5% of the variation.\nWhile this sounds low, it provides a basis for hope. Based on projections for larger sample size, it should be possible to explain 20% of the variation in education attainment through genetic factors.\nhttp://youtu.be/opGrHm4v_9o\nJason Fletcher was next, and he asked two main questions. First, how much should we believe GWAS results given how differently GWAS is done compared to normal science procedure. Second, what use are GWAS results? He spends more time on the second question and points out the usual possibilities, such as providing measures for latent variables. For example, if you don’t know the IQ of your sample but have their genomes and know how this affects intelligence, the genetic information could be used to attempt to determine the effect of IQ on a certain outcome.\nFletcher also points to the potential for exploration of gene-environment effects. He gives the example of people responding differently to tobacco taxation based on having different alleles. His paper on this topic is here.\nWithin his talk, Fletcher asks an interesting question about whether the SSGAC will become a natural monopoly in GWAS. Do we need a second SSGAC to enable people to check the results, and is it feasible for one to emerge? Others may be more viable as genetic testing becomes cheaper, but the tendency for one to dominate may still remain.\nIn the questions to Fletcher’s presentation, Benjamin makes the important point that the use of GWAS results as control variables could give much more precision to the estimates of the effect that a social science experiment is designed to measure. He gives the example of the Perry pre-school project - expensive educational interventions with a small sample, in which any added precision as to their effects would be of great value.\nhttp://youtu.be/eA52b-PzGx4\nThe last speaker, Dalton Conley, returned to the population stratification problem. His argument is that it may not be as easy to solve as it seems. Conley refers mainly to a technique called Genomic-relatedness-matrix restricted maximum likelihood (GREML) or Genome-wide complex trait analysis (GCTA) (which I have posted about before). This technique seeks to determine the contribution of all the sampled SNPs combined to variation in a trait. The output is a lower bound estimate of heritability. This technique relies, however, on an assumption that among those who are less related than second cousins (higher degrees of relatedness are removed), they share alleles in a way that is uncorrelated with any similarity in environment.\nConley argues that this assumption is false, and shows that using GREML, he can obtain a finding that birth in an urban or rural environment is heritable, in direct violation of the assumption. This result does not disappear after controlling for population stratification.\nTo deal with this problem, consideration should be given to testing for variation within families - any differences in genes between siblings will truly be random. The problem with this is that most massive datasets for which GWAS is performed don’t have pedigree data of that nature. The good news, however, is that the violation of the assumption does not seem to puncture the GWAS results. It is violated but the consequences are trivial. A paper by Conley and friends on this paper can be found here.\nhttp://youtu.be/HUhXHCHqIXQ"
  },
  {
    "objectID": "posts/genoeconomics-at-the-aea-annual-meeting.html",
    "href": "posts/genoeconomics-at-the-aea-annual-meeting.html",
    "title": "Genoeconomics at the AEA Annual Meeting",
    "section": "",
    "text": "The preliminary program for January’s American Economic Association annual meeting is available, with a session dedicated to genoeconomics. I’ve posted on the first of the papers before.\n\nJan 06, 2013 8:00 am, Manchester Grand Hyatt, Elizabeth Ballroom C American Economic Association\nGenes and Economic Behavior (D8) Presiding: DAVID LAIBSON (Harvard University)\nThe Genetic Architecture of Economic and Political Preferences DAVID CESARINI (New York University) CHRIS DAWES (New York University) CHRISTOPHER F. CHABRIS (Union College) MAGNUS JOHANNESSON (Stockholm School of Economics) DAVID I. LAIBSON (Harvard University)\nMeta-Analysis of Genome-Wide Association Studies of Educational Attainment DANIEL J. BENJAMIN (Cornell University and NBER) DAVID CESARINI (New York University) PHILIPP KOELLINGER (Erasmus University Rotterdam) MATHIJS VAN DER LOOS (Erasmus University Rotterdam) NIELS RIETVELD (Erasmus University Rotterdam)\nGenetic Modulation of the Effects of Tobacco Taxation on Use JASON FLETCHER (Yale University)\nMeta-Analysis of Genome-Wide Association Studies of Well-Being JAN-EMMANUEL DE NEVE (University College London & Centre for Economic Performance (LSE)) MEIKE BARTELS (VU University Amsterdam) BOB KRUEGER (University of Minnesota) NIELS RIETVELD (Erasmus University Rotterdam) PHILIPP KOELLINGER (Erasmus University Rotterdam)\nDiscussants: DAVID I. LAIBSON (Harvard University) ALDO RUSTICHINI (University of Minnesota) ANDREW CAPLIN (New York University) JOHN CAWLEY (Cornell University)\n\nEvolutionary biology also gets a slot in the session on the historical origins of comparative development:\n\nJan 06, 2013 10:15 am, Manchester Grand Hyatt, Randle A American Economic Association\nOn the Historical Origins of Comparative Development (O1) Presiding: HOLGER STRULIK (University of Goettingen)\nGenetic Diversity and Ethnic Civil Conflict CEMAL EREN ARBATLI (Brown University) QUAMRUL ASHRAF (Williams College) ODED GALOR (Brown University)\n\nThis paper might be an interesting addition to the debate about Ashraf and Galor’s paper on genetic diversity and economic development. The potential for conflict due to higher levels of genetic diversity is half of their argument for a hump-shaped relationship between genetic diversity and economic development, and it could do with some support."
  },
  {
    "objectID": "posts/genoeconomics-and-designer-babies-the-rise-of-the-polygenic-score.html",
    "href": "posts/genoeconomics-and-designer-babies-the-rise-of-the-polygenic-score.html",
    "title": "Genoeconomics and designer babies: The rise of the polygenic score",
    "section": "",
    "text": "When genome-wide association studies (GWAS) were first used to study complex polygenic traits, the results were underwhelming. Few genes with any predictive power were found, and those that were typically explained only a fraction of the genetic effects that twin studies suggested were there.\nThis led to divergent responses, ranging from continued resistance to the idea that genes affect anything, to a quiet confidence that once sample sizes became large enough those genetic effects would be found.\nIncreasingly large samples are now showing that the quiet confidence was justified, with a steady flow of papers emerging finding material genetic effects on traits including educational attainment, intelligence and height.\nOne source of this work are “genoeconomists”. From Jacob Ward in the New York Times:\n\nOnce a G.W.A.S. shows genetic effects across a group, a “polygenic score” can be assigned to individuals, summarizing the genetic patterns that correlate to outcomes found in the group. Although no one genetic marker might predict anything, this combined score based on the entire genome can be a predictor of all sorts of things. And here’s why it’s so useful: People outside that sample can then have their DNA screened, and are assigned their own polygenic score, and the predictions tend to carry over. This, Benjamin realized, was the sort of statistical tool an economist could use.\n…\nAs an economist, however, Benjamin wasn’t interested in medical outcomes. He wanted to see if our genes predict social outcomes.\nIn 2011, with a grant from the National Science Foundation, Benjamin launched the Social Science Genetic Association Consortium, an unprecedented effort to gather unconnected genetic databases into one enormous sample that could be studied by researchers from outside the world of genetic science. In July 2018, Benjamin and four senior co-authors, drawing on that database, published a landmark study in Nature Genetics. More than 80 authors from more than 50 institutions, including the private company 23andMe, gathered and studied the DNA of over 1.1 million people. It was the largest genetics study ever published, and the subject was not height or heart disease, but how far we go in school.\nThe researchers assigned each participant a polygenic score based on how broad genetic variations correlated with what’s called “educational attainment.” (They chose it because intake forms in medical offices tend to ask patients what education they’ve completed.) The predictive power of the polygenic score was very small — it predicts more accurately than the parents’ income level, but not as accurately as the parents’ own level of educational attainment — and it’s useless for making individual predictions.\n\nOne of the most interesting possibilities for using polygenic scores is to use them to control for heterogeneity in research subjects. Ward writes:\n\nSeveral researchers involved in the project mentioned to me the possibility of using polygenic scores to sharpen the results of studies like the ongoing Perry Preschool Project, which, starting in the early 1960s, began tracking 123 preschool students and suggested that early education plays a large role in determining a child’s success in school and life. Benjamin and other co-authors say that perhaps sampling the DNA of the Perry Preschool participants could improve the accuracy of the findings, by controlling for those in the group that were genetically predisposed to go further in school.\n\nIn a world with easy access to genetic samples, it could become common to include genetic controls in analysis of interesting societal outcomes, in the same way we now control for parental traits.\nA couple of times in the article, Ward notes that “scores aren’t individually predictive”. He writes that “The predictive power of the polygenic score was very small — it predicts more accurately than the parents’ income level, but not as accurately as the parents’ own level of educational attainment — and it’s useless for making individual predictions.”\nI’m not sure what Ward’s definition of “predictive” is for an individual, but take this example from the article:\n\nThe authors calculated, for instance, that those in the top fifth of polygenic scores had a 57 percent chance of earning a four-year degree, while those in the bottom fifth had a 12 percent chance. And with that degree of correlation, the authors wrote, polygenic scores can improve the accuracy of other studies of education.\n\nThat looks like predictive power to me. Take an individual from the sample or an equivalent population, look at their polygenic score, and then assign a probability of whether they will obtain a four-year degree.\nI recommend reading the whole article.\nA related story getting ample press is that Genomic Prediction has started to offer intelligence screening for embryos. Polygenic scores have been used with success in livestock breeding for a while now, which is often a better place to look for evidence of the future possibilities than listening to those afraid of the human implications of genetic research. From Philip Ball in The Guardian:\n\nThe company says it is only offering such testing to spot embryos with an IQ low enough to be classed as a disability, and won’t conduct analyses for high IQ. But the technology the company is using will permit that in principle, and co-founder Stephen Hsu, who has long advocated for the prediction of traits from genes, is quoted as saying: “If we don’t do it, some other company will.”\nThe development must be set, too, against what is already possible and permitted in IVF embryo screening. The procedure called pre-implantation genetic diagnosis (PGD) involves extracting cells from embryos at a very early stage and “reading” their genomes before choosing which to implant. It has been enabled by rapid advances in genome-sequencing technology, making the process fast and relatively cheap. In the UK, PGD is strictly regulated by the Human Fertilisation and Embryology Authority (HFEA), which permits its use to identify embryos with several hundred rare genetic diseases of which the parents are known to be carriers. PGD for other purposes is illegal.\nIn the US it’s a very different picture. Restrictive laws about what can be done in embryo and stem-cell research using federal funding sit alongside a largely unregulated, laissez-faire private sector, including IVF clinics. PGD to select an embryo’s sex for “family balancing” is permitted, for example. There is nothing in US law to prevent PGD for selecting embryos with “high IQ”.\n\nBall also expresses a scepticism about the value of the polygenic scores:\n\nThese relationships are, however, statistical. If you have a polygenic score that places you in the top 10% of academic achievers, that doesn’t mean you will ace your exams without effort. Even setting aside the substantial proportion of intelligence (typically around 50%) that seems to be due to the environment and not inherited, there are wide variations for a given polygenic score, one reason being that there’s plenty of unpredictability in brain wiring during growth and development.\nSo the service offered by Genomic Prediction, while it might help to spot extreme low-IQ outliers, is of very limited value for predicting which of several “normal” embryos will be smartest. Imagine, though, the misplaced burden of expectation on a child “selected” to be bright who doesn’t live up to it. If embryo selection for high IQ goes ahead, this will happen.\n\nDespite Ball’s scepticism about comparing “normal” embryos, I expect it won’t be long before Genomic Prediction or a counterpart is doing just that.\nSteve Hsu, co-founder of Genomic Prediction, comments on the press here (and provides some links to other articles). He closes by saying:\n\n“Expert” opinion seems to have evolved as follows:\n\nOf course babies can’t be “designed” because genes don’t really affect anything – we’re all products of our environment!\nGulp, even if genes do affect things it’s much too complicated to ever figure out!\nAnyone who wants to use this technology (hmm… it works) needs to tread carefully, and to seriously consider the ethical issues.\n\nOnly point 3 is actually correct, although there are still plenty of people who believe 1 and 2 :-("
  },
  {
    "objectID": "posts/genetics-and-the-increase-in-obesity.html",
    "href": "posts/genetics-and-the-increase-in-obesity.html",
    "title": "Genetics and the increase in obesity",
    "section": "",
    "text": "In a discussion on the rise of mental health issues at Core Economics, Paul Frijters touches on the increase in obesity over the last 50 years.\n\nOne can basically out of hand reject the excuses most individuals give for their problems as being the reason. The rate of increase rules out any reasonable role for genetics. The fact that the poor suffer more from obesity, whilst it is cheaper to eat less and whilst food has always been cheap for the rich, rules out any obvious effect of the lower price of food or the availability of fast-food. The sustained increase over a long time rules out any story depending on some major current crisis. Like it or loath it, but it is clear that one must look at ‘cultural factors’ to have a hope of understanding what is going on.\nA big hint comes from cross-national differences amongst rich countries, where things like wealth and food affordability don’t differ much. As you can see here, the Anglo-Saxon countries, and then particularly the US, stands out. Whilst a third of adults in the US are now obese (with about 25% of Australian adults), only 4% of Koreans and Japanese are such, and in the more egalitarian Northern European countries (Sweden, Norway, Holland) rates are below 10%. The same holds for Italy and France, though rates in those countries too are quite a bit up from what they were 50 years ago. So your one major clue is that there are major unexplained differences over countries.\n\nI’ve heard this “it can’t be genetics” argument from a few people recently. And in some respects it is right. Clearly, the genes in the population have not changed substantially over the last 50 years. However, to dismiss genetics in trying to understand obesity is ignoring an important piece of the puzzle.\nFirst is the high heritability of obesity - both before and during the increase in obesity of the last 50 years - usually measured in the range of 55 to 85 per cent. This level of heritability exceeds that measured for most behavioural traits (although it is in the realm of heritability for intelligence). This suggests that both before and after the increase in obesity, genetics plays a substantial role in who is obese. As the environment has changed (such as the “cultural factors” that Frijters alludes to), people of different genotypes have responded in different ways.\nAnother pointer to the role of genetics is in the high levels of obesity and obesity-related diseases in some populations, particularly indigenous populations with a limited history of agriculture. Pacific islanders, with almost no history of grain based agriculture, have the highest rates of obesity in the world. American Indian and Alaskan Natives (as a group) have the second highest level of obesity in the United States of any major ethnic group (behind people of Pacific Island origin). In Australia, aboriginals and Torres Strait Islanders are twice as likely to be obese as Caucasians. The costs of obesity, such as Type 2 diabetes, also tend to higher for these groups.\nFrijters’s point on the response of the rich and the poor to food prices also has a hint of genetic factors. These groups are not the same. The rich and poor differ in average levels of IQ, willpower, conscientiousness and a host of other traits with a genetic component (even if you don’t believe there is a genetic component, you still should not treat them as the same). So when we ask why obesity is higher among poor people (or less educated people), we should ask what role those traits and their underlying genetic influences play. When faced with the same choices, they are likely to select different options.\nThese traits might also be relevant for cross-country comparison. Should we be surprised that East Asian countries where populations have higher measured IQ, lower rates of time preference and higher savings rates also have lower rates of obesity? (Obviously, on a cross-national basis, this is not the complete explanation. For example, East Asians in the United States have higher levels of obesity than their counterparts still in East Asia, although they are obese at rates lower than Caucasians and other ethnic groups).\nWe should also not be too quick to dismiss price. It is not only absolute price that matters, but also relative price of different food types. As argued by Rob Brooks, Steve Simpson and David Raubenheimer, simple carbohydrates have never been cheaper relative to protein. If you are price sensitive, you may shift consumption towards simple carbohydrates. As someone who tends to avoid simple carbohydrates, I can also attest that a large part of the relative price of food is the search effort in finding a low carbohydrate option.\nThe reason this matters also has an evolutionary basis. Eating food is not a simple “eat calories and feel full” process. Different foods create different responses in appetite. Brooks and his colleagues base their argument on the protein leverage hypothesis, which is a hypothesis that humans have a stronger propensity to regulate protein intake than they do for other non-protein calories. Humans eat until we satisfy out basic daily protein need. If the food we are eating has low protein content, we need to eat more before hitting that satiation point. These extra calories are what make someone obese. Trends in carbohydrate, protein and fat consumption in the United States over the last 40 years offer support for this argument.\nArguments such as the protein leverage hypothesis also have interesting implications for any arguments about the willpower of the obese. Someone eating a diet high in simple calories would need more willpower to constrain their calorie intake than someone on a high protein diet.\nWhile Frijters points to the cross-national differences as a major clue to why obesity has increased, the above suggests that within country and cross-population differences will also be useful. The cultural changes that have resulted in the increase in obesity play out in different ways depending on who the person is. Genetics is clearly not the only factor that should be examined - look at the Anglosphere compared to Northern European countries - but any cultural explanation will need to accord with the evidence that the cultural changes do not affect all people equally."
  },
  {
    "objectID": "posts/genetically-testing-similarity.html",
    "href": "posts/genetically-testing-similarity.html",
    "title": "Genetically testing similarity",
    "section": "",
    "text": "In my last post, I questioned whether a stranger sitting next to you on a train would be more similar to you than an ancestor from 10,000 years ago and suggested that this could be tested genetically.\nA few issues arise in testing this. First, as I suggested in the last post, the particular ancestor we choose might affect the result. If an ancestor contributed through only a single ancestral line (of the approximately 10^120 lines), any similarity due to ancestry will be very low to negligible, unless that person is, say, a direct male-male ancestor and has contributed the Y-chromosome, much of which does not engage in recombination (that is, the crossover of genes between the chromosomes inherited from ones parents).\nA bigger issue in my mind is convergent evolution. Given the selection pressures that agricultural populations have been under, it is likely that a number of shared phenotypic traits (that is, traits that are expressed in the person) have emerged, with these traits having different genetic origins. Take the ability to digest lactose in adulthood, which has separate origins in Europe and sub-Saharan Africa (and possibly other locations). While the genetic mutations that cause the traits are different, the phenotypic result is similar, which reflects the common selection pressure.\nI expect that this is the case for many other traits. Distinct populations developed agriculture in a number of separate locations, which is likely to have resulted in some similar selection pressures in these locations (I won’t describe these events as independent developments of agriculture, as is sometimes done, as they aren’t independent). If the traits favoured by the adoption of agriculture are similar, despite being expressed by different genes or mutations, they would spread and increase similarity between populations in a way which may not be apparent in a genetic test.\nGiven this, as I did in my last post, I still question whether Seabright’s statement would be generally true. Further, if we could also capture similar traits expressed through different genes or mutations, we may be more similar to the modern stranger than the genetic test would suggest."
  },
  {
    "objectID": "posts/genetic-diversity-economic-development-and-policy.html",
    "href": "posts/genetic-diversity-economic-development-and-policy.html",
    "title": "Genetic diversity, economic development and policy",
    "section": "",
    "text": "It has been a few months since I wrote most of my series of posts on Quamrul Ashraf and Oded Galor’s paper The ‘Out of Africa’ Hypothesis, Human Genetic Diversity, and Comparative Economic Development. This last post in the series is on the implication of their argument for policy.\nAs a recap, Ashraf and Galor showed a hump-shaped relationship between genetic diversity and economic development across countries and populations. They proposed that two opposing effects of genetic diversity cause this hump-shaped pattern: diversity promoting innovation and productivity through the greater range of traits available in the population, and negative effects through conflict between more dissimilar individuals. This pattern is reflected in African populations with high diversity and North American populations with low diversity experiencing lower levels of economic development than European populations with moderate levels of diversity.\nAshraf and Galor do not express any policy recommendations as flowing from their findings. The closest they come is a thought experiment where they note the effect of increasing diversity in Bolivia or decreasing diversity in Ethiopia. In an article in Chance (based on an earlier blog post), Andrew Gelman considered what this means:\n\nWhat would it mean to increase Bolivia’s diversity by 1 percentage point? I assume that would mean adding some white people to the country. What kind of white person would go to Bolivia? Probably someone rich enough to increase the country’s income per capita. Hey, it works! What if some poor people from Ethiopia were taken to Bolivia? They’d increase the country’s ethnic diversity too, but I don’t see them increasing its per-capita income by 41%. But that’s okay; nobody’s suggesting filling Bolivia with poor Africans.\nWhat about Ethiopia? How do you make it less diverse? I guess you’d have to break it up into a bunch of little countries, each of which is ethnically pure. Is that possible? I don’t actually know. If you can’t do that, you’d need to throw in lots of people with less genetic diversity. Maybe, hmmm, I dunno, a bunch of whites or Asians? What sort of whites or Asians might go to Ethiopia? Not the poorest ones, certainly. Why would they want to go to a poor country in the first place? Maybe some middle-income or rich ones (if the country could be safe enough, or if there’s a sense there’s money to be made). And, there you go; per-capita income goes up again.\n\nMoving people around in the way Gelman considers is different from a situation where the founder effect shaped genetic diversity (the founder effect is the reduction in diversity that occurs when a small proportion of a population migrates and takes only a subset of the genetic diversity of the population with them). If a group of rich whites moved to Bolivia, they might increase the level of genetic diversity, but they would also be of substantial genetic distance from the Bolivians. Spolaore and Wacziarg argued that genetic distance causes differences in economic development as it acts as a barrier to technological diffusion, either through the genetic distance itself or other characteristics for which genetic diversity is a proxy. If the new sub-population in Bolivia pushed out the technological frontier through their activities, the genetic distance between them and other sub-populations may prevent technological diffusion and prevent the benefits from accruing across the country.\nWe can also consider this mix from the perspective of relatedness, which is how Ashraf and Galor frame the problem. Each of the whites and native Bolivians would have lower relatedness with each other than with their someone from their own sub-population. This might increase the negative effects of diversity more strongly than it would introduce any positive effects, as differences in relatedness between some people would be greater than in an equally diverse population shaped purely by the founder effect.\nThen there is the question of interaction between people of different races. There is considerable evidence of racial conflict, whereas the evidence for the human ability to detect small genetic differences in relatedness as Ashraf and Galor propose is weak.\nApart from Gelman’s suggestion, Ashraf and Galor’s paper does not immediately lend to other genetic-focused policy interventions. But this is not to say that their findings are useless in determining policy. For example, if Ethiopians populations could be highly productive due to their diversity, but this is negated by excessive intra-population conflict, we might ask what interventions could reduce conflict. Similarly for Bolivia, if the lack of diversity hinders expansion of the productive frontier, could Bolivia benefit from technological imports?\nAs I’ve posted about before, I am skeptical of Ashraf and Galor’s hypothesis, so I’m not sure these policy suggestions would work. But if the hypothesis is right, there is scope for it to shape policy. As Goldberger and Manski failed to see in their criticism of the use of genoeconomics in policy development, just because there is a genetic factor underlying an observed outcome does not mean that we need to implement eugenic policies or throw away non-genetic policy interventions.\nMy posts on Ashraf and Galor’s paper on genetic diversity and economic growth are as follows:\n\nA summary of the paper methodology and findings\nDoes genetic diversity increase innovation?\nDoes genetic diversity increase conflict?\nIs genetic diversity a proxy for phenotypic diversity?\nIs population density a good measure of technological progress?\nWhat are the policy implications of the effects of genetic diversity on economic development? (this post)\nShould this paper have been published?\n\nOther debate on this paper can also be found here, here, here and here."
  },
  {
    "objectID": "posts/genetic-diversity-and-economic-development-ashraf-and-galor-respond.html",
    "href": "posts/genetic-diversity-and-economic-development-ashraf-and-galor-respond.html",
    "title": "Genetic diversity and economic development: Ashraf and Galor respond",
    "section": "",
    "text": "As I noted in a postscript to my last post, Quamrul Ashraf and Oded Galor have prepared a response [Update: the response is no longer online] to the Harvard academic critique of their paper on genetic diversity and economic development (I recommend having a look through the comments on that post, where Jade d’Alpoim Guedes, Nick Patterson (both authors of the critique), Henry Harpending and others continue the debate).\nApart from the broader question of whether this work should even be undertaken, the Harvard critique focused on two issues: causation and the statistical foundations of the work. Ashraf and Galor are quick to dismiss the statistical critique:\n\n[O]ur critics have falsely suggested that we treat socioeconomic and genetic data as if populations are independent of one another. On the contrary, our empirical analysis accounts for the possibility of spatial dependence across observations, including analytical methods that correct for spatial autocorrelation in “error terms” and bootstrapping. This criticism of our work thus reflects either a misunderstanding of the techniques that we employ or a superficial reading of our work.\n\nThe response on causation is more detailed, and one of Ashraf and Galor’s arguments is one that I did not expect to see. They write:\n\nThe key is that the measure of intra-population genetic diversity that we employ should be interpreted as a proxy (i.e., a correlated summary measure) for diversity amongst individuals in a myriad of observable and unobservable personal traits that may be physiological, behavioral, socially-constructed, or otherwise. …\nA careful reading of our research should make it apparent that our use of the measure of genetic diversity from the field of population genetics does not imply that our hypothesis is one of biological determinism, nor does it imply that DNA material is directly important for economic outcomes or that some genes are more important than others for economic success. The fact that the measure of genetic diversity we use is based on variation across individuals in non-protein- coding regions of the genome (and, thus, in genomic characteristics that are not necessarily phenotypically expressed so as to be subject to the forces of natural selection) is clear reason why our findings should be interpreted through the lens of our measure serving as a proxy for diversity more broadly defined.\nThe more relevant question to ask therefore is to what extent the measure we use can reasonably be considered a proxy for diversity in unobserved phenotypic or socially-constructed characteristics. There is indeed an emerging body of scientific evidence that establishes remarkable correlations in this regard.\n\nAshraf and Galor are also quoted running this line in a Nature News piece on their paper:\n\n Galor and Ashraf told Nature that, far from claiming that genetic diversity directly influences economic development, they are using it as a proxy for immeasurable cultural, historical and biological factors that influence economies.\n\nAfter reading this, I went back to the paper to confirm my previous understanding of it, and if Ashraf and Galor intended to use genetic diversity as a proxy, it is not clear. The paper appears to finger genetic diversity and the phenotypic expression of that diversity as the relevant causal factors, with no suggestion it is a proxy. For example, they write:\n\nThe hypothesized channels through which genetic diversity affects aggregate productivity follow naturally from separate well-established mechanisms in the field of evolutionary biology and from experimental evidence from scientific studies on organisms that display a relatively high degree of social behavior in nature (e.g., living in task-directed hierarchical societies and engaging in cooperative rearing of offspring). The benefits of genetic diversity, for instance, are highlighted in the Darwinian theory of evolution by natural selection, according to which diversity, by permitting the forces of natural selection to operate over a wider spectrum of traits, increases the adaptability and, hence, the survivability of a population to changing environmental conditions. On the other hand, to the extent that genetic diversity is associated with a lower average degree of relatedness amongst individuals in a population, kin selection theory, which emphasizes that cooperation amongst genetically related individuals can indeed be collectively beneficial as it ultimately facilitates the propagation of shared genes to the next generation, is suggestive of the hypothesized mechanism through which diversity confers costs on aggregate productivity.\n\nI would like to see a more direct defence of their argument about the causal mechanisms. However, Ashraf and Galor do suggest in their response that further research on the causal mechanisms is required.\nThe timing of this debate has highlighted the extent of continued disciplinary divides. Ashraf and Galor released the working paper a couple of years ago, and they have since presented it in a raft of conferences and seminars. It was then accepted for publication in the American Economic Review, but the current debate was only triggered when the paper was mentioned in Science (gated). The pre-publication of working papers so prevalent in economics, and which is starting to gain traction in other fields, still relies on the working paper getting in front of people who might be interested in commenting. The reality is, however, that publication in a reputable journal remains the point at which a paper comes to others’ attention - or becomes “important” enough that it deserves a response.\nMy posts on Ashraf and Galor’s paper on genetic diversity and economic growth are as follows:\n\nA summary of the paper methodology and findings\nDoes genetic diversity increase innovation?\nDoes genetic diversity increase conflict?\nIs genetic diversity a proxy for phenotypic diversity?\nIs population density a good measure of technological progress?\nWhat are the policy implications of the effects of genetic diversity on economic development?\nShould this paper have been published?\n\nOther debate on this paper can also be found here, here, here and here."
  },
  {
    "objectID": "posts/genetic-distance-and-economic-development.html",
    "href": "posts/genetic-distance-and-economic-development.html",
    "title": "Genetic distance and economic development",
    "section": "",
    "text": "The History and Geography of Human Genes has heavily influenced the way I think about human evolution. Even though it is getting old at a time when masses of population genetic data are being accumulated, a flip through the maps depicting the geographic distribution of genes provides a picture that is available in few other places.\nIt was only a matter of time before some economists grabbed this population genetic data to see whether it could shed any light on economic development. In a paper published in the Quarterly Journal of Economics in 2009, Enrico Spolaore and Romain Wacziarg took data on genetic distance from the The History and Geography of Human Genes and asked whether it is correlated with differences in income between countries.\nSpolaore and Wacziarg proposed the following model. Take an initial population that branches into two sub-populations each time period, with genetic distance between the two populations being the time since they had a common ancestor. Each sub-population has a transmitted characteristic which is represented by a number. This characteristic mutates either up or down with a 50 per cent probability each generation, so it follows a random walk. As a result, the difference in characteristics (or vertical distance) between two populations is a function of their genetic distance, with the vertical characteristics more likely to have “walked” apart as the time since the shared ancestor increases.\nNext, assume that when a sub-population develops a new technology, other sub-populations’ ability to adopt that technology is a function of their vertical distance from the population at the technological frontier. If technology determines income, then the difference in income between two populations is the size of the relative vertical distance from the population that is at the frontier, which in turn is related to the genetic distance. The core insight from this model is that relative genetic distance should have a higher correlation with differences in technology than abolute genetic distance.\nWhile I am not sure this model adds much to the initial intuition, it does serve a useful purpose in that it looks to link genetic distance with income differences through differences in vertical characteristics. If genetic distance and income differences had been directly linked, we would not be left with the interesting question of what these characteristics are.\nOn the flip-side, Spolaore and Wacziarg have produced a model in which differences in vertical characteristics are a function of random drift, rather than selection. This is unsatisfying, but it is hard to see how the authors could otherwise have produced the model without a theory about what those characteristics are. The model is also agnostic about how one country may develop technology as the authors assume transmitted characteristics do not have any effect on productivity. Introducing a theory of technological development could have been interesting as if certain traits make technological development more likely, there would be two effects creating the income difference - the higher probability of technological progress coupled with the barriers to diffusion.\nWith model in hand, Spolaore and Wacziarg turned to the population genetic data. Taking data on from 42 world populations, they matched it to countries (for which they have economic data) using information on the ethnic composition of those countries. This formed the basis of determining the genetic distance between countries. They also took a set of European population data (of 26 populations) which would allow them to do a European analysis. The regressions had to depart from the model and test the link between genetic distance and income differences directly as the data does not tell us anything about the vertical characteristics of the population.\nThe authors completed a mountain of regressions in analysing the data, so here are some of the headline findings. Taking the United States as the world technological frontier in 1995 (a fair assumption), the authors regressed genetic distance against the log of income and, as expected, found that income was negatively correlated with average genetic distance from the United States population. Genetic distance also had reasonably high explanatory power, accounting for 39 per cent of the variation in the sample. The chart below gives the picture. Throwing a range of other explanatory variables into the analysis such as geography and linguistic and religious differences did not materially change this result.\nSpolaore and Wacziarg then created 9,316 pairs of countries (from 137 countries) for the world sample and 325 pairs (based on 26 countries) for the European sample and assessed the link between genetic distance and income difference. When they use this broader set of pairs, as opposed to the simple comparison with the United States technological frontier, the degree of variation accounted for by genetic distance decreases, although the genetic distance still has a material effect. For example, one standard deviation change in genetic distance accounts for 16.79% of a standard deviation change in income difference when genetic distance alone is entered into the regression.\nThe authors also examined a range of other factors, such as Jared Diamond’s thesis about differences in geography and domesticable plants and animals. While including these factors in the analysis reduced the explanatory power of the genetic difference measure, the significance remained. The data also allowed some analysis of earlier time periods, which was in fact easier as most countries’ populations were more ethnically uniform in, say, 1500. At for the later dates, the relationship still held.\nGiven the agnosticism of Spolaore and Wacziarg on what the vertical characteristics driving income differences are, I hope this paper triggers some deeper examination of what is going on. What are the microeconomic mechanisms driving this result? What are the vertical characteristics that are relevant? And how has selection affected these characteristics? Without the characteristics being subject to selection, the change in characteristics would be fairly slow. These slow changes are then hypothesised to create a substantial barrier to technological diffusion even though the populations have been separated a relatively short period. I would suggest that selection is required.\nThe authors suggest that more research on peaceful and non-peaceful interaction between societies may be useful to tease out the mechanisms that they have proposed. I agree that research may be interesting, but it leaves open the question which the model ignores - how did some countries get that technological lead in the first place? Do these vertical characteristics play a role in that? Asking why others did not follow is not as interesting as asking why some countries got the lead in the first place."
  },
  {
    "objectID": "posts/genes-and-socioeconomic-aggregates.html",
    "href": "posts/genes-and-socioeconomic-aggregates.html",
    "title": "Genes and socioeconomic aggregates",
    "section": "",
    "text": "In April, a Conference on Genetics and Behaviour was held by the Human Capital and Economic Opportunity Global Working Group at the University of Chicago.\nThe first session, with videos linked below, was on Genes and Socioeconomic Aggregates. The video and audio are average at times, and you might want to get the slides (links provided where available) as they are hard to read in the video at times. However, there are some good bits in all of the presentations.\nGregory Cochran: Genetics and Society (slides)\nCochran laid out some ideas that should be in the minds of economists, although he does not focus much attention on selling the ideas. Unfortunately, the questions at the end got derailed by epigenetics (my views approximate Cochran’s). One interesting argument by Cochran is that human environments tend be variable, as, in a Malthusian world, good times (when people breed like mad) tend to be followed by bad (too many people) which tend to be followed by good (people died in the bad). As a result, epigenetic transmission based on the current environment may be a poor strategy.\n\nEnrico Spolaore: Ancestry and the Diffusion of Economic Development: Facts and Questions (slides)\nSpolaore touches on his work concerning genetic distance and the diffusion of development (I have posted about it here, here and here). He is extending this work to look at the diffusion of fertility reduction from France (where the demographic transition first occurred), and is getting similar results.\n\nSteven Durlauf: Two Remarks on the Inference of “Macro” Genetic Effects (slides)\nI did not get much from Durlauf’s presentation, although some of the questions were interesting. Steve Hsu deflates the “it’s all too hard” message when he points out that animal breeding is now using genetic data.\n\nHenry Harpending: Some Quantitative Genetics Approaches\nHarpending discusses his work on how assortative mating can mimic strong selection. I sense this presentation might be difficult to follow if you aren’t familiar with his work. Not much value in the question session, which gets derailed by issues concerning scaling when estimating heritability.\n\nAldo Rustichini: Determinants of Inequality and Intergenerational Mobility (slides)\nA tough presentation to follow - you need to use the slides to have a chance of getting across it - and not recommended for those not mathematically inclined. The highlight is Greg Cochran trying not to jump out of his chair between the 7 and 8 minute mark due to some comments about heritability. Cochran also deflates the idea that there is a high level of false paternity in humans."
  },
  {
    "objectID": "posts/gary-klein-on-heuristics-and-biases-confirmation-bias-and-explaining-everything.html",
    "href": "posts/gary-klein-on-heuristics-and-biases-confirmation-bias-and-explaining-everything.html",
    "title": "Gary Klein on confirmation bias in heuristics and biases research, and explaining everything",
    "section": "",
    "text": "In Sources of Power: How People Make Decisions, Gary Klein writes:\n\nKahneman, Slovic, and Tversky (1982) present a range of studies showing that decision makers use a variety of heuristics, simple procedures that usually produce an answer but are not foolproof. … The research strategy was not to demonstrate how poorly we make judgments but to use these findings to uncover the cognitive processes underlying judgments of likelihood.\n…\nLola Lopes (1991) has shown that the original studies did not demonstrate biases, in the common use of the term. For example, Kahneman and Tversky (1973) used questions such as this: “Consider the letter R. Is R more likely to appear in the first position of a word or the third position of a word?” The example taps into our heuristic of availability. We have an easier time recalling words that begin with R than words with R in the third position. Most people answer that R is more likely to occur in the first position. This is incorrect. It shows how we rely on availability.\nLopes points out that examples such as the one using the letter R were carefully chosen. Of the twenty possible consonants, twelve are more common in the first position. Kahneman and Tversky (1973) used the eight that are more common in the third position. They used stimuli only where the availability heuristic would result in a wrong answer. … [I have posted some extracts of Lopes’s article here.]\nThere is an irony here. One of the primary “biases” is confirmation bias—the search for information that confirms your hypothesis even though you would learn more by searching for evidence that might disconfirm it. The confirmation bias has been shown in many laboratory studies (and has not been found in a number of studies conducted in natural settings). Yet one of the most common strategies of scientific research is to derive a prediction from a favorite theory and test it to show that it is accurate, thereby strengthening the reputation of that theory. Scientists search for confirmation all the time, even though philosophers of science, such as Karl Popper (1959), have urged scientists to try instead to disconfirm their favorite theories. Researchers working in the heuristics and biases paradigm condemn this sort of bias in their subjects, even as those same researchers perform more laboratory studies confirming their theories."
  },
  {
    "objectID": "posts/gary-klein-on-heuristics-and-biases-confirmation-bias-and-explaining-everything.html#confirmation-bias",
    "href": "posts/gary-klein-on-heuristics-and-biases-confirmation-bias-and-explaining-everything.html#confirmation-bias",
    "title": "Gary Klein on confirmation bias in heuristics and biases research, and explaining everything",
    "section": "",
    "text": "In Sources of Power: How People Make Decisions, Gary Klein writes:\n\nKahneman, Slovic, and Tversky (1982) present a range of studies showing that decision makers use a variety of heuristics, simple procedures that usually produce an answer but are not foolproof. … The research strategy was not to demonstrate how poorly we make judgments but to use these findings to uncover the cognitive processes underlying judgments of likelihood.\n…\nLola Lopes (1991) has shown that the original studies did not demonstrate biases, in the common use of the term. For example, Kahneman and Tversky (1973) used questions such as this: “Consider the letter R. Is R more likely to appear in the first position of a word or the third position of a word?” The example taps into our heuristic of availability. We have an easier time recalling words that begin with R than words with R in the third position. Most people answer that R is more likely to occur in the first position. This is incorrect. It shows how we rely on availability.\nLopes points out that examples such as the one using the letter R were carefully chosen. Of the twenty possible consonants, twelve are more common in the first position. Kahneman and Tversky (1973) used the eight that are more common in the third position. They used stimuli only where the availability heuristic would result in a wrong answer. … [I have posted some extracts of Lopes’s article here.]\nThere is an irony here. One of the primary “biases” is confirmation bias—the search for information that confirms your hypothesis even though you would learn more by searching for evidence that might disconfirm it. The confirmation bias has been shown in many laboratory studies (and has not been found in a number of studies conducted in natural settings). Yet one of the most common strategies of scientific research is to derive a prediction from a favorite theory and test it to show that it is accurate, thereby strengthening the reputation of that theory. Scientists search for confirmation all the time, even though philosophers of science, such as Karl Popper (1959), have urged scientists to try instead to disconfirm their favorite theories. Researchers working in the heuristics and biases paradigm condemn this sort of bias in their subjects, even as those same researchers perform more laboratory studies confirming their theories."
  },
  {
    "objectID": "posts/gary-klein-on-heuristics-and-biases-confirmation-bias-and-explaining-everything.html#on-explaining-everything",
    "href": "posts/gary-klein-on-heuristics-and-biases-confirmation-bias-and-explaining-everything.html#on-explaining-everything",
    "title": "Gary Klein on confirmation bias in heuristics and biases research, and explaining everything",
    "section": "On explaining everything",
    "text": "On explaining everything\nOn 3 July 1988 a missile fired from the USS Vincennes destroyed a commercial Iran Air flight taking off over the Persian gulf, killing all onboard. The crew of the Vincennes had incorrectly identified the aircraft as an attacking F-14.\nKlein writes:\n\nThe Fogarty report, the official U.S. Navy analysis of the incident, concluded that “stress, task fixation, an unconscious distortion of data may have played a major role in this incident. [Crew members] became convinced that track 4131 was an Iranian F-14 after receiving the … report of a momentary Mode II. After this report of the Mode II, [a crew member] appear[ed] to have distorted data flow in an unconscious attempt to make available evidence fit a preconceived scenario (‘Scenario fulfillment’).” This explanation seems to fit in with the idea that mental simulation can lead you down a garden path to where you try to explain away inconvenient data. Nevertheless, trained crew members are not supposed to distort unambiguous data. According to the Fogarty report, the crew members were not trying to explain away the data, as in a de minimus explanation. They were flat out distorting the numbers. This conclusion does not feel right.\nThe conclusion of the Fogarty report was echoed by some members of a five-person panel of leading decision researchers, who were invited to review the evidence and report to a congressional subcommittee. Two members of the panel specifically attributed the mistake to faulty decision making. One described how the mistake seemed to be a clear case of expectancy bias, in which a person sees what he is expecting to see, even when it departs from the actual stimulus. He cited a study by Bruner and Postman (1949) in which subjects were shown brief flashes of playing cards and asked to identify each. When cards such as the Jack of Diamonds were printed in black, subjects would still identify it as the Jack of Diamonds without noticing the distortion. The researcher concluded that the mistake about altitude seemed to match these data; subjects cannot be trusted to make accurate identifications because their expectancies get in the way.\nI have talked with this decision researcher, who explained how the whole Vincennes incident showed a Combat Information Center riddled with decision biases. That is not how I understand the incident. My reading of the Fogarty report shows a team of men struggling with an unexpected battle, trying to guess whether an F-14 is coming over to blow them out of the water, waiting until the very last moment for fear of making a mistake, hoping the pilot will heed the radio warnings, accepting the risk to their lives in order to buy some more time.\nTo consider this alleged expectancy bias more carefully, imagine what would have happened if the Vincennes had not fired and in fact had been attacked by an F-14. The Fogarty report stated that in the Persian Gulf, from June 2, 1988, to July 2, 1988, the U.S. Middle East Forces had issued 150 challenges to aircraft. Of these, it was determined that 83 percent were issued to Iranian military aircraft and only 1.3 percent to aircraft that turned out to be commercial. So we can infer that if a challenge is issued in the gulf, the odds are that the airplane is Iranian military. If we continue with our scenario, that the Vincennes had not fired and had been attacked by an F-14, the decision researchers would have still claimed that it was a dear case of bias, except this time the bias would have been to ignore the base rates, to ignore the expectancies. No one can win. If you act on expectancies and you are wrong, you are guilty of expectancy bias. If you ignore expectancies and are wrong, you are guilty of ignoring base rates and expectancies. This means that the decision bias approach explains too much (Klein, 1989). If an appeal to decision bias can explain everything after the fact, no matter what has happened, then there is no credible explanation.\n\nI’m not sure the right base rate is the proportion of aircraft challenged, but it is still an interesting point."
  },
  {
    "objectID": "posts/gandolfi-gandolfi-and-barashs-economics-as-an-evolutionary-science.html",
    "href": "posts/gandolfi-gandolfi-and-barashs-economics-as-an-evolutionary-science.html",
    "title": "Gandolfi, Gandolfi and Barash’s Economics as an Evolutionary Science",
    "section": "",
    "text": "The fundamental insight that utility in economics should be based on the concept of fitness from evolutionary biology lies at the heart of Gandolfi, Gandolfi and Barash’s Economics as an Evolutionary Science: From Utility to Fitness.\nThe first half of the book is fantastic, as the authors describe the economic way of thinking and Gary Becker’s seminal work on families, marriage and reproduction. For someone unfamiliar with Becker’s work, it is a good introduction to what Becker was setting out to achieve. The authors then take Becker’s framework and build on it to give it an evolutionary basis.\nThe authors do this by constructing a model in which people maximise their long-term fitness through balancing investments in quantity of offspring, quality of offspring and other capital investments. It is by maximising long-term intergenerational wealth, which can be converted into quantity and quality of children as required, that an agent can maximize fitness. This provides a basis for the declining fertility in modern economies, with the authors arguing that investment in quality of children is a long-term fitness maximising strategy. It is not the number of children in the next generation that matters but the ultimate number of children.\nI like this idea and approach, but ultimately, I am not convinced that it is true. To take an extreme case, what level of investment should a billionaire make in a child? Would they maximize their long-term fitness by having one or two children and bequeathing their entire fortune to them, or should they establish a harem and seek to have hundreds of children, each of whom could still have millions of dollars of investment in them. It would seem to me that the latter strategy would maximize fitness, particularly when you consider uncertainty across generations. One should reap while you can, but the rich do not appear to be doing this. There is an over investment in quality due to humans being in an environment to which they are not yet adapted.\nUltimately this is an empirical question, but the correlation between numbers of children across generations in developed countries suggests that those with a predisposition to sacrifice some quality for quantity are following the higher fitness strategy. The lack of effect of parents on child outcomes, as found through twin research, also suggests that parental investment has marginal returns at best.\nIf the current population is not optimizing fitness, the usefulness of the link between utility and fitness changes. Rather than seeking to equate each, the predispositions shaped by evolution can be used to determine the utility function, with the two differing to the extent the environment is no longer one in which those predispositions are fitness maximizing. For longer term modeling, the evolutionary system needs to be considered dynamically, as people adapt to the new environment.\nIn addition to the basic framework, the book contains chapters on sexual conflict and cooperation. While each are important considerations for the model, unfortunately they are not directly incorporated into it. In particular, it would be interesting to apply the discussion of signaling to the model, as the model does not provide an explanation for consumption above that required for survival.\nThe chapter on the evolution of cooperation treads ground covered in more detail by many other books, and unlike the earlier chapters, appears to be included for completeness rather than novelty.\nRegardless of my critique, this is an important book. The challenge is for other scholars to take the framework laid out in the book and to start to adopt it for a modern environment."
  },
  {
    "objectID": "posts/galton-trivia.html",
    "href": "posts/galton-trivia.html",
    "title": "Galton trivia",
    "section": "",
    "text": "Every time a new Francis Galton piece is published, I look forward to the Galton trivia. This time it is in an article by Steve Jones (HT: John Hawks):\n\nHe made statistical inquiries into the efficacy of prayer - he got into trouble for that for he found that those people frequently prayed for, like monarchs, lived no longer than anyone else.\n\nHe even made a beauty map of Britain, based on a secret grading of the local women on a scale from attractive to repulsive (the low point was in Aberdeen).\n\n\nJones uses Galton’s findings on height to lead into his argument that genetics has run into a problem. Geneticists are not finding specific genes for traits that we know are highly heritable.\n\nTake adult diabetes, now a major health problem, and one that certainly runs strongly in families. Genome scans reveal scores of different bits of chromosome as possible culprits but together they explain just one part in 20 of the overall inherited liability to the disease.\nThe chance of being born with a predisposition to a common illness such as diabetes or depression is a gamble with huge numbers of cards.\nSo many small cards can be shuffled that everyone who falls ill fails in their own fashion and no gene says very much about whether or not you will get the illness (although the number of cheeseburgers you eat certainly does).\n\nWhile no gene by itself is likely to say much about your chance of getting adult diabetes, Jones’s comparison between a single gene and cheeseburgers is not meaningful. We are not composed of a single gene - we have a genome. Despite genetic studies not finding the “missing heritability”, that heritability still exists. A more useful comparison might be whether family history is a better predictor than the number of cheeseburgers eaten. Or whether your twin has adult onset diabetes.\nThe other interesting thing about examples like this is that the environmental factor would have a genetic component. A host of heritable traits are likely to affect whether one eats a lot of cheeseburgers. This is one of the reasons why it is not easy to link gene X to a specific outcome."
  },
  {
    "objectID": "posts/galbraith-on-evolution-and-the-invisible-hand.html",
    "href": "posts/galbraith-on-evolution-and-the-invisible-hand.html",
    "title": "Galbraith on evolution and the invisible hand",
    "section": "",
    "text": "Paul Krugman’s oft-quoted critique of Stephen Jay Gould is one of the more brutal dismissals of his work (it is from a 1996 speech on what economists can learn from evolutionary theorists):\n\nNow it is not very hard to find out, if you spend a little while reading in evolution, that Gould is the John Kenneth Galbraith of his subject. That is, he is a wonderful writer who is beloved by literary intellectuals and lionized by the media because he does not use algebra or difficult jargon. Unfortunately, it appears that he avoids these sins not because he has transcended his colleagues but because he does not seem to understand what they have to say; and his own descriptions of what the field is about - not just the answers, but even the questions - are consistently misleading.\n\nWhile this statement is usually quoted in reference to Gould, it is also a blunt assessment of John Kenneth Galbraith’s work. I have not read much Galbraith, despite having The Affluent Society on my reading list for a while, so I am not in a position to judge Krugman’s comparison. However, I recently came across an article written by Galbraith in which he considered the invisible hand metaphor from the perspective of Darwinism. He stated that belief in the invisible hand was like belief in intelligent design, under which evolution is guided by an intelligent designer and is not the result of unguided natural selection. Galbraith writes:\n\nSmith’s Creator did not interfere. He simply wrote the laws and left them for events to demonstrate and man to discover. The greatest American economist, Thorstein Veblen, observed that “the guidance of…the invisible hand takes place…through a comprehensive scheme of contrivances established from the beginning.” What is this if not Intelligent Design?\nBut to Veblen this was, precisely, unscientific. And so he made a mighty effort back in 1898 to move economics into the Darwinian age. In a magnificent essay entitled “Why Is Economics Not an Evolutionary Science?” Veblen pointed out the problems of classical economics: too much preoccupied with classification schemes and higher purposes, too little with material process and “cumulative or unfolding sequence.” Economics could become a science, but only if it detached itself from the idea that change intrinsically led to improvement.\n\nIt is an interesting comparison but  Galbraith has lined up the wrong target. He is right that evolution provides a critique of the interpretation of the invisible hand that voluntary interactions between people always result in the optimal social outcome. While voluntary exchange is beneficial for the individual participants, evolution shows us that there is no mechanism to make sure societal benefit is maximised – there is no intelligent designer. With wasteful signalling, winner takes all contests and the potential for sub-optimal equilibria, an emergent economy might be full of waste and inefficient competition.\nHowever, Smith did not state that emergent outcomes were always positive and he recognised a score of ways in which market interactions could lead to sub-optimal outcomes. The invisible hand is an excellent metaphor for the emergent phenomena whereby “selfish” actions by individuals lead to outcomes that they do not intend. They are generally welfare enhancing (which is one reason I lean libertarian) but not necessarily so. Instead of criticizing the invisible hand metaphor, Galbraith should have used the intelligent design comparison to argue that there is no mechanism to make sure that emergent economic outcomes are positive. (Of course, this does not mean government should step in – you still need to show that government can fix the problem without creating other worse problems.)\nIn attacking neo-classical economics, Galbraith also raises the important issue of variation:\n\nEconomists still don’t understand variation; instead they write maddeningly about “representative agents” and “rational economic man.” They still teach the “marginal product theory of wages,” which excuses every gross inequality faced by the laboring poor. Alan Greenspan even recently resurrected the idea of a “natural rate of interest” to justify raising rates, though that doctrine had been extinct for 70 years. Economists still ignore the diversity of actual economic and social life.\n\nIgnoring the specific examples that Galbraith has used (I don’t understand how they line up with his points), natural selection operates on variation. If every firm or agent is the same, you cannot have firm failure or creative destruction. There cannot be comparative advantage and the benefits of specialisation are diminished. Outside of evolutionary economics, few economic models try to capture this. Despite having some trouble getting to grips with Arnold Kling’s “patterns of sustainable specialisation and trade” (I’m still short on details, and am not convinced by many of his examples), I appreciate how it introduces the ideas of variation, exploration and failure. Entrepreneurs vary in their ideas and how they search for them."
  },
  {
    "objectID": "posts/fukuyamas-biological-approach.html",
    "href": "posts/fukuyamas-biological-approach.html",
    "title": "Fukuyama’s biological approach",
    "section": "",
    "text": "I have started reading Francis Fukuyama’s The Origins of Political Order and am enjoying his starting point of human prehistory. I will write a full review when I have finished, but in the meantime, some of Fukuyama’s initial observations are worth noting. In particular, he takes biology to be the foundation of our understanding of political development.\n\nTo understand this, then, we need to go back to the state of nature and to human biology, which in some sense sets the framework for the whole of human politics. Biology presents a certain degree of solid ground resting below the turtles at the bottom of the stack, though even biology, as we will see in the next chapter, is not an entirely fixed point.\n…\nThis knowledge exists in several distinct domains, including primatology, population genetics, archaeology, social anthropology, and, of course, the overarching framework of evolutionary biology. … The recovery of human nature by modern biology, in any case, is extremely important as a foundation for any theory of political development, because it provides us with the basic building blocks by which we can understand the later evolution of human institutions.\n\nWhile Fukuyama starts with the biological basis, he adopts a Jared Diamond-esque approach to differences in political development:\n\nBiology gives us the building blocks of political development. Human nature is largely constant across different societies. The huge variance in political forms that we see both at the present time and over the course of history is in the first instance the product of variance in the physical environments that human beings came to inhabit. As societies ramify and fill different environmental niches across the globe, they develop distinctive norms and ideas in a process known as specific evolution. Groups of humans also interact with each other, and this interaction is as much a driver of change as is the physical environment.\n\nAs I have posted about before, I am not averse to a Jared Diamond style argument for developmental differences being triggered by environmental variation. However, this is not to say that human nature is from that point static.\nFukuyama’s analysis of the basis of the philosophies of Rousseau, Hobbes and Locke is also interesting. By acknowledging that humans are social and competitive animals, their philosophical positions can be argued to have a very weak foundation. Fukuyama writes:\n\nHuman beings and chimpanzees were both descended from an ancestral ape, and both modern chimpanzees and human beings, especially those living in hunter-gatherer or other relatively primitive societies, display similar forms of social behavior. For the account of the state of nature given by Hobbes, Locke, or Rousseau to be correct, we would have to postulate that in the course of evolving into modern humans, our ape ancestors somehow momentarily lost their social behaviors and emotions, and then evolved them a second time at a somewhat later stage in development. It is much more plausible to assume that human beings never existed as isolated individuals, and that social bonding into kin-based groups was part of their behavior from before the time that modern humans existed. Human sociability is not a historical or cultural acquisition, but something hardwired into human nature."
  },
  {
    "objectID": "posts/franks-the-darwin-economy.html",
    "href": "posts/franks-the-darwin-economy.html",
    "title": "Robert Frank’s The Darwin Economy",
    "section": "",
    "text": "Adam Smith’s invisible hand metaphor is one of the most powerful ideas in economics. Individual action, even in the pursuit of pure self-interest, can serve the interests of others.\nCharles Darwin’s evolution by natural selection is an even more powerful idea in the world of evolutionary biology. In Darwin’s world, individual actions can be at the expense of others, as competition for survival or mates can leave others out in the cold.\nRobert Frank’s new book The Darwin Economy: Liberty, Competition, and the Common Good has at its core the fundamental insight that many aspects of the economy are Darwinian, not Smithian. Individual action can harm the interests of others, because, as per evolutionary competition, outcomes are relative. People care about rank and many goods are positional. Only so many people can have beach views or send their children to an above average school.\nA recurring example that Frank uses through the book is work safety regulation. In the neoclassical economic world, workers trade-off safety for wages and assort according to their preferences for safety and wages. Frank argues that this understanding is incorrect, as relative wages also matter. To send your children to the best schools, you need to live in the best neighbourhood, which means that you need to earn more than most other people. As a result, workers will compete away the safety they otherwise value to obtain the income to live in the best neighbourhood. The result is low or no safety protection provided by employers, increased house prices, and the workers are worse off than they were before.\nA strong point of Frank’s argument is that he grounds markets and human motivation in evolution. People are products of evolution by natural selection, with reproductive success (and the factors underlying it) a relative concept. Those at the top of the pile were winners and are our ancestors. The motivation to have the highest status, wealth and power is still present in people today. The Darwinian framework is used by Frank to identify whether a good is positional. For example, who survives or not is governed more by relative income than leisure, so income should be more positional than leisure. There will be excessive competition for income.\nWhile I appreciate this evolutionary foundation, I question whether Frank has gone far enough in his use of the Darwinian framework. In one sense, the economy must be Darwinian as humans are animals subject to natural selection. We are shaped by and continue to be shaped by it. Is economics just a branch of ecology? Yet Frank does not take this step.\nFrank’s argument is a a strong critique of the neoclassical view of the market, and unlike many liberal critiques, does not rely on arguments about market imperfections, dominant powers, information asymmetries or irrationality. In Frank’s world, people are acting rationally and markets are highly competitive, but there is a disparity between individual and collective interests. Frank takes his fight to libertarians (his bad guy of choice) on their own turf.\nBy doing away with many of the liberal critiques of markets, Frank also eschews some typical liberal solutions. Simple nudges won’t solve these problems, nor will providing more information. Each individual is acting rationally and further information or guidance will not change that fundamental self-interest. Stronger measures are required.\nOne primary policy suggestion is agreed restraint. If everyone agreed to certain minimum safety standards, which Frank suggests the majority of people support, you won’t get a race to the bottom and the excess wage earned that would be earned in the absence of safety is not simply frittered away in a competition for positional goods. While this argument has merit, Frank creates a libertarian straw man that he beats to death, instead of seriously confronting the libertarian arguments about safety regulation. Can government determine the right level of safety? Does it bend to interest groups and companies trying to create barriers to entry? Or does government simply lack the information required to make decisions?\nAnother leg of his policy recommendations is a progressive consumption tax, which he has advocated for some years now. By taxing consumption, competitive expenditure would be curtailed, with incentives to invest and save increased. Taxing consumption and not productive activities such as labour makes sense, although I don’t share Frank’s implicit concern that government is being starved of money.\nOne of Frank’s more interesting arguments is that as high rank has a value in itself, rank is already implicitly priced in the labour market. More productive people generally do not get paid commensurate with their relative level of productivity compared to their workmates, which Frank suggests is because people are willing to be paid less if they have high rank. In other words, the labour market has implicit progressive taxation. But outside of the workplace, we compete with friends, family and neighbours, with no implicit market for rank. Frank suggests that as implicit progressive taxation is the market solution in the labour market, we should carry out progressive taxation of rank in the broader world. This is another benefit of a progressive consumption tax.\nThe latter two-thirds of the book are less focused on Frank’s Darwinian insight, but rather on some fundamental economic concepts. His chapters on efficiency and willingness to pay give excellent background on the topics for the non-economist. The chapter on Coase’s theorem might even teach something to economists. However, I might have preferred Frank to have used the space to discuss the potential critiques to his policy recommendations tailored for the Darwin economy. While I agree with his general insight, his straw man response to the libertarian position did not lay the framework needed to support his solutions.\nHaving said that, The Darwin Economy provides an important argument that must be addressed by any libertarian. As Adam Smith knew, competition does not always maximise the common good. Any coherent policy framework must deal with that fact."
  },
  {
    "objectID": "posts/franks-luxury-fever.html",
    "href": "posts/franks-luxury-fever.html",
    "title": "Frank’s Luxury Fever",
    "section": "",
    "text": "Following my reading of Robert Frank’s The Darwin Economy, I decided to read some of Frank’s back catalogue. I started with Luxury Fever: Weighing the Cost of Excess, which was first released in 1999.\nI quickly realised that The Darwin Economy is not so much a new book, but rather a refinement of Luxury Fever, updated for the events of the last 10 years and with a few of the weaker arguments replaced or improved.\nTake the central theme of both books - that people over-consume goods where there is competition for relative rank. In Luxury Fever, Frank contrasts conspicuous and inconspicuous consumption, and argues that conspicuous consumption triggers an arms race in that sphere. In The Darwin Economy, the focus is more on positional and non-positional goods. This is a sensible change, as many positional goods, such as housing in a good school district, may not be chosen purely for conspicuous consumption purposes.\nLuxury Fever also has many of the same patterns of The Darwin Economy, such as the tendency to include chapters that do not quite fit the theme but reflect Frank’s broader philosophy. The chapter suggesting a program of public employment instead of unemployment benefits is one of them. I noted before that Frank could have made his core point in The Darwin Economy in an essay, and after reading Luxury Fever, my view on this has been confirmed.\nLuxury Fever even contains the insight that underpins the theme of The Darwin Economy - that the description of competition by Charles Darwin may be more accurate in many instances than that of Adam Smith (or more precisely, many modern interpretations of Smith).\nI expect that if I had read Luxury Fever 10 years ago, it would have been a revelation to me. Today, I suggest that you go for the new and improved."
  },
  {
    "objectID": "posts/four-perspectives-on-human-decision-making.html",
    "href": "posts/four-perspectives-on-human-decision-making.html",
    "title": "Four perspectives on human decision making",
    "section": "",
    "text": "I have been rereading Gerd Gigerenzer’s collection of essays Rationality for Mortals: How People Cope with Uncertainty. It covers most of Gigerenzer’s typical turf - ecological rationality, heuristics that make us smart, understanding risk and so on.\nIn the first essay, Gigerenzer provides four categories of approaches to analysing decision making - unbounded rationality, optimisation under constraints, cognitive illusions (heuristics and biases) and ecological rationality. At the end of this post, I’ll propose a fifth.\n1. Unbounded rationality\nUnbounded rationality is the territory of neoclassical economics. Omniscient and omnipotent people optimise. They are omniscient in that they can see the future - or at least live in a world of risk where they can assign probabilities. They are omnipotent in that they have all the calculating power they need to make perfect decisions. And with that foresight and power, they make optimal decisions.\nPossibly the most important point about this model is that it is not designed to describe precisely how people make decisions, but rather to predict behaviour. And in many dimensions, it does quite well.\n2. Optimisation under constraints\nIn this approach, people are no longer omniscient. They need to search for information. As Gigerenzer points out, however, this attempt to inject realism creates another problem. Optimisation with constraints can be even harder to solve than optimisation with unbounded rationality. As a result, the cognitive power required is even greater.\nGigerenzer is adamant that optimisation under constraints is not bounded rationality - and if we use Herbert Simon’s definition of the term, I would agree - but analysis of this type commonly attracts the “boundedly rational” label. Gigerenzer’s does not want the unrealistic nature of optimisation under constraints to tar the concept of bounded rationality.\n3. Cognitive illusions - logical irrationality\nThe next category is the approach in much of  the behavioural sciences and behavioural economics. It is often labelled as the “heuristics and biases” program. This program looks to understand the processes under which people make judgements, and in many cases, seeks to show errors of judgment or cognitive illusions. This program has generated a long list of biases - just look at the Wikipedia page for a taste.\nGigerenzer picks two main shortcomings of this approach. First, although the program successfully shows failures of logic, it does not look at the underlying norms. Second, it tends not to produce testable theories of heuristics. As Gigerenzer states, “mere verbal labels for heuristics can be used post hoc to”explain” almost everything.”\nAn example is analysis of overconfidence bias. People are asked a question such as “Which city is farther north - New York or Rome?”, and asked to give their confidence that their answer is correct. When participants are 100 per cent certain of the answer, less than 100 per cent tend to be correct. That pattern of apparent overconfidence continues through lower probabilities.\nThere are several critiques of this analysis, but one of the common suggestions is that people are presented with questions that are unrepresentative of a typical sample. People typically use alternative cues to answer a question such as the above. In the case of latitude, temperature is a plausible cue. The overconfidence bias occurs because the selected cities are a biased sample where the cue fails more often than expected. If the cities are randomly sampled from the real world, the overconfidence disappears. The net result is that what appears to be a bias may be better explained by the nature of the environment in which the decision is made.\n4. Ecological rationality\nEcological rationality departs from the heuristics and biases program by examining the relationship between mind and environment, rather than the mind and logic. Human behaviour is shaped by scissors with two blades - the cognitive capabilities of the actor, and the environment. You cannot understand human behaviour without understanding both the capabilities of the decision maker and the environment in which those capabilities are exercised. Gigerenzer would apply the bounded rationality label to this work.\nOn this basis, there are three goals to the ecological rationality program. The first is to understand the adaptive toolbox - the heuristics of the decision maker and their building blocks. The second is to understand the environmental structures in which different heuristics are successful. The third is to use this analysis to improve decision making through designing better heuristics or changing the environment. This can only be done once you understand the adaptive toolbox and the environments in which different tools are successful.\nGigerenzer provides a neat example of how the ecological rationality departs from the heuristics and biases program in its analysis of a problem - in this case, optimal asset allocation. Harry Markowitz, who received a Nobel Memorial Prize in Economics for his work on optimal asset allocation, did not use the results of his analysis in his own investing. Instead, he invested his money using the 1/N rule - spread your assets equally across N assets.\nThe heuristics and biases program might look at this behaviour and note Markowitz is not following the optimal behaviour determined by himself. He is making important decisions without using all the available information. Perhaps it is due to cognitive limitations?\nAs Gigerenzer notes, optimisation is not always the best solution. Where the problem is computationally intractable or the optimisation solution lacks robustness due to estimation errors, heuristics may outperform. In the case of asset allocation, Gigerenzer notes work showing that 500 years of data would have been required for Markowitz’s optimisation rule to outperform his practice of 1/N. In a world of uncertainty, it can be beneficial to leave information on the table. Markowitz was using a simple heuristic for an important decision, but rightfully so as it is superior for the environment in which he is making the decision.\n5. Evolutionary rationality\nGigerenzer proposes four categories, but I’ll lay out a fifth (I’m not sure about the label I’ve just given it). Evolutionary rationality develops a deeper understanding of the cognitive capabilities of the decision maker through an analysis of the adaptive basis of traits. This perspective could inform all four of the above categories of decision making. It could be used to assess what is being optimised, what the constraints might be, how biases might be due to mismatch between past and present environments, and what the heuristics are.\nGigerenzer notes the possibility of going into this territory, but deliberately holds back. In the third chapter of the book, he writes:\n\n[H]uman psychologists are not able to utilize many of the lines of evidence that biologists apply to justify that a trait is adaptive. We can make only informed guesses about the environment in which the novel features of human brains evolved, and because most of us grow up in an environment very different to this, the cognitive traits we exhibit might not even have been expressed when our brains were evolving. …\nABC avoids the difficult issue of demonstrating adaptation in humans by defining ecological rationality as the performance, in terms of a given currency, of a given heuristic in a given environment. We emphasize that currency and environment have to be specified before the ecological rationality of a heuristic can be determined; thus, take-the-best is more ecologically rational (both more accurate and frugal) than tallying in noncompensatory environments but not more accurate in compensatory ones. Unlike claiming that a heuristic is an adaptation, a claim that it is ecologically rational deliberately omits any implication that this is why the trait originally evolved, or has current value to the organism, or that either heuristic or environment occurs for real in the present or past. Ecological rationality might then be useful as a term indicating a more attainable intermediate step on the path to a demonstration of adaptation.\n\nThere is a lot more interesting material in Chapter 3 on the link between Gigerenzer’s program and the approach taken by biologists. That will be the subject of a later post."
  },
  {
    "objectID": "posts/fogel-and-supersized-humans.html",
    "href": "posts/fogel-and-supersized-humans.html",
    "title": "Fogel and supersized humans",
    "section": "",
    "text": "Last week, the New York Times ran a profile of economist Robert Fogel in anticipation of the release of the book The Changing Body: Health, Nutrition, and Human Development in the Western World since 1700, of which Fogel is a co-author. During his career, Fogel and his colleagues have amassed a mound of evidence on the shape and size of the human body and how this has changed over the last few hundred years. I have not read much of Fogel’s work before, but the book looks like it is worth a look.\nThe key theme from Fogel’s data is that there have been significant gains in height and mass as countries develop. As the Times states:\n\nTo take just a few examples, the average adult man in 1850 in America stood about 5 feet 7 inches and weighed about 146 pounds; someone born then was expected to live until about 45. In the 1980s the typical man in his early 30s was about 5 feet 10 inches tall, weighed about 174 pounds and was likely to pass his 75th birthday.\nAcross the Atlantic, at the time of the French Revolution, a 30-something Frenchman weighed about 110 pounds, compared with 170 pounds now. And in Norway an average 22-year-old man was about 5 ½ inches taller at the end of the 20th century (5 feet 10.7 inches) than in the middle of the 18th century (5 feet 5.2 inches).\n\nThis rate of change over the last few hundred years is much higher than that of the previous millennia.\nThe Times talks of how “technology has sped human evolution”, although this is not evolution as biologists would call it. Fogel infers no change in underlying genotype. Rather, Fogel calls it “technophysio evolution”, and it provides one of the strongest illustrations of environmental effects on phenotype. Technophysio evolution includes changes due to the womb environment, so might be considered to include epigenetics.\nHaving said that, this is another area where we should not ignore evolution in the biological or genetic sense. As much of Fogel’s work details, Americans around the time of the civil war were surprisingly sick, and were often ill from a young age. While improvements in environment have been responsible for much (or most) of the improvement in health and increase in body stature, there would also have been strong selection pressures on people at this time. While I am not hugely familiar with the literature, I would expect that there would be some differences in fertility by stature (as John Hawks suggests here). Three hundred years is enough time for genetic changes to have occurred.\nGenetic considerations are also relevant to some of the debates about the meaning of Fogel’s results. For example, the Times quotes Angus Deaton, who is sceptical about some of the conclusions on height, and in particular, he questions whether they should be attributed mainly to nutrition. Deaton states that:\n\nWe don’t really understand why African adults and children are so much taller than Indian adults and children, but it can’t be their income, because Indians are much richer.\n\nIs this missing the obvious genetic explanation?\nHaving said that we should not ignore evolutionary factors, stature information is still a useful indicator of development. If height in a developing country is stagnant or decreasing, it is a solid signal that living conditions are not improving. But, if height differences between countries persist despite income increases, consideration of genetics might allow us to stop worrying that everyone is not the same height."
  },
  {
    "objectID": "posts/fitness-spreading.html",
    "href": "posts/fitness-spreading.html",
    "title": "Fitness spreading",
    "section": "",
    "text": "One of the issues at the core of my research is the speed of human evolution, particularly over the last 10,000 years. There are several potential arguments to suggest that the speed of human evolution is increasing, such as a larger population (creating a larger source of mutations) and the huge changes in environment that humans have experienced.\nOne of the obstacles to this argument is the largely monogamous nature of marriages, especially in more recent times and developed countries. If each man pairs with a single woman, and there are roughly equal numbers of each, inability to pair will no longer be a significant evolutionary factor. However, it is possible to argue that sexual selection can play a role even if there is pure monogamy (i.e. assuming no infidelity or serial monogamy).\nSuppose that males and females can each be ranked in order of fitness - that is, their probability of surviving to adulthood. When it is time to match, the highest fitness male and female will pair off. The second highest ranked of each sex would like to pair with the highest ranked of the opposite sex, but given the highest ranked is already paired off, the two second highest ranked settle for each other. This continues down the rankings until the lowest ranked of each sex pair.\nNow consider the offspring of these pairs. The offspring from the highest ranked pair will have the highest fitness. In fact, if you wanted to produce an offspring of the highest possible fitness, you would pair these two individuals. Similarly, if you aimed to produce the lowest possible fitness offspring, you would have done so by matching the lowest ranked pair.\nThe net result of this process is that the offspring in the population will have the largest possible range of fitness. Fitness matching concentrates harmful mutations in low fitness babies. With this large range of fitness, natural selection is faster, with the lowest fitness offspring least likely to survive and helpful genes concentrated in the high fitness offspring.\nThe result of this fitness spreading process is that sexual selection provides a platform for natural selection to have an increasing effect. Even in a perfectly monogamous society, sexual selection can still be a force."
  },
  {
    "objectID": "posts/finding-taxis-on-rainy-days.html",
    "href": "posts/finding-taxis-on-rainy-days.html",
    "title": "Finding taxis on rainy days",
    "section": "",
    "text": "A classic story on the play-list of many behavioural economics presentations is why you can’t find taxis on rainy days. The story is based on the idea that taxi drivers work to an income target. If driver wages are high due to high demand for taxis, such as when it rains, they will reach their income target earlier and go home for the day. The result is you can’t find a taxi when you need one most.\nThe story is such a favourite as it conflicts with conventional economic wisdom that people are maximisers who respond positively to incentives such as higher wages. Instead, drivers are satisficers who quit work for the day once have hit their target, even though the high wages would allow them to earn more than normal.\nThis story originates from a 1997 article by Colin Camerer and friends (I suggest following Camerer on twitter). They analysed taxi trips in New York and found that as wages went up, labour supply (taxis on the street) goes down. Their preferred explanation, based on what some drivers said, was that taxi drivers work to a daily income target. Their article did not include the reference to the rain, but it has become the way the story is traditionally told.\nBut, a new study suggests this negative relationship between wages and supply might not generally be the case for New York taxi drivers. Using a much bigger dataset of New York taxi driver activities, Henry Farber has found that, as standard economic theory would suggest, taxi drivers drive more when they can earn more. There was no evidence of income targeting in the data.\nAs another blow to the rainy day story, Farber also found that taxi drivers didn’t earn more when it was raining. As traffic was worse and they travelled less distance, their earnings didn’t increase despite the higher demand. There were less taxis on the street when it was raining, but this must be due to causes such as drivers preferring not to work when traffic is bad.\nSo how do we reconcile these conflicting findings? A starting point is in the original study. In a show of humility, Camerer and colleagues were open to the idea that their result might not be robust. They close with the following paragraph:\n\nBecause evidence of negative labor supply responses to transitory wage changes is so much at odds with conventional economic wisdom, these results should be treated with caution. Further analyses need to be conducted with other data sets (as in Mulligan [1995]) before reaching the conclusion that negative wage elasticities are more than an artifact of measurement or the special circumstances of cabdrivers. If replicated in further analyses, however, evidence of negative wage elasticities calls into question the validity of the life-cycle approach to labor supply.\n\nTo use the cliché, more research is required. And there has been a lot more research since Camerer and friends’ had their study published. While I’ve pitched the story as a new paper tearing up an almost 20-year old favourite, there has been a sequence of papers over the years with both supporting and conflicting results, including by Farber.\nFarber’s explanation for the result in his latest paper is that he had access to a larger dataset - five million shifts compared to a few thousand in Camerer and friends’ or Farber’s earlier studies. Technological progress in recording taxi data also allowed Farber’s work to be at a much finer level of detail than was possible at the time of the original study. Other studies also had small datasets or used less reliable data such as from surveys (such as this one from Singapore), but there have also been at least one involving similarly large sets of taxi data that did find a negative relationship (such as a second from Singapore, although in that case the negative relationship seemed of too low a magnitude to support income targeting).\nAnother explanation might lie in the methodological battle about how you should measure the relationship between wages and supply for taxi drivers. Farber’s 2005 paper picked apart the original methodology, particularly around their assumptions on wages, and he chose a different approach based on drivers deciding whether to continue or not at the end of each ride. When I previously invested some time to understand it, I found Farber’s critique reasonably persuasive. However, I haven’t taken the time to understand the finer points of Farber’s new analysis and to what extent methodology determines the result, so it will be interesting to see some responses to this latest salvo.\nAnother potential distinction is that Camerer and friends’ original study was able to distinguish between owner-operators and employee drivers, each of which face different incentives. Farber wasn’t able to tease the two apart. However, Camerer and friends found a negative relationship for both groups, so at a minimum, Farber’s work suggests that the finding would not hold across both. Farber did consider whether there might be many different types of driver, which there may be. But if the satisficers do exist, there are not many of them.\nOn a brighter note, there is some hope that we will be better able to catch a taxi on rainy days in the future. With current taxi regulation and fixed pricing, the inconvenience of driving in bad traffic results in less taxis on the road. But with new entrants such as Uber able to charge more and adjust pricing at times of high demand, we might actually get more taxis or other vehicles on the road when we need them most. And we can have some comfort that when those taxis are needed most, there will be plenty of maximisers around to fill our need."
  },
  {
    "objectID": "posts/fertility-is-going-to-go-up.html",
    "href": "posts/fertility-is-going-to-go-up.html",
    "title": "Fertility is going to go up",
    "section": "",
    "text": "In my latest working paper, co-authored with Oliver Richards, we argue that recent fertility increases in developed countries may only be the beginning. From the abstract:\n\nWe propose that the recent rise in the fertility rate in developed countries is the beginning of a broad-based increase in fertility towards above-replacement levels. Environmental shocks that reduced fertility over the past 200 years changed the composition of fertility-related traits in the population and temporarily raised fertility heritability. As those with higher fertility are selected for, the “high-fertility” genotypes are expected to come to dominate the population, causing the fertility rate to return to its pre-shock level. We show that even with relatively low levels of genetically based variation in fertility, there can be a rapid return to a high-fertility state, with recovery to above-replacement levels usually occurring within a few generations. In the longer term, this implies that the proportion of elderly in the population will be lower than projected, reducing the fiscal burden of ageing on developed world governments. However, the rise in the fertility rate increases the population size and proportion of dependent young, presenting other fiscal and policy challenges.\n\nWe’re certainly not the first to hint at the idea that selection of high fertility individuals will increase fertility. Fisher noted the power of higher fertility groups in The Genetical Theory of Natural Selection. I’ve seen Razib Kahn, Robin Hanson and John Hawks mention the idea in blog posts. There is one great paper by Murphy and Wang (which I will blog about soon) that has part of this argument buried in the micro-simulation. Many papers on the heritability of fertility hint at it. Rowthorn’s paper on fertility and religiosity also points in this direction. But what we couldn’t find was someone who sought to tie down the idea - particularly in the way we have.\nI actually thought a paper of this nature would already be written. We were interested in the economic implication of the argument, but because there was no clear statement of the evolutionary foundations that we could use in the way we wanted, we decided to build our argument from the ground up. We’re hoping that this working paper receives some solid critique that will allow us to decide whether our angle of attack is useful or can be improved. We have constructed three basic genetic models, but are they useful? Are there better alternatives? Once we address those questions, we have some ideas for empirical tests and we hope to use the concept in some more detailed economic and cultural analysis. Ultimately, this paper will need to be tied in with a large and growing literature on the biosocial basis of fertility.\nI’m the first to admit we could be wrong in the prediction of a fertility increase. What other shocks are still to come? Will the continually changing environment drown out the underlying evolutionary dynamics? Our instinct is that most of the shocks that can affect fertility have played out in the developed world - increased incomes, effective contraception, female choice and so on. But what further shocks could reduce fertility?\nIn presenting this paper, we tend to receive  two major classes of response. The first and obvious question is whether these dynamics will play out in time frames that matter. As its been 200 years since some populations underwent the demographic transition, there has  been enough time for selection to have occurred on a trait as important to fitness as fertility (in some populations we have evidence of this). The more interesting question is what will be the magnitude of the effect over the next 50 or 100 years? I’m not sure of the answer to this, but even a small total fertility rate increase of 0.1 children per female can have material effects on population size and structure.\nThe other response we tend to receive is that fertility is affected by policy, incentives, female opportunities and so on. Any trend we see today is a response to those factors. And that may be true. But to the extent there is variation in the response to the policy, incentives or opportunities and there is a genetic basis to that variation, we can see selection of those with higher fertility.\nHaving said that, throughout the paper we are deliberately agnostic about the merits of the various theories of what has caused the fertility decline in the developed world to date. For the purposes of our hypothesis, it is sufficient to know that there was a decline in fertility and that variation in fertility is heritable after the decline. As the first law of behaviour genetics is that all human behaviour is heritable, its not a very high bar to clear. It would be difficult if it was otherwise, particularly when you consider the raft of current theories. And even if you believe a certain factor is behind the decline, what is the causative pathway? As an example, consider the spread of the pill and the factors which are relevant to it reducing fertility. First, there is desire of someone with access to the pill to have children. Then there is their desire to take the pill to control pregnancy. Do they take the pill as instructed (possibly related to conscientiousness)? Is the pill physiologically effective? Do they experience side-effects that deter continued use? Variation along any of those dimensions would affect fertility.\nThe biggest simplification in the way we present our models is that, unlike our models, developed countries did not receive a single fertility shock across the population. Rather, multiple shocks hit different parts of the population at different times. This is why fertility has generally declined for much of the last 200 years, rather than suddenly suffering a single large drop. Of note, fertility tended to decrease among the wealthy first. As our framework would suggest that fertility rates will increase first among groups that experienced the shock earlier, we would predict that groups with a history of higher socioeconomic status will tend to increase their fertility rates earlier.\nImmigration also presents some interesting issues. Immigrants tend to have higher fertility when arriving in a country that has undergone the demographic transition. But our framework would suggest that following generations will experience a decline as they undergo the fertility shock. To the extent that the immigrant population has not been exposed to the shock before, their fertility may decline and recover later than native populations.\nThese issues offer some basis for testing the hypothesis. But first, we’re keen to nail down some good ways of thinking about the problem. The working paper and the models within are part of that process. So, if you have any thoughts or criticisms, we’d be grateful to hear them."
  },
  {
    "objectID": "posts/ferguson-on-malthus.html",
    "href": "posts/ferguson-on-malthus.html",
    "title": "Ferguson on Malthus",
    "section": "",
    "text": "Last week I came across a 2007 article by Niall Ferguson on increasing food prices and the potential for future shortages.  Leaving aside Ferguson’s predictions of the return of Malthusian misery, he makes an important and often forgotten point about what Malthus described in his An Essay on the Principle of Population. Ferguson writes:\n\nMalthus concluded from this inexorable divergence between population and food supply that there must be “a strong and constantly operating check on population”.\nThis would take two forms: “misery” (famines and epidemics) and “vice”, by which he meant not only alcohol abuse but also contraception and abortion (he was, after all, an ordained Anglican minister).\n“The vices of mankind are active and able ministers of depopulation,” wrote Malthus in an especially doleful passage of the first edition of his Essay. “They are the precursors in the great army of destruction; and often finish the dreadful work themselves.\n“But should they fail in this war of extermination, sickly seasons, epidemics, pestilence, and plague advance in terrific array, and sweep off their thousands and tens of thousands. Should success be still incomplete, gigantic inevitable famine stalks in the rear, and with one mighty blow levels the population with the food of the world.”\n……\n[V]ice and misery have been operating just as Malthus foresaw to prevent the human population from exploding geometrically.\nOn the one hand, contraception and abortion have been employed to reduce family sizes. On the other hand, wars, epidemics, disasters and famines have significantly increased mortality.\nTogether, vice and misery have ensured that the global population has grown at an arithmetic rather than a geometric rate. Indeed, they’ve managed to reduce the rate of population growth from 2.2 per cent per annum in the early Sixties to around 1.1 per cent today.\n\nWhile I am not sure that Ferguson’s statement on increased mortality is correct, developed countries have experienced a preventative fertility check in the form of active birth control. What would the developed world look like without that check? I expect population advocates such as Julian Simon would argue that we’d be richer, Malthusians the opposite.\nIn A Farewell to Alms, Gregory Clark made a similar point where he noted that higher living standards in many Pacific islands (relative to pre-Industrial Revolution England) was due to practices such as infanticide. When considering the Malthusian model, we should note both preventative and positive fertility checks.\nThe question that always interests me about preventative checks is how long the “vice” restrictions on population can operate. As the parents of the next generation are those immune to the vices, will the vices provide only a temporary check?"
  },
  {
    "objectID": "posts/family-friendly-backfires.html",
    "href": "posts/family-friendly-backfires.html",
    "title": "Family friendly backfires",
    "section": "",
    "text": "Last month a NYT article by Claire Cain Miller documented some of the backfires associated with family friendly policies. For instance:\n\nUnlike many countries, the United States has few federal policies for working parents. One is the Family and Medical Leave Act of 1993, which provides workers at companies of a certain size with 12 weeks of unpaid leave.\nWomen are 5 percent more likely to remain employed but 8 percent less likely to get promotions than they were before it became law, according to an unpublished new study by Mallika Thomas, who will be an assistant professor of economics at Cornell University. …\nThe child-care law in Chile, the most recent version of which went into effect in 2009, was intended to increase the percentage of women who work, which is below 50 percent, among the lowest rates in Latin America. It requires that companies with 20 or more female workers provide and pay for child care for women with children under 2, in a location nearby where the women can go to feed them.\nIt eases the transition back to work and helps children’s development, said María F. Prada, an economist at the Inter-American Development Bank and lead author of a new study on the effects of the law. But it has also led to a decline in women’s starting salaries of between 9 percent and 20 percent.\n\nI am not sure there exists a family friendly policy that doesn’t “backfire” in some dimension. These policies tend to have multiple objectives and there are trade-offs between these objectives. That holds even if these policies are publicly funded and place no burden on employers.\nFirst, those objectives. Policy makers want women to be able to have children while having a career. They are pronatalist. They want women (and sometimes men) to be able to take time out of the workforce to care for their children. They want high-quality care for children. And they want men and women to have the same level of pay. You can’t have them all.\nConsider this trade-off in the light of the AER article by Claudia Goldin suggesting gender pay disparities are because women find it difficult  to work the long hours many jobs now require. How can you facilitate those long hours?\nOne option is to discourage having a family, which tends not to be the preferred route (unless you are a tech company offering to subsidise egg freezing services).\nAlternatively, you can provide subsidised or free childcare, which will reduce the cost of having a family and enable the mother to return to work faster. But this also increases the attractiveness of having a family, which increases the number of children and constrains potential hours worked. And even with free childcare, the mother will likely take some time off.\nAlternatively, take Paul Seabright’s argument in War of the Sexes that much of the wage gap is due to gender differences in networks. When you take time out of the workforce, your networks suffer. Women are more likely to take time out of the workforce.\nOne of Seabright’s ideas to counter this is compulsory paternity leave. Gender neutral parental leave arrangements are also floated at the end of the NYT article. However, compulsory paternity leave effectively converts the penalty on women into a penalty on families. The gap will be between those with and without children. And now that the birth may be timed to fit in with the man’s career, it is possible that the birth may be timed even less suitably for the woman."
  },
  {
    "objectID": "posts/failure-to-replicate-ego-depletion-edition.html",
    "href": "posts/failure-to-replicate-ego-depletion-edition.html",
    "title": "Failure to replicate: ego depletion edition",
    "section": "",
    "text": "Ego depletion is the idea that we have a limited supply of willpower. As we use it through the day, we become depleted and more likely to experience a willpower failure.\nThere is a mountain of published experiments providing evidence of ego depletion. Meta-analyses of the studies have supported the concept. The typical trick in these experiments is to get someone to engage in an ego depleting task - such as resisting chocolate - and then you watch them cave in more quickly on a later task than those who haven’t been subject to the earlier ego depletion.\nBut now the evidence is looking shaky. A pre-registered replication involving 23 labs and over 2,000 subjects will be published in Psychological Science. A smaller scale attempt to replicate was also published in PLOS One. The result? If there is any effect of ego depletion, it is close to zero.\nDaniel Engber at Slate has the full story. One of the interesting points is how the meta-analysis didn’t show any problems:\n\nTo figure out what went wrong, Carter reviewed the 2010 meta-analysis—the study using data from 83 studies and 198 experiments. The closer he looked at the paper, though, the less he believed in its conclusions. First, the meta-analysis included only published studies, which meant the data would be subject to a standard bias in favor of positive results. Second, it included studies with contradictory or counterintuitive measures of self-control. One study, for example, suggested that depleted subjects would give more money to charity while another said depleted subjects would spend less time helping a stranger. When he and his adviser, Michael McCullough, reanalyzed the 2010 paper’s data using state-of-the-art analytic methods, they found no effect. For a second paper published last year, Carter and McCullough completed a second meta-analysis that included different studies, including 48 experiments that had never been published. Again, they found “very little evidence” of a real effect.\n\nRoy Baumeister, one of the founders on this work on ego depletion, provided a response to Slate. It’s typical of many responses to this growing replication ‘crisis’ in psychology - suggest that those replicating the experiments haven’t captured all the experimental nuances, or that the effect is context specific.\n\nIn his lab, Baumeister told me, the letter e task [the task used in the replication] would have been handled differently. First, he’d train his subjects to pick out all the words containing e, until that became an ingrained habit. Only then would he add the second rule, about ignoring words with e’s and nearby vowels. That version of the task requires much more self-control, he says.\nSecond, he’d have his subjects do the task with pen and paper, instead of on a computer. It might take more self-control, he suggested, to withhold a gross movement of the arm than to stifle a tap of the finger on a keyboard.\nIf the replication showed us anything, Baumeister says, it’s that the field has gotten hung up on computer-based investigations. “In the olden days there was a craft to running an experiment. You worked with people, and got them into the right psychological state and then measured the consequences. There’s a wish now to have everything be automated so it can be done quickly and easily online.” These days, he continues, there’s less and less actual behavior in the science of behavior. “It’s just sitting at a computer and doing readings.”\n\nEngber nicely points out the consequence of this line of defence. The big idea - and you only need to read Willpower to see that Baumeister and friends sell ego depletion as a big idea - loses its power:\n\nOne of the idea’s major selling points is its flexibility: Ego depletion applied not just to experiments involving chocolate chip cookies and radishes, but to those involving word games, conversations between white people and black people, decisions on whether to purchase soap, and even the behavior of dogs. In fact, the incredible range of the effect has often been cited in its favor. How could so many studies, performed in so many different ways, have all been wrong?\nYet now we know that ego depletion might be very fragile. It might be so sensitive to how a test is run that switching from a pen and paper to a keyboard and screen would be enough to make it disappear. If that’s the case, then why should we trust all those other variations on the theme? If that’s the case, then the Big Idea has shrunk to something very small.\n\nPersonally, I don’t believe that this is a case of experimental outcomes being subject to specific experimental context. Rather, the ‘experimental context’ is the ‘garden of forking paths’, p-hacking and publication bias."
  },
  {
    "objectID": "posts/explaining-the-hot-hand-fallacy-fallacy.html",
    "href": "posts/explaining-the-hot-hand-fallacy-fallacy.html",
    "title": "Explaining the hot-hand fallacy fallacy",
    "section": "",
    "text": "Since first coming across Joshua Miller and Adam Sanurjo’s great work demonstrating that the hot-hand fallacy was itself a fallacy, I’ve been looking for a good way to explain simply the logic behind their argument. I haven’t found something that completely hits the mark yet, but the following explanation from Miller and Sanjurjo in The Conversation might be useful to some:\n\nIn the landmark 1985 paper “The hot hand in basketball: On the misperception of random sequences,” psychologists Thomas Gilovich, Robert Vallone and Amos Tversky (GVT, for short) found that when studying basketball shooting data, the sequences of makes and misses are indistinguishable from the sequences of heads and tails one would expect to see from flipping a coin repeatedly.\nJust as a gambler will get an occasional streak when flipping a coin, a basketball player will produce an occasional streak when shooting the ball. GVT concluded that the hot hand is a “cognitive illusion”; people’s tendency to detect patterns in randomness, to see perfectly typical streaks as atypical, led them to believe in an illusory hot hand.\n…\nIn what turns out to be an ironic twist, we’ve recently found this consensus view rests on a subtle – but crucial – misconception regarding the behavior of random sequences. In GVT’s critical test of hot hand shooting conducted on the Cornell University basketball team, they examined whether players shot better when on a streak of hits than when on a streak of misses. In this intuitive test, players’ field goal percentages were not markedly greater after streaks of makes than after streaks of misses.\nGVT made the implicit assumption that the pattern they observed from the Cornell shooters is what you would expect to see if each player’s sequence of 100 shot outcomes were determined by coin flips. That is, the percentage of heads should be similar for the flips that follow streaks of heads, and the flips that follow streaks of misses.\nOur surprising finding is that this appealing intuition is incorrect. For example, imagine flipping a coin 100 times and then collecting all the flips in which the preceding three flips are heads. While one would intuitively expect that the percentage of heads on these flips would be 50 percent, instead, it’s less.\n\n\nHere’s why.\nSuppose a researcher looks at the data from a sequence of 100 coin flips, collects all the flips for which the previous three flips are heads and inspects one of these flips. To visualize this, imagine the researcher taking these collected flips, putting them in a bucket and choosing one at random. The chance the chosen flip is a heads – equal to the percentage of heads in the bucket – we claim is less than 50 percent.\n The percentage of heads on the flips that follow a streak of three heads can be viewed as the chance of choosing heads from a bucket consisting of all the flips that follow a streak of three heads. Miller and Sanjurjo, CC BY-ND\nTo see this, let’s say the researcher happens to choose flip 42 from the bucket. Now it’s true that if the researcher were to inspect flip 42 before examining the sequence, then the chance of it being heads would be exactly 50/50, as we intuitively expect. But the researcher looked at the sequence first, and collected flip 42 because it was one of the flips for which the previous three flips were heads. Why does this make it more likely that flip 42 would be tails rather than a heads?\n Why tails is more likely when choosing a flip from the bucket. Miller and Sanjurjo, CC BY-ND\nIf flip 42 were heads, then flips 39, 40, 41 and 42 would be HHHH. This would mean that flip 43 would also follow three heads, and the researcher could have chosen flip 43 rather than flip 42 (but didn’t). If flip 42 were tails, then flips 39 through 42 would be HHHT, and the researcher would be restricted from choosing flip 43 (or 44, or 45). This implies that in the world in which flip 42 is tails (HHHT) flip 42 is more likely to be chosen as there are (on average) fewer eligible flips in the sequence from which to choose than in the world in which flip 42 is heads (HHHH).\nThis reasoning holds for any flip the researcher might choose from the bucket (unless it happens to be the final flip of the sequence). The world HHHT, in which the researcher has fewer eligible flips besides the chosen flip, restricts his choice more than world HHHH, and makes him more likely to choose the flip that he chose. This makes world HHHT more likely, and consequentially makes tails more likely than heads on the chosen flip.\nIn other words, selecting which part of the data to analyze based on information regarding where streaks are located within the data, restricts your choice, and changes the odds.\n\nThere are a few other pieces in the article that make it worth reading, but here is an important punchline to the research:\n\nBecause of the surprising bias we discovered, their finding of only a negligibly higher field goal percentage for shots following a streak of makes (three percentage points), was, if you do the calculation, actually 11 percentage points higher than one would expect from a coin flip!\nAn 11 percentage point relative boost in shooting when on a hit-streak is not negligible. In fact, it is roughly equal to the difference in field goal percentage between the average and the very best 3-point shooter in the NBA. Thus, in contrast with what was originally found, GVT’s data reveal a substantial, and statistically significant, hot hand effect."
  },
  {
    "objectID": "posts/evonomics-is-live.html",
    "href": "posts/evonomics-is-live.html",
    "title": "Evonomics is live!",
    "section": "",
    "text": "The web magazine Evonomics is now live. The blurb:\n\nA revolution in economics and business is taking place. Orthodox economics is quickly being replaced by the latest science of human behavior and how social systems work. Few are aware of these deep and profound changes underway that have power to transform the world.  Not anymore!  Evonomics is the home for thinkers who are applying the ground-breaking science to their lives and who want to see their ideas influence society.\n\nI’d recommend these articles as a starting point:\nWhy Neoclassical and Behavioral Economics Doesn’t Make Sense without Darwin, by Terry Burnham\nThe Real Power of Free Markets, by Rory Sutherland\nA Different Way to Look at the Economy, by W. Brian Arthur\nEvolution Takes over One of the World’s Best Business Schools, by Jonathan Haidt\nWho Is the Real Father of Economics?, an interview of Robert Frank by David Sloan Wilson\nAlso on Evonomics is my recent talk at the MSiX conference, Please, Not Another Bias! The Problem with Behavioral Economics.\nThe first batch of articles bashes neoclassical economics a lot. I hope that as the magazine evolves there will be some interesting interaction between the proponents of new approaches. Beneath the surface, there are some substantial differences between the proponents as to what this new approach looks like, and it will be interesting to see those differences teased out."
  },
  {
    "objectID": "posts/evolutionary-science-as-the-new-classics.html",
    "href": "posts/evolutionary-science-as-the-new-classics.html",
    "title": "Evolutionary science as the new “classics”",
    "section": "",
    "text": "Carole Jahme at the Guardian reports on Richard Dawkins’s proposition that evolutionary science will be the new “classics”:\n\nHe explained that whereas classicists have traditionally been assumed to be the scholars most able to branch into any area of research, today – with advances in evolutionary study – it will be those with scholarship in evolutionary science who will supersede classicists in depth, breadth and usefulness.\n\nDawkins sees his new polymathic course at the New College of the Humanities as part of this trend:\n\nHe predicted that those who took his new degree course would achieve a “polymathic status”. He said the course “places evolution at the centre but brings in lots and lots of other subjects such as economics, social science, philosophy, engineering, medicine, agriculture, linguistics, physics, cosmology and history of science.”\nDawkins went into some detail to justify this statement, explaining the relevance of the various disciplines, starting with behavioural economics: “Everything has to be paid for, there is no such thing as a free lunch. You have to pay for whatever you do now in the form of lost opportunities to do other things in the future.”\nHe claimed that in the areas of sexual selection, parent–offspring relationships and sex ratio theory, economic thinking was “rife” within evolutionary research.\n\nIt is interesting that Dawkins intends to let ideas run in both directions between evolutionary biology and other fields, and that he is not engaging in pure evolutionary imperialism. However, I wonder what further ideas economics can offer evolutionary biology beyond those already well entrenched in evolutionary thinking. Are there any more insights or tools that evolutionary biologists should be borrowing from economics?"
  },
  {
    "objectID": "posts/evolutionary-psychology-and-the-left.html",
    "href": "posts/evolutionary-psychology-and-the-left.html",
    "title": "Evolutionary psychology and the left",
    "section": "",
    "text": "Belief in evolution is often considered the domain of “the left”. Apart from being true, evolutionary theory provides a ground for opposition to creationists. It is often used to argue that competition can be wasteful and that self-organising systems (such as the economy) do not always operate for the good of society. However, evolutionary psychology has not been embraced to the same extent. Ever since Gould and Lewontin led the sociobiology wars against E O Wilson and others, the concept that evolution shaped human minds has faced much opposition, even among those who otherwise accept that evolution is true.\nIn support of my sweeping statements, in an article published this month in the Journal of Social, Evolutionary and Cultural Psychology, Andrew Ward and his colleagues report on a study of attitudes to evolution and what the authors describe as some key tenets of evolutionary psychology (also blogged on by Robert Kurzban). Those tenets included that men are more interested than women in one night stands, men are more interested in attractiveness, and women value good financial prospects in a mate more than men do.\nI feel that Ward and colleagues chose the topic of human mating specifically to get the paradoxical result. As the authors note, there would be value in also testing attitudes to evolutionary explanations for cognition, perception, or language, which I would expect would have a higher level of acceptance for many people.\nWard and colleagues tested 99 participants at a train station for their attitudes to evolution and the implications of applying evolutionary concepts to human mating. They then tested a similar set of questions (with a more generic definition of evolution) on 452 participants from a train station, a college and two churches.\nIn both studies, the opponents of evolution, who were generally older and more conservative, were more likely to endorse the evolutionary implications on human mating systems. Interestingly, when half of the survey respondents in the second survey were explicitly told that the evolutionary psychology questions were “based on the THEORY OF EVOLUTION, as applied to the fields of psychology and biology” (actual text from survey), it reduced the level of support from evolution opponents but made no difference to the response of evolution supporters.\nThe most obvious explanation for this is that the evolution supporters simply disagree that the mating differences are a consequence of evolutionary psychology and they have rationalised a reason for this. However, this only raises a follow-up question about why they would consistently create this reason, while the creationists remained happy to support the statements (although to a lesser extent), even when told that the statements were based on the theory of evolution. Further, as noted by Ward and his colleagues, it is unlikely that many of the evolution supporters that were surveyed would have actually thought about and rationalised through this process.\nThe slightly dissatisfying thing about this study is that, while showing an interesting paradox, it does little to explore the foundation for the paradox or whether there are any conflating factors such as education. That will be left for future studies. For now, the authors leave us holding the aphorism: “Never let the data get in the way of a good theory”."
  },
  {
    "objectID": "posts/evolutionary-economics.html",
    "href": "posts/evolutionary-economics.html",
    "title": "What is evolutionary economics?",
    "section": "",
    "text": "I am called an evolutionary economist often enough that I have been tempted to write a post titled “Why I am not an evolutionary economist”. In the absence of that post, Peter Turchin quizzed Ulrich Witt on what evolutionary economics is, and provides a useful description:\n\n[T]here are two main currents in evolutionary economics, which have developed largely independently of each other. One research direction, within which Ulrich himself has been working, begins by questioning the assumption of homo economicus, a rational agent that choses those actions that yield the best balance of rewards versus costs. Real human beings behave in a very different way. …\nHowever, it’s not enough to say that we fail to measure up to the lofty standards assumed by the rational choice theory. What would be particularly interesting is to understand in what ways our behavior deviates from ‘perfect rationality’ and why we evolved to behave in these ways. In the last couple of decades the fields of evolutionary psychology and behavioral economics have been making great strides in answering such questions.\nThe second current in evolutionary economics is sometimes called the ‘Universal Darwinism’ … Darwin developed his theory to explain biological evolution. But his basic insight has a lot of value when considering the dynamics of economic agents (especially, organizations such as firms and corporations) competing in the market. In biological organisms evolutionary ‘fitness’ is maximized when they increase their chances of survival and reproduction. Firms also have fitness, which is maximized when they increase their revenues and cut costs.\n\nI haven’t heard evolutionary economics described in this split way before, and have tended to see both “currents” as encompassing the cultural evolution of economic systems. Witt (and others) place the evolutionary focus on the evolved human tendencies that underlie the economic dynamics. The second approach (as epitomised by Nelson and Winter’s seminal work) places the evolutionary emphasis on the dynamics themselves. Importantly, Witt’s approach displays some skepticism toward Darwinian approaches to cultural evolution, with non-Darwinian considerations such as diffusion and learning also playing roles.\nUnlike Turchin, I would not describe evolutionary psychology or behavioural science (I prefer “behavioural science” to “behavioural economics”) as having made great strides in this area. Behavioural science is a body of work crying out for the theoretical backbone that evolutionary biology could provide. Yet evolutionary biology is not considered in most behavioural science work. It is why we have so many lists of heuristics and biases and so little theoretical unification (giving a bias a name is not theory). Evolutionary psychology has been more fruitful, but is not often enough turned toward economic applications or the empirical findings of behavioural science.\nThe work of Witt is a rare example of those evolutionary insights into human behaviour being used in an economic context (here is another), but I wonder if the limited spread of these ideas through economics is due to it being within the evolutionary economic framework. Evolutionary economics is a heterodox field on the fringes of economics, so combining the evolutionary approach to human behaviour with a second heterodox approach creates one barrier too many.\nThen again, many of those working on the evolution of preferences have expressly taken more orthodox approaches (for example, Arthur Robson notes this on his homepage), but are also some way from transforming economics. In that case, the extent of their impact might be attributed to the failure of that evolutionary approach to explain many of the empirical observations developed by behavioural scientists. Evolutionary approaches have a habit of generating rational preferences.\nWhat does give me hope is that much recent work at the frontiers of behavioural science is driven not by economists or psychologists, but by evolutionary biologists. At times it seems as though they are repeating empirical work done by others before them, but with an evolutionary framework with which to analyse it, those observations take on a whole new light.\nAs for the label for my work, I am tending towards “Darwinian economics”. I’m not sure if it will work, but it avoids the baggage that comes with “evolutionary economics”, “bioeconomics” and other labels with longer traditions. It is also less of a mouthful than “the integration of economics and evolutionary biology”."
  },
  {
    "objectID": "posts/evolutionary-biology-in-economics-a-review.html",
    "href": "posts/evolutionary-biology-in-economics-a-review.html",
    "title": "Evolutionary Biology in Economics: A Review",
    "section": "",
    "text": "I’ve just had a new article published in the Economic Record - Evolutionary Biology in Economics: A Review (pdf).\n\nEvolutionary Biology in Economics: A Review\nJason Collins, Boris Baer and Ernst Juerg Weber\nAs human traits and preferences were shaped by natural selection, there is substantial potential for the use of evolutionary biology in economic analysis. In this paper, we review the extent to which evolutionary theory has been incorporated into economic research. We examine work in four areas: the evolution of preferences, the molecular genetic basis of economic traits, the interaction of evolutionary and economic dynamics, and the genetic foundations of economic development. These fields comprise a thriving body of research, but have significant scope for further investigation. In particular, the growing accessibility of low-cost molecular data will create more opportunities for research on the relationship between molecular genetic information and economic traits.\n\nI previously posted about an earlier version of this paper when it was called The Evolutionary Foundations of Economics. You can access that earlier paper here.\nIt’s not the most exciting article - it was the introductory chapter of my PhD thesis and I wrote it to provide the foundations for the substantive chapters rather than to spark a revolution. However, it will give you a decent snapshot of what is going on."
  },
  {
    "objectID": "posts/evolution-of-time-preference-by-natural-selection.html",
    "href": "posts/evolution-of-time-preference-by-natural-selection.html",
    "title": "Evolution of time preference by natural selection",
    "section": "",
    "text": "One of the few areas where there is active research on the link between evolutionary biology and economics is the evolution of economic preferences (some papers in this area are in my economics and evolutionary biology reading list). Economic preferences are the way an actor will rank a set of choices based on characteristics such as the amount received, the probability of certain outcomes, and the timing with which the outcomes are received.\nPreferences about the timing of an outcome are known as time preference (often measured as a discount rate). Someone who values goods now much more than goods received later has a high rate of time preference, while someone who gives goods in the future a relatively higher value has a low rate of time preference. We would call someone with a low rate of time preference patient.\nSo, what rate of time preference would have evolved under the forces of natural selection? Discounting the future makes sense in an evolutionary context as future outcomes might not be realised, population growth might make benefits received in the future worth less, and there is the chance of events such as death.\nOne of the seminal papers in the analysis of the evolution of time preference is Alan Rogers’s Evolution of Time Preference by Natural Selection (ungated version here). Time preference had been touched on before by R.A. Fisher, and in an earlier paper by Hansson and Stuart who examined intergenerational time preference. But Rogers’s paper was the first to look at this question on an intragenerational basis, which is the context in which economists usually consider it.\nRogers examined the optimal same-age transfer  that would be made from a mother to her daughter (e.g. from the 20-year old mother to the 20-year old daughter). In making such decisions, the mother would need to consider the remaining reproductive life of her daughter, that her daughter is only 50 per cent related to the mother, and the rate of population growth. As the transfer is same-age, the mother and daughter have the same remaining reproductive life. If there is no population growth (which was effectively the case for most of human history), only the degree of relatedness would matter and the discount factor is effectively one half per generation. Under these conditions, Rogers argued that the long-term real interest rate should be about 2 per cent per year.\nAs for the analysis by Hansson and Stuart, this rate appears low against measured discount rates, which are typically above 10 per cent per year. A simple static analysis of this nature is missing something. Some other papers grapple with this issue, and I will post about them soon.\nSubsequently, Robson and Szentes argued that there are “serious problems with Rogers’ model” (ungated version here and extended version here), and that a particular rate of time preference should not flow from the analysis. They argued that unrealistic assumptions drive the results, including the assumption of identical offspring, which is not the case when offspring vary with age, and the assumption of a single same-age transfer where there are many possible same-age transfers (e.g. 20-year old mother to 20-year-old offspring, 30-year old mother to 30-year old offspring). Robson and Szentes also showed that the rate of time preference would depend upon the nature of the survival function faced at each age (i.e. the probability of death).\nRegardless, Rogers’s paper is an important one, and despite a couple of earlier pieces of work on the evolution of time preference by other authors, Rogers’s paper is often seen as the seminal paper that kick started the evolutionary analysis of preferences. That is not a bad legacy to have.\nUpdate: Alan Rogers responds in the comments."
  },
  {
    "objectID": "posts/evolution-and-obesity.html",
    "href": "posts/evolution-and-obesity.html",
    "title": "Evolution and obesity",
    "section": "",
    "text": "As I indicated in my recent post on Rob Brooks’s Sex, Genes and Rock ‘n’ Roll: How Evolution Has Shaped the Modern World, Brooks devotes some time to the issue of obesity. Rob has also blogged about obesityand published a paper with Steve Simpson and David Raubenheimer on it (although the book covers more ground).\nFirst, why is obesity an evolutionary problem? In his book, Brooks set out why:\n\nSome researchers use the fact that the obesity crisis emerged so recently as a reason to reject or ignore evolutionary or biological explanations for the crisis. After all, they argue, evolution takes thousands of generations. While they are certainly correct that we are not evolving to become fatter, obesity certainly has deep evolutionary roots among its causes. …\nThe combinations of genes that we inherited from our ancestors are adapted to the environments where those ancestors lived and bred, not the environments we inhabit today.\n\nFor most of our evolutionary history, humans ate a diet high in protein and fat, with some complex carbohydrates. With the neolithic revolution, some populations increased the proportion of their diet that consisted of carbohydrates. Over the last 50 or so years, the level of simple carbohydrates in diets has soared. Humans are not adapted to this modern diet and populations that have only very recently been exposed to this modern diet are more likely to face problems. This can be seen in the high rates of obesity on many Pacific islands or the high rates of diabetes and alcoholism among indigenous populations.\nOne of the arguments as to why the modern diet poses a problem is the protein leverage hypothesis. The basic idea is that humans have a stronger propensity to regulate protein intake than non-protein calories. As humans have a basic daily protein need - we eat until we satisfy our protein requirements. If the food we are eating has low protein content, we need to eat more before hitting that satiation point. These extra calories are what make us obese.\nBrooks and his paper co-authors then took this question into the modern supermarket and looked at the prices of protein and carbohydrates. In his book, Brook’s summarises their findings:\n\nI did a quick assessment of the costs of 111 common foods in my local supermarket and takeaway outlets. I was amazed that every megajoule (1000 kilojoules or 239 calories) of energy from protein adds US$3.26 to the average price of a food, but every megajoule of carbohydrate actually reduces the cost of food by 38 cents. …. Because sugars and starches are cheaper relative to protein than at any other time in human history, economic costs are likely to bias the foods we buy and eat toward energy-rich yet protein-poor diets. Within industrialised societies this effect is likely to be most extreme for poor people who have access to a wide range of foods but who are constrained in which foods they can afford to buy.\n\nThis led them to estimate that to cut kilojoule intake by 1,600 kilojoules (the estimated jump in calorie intake since the early 1970s) would require a subsidy of US$0.72 per person per day. This is around $262 per obese person per year, or around one-fifth of the estimated annual medical spending due to obesity.\nIt provides a case for government action, with the benefits well excess of the cost. Brooks puts it as follows:\n\nOne possibility is to subsidise high-protein foods such as lentils, lean meat and fish. Another is to reduce subsidies or tariff protection on sugar and cereal staples. An alternative to the politically perilous business of intervening in commodities markets is to tax products that clearly generate a large part of the public health burden. Reductions in carbohydrate intake might more effectively be achieved by raising the price of carbohydrate energy than by lowering the price of protein. Products like soft drinks, cordials, fried potato products and ice cream constitute a large proportion of the energy intake of adults and children at risk of obesity but contain little or no protein. Special taxes on cheap carbohydrates could well prove to be particularly effective.\n\nGetting rid of subsidies and tariffs is an excellent idea, but I am not sure this is a one-way street for obesity reduction. Sugar quotas in the United States raise the price of sugar. Or take high-fructose corn syrup. Corn prices are artificially inflated by the ridiculous ethanol related subsidies in the United States. Would a truly unsubsidised, free market in corn raise or lower the price of corn syrup? I’m not sure, although that is one experiment I would like to see.\nOn the subsidisation of high protein foods, I am not sure whether this would succeed. Assuming the subsidisation occurred at the point of sale, we would see an increase in demand for these products and some substitution from carbohydrate heavy foods to these subsidised foods. However, we would also see an income effect, whereby the person could use the extra income freed up by the lower price of protein to buy more ice cream. We might also see underlying protein prices increase, with increased demand leading to much of the subsidy going to the fixed factor - the primary producer of the protein.\nI’d be more optimistic about the effect of a tax on high-sugar products. There is reasonable evidence that soft-drinks and the like have a high price elasticity (demand is responsive to changes in price). However, there is also likely to be an income effect, whereby reduced income caused by the higher price of sugar might cut protein consumption. I would not expect the reduction in protein purchases due to reduced income to be larger than the incentive to substitute protein for sugars, but we should consider it.  This is particularly the case where obesity is most concentrated in the low socio-economic income groups.\nA good step would be to test this by running some randomised trials (if they haven’t already been done) to see if the tax converts into reduced obesity. The evidence on food labelling and calorie disclosure is that it is ineffective, while there is a long history of sin taxes successfully reducing “sinful” consumption. However, I’m wary of these sorts of ideas as there are always unintended consequences. Sin taxes hit a wide number of people who are using the product sensibly. I’d like to see an estimate of the cost of their loss of enjoyment. Even among those who are obese, a substantial part of their life enjoyment might come from eating high-sugar foods. Perhaps they have decided it is worth the cost of obesity (and if you consider that they should not be allowed to impose the costs of their decision on the health system - don’t let them).\nThankfully, I’d be reasonably immune from these taxes as I like to stick to the edges of the supermarket. I’d encourage anyone to do this. However, this makes me even more reluctant to support a sugar tax. One should be wary of advocating intrusive actions when the intrusion is not on yourself."
  },
  {
    "objectID": "posts/evolution-and-education-policy.html",
    "href": "posts/evolution-and-education-policy.html",
    "title": "Evolution and education policy",
    "section": "",
    "text": "A couple of months ago, David Sloan Wilson posted on a project he has been involved in with in the Binghamton City School District, which is also the subject of an article in PLoS ONE by Wilson and his colleagues. The concept behind the project is that “[K]nowledge derived from general evolutionary principles and our own evolutionary history can be used to enhance cooperation in real-world situations, such as a program for at-risk high school students.”\nAmong other things, the authors drew on the work of Elinor Ostrom and the design features that she identified as contributing to group success. The authors also looked at bodies of evolutionary knowledge about development, psychological function and learning.\nFrom this knowledge, specific measures were developed. As regards cooperation, the first three days of school comprised group identity building activities. Students were consulted to set up the rules. Staff meetings were held twice weekly. Praise was plentiful but rules clear and enforced. And so on (the full list of measures is in the paper here).\nStudents in the modified program significantly outperformed the comparison group in a randomly controlled trial. The standard of performance was up to average for a Binghamton student, despite the sample coming from students who had failed three or more subjects in the previous year.\nThis is a positive result, but it reminded me of a section in Ian Ayres’s book Super Crunchers: Why Thinking-by-Numbers Is the New Way to Be Smart. Ayres describes a method of teaching known as Direct Instruction, which is the form of teaching that George W Bush was watching when he was informed of the 9/11 attacks. Ayres writes:\n\nDirect Instruction forces teachers to follow a script. The entire lesson - the instructions (“Put your finger under the first word.”), the questions (“What does that comma mean?”), and the prompts (“Go on.’) - is written out in the teacher?s instruction manual. …\nEach student is called upon to give up to ten responses each minute. How can a single teacher pull this off? The trick is to keep a quick pace and to have the students answer in unison. The script asks the students to “get ready” to give their answers and then after a signal from the teacher, the class responds simultaneously. Every student is literally on call for just about every question.\n\nEvery time I read about Direct Instruction, I struggle to understand how it could work. It seems constrained. It puts everyone at the same pace. But it works. Ayres continues:\n\nThe result was Project Follow Through, an ambitious effort that studied 79,000 children in 180 low-income communities for twenty years at a price tag of more than $600 million. … Project Follow Through looked at the impact of seventeen different teaching methods, ranging from models like DI, where lesson plans are carefully scripted, to unstructured models where students themselves direct their learning by selecting what and how they will study. …\nDirect Instruction won hands down. Education writer Richard Nadler summed it up this way: “When the testing was over, students in DI classrooms had placed first in reading, first in math, first in spelling, and first in language. No other model came close.” And DI’s dominance wasn’t just in basic skill acquisition. DI students could also more easily answer questions that required higher-order thinking.\n\nCan evolutionary theory provide an explanation? I don’t know - but it does not matter from a policy perspective if one can’t be developed. This is because policy decisions such as teaching method do not need to be made though a non-repeatable top down decision. Instead, randomised controlled trials can be used to test all the teaching ideas, crazy or not, and see which delivers the best result. Evolutionary theory can be used to develop ideas to test, but it is the results that matter.\nEven better, competition between schools can provide a basis by which teaching methods compete. Student and parent choice would drive outcomes. As a result, I am more interested in seeing competition between schools in the marketplace than having them consider what evolutionary strategy they should use to teach.\nSo, while Wilson’s work in Binghamton is impressive and his evolutionary approach improved outcomes, I don’t know if he has produced the best possible result. Competition and randomised controlled trials are the way to find out.\n(And thanks to Sex, Genes & Rock ‘n’ Rollfor the reminder about Wilson’s post.)"
  },
  {
    "objectID": "posts/eugenics-versus-economics.html",
    "href": "posts/eugenics-versus-economics.html",
    "title": "Eugenics versus economics",
    "section": "",
    "text": "In outing Irving Fisher as a Social Darwinist, Bryan Caplan writes on how Fisher reconciled eugenics and economics. First, Caplan quotes Fisher:\n\nThe core of the problem of immigration is, however, one of race and eugenics. If we could leave out of account the question of race and eugenics I should, as an economist, be inclined to the view that unrestricted immigration, although injurious to some classes, is economically advantageous to a country as a whole, and still more to the world as a whole. But such a view would ignore the supremely important factors… Our problem is to make the most of this inheritance [of the 8,000 immigrants who arrived before 1741]. We can not do so if that racial stock is overwhelmed by the inferior stock which “assisted” immigration has recently brought. (“Impending Problems of Eugenics” [1921])\n\nCaplan considers that the two approaches diverge due to the inherent misanthropy of eugenics:\n\nEconomics doesn’t point to people and say, “Look what they can’t do.” Economics instead asks, “Well, what can they do?” If the answer is “something productive,” then the Law of the Comparative Advantage implies gains to trade. Economics, known for its hard-headed methods, culminates in an optimistic and humane conclusion: Regardless of their Darwinian “fitness,” the existence of people - even those well below average - makes the world a better place.\n\nI would argue that the difference is more subtle than Caplan suggests, as Fisher and many other eugenicists were well aware of comparative advantage. Rather, they had different goals, particularly about the distribution of benefits, and they often came to a different conclusion about how someone’s negative externalities balance with the “something productive” that one can offer. At the limit, we regularly put people in prison for those externalities. The question is where the line gets drawn - Fisher would likely put more people into the negative externality basket than Caplan.\nFurther, eugenicists often understood that the poor had a comparative advantage (although I’d suggest they did not fully understand the full implications of this) when framing their policy preferences. Thomas Leonard writes:\n\nProgressive economists, like their neoclassical critics, believed that binding minimum wages would cause job losses. However, the progressive economists also believed that the job loss induced by minimum wages was a social benefit, as it performed the eugenic service ridding the labor force of the “unemployable.” Sidney and Beatrice Webb (1897 [1920], p. 785) put it plainly: “With regard to certain sections of the population [the “unemployable”], this unemployment is not a mark of social disease, but actually of social health.” “[O]f all ways of dealing with these unfortunate parasites,” Sidney Webb (1912, p. 992) opined in the Journal of Political Economy,  “the most ruinous to the community is to allow them to unrestrainedly compete as wage earners.” A minimum wage was seen to operate eugenically through two channels: by deterring prospective immigrants (Henderson, 1900) and also by removing from employment the “unemployable,” who, thus identified, could be, for example, segregated in rural communities or sterilised. …\nWorthy wage-earners, Seager (1913a, p. 12) argued, need protection from the “wearing competition of the casual worker and the drifter” and from the other “unemployable” who unfairly drag down the wages of more deserving workers (1913b, pp. 82–83). The minimum wage protects deserving workers from the competition of the unfit by making it illegal to work for less. Seager (1913a, p. 9) wrote: “The operation of the minimum wage requirement would merely extend the definition of defectives to embrace all individuals, who even after having received special training, remain incapable of adequate self-support.” Seager (p. 10) made clear what should happen to those who, even after remedial training, could not earn the legal minimum: “If we are to maintain a race that is to be made of up of capable, efficient and independent individuals and family groups we must courageously cut off lines of heredity that have been proved to be undesirable by isolation or sterilization . . . .”\n\nThese paragraphs also indicate the often missed difference between eugenics and Social Darwinism. Social Darwinists tend to be much more rosy about the effects of competition on the human race, while eugenicists would prefer to give it a push in their preferred direction."
  },
  {
    "objectID": "posts/envy-has-its-benefits.html",
    "href": "posts/envy-has-its-benefits.html",
    "title": "Envy has its benefits",
    "section": "",
    "text": "Bryan Caplan writes:\n\nIf people envy people richer than themselves, I say we should fight envy, not inequality. A number of people have objected that “Envy is ‘hard-wired.’” They’re right - but it doesn’t matter.\n…\n“[H]ard-wired” does not mean fixed. All humans may feel these emotions to some extent. But there’s plenty of room to maneuver. You can become less envious than you are. Make an effort to monitor your thoughts and behavior. Count your blessings. Give credit where credit is due. Focus on improving yourself instead of comparing yourself to other people. Spend more time with less envious people.\n\nWe are “hard-wired” to feel envy as, historically, those who felt envy were more reproductively successful. Presumably it is a driver behind the success of some people -they aspire to the levels of status, wealth and power of those they envy – and as a result, they were better able to attract mates. In a post supportive of Caplan’s position, David Henderson characterises envyas “self-destructive”. In an evolutionary sense, envy could not have been self-destructive (on average) for it to become “hard‑wired” and so ubiquitous. I am not aware of any evidence that this has changed.\nSo what would the world look like if there was less or no envy? It may be a less dynamic, interesting and creative world than the one we live in. How many people have created an invention or business with envy of others a motivating factor? Envy has its benefits.\nIf Caplan’s encouragement to be less envious did work (although I consider convincing many people is a lost cause), it may be a temporary result. The less envious person might be happier. But they may also have less status, wealth and power and a reduced ability to attract a mate. To the extent there are less “envy free” people in next generation, envy will be back.\nHaving said the above, I do try to follow Caplan’s advice - but not always successfully."
  },
  {
    "objectID": "posts/elite-envy.html",
    "href": "posts/elite-envy.html",
    "title": "Elite envy",
    "section": "",
    "text": "Robin Hanson writes:\n\nConsider: what elites did foragers worry most about? Foragers worried most about elite capacity for violence, and an inclination to use it. They also worried lots about unequal access to food and shelter, and to tools useful for all these things. So foragers enforced strong norms against giving orders or doing violence, and norms favoring sharing of food, shelter, and tools. In these senses foragers were egalitarian.\nHowever, foragers worried far less about unequal capacities for art, music, conversation, charm, social popularity, or sex appeal. After all, in a forager world unequal capacities of these sort just couldn’t go anywhere near as horribly wrong as unequal violence or food. Because of this humans seem evolved to tolerate, and even celebrate, unequal abilities in art, popularity, or sex appeal.\n\nAs human populations are the descendants of those who survived AND were reproductively successful, I am not sure that Hanson is drawing a useful distinction. If men tolerated inequality in resources OR inequality in mating opportunities when they could do something about it, they would have lower than maximum fitness.\nThe other conflating factor is that these features - wealth, art, appearance, popularity and so on - are all signals to the opposite sex. Lack of tolerance of income inequality is likely to reflect the effect on reproductive success as much as the consequences for survival. Further, individuals are likely to be better across a spectrum of these traits, with more popular and attractive males also having higher income.\nAssuming Hanson is correct in assuming that people worry more about resource scarcity than inequality in abilities in art, sex appeal or popularity, is this because abilities in art, sex appeal or popularity are more difficult to fake? In contrast, excess resources could be acquired through violence (although ability to successfully acquire resources through violence is also difficult to fake). Inequality in sex appeal or popularity can go wrong, but there are more limited means to rectify it. If the ugliest man in the tribe kills the most attractive, he is still the ugliest. The benefits to killing the attractive man would be because he has resources and the killing may raise status, not because it reduces sex appeal inequality.\nOne interesting piece of evidence about income versus reproductive inequality is about to emerge. As the excess of men in China and India pass through their reproductive prime in the next 20 years, what will trigger the more trouble - inequality in resources or inequality in mating opportunities? We should be able to contrast different regions in those two countries. There may be difficulty disentangling the two as income inequality and inequality in reproductive success are likely to be highly correlated, but the evidence is mounting that excess men are a recipe for trouble."
  },
  {
    "objectID": "posts/education-in-the-developing-world.html",
    "href": "posts/education-in-the-developing-world.html",
    "title": "Education in the developing world",
    "section": "",
    "text": "Following from my recent blog on over-education, Tyler Cowen’s words on under-education for most of the world should be noted.\n\nOf course Bryan favors rising wealth and falling fixed costs, as do I.  But in the meantime he also should admit that a) education “parachuted” in from outside can have a high marginal return, b) collectively stronger pro-education norms raise demand and can alleviate the high fixed costs problem, c) there are big external benefits, some operating through the education channel, to lowering the fixed costs, d) stronger pro-education norms put a region closer to a “big breakthrough” and weaker education norms do the opposite, and e) a-d still impliy “too little education” is the correct judgment.  On b), some evangelical groups in Latin America do seem to have stronger pro-education norms in their converts and it appears to be much better for the children of these families and no I’m not going to buy any response which ascribes the whole effect to selection.\n…..\nSignaling models are important but they are not the only effect and of course a lot of signaling is welfare-improving for reasons of screening and sorting and character reenforcement.  The traditional story of high social returns to education is supported by evidence from a wide variety of different fields and methods, including cross-sectional growth models, labor economics, political science, public opinion research, anthropology, education research, and much more.  You can knock some of this down by stressing the endogeneity of education, but at the end of the day the pile of evidence, and the diversity of its directions, is simply too overwhelming."
  },
  {
    "objectID": "posts/economists-on-autopilot.html",
    "href": "posts/economists-on-autopilot.html",
    "title": "Economists on autopilot",
    "section": "",
    "text": "One blog in my feed is the excellent synthesis blog, which brings you the musings of Greg Fisher, Paul Ormerod and others. From their about page:\n\nWe believe that this new approach, centred on the study of complex networks, has enormous potential to integrate, reconcile, and synthesise the social sciences. Viewing society as a complex network requires us to focus on actual human psychology and the nature of human interaction, rather than some inaccurate abstraction (such as “the rational agent”). We can drill down further and understand that neuroscience can help us to understand the nature of human cognition (the brain being a complex network itself), action, and interaction. Starting from these micro levels we can build a more representative understanding of social groups, including what we understand as the “macro level”. Therefore, complex networks offer a synthesis between subjects that until now have been largely kept apart, such as psychology and economics.\n\nI recommend subscribing to their feed.\nTheir most recent post is by Ormerod, who asks why Daniel Kahneman’s observations about human decision-making have not been integrated into economic theory.\n\nPerhaps Kahneman’s own work gives us an insight. He distinguishes between System 1 and System 2 thinking. System 1 is when the brain is almost on autopilot. He illustrated this in his talk last night. ‘If I mention the word “vomit”, your brain reacts. If I ask “what is 2 plus 2?”, the answer comes in your mind automatically’. System 2 thinking requires much more effort – most people, he said, cannot multiply 24 and 17 whilst at the same time negotiating a right turn in heavy traffic.\nActually, I guess that most economists could do this. Many of them could even carry out the maths required to optimise a particular function at the same time! In other words, economists are so steeped in calculus, they have performed these mathematical operations so many time, that for them, the maths of calculus has become System 1 thinking.\n\nAs mathematics is so ingrained in economists’ minds, when a new problem arises, the instinctive response is to write out the equations as a rational optimisation problem. It is much harder to integrate Kahneman’s insights, which require considerable System 2 thinking. Economists take the easy route."
  },
  {
    "objectID": "posts/economists-are-different.html",
    "href": "posts/economists-are-different.html",
    "title": "Economists are different?",
    "section": "",
    "text": "In Robert Frank’s Passions Within Reason: The Strategic Role of the Emotions (this is another snippet pending my finding time to write a decent review), Frank describes the free riding behaviour of economists in the public goods game:\n\nIt is interesting to note that the only group for which the strong free-rider hypothesis receives minimal support in this vast experimental literature turns out to be a group of economics graduate students. In experiments essentially like the ones run by Dawes et al., Marwell and Ames discovered that economics students were significantly more likely to defect that any of the other groups they studied. This findings agrees with the finding of Kahneman et al. that commerce students are more likely than psychology students to make one-sided offers in ultimatum bargaining games.\n\nFrank offers two explanations for these observations. The first is that economists are influenced by economic theory to act in this way, with the “most charitable” explanation being that economists treat it as little more than an IQ test. The second is that certain types of people self select into the field.\nFrank suggests it is a mix of the two, and I would tend to agree. However, Frank’s observation reminded me of a class I was in a few years ago, where after learning about the strategy of the ultimatum game, we then played it. A number of people in the class became frustrated that some of the responders were not playing the equilibrium strategy of accepting the pittance they were offered. Those who play the Nash equilibrium strategy tend to suffer against many competitors, as this strategy does not always work against “irrational behaviour”.\nFrank also quotes a 1986 piece from The Chronicle of Higher Education, where Robert Solow reflects on an essay by Francis Amasa Walker:\n\nEconomists, Walker argued, disregard important international differences in laws, customs, and institutions that affect economic issues. They also ignore, he said, the customs and beliefs that tie individuals to their occupations and locations and lead them to act in ways contrary to the predictions of economic theory.\nWalker, Mr Solow said, could have been talking about the economists of the 1980’s.\n\nEconomists are not immune from the flawed assumption that everyone else thinks like you do."
  },
  {
    "objectID": "posts/economists-1-biologists-0.html",
    "href": "posts/economists-1-biologists-0.html",
    "title": "Economists 1, Biologists 0",
    "section": "",
    "text": "Sorry for the slightly inflammatory post title - but I went to a speech tonight that reminded me of one case where an economist was well ahead of evolutionary biologists in cracking a puzzle.\nThe speaker was Michael Spence, 2001 Bank of Sweden prize winner and author of the recently released The Next Convergence: The Future of Economic Growth in a Multispeed World (the subject of his speech).\nIn 1973, Spence wrote a paper called Job Market Signalling, one of the main reasons he won the prize. The idea of his paper was simple. Take two groups of people with different productivity (call them low and high). This productivity cannot be directly observed by the employer, but the employer can observe their level of education. Spence showed that if the low productivity workers had a higher cost of education than the high productivity workers, a separating equilibrium could arise where only the high productivity workers would educate themselves and the employers would pay these workers a higher wage. The assumption of the different costs of education was vital for this equilibrium to exist, or else low productivity workers would fake the signal. And it is a reasonable assumption. For example, if the worker is of low productivity due to low intelligence, they will have difficulty understanding lectures and doing assignments and will have to invest more time that the high productivity worker to get the same educational result.\nThe following diagram shows Spence’s result. Each type of worker could be paid high or low wages. The employer will only pay high wages to workers with an education level above y*. In this case, the cost of education for the low productivity “Group I” is such that they do not undertake education, while members of the high productivity “Group II” educate themselves to y*. The benefit of education outweighs the costs for “Group II” only.\n\nTwo years after Spence’s paper was published, Amotz Zahavi had a paper published titled Mate selection - a selection for a handicap. This paper spelt out Zahavi’s handicap principle, which described how honest signals of quality between animals could evolve. The signals are honest because they impose a handicap on the signaller that only a high quality signaller can bear.\nThe handicap principle was not accepted at first. Richard Dawkins wrote in an early edition of The Selfish Gene:\n\nI do not believe this theory, although I am not quite so confident in my scepticism as I was when I first heard it. I pointed out then that the logical conclusion to it should be the evolution of males with only one leg and only one eye. Zahavi, who comes from Israel, instantly retorted: ‘Some of our best generals have only one eye!’ Nevertheless, the problem remains that the handicap theory seems to contain a basic contradiction. If the handicap is a genuine one-and it is of the essence of the theory that it has to be a genuine one-then the handicap itself will penalize the offspring just as surely as it may attract females. It is, in any case, important that the handicap must not be passed on to daughters.\n\nJohn Maynard Smith published papers (such as this) suggesting that no model could be found in which the handicap principle could hold (although he did not rule out someone else finding one).\nFinally, in 1990, Alan Grafen published two papers in which he established the population genetic and game theoretic foundations to the handicap principle. Mathematically, it could work. It convinced people such as Dawkins that the handicap principle could be right. And the key element in Grafen’s analysis was different costs - as used by Spence in 1973. While Grafen’s papers are quite technical, the following diagram by Rufus Johnstone provides a simple illustration of how it works - and how similar it is to the work of Michal Spence. If two different quality individuals face differential costs and the same benefits (or differential benefits and the same costs), they will signal at different levels, making their signal a reliable indicator of their quality. The high-quality individual maximises costs relative to benefits at s_{high}, while the low-quality individual maximises their benefits relative to costs at s_{low}.\n\nThe seventeen year gap between Spence and Grafen’s papers appears quite long for a solution to essentially the same problem - although obviously they were framed in quite different ways. How many other instances have there been of fields facing a parallel problem but the solutions were reached such a long-time apart (not to mention independently - Spence’s work was not used used in solving the handicap principle problem)?"
  },
  {
    "objectID": "posts/economics-versus-history-is-this-the-right-debate.html",
    "href": "posts/economics-versus-history-is-this-the-right-debate.html",
    "title": "Economics versus history - is this the right debate?",
    "section": "",
    "text": "Over the past few days, Tim Harford has been engaging in a debate with Gideon Rachman (along with a few other bloggers) regarding Rachman’s contention that economists should be swept from their throne and historians given greater due.\nA debate like this is always going to be at cross purposes. Both history and economics are large fields with differing schools of thought (and battles within). To tar all of economics with the same brush is to ignore the breadth of economics. Paul Krugman’s characterisation of saltwater and freshwater economists is a case in point. The strongest and most coherent critics are generally on the inside.\nSimilarly, history has a variety of schools of thought. Which historian is representative? Niall Ferguson, for example, has built his career on controversial and counter-intuitive ideas. If he is wrong, should that be a black mark on all historians? Or are historians his strongest critics and counterbalance? And of course, Ferguson is constantly making predictions, the very crime that Rachman accuses economists (but not historians) of committing.\nMy two cents worth is that Rachman is right the influence of economics should be tempered. In his Nobel Banquet speech, Freidrich Hayek stated that he would not have created the “Bank of Sweden Prize”. He stated:\n\n… I must confess that if I had been consulted whether to establish a Nobel Prize in economics, I should have decidedly advised against it. … the Nobel Prize confers on an individual an authority which in economics no man ought to possess. This does not matter in the natural sciences. Here the influence exercised by an individual is chiefly an influence on his fellow experts; and they will soon cut him down to size if he exceeds his competence. But the influence of the economist that mainly matters is an influence over laymen: politicians, journalists, civil servants and the public generally.\n\nIn his prize speech, Hayek also touched on the mess made by economists and the “scientistic” approach which may lead to error.\nHayek’s message that economics is full of fashion and pretends to be a harder science than it should be still holds today. But at the same time, there are pockets of theory and thought that have stood the test of time (read Adam Smith). The innovative uses of microeconomics continue to astound, with examination of incentives providing a framework for predictions. Amongst the problems, mistakes and unfortunate influence, there is value. But since there are internal differences, we should be careful about the extent of the influence and power of any one individual or school of thought.\nAnd in that sense, it is probably no different to history."
  },
  {
    "objectID": "posts/economics-from-a-biological-viewpoint.html",
    "href": "posts/economics-from-a-biological-viewpoint.html",
    "title": "Economics from a biological viewpoint",
    "section": "",
    "text": "One of the earlier advocates of using evolutionary biology in economics was Jack Hirshleifer, a professor of economics at the University of California, Los Angeles. Hirshleifer was author of The Dark Side of the Force: Economic Foundations of Conflict Theory, which includes evolutionary analysis of cooperation and conflict, and some discussion of the unification of law, economics and evolutionary biology. The subject of this post is his 1997 article Economics from a Biological Viewpoint from the Journal of Law and Economics. Coming on the heels of E.O. Wilson’s Sociobiology, Hirshleifer states that there is no argument about the utility of using sociobiology in economics. The only open question is how much utility can be gained.\nUnlike Becker’s 1976 article linking economics and sociobiology, Hirshleifer does not draw the parallels and then imperialistically march economics into evolutionary biology. Hirshleifer focuses on the message that sociobiology has for economics, not on “how we can set the biologists straight”.\nHirshleifer is scathing of the disinterest of economists in understanding the foundation of tastes and preferences. Hirshleifer asks if economists are waiting for someone in another field to do the work, and suggests that since it is not being done, economists will need to take on this role. Hirshleifer considers that preferences are governed by how they affect fitness, and adopts a strong version of this approach. He prefers to explain the apparently fitness-reducing actions of modern humans on the basis that the fitness benefits are simply not apparent to us, and not that they might be maladaptive.\nMost of Hirshleifer’s discussion of preferences focuses on altruism, where he is critical of Becker’s approach. Hirshleifer points out that Becker gives the agents arbitrary levels of altruism, with altruistic behaviour emerging as long as there is one altruist. Becker is not using evolutionary biology to look at what the tastes might be, but is trying to supplant an evolutionary explanation of why altruism emerges.\nHirshleifer does note a benefit of Becker’s approach in his discussion of cheating, as Becker’s rotten kid theorem shows that altruism on only one side of the transaction may be enough to prevent it. Hirshleifer also discusses how honest signals can evolve to reduce cheating, but the absence of the handicap principle (only just proposed in 1975) from the discussion highlights why the principle is so important in understanding how signals work.\nHirshleifer is sympathetic to group selection arguments and covers some of the classic group selection models, although he does not subject them to serious analysis as to whether they might be the right explanation. He also relies on what appears to be a version of multilevel selection theory to argue that mixed levels of altruism and free riding can exist in a population, whereby reassortment allows altruists to continue to survive despite their fitness disadvantage relative to free-riders in the same group.\nThe most novel part of the paper (to me) was Hirshleifer’s discussion of specialisation. Specialisation limits competition, but there is a dichotomy between the competitive and cooperative division of labour. In competitive specialisation, species try to avoid direct competition by choosing a narrow niche, while cooperative specialisation allows for gains from trade. Hirshleifer argues that biologists have more subtle understanding of specialisation as they recognise the variety of dimensions across which it occurs.\nOne dimension of specialisation is the distinction between “K-strategies” that make superior use of resources in constrained environments and “r-strategies” that pioneer and settle unfilled environments. Hirshleifer sees selection between these strategies as having had an effect in American history:\n\n[H]uman individuals, families, races etc. are biological entities which may be regarded as choosing competitive strategies. Martial races may concentrate on success through politics, conflict, or violence (“interference strategy”); others may have proliferated and extended their sway through high birth rates; others through lower birth rates but superior efficiency in utilizing resources (“exploitation strategy”). The r-strategist pioneering human type was presumably selected for in the early period of American history - a period long enough for genetic evolution, though cultural adaptation may have been more important. This type was not entirely antisocial; altruist “pioneer” virtues such as mutual defense and sharing in adversity can emerge under r-selection. In the present more crowded conditions the preferred forms of altruism represent “urban” virtues of a negative rather than positive sort: tolerance, nonaggressiveness, and reproductive restraint. Even today it seems likely that a suitable comparison of populations in environments like Alaska on the one hand and New York City on the other would reveal differential genetic (over and beyond merely cultural) adaptations.\n\nOne other interesting point that Hirshleifer covers is teleology. Biologists generally consider evolution to be directionless, in contrast to the economic story of the invisible hand leading to positive outcomes. Hirshleifer argues that the imperfections in nature are largely due to a lack of property rights founded on law and government. As a result, there is more chance of an optimal outcome in human economies than in nature. However, Hirshleifer is not naive, and he notes that the institutions that provide the rule of law and government may also be used to steer outcomes away from optimality.\nThere is plenty of other material in the article worth reading, although some of it feels dated. This includes a section of the article on the evolutionary economics of Alchian, Nelson and Winter, which Hirshleifer terms “quasi-biological” and is worth reading for the discussion of whether businesses actually maximise."
  },
  {
    "objectID": "posts/economic-mobility-and-reproductive-success.html",
    "href": "posts/economic-mobility-and-reproductive-success.html",
    "title": "Economic mobility and reproductive success",
    "section": "",
    "text": "Tyler Cowen writes:\n\nHow much of immobility is due to “inherited talent plus diminishing role for random circumstance”?  Is not this cause of immobility very different — both practically and morally — from such factors as discrimination, bad schools, occupational licensing, etc.?  What are you supposed to get when you combine genetics with meritocracy?\n\nI have  written before about how decreased social mobility may be a sign of equalised opportunity. In a perfect meritocracy, assortment will largely be through genes.\nHowever, there can still be social mobility even though assortment is through genetic factors. This is through differential reproductive success. If an economic class has  more children, many of those children move up or down the social scale as there is no longer room for them in their cohort.\nThis social mobility can have some interesting effects. If, for example, the bottom 80 per cent of the economic distribution has much higher fertility than the top 20 per cent, some of those born in the bottom 80 per cent will move into the top  quintile. If those who move up the scale do not experience any increase in income (as their productivity has not increased, only their prevalence has changed), the relative wealth of those born to someone from the initial top 20 per cent may increase. Social mobility appears to increase inequality through concentrating wealth in the less fertile top.\nGreg Clark describes the converse situation in A Farewell to Alms. In pre-Industrial Revolution England, the rich had higher reproductive success. There was necessarily downward social mobility. Over time, the population became largely composed of those from the top. Productivity increased among the lower classes, but many of them weren’t from the lower classes at all."
  },
  {
    "objectID": "posts/economic-cosmology-the-rational-egotistical-individual.html",
    "href": "posts/economic-cosmology-the-rational-egotistical-individual.html",
    "title": "Economic cosmology - The rational egotistical individual",
    "section": "",
    "text": "The social sciences have recently become a common battleground for debates about group selection. From Jonathan Haidt’s use of group selection to explain “groupish” traits in humans, to some of the recent rumblings about cultural group selection (as expressed in a debate triggered by Steven Pinker), and even at last years Consilience Conference, group selection has undergone a revival in the social sciences.\nOne social science in which group selection has re-emerged is economics, with the most recent occurrence in the Journal of Economic Behavior & Organization special issue, Evolution as a General Theoretical Framework for Economics and Public Policy.\nIn that special issue, group selection is one of the central threads of an article by John Gowdy and colleagues, who take apart three Western “cosmologies” that the authors consider have influenced economic thought. These cosmologies are that “natural man” is a rational, self-sufficient and egotistical individual, that competition between individuals can lead to a well-functioning society, and that there exists an optimal state of nature (equilibrium). I will deal with each of these three cosmologies in separate posts, with this post on the nature of “natural man”.\nGowdy and colleagues hint at some of the previous debate over whether man is purely self-interested, pointing to the origins of the cosmology in work by Pareto, Jevons and Walras, along with critiques of this view by Edgeworth, Veblen and some modern writers. However, the bulk of the argument developed in the paper relates to the nature of groups.\nTheir opening argument is that group level phenomena can affect individual behaviour, which can then affect the economic system as a whole. This is a fair point, and was one of the central themes of An Economic Theory of Greed, Love, Groups and Networks that I recently reviewed. Humans are highly social and highly responsive to group-level incentives.\nThis point, however, leads Gowdy and colleagues to group or multilevel selection. As they frame the question:\n\nHow can natural selection favor traits that are “for the good of the group” when they are selectively disadvantageous within groups? The answer is, by a process of between-group selection. Groups of solid citizens outcompete other groups, even if solid citizens are not selectively advantageous within their own groups.\n\nThe authors note the arguments of George Williams and friends of the 1960s, who generally considered that while group level selection can theoretically occur, between group selection was invariably weak as there was too much mixing between groups. Selfish individuals within a group undermine any potential for group selection. At this point, however, Gowdy and colleagues rewrite history when they suggest that the second of these points - the weak potential for group selection - has been overturned. They go as far as stating that:\n\nThe consensus among evolutionists that humans are a highly group-selected species (as conceptualized within multi-level selection theory) challenges the individualistic assumption of economics at its core.\n\nThis is where I need to be careful with language, particularly given the mention of multilevel selection theory.\nThe phrase “group selection” is often used in reference to selection between populations, and this was the context for the debates between Wynne-Edwards, Williams, Maynard Smith, Dawkins and others. In the mid to late 1970s, David Sloan Wilson (one of the authors of the paper the subject of this post) developed a different conception of group selection, generally termed multilevel selection, which looks at the development of individual traits within group structured populations. The groups are not population size groups, but are rather “trait groups” within populations that occur through non-random assortment of altruistic genes. These are not groups in the sense that people typically use the word. For example, when I engage in a trade with someone, we could be considered a group under this multilevel selection framework.  (I have written extended posts explaining the concept of multilevel selection here and here.)\nIn multilevel selection theory, selection occurs at all levels and the multilevel selection framework allows you to partition the selection effects between these levels. Importantly, multilevel selection theory is generally accepted as a different way to conceptualising the same evolutionary processes as are captured by the concept of inclusive fitness (kin selection). Gowdy and colleagues hat tip to this concept by noting that altruistic actions can be made to appear selfish by altering the frame of comparison.\nBut when Gowdy and colleagues flick back and forth between the group and multilevel selection terms, and particularly in their reference to Wynne-Edwards and Williams, I become confused about which group selection concept they are talking about. Are they talking about the old group selection concept as debated in the 1960s? If so, then their statement that evolutionary biologists accept that humans are a highly group selected species would be wrong. That idea appears as contested today as it ever was.\nBut what of the newer multilevel selection theory? Yes, biologists would agree that multilevel selection occurs in humans, although they differ in their opinion as to its usefulness as a frame of analysis. But since it is just a frame of analysis, what does it mean to say that humans are a group selected species? By using the other frame of reference, are we also a highly kin selected species?\nThe ability to change the frame of comparison can be seen in the examples of traits they use to illustrate that humans are a highly group selected species. We are other-regarding as to kin. There are clear benefits to self through having a social brain and the theory of mind to put ourselves in others’ shoes. And there can be huge personal benefits to being cooperative (particularly with kin).\nNow, this is not to say that I do not agree with the authors’ overarching point. Despite many occasions where other-regarding considerations have been included in economics analysis (economics is a huge field with many practitioners), it does not happen as often as it possibly should. Pressures within groups, our social natures and our “groupish” traits have major effects on our actions.\nBut I am not certain that the multilevel selection approach is the optimal way to sell the concept that these groupish, social traits need to be considered in economic analysis. If evolutionary biologists tend to disagree on that point, are you going to be able to sell it to the economists? A starting point would be to at least get the nature of those traits on the table, present the design process from both frames of reference, and make it clear that it is not the old group selection concept that you are talking about.\nAs an end note, the papers in the special issue are easy to read and are not full of the usual mathematical signalling contained in economics journals. I recommend reading them yourself.\nMy series of posts on the Journal of Economic Behavior & Organization special issue, Evolution as a General Theoretical Framework for Economics and Public Policy, are as follows:\n\nSocial Darwinism is back - a post on one of the popular press articles that accompanied the special issue, a piece by David Sloan Wilson called A good social Darwinism.\nFour reasons why evolutionary theory might not add value to economics - a post on David Sloan Wilson and John Gowdy’s article Evolution as a general theoretical framework for economics and public policy\nEconomic cosmology - The rational egotistical individual (this post) - a post on John Gowdy and colleagues’ article Economic cosmology and the evolutionary challenge \nEconomic cosmology - The invisible hand - a second post on Economic cosmology and the evolutionary challenge \nEconomic cosmology - Equilibrium - a third post on Economic cosmology and the evolutionary challenge\nDesign principles for the efficacy of groups - a post on David Sloan Wilson, Elinor Ostrom and Michael E. Cox’s article Generalizing the core design principles for the efficacy of groups"
  },
  {
    "objectID": "posts/economic-cosmology-equilibrium.html",
    "href": "posts/economic-cosmology-equilibrium.html",
    "title": "Economic cosmology - Equilibrium",
    "section": "",
    "text": "Although most of my interest in integrating evolutionary biology into economics concerns treating people as evolved (or evolving) animals, economists can also learn a lot from the dynamic analysis of biological systems. This thought is shared by John Gowdy and colleagues and is the subject of their third economic cosmology, equilibrium, in their article Economic cosmology and the evolutionary challenge from the Journal of Economic Behavior & Organization special issue, Evolution as a General Theoretical Framework for Economics and Public Policy. (The first two cosmologies, the rational man and the invisible hand, are the subject of two earlier posts.)\nAs Gowdy and colleagues put it, ecosystems are complex, often out of equilibrium and rarely tend towards an equilibrium state. It was not always seen that way and some of the equilibrium mind-state remains (particularly in common conceptions), but an equilibrium view of ecosystems has been abandoned. That is not to say that individual agents or groups within an ecosystem should never be treated as tending toward an equilibrium (say, a gene moving to fixation), but as a whole, ecosystems do not maximise anything.\nGowdy and colleagues suggest that economic concepts of equilibrium should be seen in the same way, and discarded as was the concept of harmonious natural order in biology. I am sympathetic to their argument, although Gowdy and colleagues pick some interesting points with which to make it.\nTheir first relates to Milton Friedman’s The Methodology of Positive Economics (worth a read), in which Friedman argued that inefficient firms will be driven out of business, leaving only the efficient profit maximising firms in the competitive market. This allows firms to be treated as pure profit maximisers. Gowdy and colleagues suggest that the problem with Friedman’s approach is not that it is evolutionary, but rather that it is not evolutionary enough. This is a fair enough point, as adaptionist hypotheses such as this require testing to move beyond a ‘just-so’ story. For example, Gowdy and colleagues refer to studies suggesting that firms that are narrowly focussed on profit are more likely to go out of business. However, this might not be evidence that profit maximisation does not lead to firm success, and could point more to the complexity of running a business in a modern economy. Gowdy and colleagues highlight this complexity when they relate Nelson and Winter’s important point that firms shape the environment themselves, adding a further layer of complication to any strategic consideration.\nMore importantly, however, what does this mean for the concept of equilibrium? Even though an ecosystem may not maximise anything, the individuals within it may. In studying them, the biological agents are often treated as maximisers. Similarly, an economy doesn’t maximise, but treating firms as maximisers may serve some purposes. This allows us to get to the more substantial point. In a complex, shifting landscape, as firms struggle to maximise profits with varying strategies and degrees of success, changing the environment as they go, there is no guarantee that they will maximise profits across the economy at any time. A strategy that maximises profits at one moment may not the next. And even if profits tend to be maximised, what of general wellbeing or other economic measures?\nThe flip side to this observation is the massive increases in wealth of the last 200 years. Even though there is no guarantee that an economy will tend towards an equilibrium that maximises wellbeing, modern economic structures have done a pretty good job of creating stable upward growth.\nGowdy and colleagues focus more attention on the policy implications of overturning the concept of equilibrium than they do for the other two cosmologies. They note that although there has been a marked increase in material prosperity driven by market competition, there has been increased risk taking such as environmental degradation and resource depletion. Myopia, biased time preferences or other behavioural anomalies may drive that risk-taking, with Gowdy and colleagues noting that markets have not yet constrained them.\nFurther, they consider that evolutionary theory may be of use in managing threats to prosperity. This could be through an understanding of how small groups interact (a subject of a later article in the special issue) and of the constraints that must be applied to self-interest to achieve the common good. It is on this point that I am more skeptical. While we should take the lesson of biology that equilibrium is a shaky concept, the lack of equilibrium does not immediately point to the benefit of economic interventions. After all, if firms can’t even maximise their own profits, we need to be humble about our ability to control higher level outcomes. Our experience in trying to manage ecosystems suggests that ‘managing’ an economy is a difficult task.\nMy series of posts on the Journal of Economic Behavior & Organization special issue, Evolution as a General Theoretical Framework for Economics and Public Policy, are as follows:\n\nSocial Darwinism is back - a post on one of the popular press articles that accompanied the special issue, a piece by David Sloan Wilson called A good social Darwinism.\nFour reasons why evolutionary theory might not add value to economics - a post on David Sloan Wilson and John Gowdy’s article Evolution as a general theoretical framework for economics and public policy\nEconomic cosmology - The rational egotistical individual - a post on John Gowdy and colleagues’ article Economic cosmology and the evolutionary challenge \nEconomic cosmology - The invisible hand - a second post on Economic cosmology and the evolutionary challenge \nEconomic cosmology - Equilibrium (this post) - a third post on Economic cosmology and the evolutionary challenge\nDesign principles for the efficacy of groups - a post of David Sloan Wilson, Elinor Ostrom and Michael E. Cox’s article Generalizing the core design principles for the efficacy of groups"
  },
  {
    "objectID": "posts/dysgenics-and-war.html",
    "href": "posts/dysgenics-and-war.html",
    "title": "Dysgenics and war",
    "section": "",
    "text": "Bryan Caplan has picked up on an interesting interview of Irving Fisher in the New York Times archives. Fisher states:\n\nIf war would weed out only the criminal, the vicious, the feeble-minded, the insane, the habitual paupers, and others of the defective classes, it might lay claim, with some show of justice, to the beneficent virtues sometimes ascribed to it.\nBut the truth is that its effects are diametrically opposite. It eliminates the young men, who should be the fathers of the next generation - men medically selected as the largest, strongest, most alert, and best endowed in every way, and at the very age when they normally would be performing the most important function which men can perform, that of fathering posterity.\nTheir less endowed fellows, medically rejected from military service, because of defects in stature, eyesight, hearing, mentality, &c, are left at home to reproduce the race.\nThe result must be a tendency toward race degeneration, and that we may look forward to as a result of this great war. …\n\nIf Fisher had of known of the events of World War II and the systematic murder of Jews, he might have been more even horrified about the dysgenic effects of war. His feeling that war was decimating Europe while leaving the United States as the last remnant of civilization might have increased if he had of seen the flow of Jewish refugees from Europe to the United States (which I consider to be one of the most important boosts that the United States has ever received).\nWould Fisher have considered modern wars dysgenic? I have seen estimates (not sure how accurate) that the average IQ of the United States military is slightly above 100. However, I expect that the troops most likely to be killed (i.e. not the officers or airforce) would likely be below that mark. Most wars are now fought in developing or unstable countries, which given Fisher’s views on race, he would have seen as more benign than a war decimating Europe.\nSo if modern wars were eugenic, would Fisher end his pacifism and wish for world-peace?\nAt the end of Caplan’s post, after asking some good questions that the interviewer should have asked Fisher (I would guess that Fisher would have been comfortable answering a couple of them), Caplan summaries as follows:\n\nWe’ve learned so much from human genetic research. But when I read Fisher, I understand why the subject terrifies so many people. Hereditarianism combined with inane, half-baked moral philosophy does indeed logically imply Nazi-style homicidal mania. But don’t blame the facts of human genetics. Blame the inane, half-baked moral philosophy.\n\nWhat is the appropriate boundary when discussing genetics and policy? Take this footnote from Caplan’s Cato Unbound essay from earlier this year, in which Caplan gives a slightly eugenic flavour to his policy recommendation:\n\nThere is at least one major reason to think that natalist tax credits are better than simple estimates suggest. Quebec’s program paid baby bonuses to everyone. My proposal, in contrast, only rewards parents who actually pay taxes. Since income runs in families, the extra children born are especially likely to be net taxpayers.\n\nSimilarly, I have noted the effect of incarcerating violent males in their mating prime. Obviously, the observations of Caplan and myself are at opposite ends of the spectrum to Fisher’s interventionist approach. But at what point is it fair to note a eugenic or dysgenic effect? Or use them as a pillar of an argument?"
  },
  {
    "objectID": "posts/dubreuils-human-evolution-and-the-origins-of-hierarchies.html",
    "href": "posts/dubreuils-human-evolution-and-the-origins-of-hierarchies.html",
    "title": "Dubreuil’s Human Evolution and the Origins of Hierarchies",
    "section": "",
    "text": "Benoit Dubreuil’s Human Evolution and the Origins of Hierarchies seeks to explain two historical transitions in social hierarchies in human (and pre-human) history. The first is the transition from dominance hierarchies, such as those lived in by our common ancestor with the chimpanzee, to egalitarian social relationships. The second is from those egalitarian relationships to the large-scale, state-based hierarchies we see today.\nThe assumption as to the existence of the first transition is largely taken from the work of Christopher Boehm (Hierarchy in the Forest: The Evolution of Egalitarian Behavior). I have not read Boehm’s work yet, so I will leave this assumption alone apart from noting that when talking about the end of dominance hierarchies, this is not to suggest that there are no hierarchies. Rather, they are based on cooperation, coalitions and other socially based arrangements, as opposed to raw strength and aggression. As noted in the work of Napoleon Chagnon, egalitarian societies can have very unequal distributions in areas such as access to mates.\nDubreuil’s argument is that each transition was driven by the evolution of the human mind. Dominance hierarchies were destabilised as early humans gained the ability to cooperate and coordinate to bring down dominant individuals. The re-emergence of hierarchies, although different in form, was driven by the cognitive changes associated with the behavioural modernisation of humans. The institutions necessary for efficient enforcement of norms in a large-scale society require cognitive abilities that humans did not have at the time of their earlier egalitarian societies.\nOn one level, the argument behind the second transition must be right. The common ancestors of humans and chimpanzees, and many earlier humans, simply did not have the cognitive firepower to create the institutions underlying a modern state. The more particular claim of Dubreuil that the emergence of state-based hierarchies are directly linked to these cognitive changes faces some challenges due to the gap in time between the emergence of behaviourally modern humans (as identified by evidence such as from the fossil record) and the establishment of agriculture and its associated large-scale societies. One potential reconciliation is that humans continued to evolve beyond the time when he (and most anthropologists) would consider them to be behaviourally modern. As I would claim, humans have continued to evolve through to today.\nDubreuil’s thesis does create an interesting challenge for those that claim that human nature supports egalitarian social structures. If the departure from those structures was driven by the evolution of the mind, has human nature changed such that these egalitarian relationships are no longer stable or feasible? It may be possible to argue that an egalitarian society is the preferred social arrangement, but the argument must be more sophisticated than simply claiming that egalitarian structures were the predominant evolutionary environment faced by early humans.\nDubreuil’s book started life as a thesis, and to some extent, it still feels like one. The first two chapters, in which Dubreuil establishes the foundations of norm enforcement in humans, form an excellent literature review on experiments about cooperation, fairness, punishment and envy. It is worth grabbing the book for these alone. In the other three chapters, Dubreuil crafts the argument. Dubreuil convinced me of the general idea, but with issues such as the timing of the transition to modern behavior, there is some way to go before the finer points of his argument will be convincing."
  },
  {
    "objectID": "posts/domingoss-the-master-algorithm-how-the-quest-for-the-ultimate-learning-machine-will-remake-our-world.html",
    "href": "posts/domingoss-the-master-algorithm-how-the-quest-for-the-ultimate-learning-machine-will-remake-our-world.html",
    "title": "Pedro Domingos’s The Master Algorithm: How the Quest for the Ultimate Learning Machine Will Remake Our World",
    "section": "",
    "text": "My view of Pedro Domingos’s The Master Algorithm: How the Quest for the Ultimate Learning Machine Will Remake Our World depends on which part of the book I am thinking about.\nThe opening and the close of the book verge on techno-Panglossianism. The five chapters on the various “tribes” of machine learning, plus the chapter on learning without supervision, are excellent. And I simply don’t have the knowledge to judge the value of Domingos’s reports on his own progress to the master algorithm.\nBefore getting to the details, The Master Algorithm is a book on machine learning. Machine learning involves the development of algorithms that can learn from data. Domingos describes it as computers programming themselves, but I would prefer to describe it as humans engaging in a higher level of programming. Give the computer some data and the objective, provide a framework for developing the solution (each of Domingos’s tribes has a different approach to this), and let the computer develop it.\nMachine learning’s value is becoming more apparent with increasing numbers of problems involving “big data” and mountains of variables that cannot be feasibly be incorporated into explicitly designed programs. Tasks such as predicting the tastes of Amazon’s customers or deciding which updates to show each Facebook user are effectively intractable given the millions of choices available. In response, the Facebooks and Amazons of the world are designing learning algorithms that can use the massive amounts of data available to attempt to determine what their customers or users want.\nSimilarly, explicitly programming a self-driving car for every possible scenario is not feasible. But train it on massive amounts of data and it can learn to drive itself.\nThe master algorithm of the book’s title is a learning algorithm that can be used across all domains. Today there are five tribes (as categorised by Domingos), each with their own master algorithm. The ultimate master algorithm combines them into a general purpose learning machine.\nThe first tribe, the symbolists, believe in the power of logic. Their algorithms build sets of rules that can classify the examples in front of it. Induction, or as Domingos notes, inverse deduction, can be used to generate further rules to fill in the gaps.\nTo give the flavour of this approach, suppose you are trying to find the conditions under which certain genes are expressed. You run a series of experiments and your algorithm generates an initial set of rules from the results.\n\nIf the temperature is high, gene A is expressed.\nIf the temperature is high, genes B and D are not expressed.\nIf gene C is expressed, gene D is not.\n\nGaps in these rules can then be filled in by inverse deduction. From the above, the algorithm might induce If gene A is expressed and gene B is not, gene C is expressed. This could then be tested in experiments and possibly form the basis for further inductions. These rules are then applied to new examples to predict whether the gene will be expressed or not.\nOne tool in the symbolist toolbox is the decision tree. Start at the first rule, and go down the branch pointed to by the answer. Keep going until you reach the end of a branch. Considering massive bodies of rules together is computationally intensive, but the decision tree saves on this by ordering the rules and going through them one-by-one until you get the class. (This also solves the problem of conflicting rules.)\nThe second tribe are the connectionists. The connectionists take their inspiration from the workings of the brain. Similar to the way that connections between neurons in our brain are shaped by experience, the connectionists build a model of neurons and connect them in a network. The strength of the connections between the neurons is then determined by training on the data.\nOf the tribes, the connectionists could be considered to be in the ascendency at the moment. Increases in computational power and data have laid the foundations for the success of their deep learning algorithms - effectively stacks or chains of connectionist networks - in applications such as image recognition, natural language processing and driving cars.\nThe third tribe are the evolutionaries, who use the greatest algorithm on earth as their inspiration. The evolutionaries test learning algorithms by their “fitness”, a scoring function as to how well the algorithm meets its purpose. The fitter algorithms are more likely to live. The successful algorithms are then mutated and recombined (sex) to produce new algorithms that can continue the competition for survival. Eventually an algorithm will find a fitness peak where further mutations or recombination do not increase the algorithm’s success.\nA major contrast with the connectionists is the nature of evolutionary progress. Neural networks start with a predetermined structure. Genetic algorithms can learn their structure (although a general form would be specified). Backpropogation, the staple process by which neural networks are trained, starts from an initial random point for a single hypothesis but then proceeds deterministically in steps to the solution. A genetic algorithm has a sea of hypotheses competing at any one moment, with the randomness of mutation and sex potentially producing big jumps at any point, but also generating many useless algorithm children.\nThe fourth tribe are the Bayesians. The Bayesian’s start with a set of hypotheses that could be used to explain the data, each of which has a probability of being true (their ‘priors’). Those hypotheses are then tested against the data, with those hypotheses that better explain the data increasing in their probability of being true, and those that can’t decreasing in their probability. This updating of the probability is done through Bayes’ Rule. The effective result of this approach is that there is always a degree of uncertainty - although often the uncertainty relating to improbable hypotheses is negligible.\nThis Bayesian approach is typically implemented through Bayesian networks, which are arrangements of events that each have specified probabilities and conditional probabilities (the probability that an event will occur conditional on another event or set of events occurring). To prevent explosions in the number of probability combinations required to specify a network, assumptions about the degree of independence between events are typically made. Despite these possibly unrealistic assumptions, Bayesian networks can still be quite powerful.\nThe fifth and final tribe are the analogisers, who, as the name suggests, reason by analogy. Domingos suggests this is perhaps the loosest tribe, and some members might object to being grouped together, but he suggests their common reliance on similarity justifies their common banner.\nThe two dominant approaches in this tribe are nearest neighbour and support vector machines. Domingos describes nearest neighbour as a lazy learner, in that there is no learning process. The work occurs when a new test example arrives and it needs to be compared across all existing examples for similarity. Each data point (or group of data points for k-nearest neighbour) is its own classifier, in that the new example is classified into the same class as that nearest neighbour. Nearest neighbour is particularly useful in recommender systems such as those run by the Netflixes and Amazons of the world.\nSupport vector machines are a demonstration of the effectiveness of gratuitously complex models. Support vector machines classify examples by developing boundaries between the positive and negative examples, with a specified “margin” of safety between the examples. They do this by mapping the points into a hyper-dimensional space and developing boundaries that are straight lines. The examples along the margin are the “support vectors”.\nOf Domingos’s tribes, I feel a degree of connection to them all. Simple decision trees can be powerful decision tools, despite their simplicity (or possibly because of it). It is hard not to admire the progress of the connectionists in recent years in not just technical improvement but also practical applications in areas such as medical imaging and driverless cars. Everyone seems to be a Bayesian nowadays (or wants to be), including me. And having played around with support vector machines a bit, I’m both impressed and perplexed by their potential.\nFrom a machine learning perspective, it is the evolutionaries I feel possibly the least connection with. Despite my interest and background in evolutionary biology, it’s the one group I haven’t seen practically applied in any of the domains I operate. I’ve read a few John Holland books and articles (Holland being one of the main protagonists in the evolutionary chapter) and always appreciate the ideas, but have never felt close to the applications.\nOutside of the chapters on the five tribes, Domingos’s Panglossianism grates, but it is relatively contained to the opening and closing of the book. In Domingos’s view, the master algorithm will make stock market crashes fewer and smaller, and the play of our personal algorithms with everyone else’s will make our lives happier, longer and more productive. Every job will be better than it is today. Democracy will work better because of higher bandwidth communication between voters and politicians.\nBut Domingos’s gives little thoughts to what occurs where people have different algorithms, different objectives, different data they have trained their algorithm on and, in effect, different beliefs. Little thought is given to the complex high-speed interaction of these algorithms.\nThere are a few other interesting threads in the books worth highlighting. One is the idea that you need bias to learn. If you don’t have preconceived notions of the world, you could conceive of a world where everything you haven’t seen is the opposite of what you predict (known as the ‘No free lunch theorem’).\nAnother is the idea that once computers get to a certain level of advancement, the work of scientists will largely be trying to understand the outputs of computers rather than generate the outputs themselves.\nSo all up, a pretty good read. For a snapshot of the book, the Econtalk episode featuring Domingos is (as usual) excellent."
  },
  {
    "objectID": "posts/does-presuming-you-can-take-a-persons-organs-save-lives.html",
    "href": "posts/does-presuming-you-can-take-a-persons-organs-save-lives.html",
    "title": "Does presuming you can take a person’s organs save lives?",
    "section": "",
    "text": "I’ve pointed out several times on this blog the confused story about organ donation arising from Johnson and Goldstein’s Do Defaults Save Lives? (ungated pdf). Even greats such as Daniel Kahneman are not immune from misinterpreting what is going on.\nAgain, here’s Dan Ariely explaining the paper:\n\nOne of my favorite graphs in all of social science is the following plot from an inspiring paper by Eric Johnson and Daniel Goldstein. This graph shows the percentage of people, across different European countries, who are willing to donate their organs after they pass away. …\nBut you will notice that pairs of similar countries have very different levels of organ donations. For example, take the following pairs of countries: Denmark and Sweden; the Netherlands and Belgium; Austria and Germany (and depending on your individual perspective France and the UK). These are countries that we usually think of as rather similar in terms of culture, religion, etc., yet their levels of organ donations are very different.\nSo, what could explain these differences? It turns out that it is the design of the form at the DMV. In countries where the form is set as “opt-in” (check this box if you want to participate in the organ donation program) people do not check the box and as a consequence they do not become a part of the program. In countries where the form is set as “opt-out” (check this box if you don’t want to participate in the organ donation program) people also do not check the box and are automatically enrolled in the program. In both cases large proportions of people simply adopt the default option.\n\n\n\nJohnson and Goldstein (2003) Organ donation rates in Europe\n\n\n\nI keep hearing this story in new places, so it’s clearly got some life to it (and I’ll keep harping on about it). The problem is that there is no DMV form. These aren’t people “willing” to donate their organs. And a turn to the second page of Johnson and Goldstein’s paper makes it clear that the translation from “presumed consent” to donation appears mildly positive but is far from direct. 99.98% of Austrians (or deceased Austrians with organs suitable for donation) are not organ donors.\nAlthough Johnson and Goldstein should not be blamed for the incorrect stories arising from their paper, I suspect their choice of title - particularly the word “default” - has played some part in allowing the incorrect stories to linger. What of an alternative title “Does presuming you can take a person’s organs save lives?”\nOne person who is clear on the story is Richard Thaler. In his surprisingly good book Misbehaving (I went in with low expectations after reading some reviews), Thaler gives his angle on this story:\n\nIn other cases, the research caused us to change our views on some subject. A good example of this is organ donations. When we made our list of topics, this was one of the first on the list because we knew of a paper that Eric Johnson had written with Daniel Goldstein on the powerful effect of default options in this domain. Most countries adopt some version of an opt-in policy, whereby donors have to take some positive step such as filling in a form in order to have their name added to the donor registry list. However, some countries in Europe, such as Spain, have adopted an opt-out strategy that is called “presumed consent.” You are presumed to give your permission to have your organs harvested unless you explicitly take the option to opt out and put your name on a list of “non-donors.”\nThe findings of Johnson and Goldstein’s paper showed how powerful default options can be. In countries where the default is to be a donor, almost no one opts out, but in countries with an opt-in policy, often less than half of the population opts in! Here, we thought, was a simple policy prescription: switch to presumed consent. But then we dug deeper. It turns out that most countries with presumed consent do not implement the policy strictly. Instead, medical staff members continue to ask family members whether they have any objection to having the deceased relative’s organs donated. This question often comes at a time of severe emotional stress, since many organ donors die suddenly in some kind of accident. What is worse is that family members in countries with this regime may have no idea what the donor’s wishes were, since most people simply do nothing. That someone failed to fill out a form opting out of being a donor is not a strong indication of his actual beliefs.\nWe came to the conclusion that presumed consent was not, in fact, the best policy. Instead we liked a variant that had recently been adopted by the state of Illinois and is also used in other U.S. states. When people renew their driver’s license, they are asked whether they wish to be an organ donor. Simply asking people and immediately recording their choices makes it easy to sign up. In Alaska and Montana, this approach has achieved donation rates exceeding 80%. In the organ donation literature this policy was dubbed “mandated choice” and we adopted that term in the book."
  },
  {
    "objectID": "posts/does-genetic-diversity-increase-innovation.html",
    "href": "posts/does-genetic-diversity-increase-innovation.html",
    "title": "Does genetic diversity increase innovation?",
    "section": "",
    "text": "Last week I presented a summary of the method and findings of Ashraf and Galor’s American Economic Review paper The ‘Out of Africa’ Hypothesis, Human Genetic Diversity, and Comparative Economic Development (for the latest ungated version, go here). As discussed in that post, one limb of Ashraf and Galor’s argument is that genetic diversity provides a greater range of traits for the development and implementation of new technologies (which I’ll call innovation). In this post, I look at that claim in more detail.\nThe measure of genetic diversity used by Ashraf and Galor is based on non-protein coding regions of the genome. This is common in population studies to prevent selection from distorting attempts to track evolutionary history. However, Ashraf and Galor note that the genetic diversity shaped by the founder effect as populations moved out of Africa likely included other phenotypically expressed genetic diversity. For example, they note one study in which craniometric (head shape) diversity declines with distance from Africa.\nAt a population level, increased genetic diversity can have evolutionary benefits. Increased diversity provides a broader suite of traits on which natural selection can act. Where the environment changes, it is more likely that traits favourable in that environment will be present, and if the environment is particularly unstable, genetic variation will provide a basis for some of the population to be viable through the range of conditions.\nAshraf and Galor refer to a couple of studies in support of their hypothesis, but their examples do not build a strong case for their specific argument linking genetic diversity and innovation. In one case, they reference a study in which it was found that less diverse, inbred populations of fruit fly became extinct at lower concentrations of salt. However, the authors of that study were unable to differentiate whether inbreeding or lack of diversity drove the outcome. Other studies that isolated the effects of low diversity did not always show the same result (for example, in this study of flies).\nAnother more relevant study found that more diverse honeybee colonies have higher workforce efficiency (I can’t give a a link to this study, but a summary of work in the area is here). This finding provides evidence that diversity has an effect on production outcomes. Ashraf and Galor expand their discussion of the effect of diversity on bee colonies in the Web Appendix, where they discuss an experiment published in Science. In that study, colonies with various levels of diversity were released, with more diverse colonies founding their colonies faster and accumulating food stores more quickly. The authors of the study proposed that greater response variation to changing conditions may be behind the higher fitness of diverse colonies. If there is a trade-off between response thresholds and activity efficiency, or if response thresholds result in some behaviours missing from a worker’s repertoire, higher genetic diversity will provide a basis for the full suite of required traits.\nOne question overhanging these bee studies is the direction of the effect. As the Ashraf and Galor hypothesis has a countervailing force whereby diversity decreases cooperation, diversity could also be expected to harm colony production. Whether diversity is beneficial or not would depend on the relative effect of the two forces. Some of the bee studies note the potential for intra-colonial conflict and ask whether factors such as recombination rates may reduce conflict. But when comparing to the hump shaped relationship between human genetic diversity and economic development, it is not easy to place the bee colonies on the curve. Any effect of diversity on bee colony success could be justified after the fact as being at a certain point on the curve.\nTowards the end of the paper Ashraf and Galor examine the hypothesis for human populations by regressing the number of scientific papers published per year per person against genetic diversity and a range of controls including social infrastructure, years of schooling, risk of malaria and distance to waterways. Continent fixed effects are used, which eliminates comparisons across continents, as well as other controls for sub-Saharan Africa and OPEC. The result of the regression is that genetic diversity is a significant factor in scientific output, with a one per cent increase in diversity linked to an increase in scientific articles per person per year of 0.02.\nDespite this statistical evidence, I am not convinced. Partly, I suspect missing variables - that is, the qualitative traits that have been under selection since the Out of Africa event. The inclusion of controls such as social infrastructure also makes the analysis difficult, as it is closely related to potentially relevant factors such as IQ and levels of trust (the data is available on the AER website, so I should test my musings).\nStill, if I were to favour a hypothesis of diversity affecting innovation, I can see two hypothetical pathways. First diversity may increase innovative activity as it allows for gains to trade. If there are different kinds of intelligence or other innovative traits, a broader basket of traits may provide more opportunity for technological innovation. The bee studies seem to fall into this camp.\nSecond, diversity may provide a higher probability of a favourable innovative trait being present in a new environment. Diversity would lead to a useful trait being present, but that trait would then spread and genetic diversity related to that trait would be eliminated by selection.\nI lean toward the second pathway. Different forms of intelligence are highly correlated (hence the search for the g factor), and we would expect that a group with higher absolute intelligence would outperform a group with more diverse intelligence levels (unless, of course, those at the top end are exceptional).\nHowever, having favoured this second pathway, I am not convinced that either is the case. I prefer a hypothesis that selection pressures pushed traits in a certain direction, with diversity a secondary factor (if a factor at all). Partly this comes from the nature of innovative activity, which is affected by diversity but relies heavily on the innovative capacity of those involved. Innovative capacity is also only a subset of the traits that affect evolutionary success. If the hypothesis was one of evolutionary success, I would give more weight to a diversity hypothesis. This is also one of the many reasons that extrapolating bee or other studies to humans can be problematic. For the bees, we are measuring proxies for evolutionary success, while innovative activity is only one of many factors that may affect evolutionary success for humans.\nGiven the above, further research is required to give the diversity-innovation hypothesis standing. But what approach should be used to tease out this question? Cross-species comparison provides one potential avenue, but how would levels of diversity across species be compared or levels of innovation measured? The low genetic diversity in humans relative to other species may complicate the analysis. I also suspect that a cross-species approach might be more useful in assessing the effect of diversity on levels of cooperation (the subject of a forthcoming post) as it may be too difficult to develop useful indexes of innovative behaviour in other species. Alternatively, further work on isolated human populations could be useful.\nMy preferred direction of research would be to directly analyse the selection that has occurred on the various populations, genetic evidence permitting. I would also examine what other controls are useful in the analysis. IQ is one option, although a Flynn-like hypothesis of economic development and IQ ratcheting up together would make that difficult.\nAs an end note, some of the debate about the paper raises the question of whether Ashraf and Galor were directly relating genetic diversity to economic development, or whether genetic diversity is a proxy for phenotypic diversity unrelated to that genetic diversity (such as language). I have deliberately skirted that issue for this post, but as you can see below, my thoughts are forthcoming.\nMy posts on Ashraf and Galor’s paper on genetic diversity and economic growth are as follows:\n\nA summary of the paper methodology and findings\nDoes genetic diversity increase innovation? (this post)\nDoes genetic diversity increase conflict?\nIs genetic diversity a proxy for phenotypic diversity?\nIs population density a good measure of technological progress?\nWhat are the policy implications of the effects of genetic diversity on economic development?\nShould this paper have been published?\n\nEarlier debate on this paper can also be found here, here, here and here."
  },
  {
    "objectID": "posts/does-equality-increase-conspicuous-consumption.html",
    "href": "posts/does-equality-increase-conspicuous-consumption.html",
    "title": "Does equality increase conspicuous consumption?",
    "section": "",
    "text": "In an interesting paper in the Journal of Consumer Research, Nailya Ordabayeva and Pierre Chandon propose that conspicuous consumption may be higher in a more equal society as it provides an opportunity for a larger increase in relative rank.\nThe benefits of conspicuous consumption are highest when everyone is similar as a small signal can allow someone to jump ahead of the greater number of people clustered in similar income groups. Contrast this to the often heard argument that inequality increases conspicuous consumption, particularly among the lower classes, as there are greater pressures to “keep up with the Joneses”. The lower savings rate of lower-income groups is often cited in support of this claim.\nBefore getting to the empirical side of the study, the logical argument is interesting. If everyone faces the same incentives to increase their conspicuous consumption and have the same capacity to engage in conspicuous consumption (which equality would imply), then they will all do so and no-one’s rank would change. Although the first person to increase their conspicuous consumption might intend to jump a large group of people in status, the reality is that most people would be increasing their conspicuous consumption simply to maintain their relative ranking. The high conspicuous consumption state is the only stable equilibrium.\nThis argument changes if people have different preferences or compete in different arenas for status. Then conspicuous consumption might result in a change in rank if people engage in conspicuous consumption to different degrees.\nMoving to the empirical work, the authors conducted five studies in which they primed the subjects with various scenarios of equality and inequality and surveyed the subjects’ consumption intentions (I should note that I always feel a slight sense of discomfort about the robustness of studies involving priming). Across the studies, the authors found that the people at the bottom felt more satisfied in more equal scenarios, but they were also more likely to engage in conspicuous consumption.\nThe most interesting of these studies involved also priming the subjects with competitive or group assimilation goals. Where there is a more cooperative social context, conspicuous consumption did not increase when equality was increased. However, where people care about social gain in a competitive context, equality boosted conspicuous consumption. The effects of equality on conspicuous consumption could be mediated through the social context.\nSo is this work significant outside of the experimental setting? Even though there is society-wide inequality, inequality is likely to be lower within the groups that people associate and there would be gains to conspicuous consumption within those groups. A low-income person may not be able to compete with the investment banker, but they are unlikely to be pitching to the same audience. However, would exposure to other groups through television or other social mediums affect the perceived gains to conspicuous consumption?\nThe policy implications of this work could be important. As stated by the authors:\n\n[W]e cannot simply assume that increasing equality will reduce consumption and that marketers and policy makers should build a more holistic view of redistribution policies and their consequences. Specifically, our results suggest that the implications of redistribution policies need to be reconsidered for different social environments. For example, we find that increasing income equality succeeds in reducing conspicuous consumption in cooperative environments and when people are indifferent to the social context. This suggests that redistribution policies may be particularly effective if supplemented with policies to promote resistance to social pressure, which focus on relationships with friends and family. Echoing Putnam (2007), the promotion of a broad sense of “we” through popular culture, national symbols, education, and common experiences may not only increase trust but could also reduce conspicuous arms races.\n\nThe question of unintended consequences to policy decision raises its head again."
  },
  {
    "objectID": "posts/does-a-moral-reminder-decrease-cheating.html",
    "href": "posts/does-a-moral-reminder-decrease-cheating.html",
    "title": "Does a moral reminder decrease cheating?",
    "section": "",
    "text": "In The (Honest) Truth About Dishonesty, Dan Ariely describes an experiment to determine how much people cheat:\n\n[P]articipants entered a room where they sat in chairs with small desks attached (the typical exam-style setup). Next, each participant received a sheet of paper containing a series of twenty different matrices … and were told that their task was to find in each of these matrices two numbers that added up to 10 …\nWe also told them that they had five minutes to solve as many of the twenty matrices as possible and that they would get paid 50 cents per correct answer (an amount that varied depending on the experiment). Once the experimenter said, “Begin!” the participants turned the page over and started solving these simple math problems as quickly as they could. …\n\nHere’s an example matrix:\n\n\nmatrix\n\n\n\n\nThis was how the experiment started for all the participants, but what happened at the end of the five minutes was different depending on the particular condition.\nImagine that you are in the control condition… You walk up to the experimenter’s desk and hand her your solutions. After checking your answers, the experimenter smiles approvingly. “Four solved,” she says and then counts out your earnings. … (The scores in this control condition gave us the actual level of performance on this task.)\nNow imagine you are in another setup, called the shredder condition, in which you have the opportunity to cheat. This condition is similar to the control condition, except that after the five minutes are up the experimenter tells you, “Now that you’ve finished, count the number of correct answers, put your worksheet through the shredder at the back of the room, and then come to the front of the room and tell me how many matrices you solved correctly.” …\nIf you were a participant in the shredder condition, what would you do? Would you cheat? And if so, by how much?\nWith the results for both of these conditions, we could compare the performance in the control condition, in which cheating was impossible, to the reported performance in the shredder condition, in which cheating was possible. If the scores were the same, we would conclude that no cheating had occurred. But if we saw that, statistically speaking, people performed “better” in the shredder condition, then we could conclude that our participants overreported their performance (cheated) when they had the opportunity to shred the evidence. …\nPerhaps somewhat unsurprisingly, we found that given the opportunity, many people did fudge their score. In the control condition, participants solved on average four out of the twenty matrices. Participants in the shredder condition claimed to have solved an average of six—two more than in the control condition. And this overall increase did not result from a few individuals who claimed to solve a lot more matrices, but from lots of people who cheated by just a little bit.\n\nThe question then becomes how to reduce cheating. Ariely describes one idea:\n\n[O]ur memory and awareness of moral codes (such as the Ten Commandments) might have an effect on how we view our own behavior.\n… We took a group of 450 participants and split them into two groups. We asked half of them to try to recall the Ten Commandments and then tempted them to cheat on our matrix task. We asked the other half to try to recall ten books they had read in high school before setting them loose on the matrices and the opportunity to cheat. Among the group who recalled the ten books, we saw the typical widespread but moderate cheating. On the other hand, in the group that was asked to recall the Ten Commandments, we observed no cheating whatsoever. And that was despite the fact that no one in the group was able to recall all ten.\nThis result was very intriguing. It seemed that merely trying to recall moral standards was enough to improve moral behavior.\n\nThis experiment comes from a paper co-authored by Nina Mazar, On Amir and Ariely (pdf). (I’m not sure where the 450 students in the book comes from - the paper reports 229 students for this experiment. A later experiment in the paper uses 450. There were also a few differences in this experiment to the general cheating story above. People took their answers home for “recycling”, rather than shredding them, and payment was $10 per correct matrix to two randomly selected students.)\nThis experiment has now been subject to a multi-lab replication by Verschuere and friends. The abstract of the paper:\n\nThe self-concept maintenance theory holds that many people will cheat in order to maximize self-profit, but only to the extent that they can do so while maintaining a positive self-concept. Mazar, Amir, and Ariely (2008; Experiment 1) gave participants an opportunity and incentive to cheat on a problem-solving task. Prior to that task, participants either recalled the 10 Commandments (a moral reminder) or recalled 10 books they had read in high school (a neutral task). Consistent with the self-concept maintenance theory, when given the opportunity to cheat, participants given the moral reminder priming task reported solving 1.45 fewer matrices than those given a neutral prime (Cohen ́s d = 0.48); moral reminders reduced cheating. The Mazar et al. (2008) paper is among the most cited papers in deception research, but it has not been replicated directly. This Registered Replication Report describes the aggregated result of 25 direct replications (total n = 5786), all of which followed the same pre-registered protocol. In the primary meta-analysis (19 replications, total n = 4674), participants who were given an opportunity to cheat reported solving 0.11 more matrices if they were given a moral reminder than if they were given a neutral reminder (95% CI: -0.09; 0.31). This small effect was numerically in the opposite direction of the original study (Cohen ́s d = -0.04).\n\nAnd here’s a chart demonstrating the result (Figure 2):\n\n\nFigure 2\n\n\n\nMulti-lab experiments like this are fantastic. There’s little ambiguity about the result.\nThat said, there is a response by Amir, Mazar and Ariely. Lots of fluff about context. No suggestion of “maybe there’s nothing here”."
  },
  {
    "objectID": "posts/do-economists-satisfice.html",
    "href": "posts/do-economists-satisfice.html",
    "title": "Do economists satisfice?",
    "section": "",
    "text": "In Herbert Simon’s 1978 Swedish Bank prize lecture (pdf), he stated the following:\n\nMilton Friedman sums up his celebrated polemic against realism in theory (1953, p. 41, italics supplied):\n\nComplete “realism” is clearly unattainable, and the question whether a theory is realistic “enough” can be settled only be seeing whether it yields predictions that are good enough for the purpose in hand or that are better than predictions from alternative theories.\n….. This is sometimes even interpreted to mean that economic theories of decision making are not falsified in any interesting or relevant sense when their empirical predictions of microphenomena are found to be grossly incompatible with the observed data. Such theories, we are told, are still realistic “enough” provided that they do not contradict aggregate observations of concern to political economy. Thus economists who are zealous in insisting that economic actors maximize turn around and become satisficers when the evaluation of their own theories is concerned. They believe that businessmen maximize, but they know that economic theorists satisfice.\n\n\nAs I read this quote, I pictured Friedman’s response (I am not aware whether he did respond). Friedman might say that it does not matter whether the assumption about the behaviour of economists is accurate. If we wanted to predict economists’ behaviour, would these predictions be any different based on whether they are satisficers or maximisers? If not, the assumption does not matter either way.\nOn this dimension, I would argue that this assumption matters. If economists were maximisers and not satisficers in the way they built their models and made predictions, economics courses might have changed since the 1960s. Moribund theories would be more readily discarded. And we’d more readily tear down the house that we built.\nOf course, this is probably looking at the wrong dimension. Economists may be satisficers in their models as they are maximisers in other dimensions - career, status, prestige. To achieve those things, it may not matter whether you have made an economic model that accurately reflects the world."
  },
  {
    "objectID": "posts/disease-and-liberalisation.html",
    "href": "posts/disease-and-liberalisation.html",
    "title": "Disease and liberalisation",
    "section": "",
    "text": "Ronald Bailey has written an article for Reason on Randy Thornhill and Corey Fincher’s work linking disease and liberalisation. Bailey writes:\n\nThornhill and Fincher argue that the risk of infectious disease affects elites’ willingness to share power and resources, the general social acceptance of hierarchical authority, and the population’s openness to innovation. Their central idea is that ethnocentrism and out-group avoidance function as a kind of behavioral immune system. Just as individuals have immune systems that fight pathogens, groups of people evolve with local parasites and develop some resistance to them. People who are not members of one’s group may carry new diseases to which the group has not developed defenses. “Thus,” Thornhill and Fincher write, “xenophobia, as a defensive adaptation against parasites to which there is an absence of local adaptation, is expected to be most pronounced in regions of high parasite stress.” …\nThornhill and Fincher believe that more recent advances in medicine and public health are implicated in the post-1950s wave of liberalization that swept over the United States and Western Europe. The advent of penicillin, the arrival of polio vaccines, the elimination of malaria, the chlorination of drinking water, and the reduction in food-borne illnesses all combined to dramatically reduce disease. The authors suggest that if people experience few infections as they grow up, they perceive strangers and novel ways of life as safe; tolerance and the embrace of social, economic, and technological innovation follow. They note that areas of the world in which disease rates remain high have not experienced such liberalization.\n\nDisease sounds a plausible reason for avoiding out-groups, but what is the incentive of an elite to share resources and power in the absence of disease? And can ethnocentrism and out-group avoidance be more simply explained by kin selection or strategic considerations?\nBailey notes the policy implications of this theory:\n\nIf Sachs, Thornhill, Fincher, and  Gelfand are right, reducing a country’s disease burdens should promote the rise of liberal institutions. “If the parasite hypothesis of democratization is supported by additional research,” Thornhill and Fincher write, “humanitarian efforts to reduce human rights violations and to increase human liberties and democracy in general will be most effective if focused on the most fundamental causal level of infectious disease reduction.”\n\nI agree with the objective, but is this causal link in the right direction? Do people with a low disease burden support liberal institutions, or do liberal institutions create the framework under which disease can be controlled?\nIf I had to argue for the former, I would probably use Garrett Jones’s hypothesis that high-IQ is a significant factor in determining the level of trust in a society. If there is a link between disease and IQ, as Thornhill and Fincher have argued, the effect of disease reduction on liberalisation would be largely via the IQ mechanism.\nHowever, I am not convinced of this causal link. More generally, the direction of causation is the central issue with the suite of articles linking parasites to religion, IQ, liberalisation and armed conflict (and I think there are others) that Thornhill and Fincher have produced over the last few years. What Thornhill and Fincher have successfully shown is that parasite loads and a host of characteristics of poorer nations are correlated.\nUltimately, I expect that the causal mechanisms between disease, IQ and institutions largely flow from IQ and institutions to disease. That is not to say that disease cannot reduce IQ - it undoubtedly does in some instances and is possibly part of the feedback loop during development. However, an IQ to disease causal relationship provides a mechanism by which disease declines - high IQ groups develop more liberal institutions, have much more wealth and can more effectively control disease. If causation flows in the other direction, it leaves open why disease prevalence varies greatly between areas that, based on climate, geography and history, should have similar levels of disease."
  },
  {
    "objectID": "posts/design-principles-efficacy-groups.html",
    "href": "posts/design-principles-efficacy-groups.html",
    "title": "Design principles for the efficacy of groups",
    "section": "",
    "text": "In Tim Harford’s article contrasting Lin Ostrom and Garrett Hardin’s approaches to the tragedy of the commons, he writes:\n\nShe [Ostrom] persevered and secured her PhD after studying the management of fresh water in Los Angeles. In the first half of the 20th century, the city’s water supply had been blighted by competing demands to pump fresh water for drinking and farming. By the 1940s, however, the conflicting parties had begun to resolve their differences. In both her PhD, which she completed in 1965, and subsequent research, Lin showed that such outcomes often came from private individuals or local associations, who came up with their own rules and then lobbied the state to enforce them. In the case of the Los Angeles water producers, they drew up contracts to share their resources and the city’s water supply stabilised.\nIt was only when Lin saw Hardin lecture that she realised that she had been studying the tragedy of the commons all along. …\nIn his essay, Hardin explained that there was no way to manage communal property sustainably. The only solution was to obliterate the communal aspect. Either the commons could be nationalised and managed by the state – a Leviathan for the age of environmentalism – or the commons could be privatised, divided up into little parcels and handed out to individual farmers, who would then look after their own land responsibly. …\nBut Lin Ostrom could see that there must be something wrong with the logic. Her research on managing water in Los Angeles, watching hundreds of different actors hammer out their messy yet functional agreements, provided a powerful counter-example to Hardin. She knew of other examples, too, in which common resources had been managed sustainably without Hardin’s black-or-white solutions.\n\nOstrom identified eight design principles that allow groups to manage common pool resources sustainably. Without them, we’re closer to Hardin’s conception of the tragedy. These principles are:\n\nClearly defined boundaries\nCongruence between appropriation and provision rules and local conditions (i.e. proportional equivalence between benefits and costs)\nCollective-choice arrangements - those affected by the rules can participate in modifying the rules.\nMonitoring\nGraduated sanctions\nConflict-resolution mechanisms\nMinimal recognition of rights to organize\nFor larger systems: Appropriation, provision, monitoring, enforcement, conflict resolution, and governance activities are organized in multiple layers of nested enterprises.\n\nIn the special issue of the Journal of Economic Behavior & Organization, Evolution as a General Theoretical Framework for Economics and Public Policy, Ostrom, together with David Sloan Wilson and Michael Cox, proposed that these principles can be generalised beyond the common pool resource problem. First, where these principles hold, they also provide the ideal social environment for the evolution of group-level adaptations. If you have read any of my earlier posts on the special issue (links below), it comes as no surprise that multilevel selection theory forms a strong part of Wilson, Ostrom and Cox’s evolutionary argument. Second, the principles can be applied to a wider range of groups.\nThe most interesting application of this argument relates to the first design principle, the need for clearly defined boundaries. When group selection was first framed as a concept, it was seen as competition between groups in the way that an ordinary person might see it; say, different tribes fighting against each other. But this style of group selection ran into problems in that the group boundaries are not as clear as one might think. People move between groups. The victor in war takes the women, children and surviving males into their own group. This lack of clarity was a central plank of many criticisms of this style of group selection.\nThe conception of group selection labelled as multilevel selection theory averts this problem through adopting a more flexible concept of groups. Individuals take part in many groups, which each group differing by context. For example, a person may have many trading relationships, each being conceptualised as a group. They have family groups. They form coalitions. As such, there can be considered to be clearly defined boundaries for each of these different groups. (For earlier posts discussing how this multilevel selection framing works, see here and here) But when put this way, the group boundaries can appear to be more a question of framing than substance. And given the equivalence of a multilevel selection framework to inclusive fitness, it is possible to ignore the framing of group boundaries and simply consider whether an individual and their relations benefits from their action relative to the broader population.\nThis equivalence of framing is also apparent in the second design principle. The costs and benefits within and between groups are the core of the multilevel selection calculation, whereas an inclusive fitness framework utilises Hamilton’s rule to balance costs and benefits weighted by relatedness.\nOf the other design principles, the presence of monitoring, sanctions and conflict resolution mechanisms create the costs and benefits that induce cooperative, trusting behaviour. Where these principles hold, we might expect social behaviours to be fitness enhancing. Also, when people interact it is easy to frame those social behaviours such that most of the multilevel selection dynamics are at the group level. Simply find those who socially interact and call them a group.\nThis is the point where I tend to question whether a multilevel selection framing is the best approach. Once the definition of a group drifts from that that is commonly used, it loses simplicity and transparency. It also hides the reason for the group interaction. When someone undertakes a trade, do they consider foremost their standing relative to their particular trading partner, or to a larger population? The evolved psychological mechanisms that lead to the formation of the “group” in which the trade occurs may also be hidden in the multilevel selection framing.\nAs a result, I can see the alignment between Ostrom’s design principles and the multilevel selection framework. But when the definition of a group is so flexible, I’m not convinced that this is a useful way of examining the problem.\nI don’t have much to add on the second claim - that Ostrom’s design principles can be applied to a wider range of groups - so will leave that point for now.\nMy series of posts on the Journal of Economic Behavior & Organization special issue, Evolution as a General Theoretical Framework for Economics and Public Policy, are as follows:\n\nSocial Darwinism is back - a post on one of the popular press articles that accompanied the special issue, a piece by David Sloan Wilson called A good social Darwinism.\nFour reasons why evolutionary theory might not add value to economics - a post on David Sloan Wilson and John Gowdy’s article Evolution as a general theoretical framework for economics and public policy\nEconomic cosmology - The rational egotistical individual - a post on John Gowdy and colleagues’ article Economic cosmology and the evolutionary challenge \nEconomic cosmology - The invisible hand - a second post on Economic cosmology and the evolutionary challenge \nEconomic cosmology - Equilibrium - a third post on Economic cosmology and the evolutionary challenge\nDesign principles for the efficacy of groups (this post) - a post on David Sloan Wilson, Elinor Ostrom and Michael E. Cox’s article Generalizing the core design principles for the efficacy of groups"
  },
  {
    "objectID": "posts/delong-on-the-pace-of-evolution.html",
    "href": "posts/delong-on-the-pace-of-evolution.html",
    "title": "DeLong on the pace of evolution",
    "section": "",
    "text": "Any theory that seeks to invoke human evolution as a factor in the Industrial Revolution needs to deal with how quickly humans can evolve and whether this rate of change is fast enough to be a factor.\nI was recently browsing Gregory Clark’s web-page for his book A Farewell to Alms and came across a video of a seminar in 2007 involving Clark, Brad DeLong and Tyler Cowen. There were some interesting points throughout the session (and it is worth watching it all) but one interesting point was an argument by DeLong on the pace of evolution.\n\nHis argument was based on the following example. Suppose there is a patience gene in the population. Assume that each person with the patience gene has a two-thirds chance of being patient while those without the gene have a one-third chance. Of those who are patient, they have a two-thirds chance of being rich, versus one-third for the others. Finally, assume that those who are rich have a two-thirds chance of having children, while the rest have a one-third chance.\nNone of the numbers in the example seem implausible, although there is plenty of room to debate the specifics. Based on these numbers, DeLong noted that those with the patience gene have a 14 in 27 chance of having children, while those without have a 13 in 27 chance. DeLong translated this to a proportional growth rate of 1/27 or approximately 0.04 for those with the patience gene. Assuming 25 years per generation, it would take about 500 years to double the proportion of the patience gene in the population from, say, 1 per cent to 2 per cent.\nThe following table indicates how he came to that conclusion (the numbers are how many of each type):\n\nDeLong took this slow rate of change to be a challenge for any genetically based theory of the Industrial Revolution. If those numbers were the last word, I would be inclined to agree. However, I would not rule out a scenario where a change in a relatively small part of the population could have large effects if s small group of individuals were responsible for a large proportion of innovation in an economy or there were positive feedback loops.\nMore importantly, a closer look at the numbers can change the assessment. The first issue is DeLong’s interpretation of his own example. While DeLong’s estimate of 0.04 for the rate of growth is approximately right when the population is composed of equal numbers of each genotype, it underestimates the growth rate when there are proportionally less patient genotypes. Take the situation where the population has 1 per cent patient genotypes. In such a case, the increase of patient genotypes is effectively their absolute growth rate as they make little difference to the total population. Therefore, they increase as a proportion of the population at a rate of 1/13, or approximately 8 per cent per generation.\nThis would see the patient genotypes increase to 2 per cent of the population in less than 10 generations, or around 250 years. They would quadruple their proportion of the population in 500 years. As their proportion grows, their proportional growth rate slows. However, an argument that patient genotypes increased from 5 to 20 per cent of the population over 500 years is certainly a basis for significant macroeconomic effects.\nFurther playing with the numbers gives us some other possibilities. If instead of using two-thirds, one-third as the basis of our calculations, we could use three-quarters-one quarter, giving the following:\n\nThese calculations yield us a proportional growth rate of 12 per cent when there is a low proportion of patient genotypes, and 6 per cent when the population is composed of around 50 per cent patient genotypes. That is a doubling in proportion every 6 generations or 150 years when there is a low proportions of patient genotypes.\nAnother alternative is to simply cut out a step and assume that patient genotypes have a two-thirds chance of being rich, while the others have a one-third chance. This dramatically increases the potential growth rates:\n\nAt low prevalence, the patient genotypes increase in proportion of the population at a rate of 25 per cent per generation. Every three to four generations, patient genotypes would double in proportion of the total population.\nAll of the above is fairly crude and open to debate. However, it seems to indicate that genetically based hypotheses about the Industrial Revolution are robust to this particular back of the envelope calculation."
  },
  {
    "objectID": "posts/defending-economics-from-the-anthropologists.html",
    "href": "posts/defending-economics-from-the-anthropologists.html",
    "title": "Defending economics from the anthropologists",
    "section": "",
    "text": "Well, one anthropologist anyway. I’m normally the first person to admit that economics would benefit from incorporating findings from other fields into its understanding of human behaviour, be that from anthropology, biology, evolutionary psychology or whatever other field might yield useful insight. After all, that is one of my focuses for this blog. But sometimes the caricatures of economics become too much to bear. So for this post, I want to take a moment to defend economics.\nIn a post over at The Primate Diaries, Eric Michael Johnson argues that few economists will admit that economics is inextricably tied to moral behaviour. I’m not sure if Johnson has set foot in an economics seminar before, or read the debates on economic blogs, but morality is often at the forefront of economics debates (even if it’s not called “morality”). Adam Smith, who he picks on at one point for getting hunter-gatherer economics wrong, wrote a whole book on morality. It’s hardly a back seat consideration.\nBut rather than focus on the morality question, I want to look at Johnson’s framing of the state of economics. There’s one paragraph toward the end of the post where Johnson lets loose in particular, so I’ll pull that paragraph apart one piece at a time. First:\n\nSocial scientists are increasingly reaching the conclusion that economics is a field in disarray.\n\nSo what is the benchmark for disarray? Johnson provides two potential reasons:\n\nNot only is the economic outcome so often in conflict with our most celebrated moral principles, …\n\nFollowing Johnson’s link, it seems that economics is in disarray because the Pope is concerned about our economic system. So economics in disarray because the subject of study isn’t behaving in the way that someone wants?\nFinishing Johnson’s sentence:\n\n… leading economists can’t even agree on the core assumptions in their field.\n\nThe article that Johnson links to is a piece in Evolution: This View of Life by Terence Burnham, who points to disagreement between neoclassical and behavioural economists about their assumptions of human behaviour. To illustrate, Burnham notes the dispute between two University of Chicago economists, Richard Thaler on the behavioural economics side and Eugene Fama on the neoclassical. Fama doesn’t think that some of Thaler’s articles should have been published (who hasn’t thought that about an article before?) whereas Thaler has picked on Fama for being “the only guy on earth who doesn’t think there was a bubble in Nasdaq in 2000.”\nBut how much disagreement is a sign of a field in disarray? Take the bitter struggle over the origins of kindness between groups of evolutionary biologists. Or Dawkins versus Gould. Or David Sloan Wilson and E.O. Wilson against most evolutionary biologists on group selection? It seems sociobiology is in disarray too.\nTurning to Johnson’s own field, cultural and evolutionary anthropologists still debate about the blank slate picture of human nature. Napoleon Chagnon’s recent book, Noble Savages, triggered another round of battles between anthropologists about his fieldwork with the Yanomamö, with Marshall Sahlins resigning from the National Academy of Sciences following Chagnon’s election as a member. At least Thaler and Fama still work at the same university.\nAnd of course, Fama and Thaler have much common ground. Even Thaler thinks that the selfish, rational agent model is the best theory of economic behaviour available. The question is when is the right time to use that abstraction, and when not.\nAs a a result, I’ll accept that economics is in disarray if evolutionary biology, sociobiology and anthropology are in disarray too.\nMoving to Johnson’s next sentence,\n\nDespite the millions of dollars that go into economic think tanks and the construction of complicated financial models we can’t even come to a consensus on whether government spending helps or hurts an economy in recession.\n\nActually, there’s a surprising level of consensus. A poll of Chicago Booth’s Economic Experts Panel found 80 per cent agreed unemployment was lower in 2010 than it would have been without the stimulus bill (and only 86 per cent responded, making it 2 per cent uncertain and 4 per cent disagreeing). There was less (but still moderate) agreement about the long-term costs and benefits once future tax changes and the like were taken into account, but how can there be consensus when you don’t even know what form those future tax and policy changes will take? So who are these disagreeing people who Johnson refers to? Follow the link and you find its Republicans and Democrats.\nTo be fair to Johnson, there is a lot that economists do not know about stimulus. In an update to his post, he quotes Doyne Farmer, Duncan Foley, Olivier Blanchard and friends on that point. But it’s hardly surprising. We have a highly complex system involving millions of people and firms, complex networks and masses of complex regulation and bureaucracy, together with an astounding range of ways in which stimulus might be implemented. In Australia, one of the prime forms of stimulus after the financial crisis was installation of insulation bats in house roofs. How is that going to play out in your economic model? (So far, the result seems to be a boost to employment in bat installation, more energy efficient houses, house fires, fraud, the occasional electrocution, a couple of government inquiries and continuing debate about whether it was worth it.)\nAll up, Johnson’s caricatures distract potential supporters of his more important argument. The paragraph I pulled apart could have simply been omitted from Johnson’s post and other statements tweaked, and his central argument would not have been any weaker (and we might be responding to and talking about that instead of the caricature).\nAlternatively, Johnson could have acknowledged that economics is a huge field with many economists working on fairness, cooperation and altruism. He could have noted that most economists are actually concerned with “how people behave in the real world” (any popular economics book gives a taste of that) and that morality is often a central consideration in economic thought. Even Alan Greenspan influencer Ayn Rand, who Johnson has targeted before, wrote Atlas Shrugged as part of her work on moral philosophy. And then Johnson could have said that research from anthropology shows that these conceptions of morality are wrong or under-utilised, and that anthropology could provide evidence to resolve disagreements within the field. Not as exciting or politically charged, but more likely to allow focus on the core argument than the burning straw man.\nAnother alternative would be to get a deeper understanding of which parts of the economics profession deserve to be set on fire and address them head on. There are plenty of potential targets, from the confidence with which economic forecasts are made, to the abuse of complex statistical techniques in econometric analysis, to the question of when it is appropriate to ease the assumption of humans as rational animals in analysis of financial markets. Even drawing a distinction between macro and microeconomics would help (each have their own problems).\nA more focussed approach would have the benefit of not turning off those who might learn from Johnson, and give a better picture of what Johnson’s conception of morality would actually add to economics. Because the question I have at the end of Johnson’s post is, “what exactly would change if economists adopted Johnson’s conception of morality in their analysis?” I expect it’s not determining the effects of economic stimulus.\n*In the comments to Johnson’s post, Chris Auld makes some good points."
  },
  {
    "objectID": "posts/debating-the-conjunction-fallacy.html",
    "href": "posts/debating-the-conjunction-fallacy.html",
    "title": "Debating the conjunction fallacy",
    "section": "",
    "text": "From Eliezer Yudkowsky on Less Wrong (a few years old, but worth revisiting in the light of my recent Gigerenzer v Kahneman and Tversky post):\n\nWhen a single experiment seems to show that subjects are guilty of some horrifying sinful bias - such as thinking that the proposition “Bill is an accountant who plays jazz” has a higher probability than “Bill is an accountant” - people may try to dismiss (not defy) the experimental data. Most commonly, by questioning whether the subjects interpreted the experimental instructions in some unexpected fashion - perhaps they misunderstood what you meant by “more probable”.\nExperiments are not beyond questioning; on the other hand, there should always exist some mountain of evidence which suffices to convince you.\n…\nHere is (probably) the single most questioned experiment in the literature of heuristics and biases, which I reproduce here exactly as it appears in Tversky and Kahneman (1982):\n\nLinda is 31 years old, single, outspoken, and very bright. She majored in philosophy. As a student, she was deeply concerned with issues of discrimination and social justice, and also participated in anti-nuclear demonstrations.\nPlease rank the following statements by their probability, using 1 for the most probable and 8 for the least probable:\n(5.2) Linda is a teacher in elementary school.\n(3.3) Linda works in a bookstore and takes Yoga classes.\n(2.1) Linda is active in the feminist movement. (F)\n(3.1) Linda is a psychiatric social worker.\n(5.4) Linda is a member of the League of Women Voters.\n(6.2) Linda is a bank teller. (T)\n(6.4) Linda is an insurance salesperson.\n(4.1) Linda is a bank teller and is active in the feminist movement. (T & F)\n\n(The numbers at the start of each line are the mean ranks of each proposition, lower being more probable.)\nHow do you know that subjects did not interpret “Linda is a bank teller” to mean “Linda is a bank teller and is not active in the feminist movement”? For one thing, dear readers, I offer the observation that most bank tellers, even the ones who participated in anti-nuclear demonstrations in college, are probably not active in the feminist movement. So, even so, Teller should rank above Teller & Feminist.  …  But the researchers did not stop with this observation; instead, in Tversky and Kahneman (1983), they created a between-subjects experiment in which either the conjunction or the two conjuncts were deleted. Thus, in the between-subjects version of the experiment, each subject saw either (T&F), or (T), but not both. With a total of five propositions ranked, the mean rank of (T&F) was 3.3 and the mean rank of (T) was 4.4, N=86. Thus, the fallacy is not due solely to interpreting “Linda is a bank teller” to mean “Linda is a bank teller and not active in the feminist movement.”\n…\nAnother way of knowing whether subjects have misinterpreted an experiment is to ask the subjects directly. Also in Tversky and Kahneman (1983), a total of 103 medical internists … were given problems like the following:\n\nA 55-year-old woman had pulmonary embolism documented angiographically 10 days after a cholecstectomy. Please rank order the following in terms of the probability that they will be among the conditions experienced by the patient (use 1 for the most likely and 6 for the least likely). Naturally, the patient could experience more than one of these conditions.\n\nDyspnea and hemiparesis\nCalf pain\nPleuritic chest pain\nSyncope and tachycardia\nHemiparesis\nHemoptysis\n\n\nAs Tversky and Kahneman note, “The symptoms listed for each problem included one, denoted B, that was judged by our consulting physicians to be nonrepresentative of the patient’s condition, and the conjunction of B with another highly representative symptom denoted A. In the above example of pulmonary embolism (blood clots in the lung), dyspnea (shortness of breath) is a typical symptom, whereas hemiparesis (partial paralysis) is very atypical.”\nIn indirect tests, the mean ranks of A&B and B respectively were 2.8 and 4.3; in direct tests, they were 2.7 and 4.6. In direct tests, subjects ranked A&B above B between 73% to 100% of the time, with an average of 91%.\nThe experiment was designed to eliminate, in four ways, the possibility that subjects were interpreting B to mean “only B (and not A)”. First, carefully wording the instructions:  “…the probability that they will be among the conditions experienced by the patient”, plus an explicit reminder, “the patient could experience more than one of these conditions”. Second, by including indirect tests as a comparison. Third, the researchers afterward administered a questionnaire:\n\nIn assessing the probability that the patient described has a particular symptom X, did you assume that (check one):\nX is the only symptom experienced by the patient?\nX is among the symptoms experienced by the patient?\n\n60 of 62 physicians, asked this question, checked the second answer.\nFourth and finally, as Tversky and Kahneman write, “An additional group of 24 physicians, mostly residents at Stanford Hospital, participated in a group discussion in which they were confronted with their conjunction fallacies in the same questionnaire. The respondents did not defend their answers, although some references were made to ‘the nature of clinical experience.’  Most participants appeared surprised and dismayed to have made an elementary error of reasoning.”\n…\nDoes the conjunction fallacy arise because subjects misinterpret what is meant by “probability”? This can be excluded by offering students bets with payoffs. In addition to the colored dice discussed yesterday, subjects have been asked which possibility they would prefer to bet $10 on in the classic Linda experiment. This did reduce the incidence of the conjunction fallacy, but only to 56% (N=60), which is still more than half the students.\nBut the ultimate proof of the conjunction fallacy is also the most elegant. In the conventional interpretation of the Linda experiment, subjects substitute judgment of representativeness for judgment of probability: Their feelings of similarity between each of the propositions and Linda’s description, determines how plausible it feels that each of the propositions is true of Linda. …\nYou just take another group of experimental subjects, and ask them how much each of the propositions “resembles” Linda. This was done - see Kahneman and Frederick (2002) - and the correlation between representativeness and probability was nearly perfect.  0.99, in fact.\n…\nThe conjunction fallacy is probably the single most questioned bias ever introduced, which means that it now ranks among the best replicated. The conventional interpretation has been nearly absolutely nailed down.\n\nThere are a few additional experiments in Yudkowsky’s post that I have not included here."
  },
  {
    "objectID": "posts/david-leiser-and-yhonatan-shemeshs-how-we-misunderstand-economics-and-why-it-matters-the-psychology-of-bias-distortion-and-conspiracy.html",
    "href": "posts/david-leiser-and-yhonatan-shemeshs-how-we-misunderstand-economics-and-why-it-matters-the-psychology-of-bias-distortion-and-conspiracy.html",
    "title": "David Leiser and Yhonatan Shemesh’s How We Misunderstand Economics and Why it Matters: The Psychology of Bias, Distortion and Conspiracy",
    "section": "",
    "text": "From a new(ish) book by David Leiser and Yhonatan Shemesh, How We Misunderstand Economics and Why it Matters: The Psychology of Bias, Distortion and Conspiracy:\n\nWorking memory is a cognitive buffer, responsible for the transient holding, processing, and manipulation of information. This buffer is a mental store distinct from that required to merely hold in mind a number of items and its capacity is severely limited. The complexity of reasoning that can be handled mentally by a person is bounded by the number of items that can be kept active in working memory and the number of interrelationships between elements that can be kept active in reasoning. Quantifying these matters is complicated, but the values involved are minuscule, and do not exceed four distinct elements …\nLTM [long-term memory] suffers from a different failing. … It seems there is ample room for our knowledge in the LTM. The real challenge relates to retrieval: people routinely fail to use knowledge that they possess – especially when there is no clear specification of what might be relevant, no helpful retrieval cue. …\nThe two flaws … interact with one another. Ideas and pieces of knowledge accumulate in LTM, but those bits often remain unrelated. Leiser (2001) argues that, since there is no process active in LTM to harmonize inconsistent parts, coordination between elements can only take place in working memory. And in view of its smallness, the scope of explanations is small too. &gt;…\nLimited knowledge, unavailability of many of the relevant economic concepts and variables, and restricted mental processing power mean that incoherencies are to be expected, and they are indeed found. One of the most egregious is the tendency, noted by Furnham and Lewis (1986) who examined findings from the US, the UK, France, Germany, and Denmark, to demand both reductions in taxation and increased public expenditure (especially on schools, the sick, and the old). You can of course see why people would rather pay less in taxes, and also that they prefer to benefit from more services, but it is still surprising how often the link between the two is ignored. This is only possible because, to most people, taxes and services are two unrelated mental concepts, sitting as it were in different parts of LTM, a case of narrow scoping, called by McCaffery and Baron (2006) in this context an “isolation effect.”\n…\nBastounis, Leiser, and Roland- Levy ( 2004 ) ran an extensive survey on economic beliefs in several countries (Austria, France, Greece, Israel, New Zealand, Slovenia, Singapore, and Turkey) among nearly 2000 respondents, and studied the correlations between answers to the different questions. No such broad clustering of opinions as that predicted by Salter was in evidence. Instead, the data indicate that lay economic thinking is organized around circumscribed economic phenomena, such as inflation and unemployment, rather than by integrative theories. Simply put, knowing their answers about one question about inflation was a fair predictor of their answer to another, but was not predictive of their views regarding unemployment.\n\nA refreshing element of the book is that it draws on a much broader swathe of psychology than just the heuristics and biases literature, which often becomes the focus of stories on why people err. However, I was surprised by the lack of mention of intelligence.\nA couple of other interesting snippets, the first on the ‘halo effect’:\n\nThe tendency to oversimplify complex judgments also manifests in the “halo” effect. … [K]nowing a few positive traits of a person leads us to attribute additional positive traits to them. … The halo effect comes from the tendency to rely on global affect, instead of discriminating among conceptually distinct and potentially independent attributes.\nThis bias is unfortunate enough by itself, as it leads to the unwarranted attribution of traits to individuals. But it becomes even more pernicious when it blinds people to the possibility of tradeoffs, where two of the features are inversely correlated. To handle a tradeoff situation rationally, it is essential to disentangle the attributes, and to realize that if one increases the other decreases. When contemplating an investment, for instance, a person must decide whether to invest in stocks (riskier, but with a greater potential return) or in bonds (safer, but offering lower potential returns). Why not go for the best of both worlds – and buy a safe investment that also yields high returns? Because no such gems are on offer. A basic rule in investment pricing is that risk and return are inversely related, and for a good reason. …\nStrikingly, this relation is systematically violated when people are asked for an independent evaluation of their risk perception and return expectations. Shefrin (2002) asked portfolio managers, analysts, and MBA students for such assessments, and found, to his surprise, that expected return correlates inversely with perceived risk. Respondents appear to expect that riskier stocks will also produce lower returns than safer stocks. This was confirmed experimentally by Ganzach (2000). In the simplest of his several experiments, participants received a list of (unfamiliar) international stock markets. One group of participants was asked to judge the expected return of the market portfolio of these stock markets, and the other was asked to judge the level of risk associated with investing in these portfolios. … The relationship between judgments of risk and judgments of expected return, across the financial assets evaluated, was large and negative (Pearson r = −0.55). Ganzach interprets this finding as showing that both perceived risk and expected return are derived from a global preference. If an asset is perceived as good, it will be judged to have both high return and low risk, whereas if it is perceived as bad, it will be judged to have both low return and high risk.\n\nAnd on whether some examinations of economic comprehension are actually personality tests:\n\nLeiser and Benita (in preparation) asked 300 people in the US for their view concerning economic fragility or stability, by checking the extent to which they agreed with the following sentences:\n\nThe economy is fundamentally sound, and will restore itself after occasional crises.\nThe economy is capable of absorbing limited shocks, but if the shocks are excessive, a major crisis and even collapse will ensue.\nDeterioration in the economy, when it occurs, is a very gradual process.\nThe economy’s functioning is delicate, and always at a risk of collapse.\nThe economy is an intricate system, and it is all but impossible to predict how it will evolve.\nEconomic experts can ensure that the economy will regain stability even after major crises.\n\nThese questions relate to the economy, and respondents answered them first. But we then asked corresponding questions, with minimal variations of wording, about three other widely disparate domains: personal relationships, climate change, and health. Participants rated to what extent they agree with each of the statements about each additional domain. The findings were clear: beliefs regarding economic stability are highly correlated with parallel beliefs in unrelated social and natural domains. People who believe that “The economy’s functioning is delicate, and always at a risk of collapse” tend to agree that “Close interpersonal relationships are delicate, and always at a risk of collapse” … And people who hold that “The economy is capable of absorbing limited shocks, but if the shocks are excessive, a major crisis will occur” also tend to judge that “The human body is capable of absorbing limited shocks, but beyond a certain intensity of illness, body collapse will follow.”\nWhat we see in such cases is that people don’t assess the economy as an intelligible system. Instead, they express their general feelings towards dangers. … [T]hose who believe that the world is dangerous and who see an external locus of control see all four domains (economics, personal relations, health, and the environment) as unstable and unpredictable. Such judgments have little to do with an evaluation of the domain assessed, be it economic or something else. They attest personal traits, not comprehension."
  },
  {
    "objectID": "posts/darwin-on-female-preferences.html",
    "href": "posts/darwin-on-female-preferences.html",
    "title": "Darwin on female preferences",
    "section": "",
    "text": "I am slowly re-reading Darwin’s The Descent of Man and came across the following gem:\n\nThe female, on the other hand, with the rarest exceptions, is less eager than the male. As the illustrious Hunter … long ago observed, she generally “requires to be courted;” she is coy, and may often be seen endeavouring for a long time to escape from the male. … Or she may accept, as appearances would sometimes lead us to believe, not the male which is the most attractive to her, but the one which is the least distasteful. The exertion of some choice on the part of the female seems a law almost as general as the eagerness of the male."
  },
  {
    "objectID": "posts/dangerous-ideas.html",
    "href": "posts/dangerous-ideas.html",
    "title": "Dangerous ideas",
    "section": "",
    "text": "Recently, I was asked whether the idea that I was espousing - considering human evolution in economics - was dangerous. For a perspective on debating dangerous ideas, it was suggested that it was worthwhile reading Steven Pinker’s introduction to the book What is Your Dangerous Idea? (HT Erik Postma).\nPinker argues both for and against treating some ideas as dangerous and possibly limiting their discussion. In addition to the usual arguments such as sunlight being the best disinfectant and that people will twist the debates to suit their own purposes, Pinker made some interesting observations.\nOne of them mirrors an argument that I use when people suggest that there are eugenic or Social Darwinistic implications to accepting that there is a genetic basis to human behaviour. Pinker notes that discrimination and oppression are deplorable, but that:\n\n[N]one of them actually follows from the supposedly dangerous idea. Even if it turns out, for instance, that groups of people are different in their averages, the overlap is certainly so great that it would be irrational and unfair to discriminate against individuals on that basis. Likewise, even if it turns out that parents don’t have the power to shape their children’s personalities, it would be wrong on grounds of simple human decency to abuse or neglect one’s children.\n\nThe example I often use is IQ and education. If we accept that there is a genetic basis to IQ, this does not mean that we should shunt those who fall below a certain threshold up to the salt mines. Instead, schools can tailor their teaching to a range of capabilities and potentials.\nThe second point I found interesting related to who is making the argument. Pinker writes:\n\nWe must be especially suspicious when the danger in a dangerous idea is to someone other than its advocate.  Scientists, scholars, and writers are members of a privileged elite. They may have  an interest in promulgating ideas that justify their privileges, that blame or make light of society’s victims, or that earn them attention for cleverness and iconoclasm.\n\nI expect that I am in the target audience for this point. I am a white, educated male in a developed country and am exploring how human evolution is relevant to economic questions such as the origin of economic growth. My research, if twisted in certain ways, could be argued to be justifying privilege. It could certainly be argued that I have an interest in earning attention for cleverness and iconoclasm.\nSo, how should I respond to this point? I like to think I am exploring these questions because I find them interesting and I don’t know all the answers. I want to understand them better. Perhaps it is signal that I should always bear in mind what my motivations are.\n*As a footnote, I tried to buy an ebook copy of this book last night but the sellers blocked the purchase as I am in a territory in which it is not available. As is often the case, I found a free substitute. For those interested in reading the essays that formed the basis of the book, you can find them here (click on the contributors name for the essay). The essays range between the interesting and rehashes of old arguments that we’ve had for a very long time."
  },
  {
    "objectID": "posts/critique-of-conspicuous-consumption-and-economic-growth.html",
    "href": "posts/critique-of-conspicuous-consumption-and-economic-growth.html",
    "title": "Critique of conspicuous consumption and economic growth",
    "section": "",
    "text": "Last week, I presented my paper on conspicuous consumption and economic growth at the annual PhD Conference in Economics and Business. The basic argument of the paper is that the evolution of the propensity to engage in conspicuous consumption is a factor underlying modern economic growth, as conspicuous consumption requires productive activity to produce the resources to consume.\nOne of the great features of the conference is that each presenter is allocated a discussant who reviews the paper and presents a critique - and the task is taken seriously. Paul Frijters from the University of Queensland was my discussant, and he made a few interesting points (a copy of his presentation is here).\nFrijters’s major criticism is that I should not link the evolution of conspicuous consumption to the Industrial Revolution. This critique suggests that I have done a poor job at framing my argument, as it is not my intention to argue that the evolution of the propensity to signal quality through conspicuous consumption was a specific trigger of the Industrial Revolution. Rather, it provides one of the foundations for the Industrial Revolution to occur - and that foundation could have been laid some time before it.\nTo move away from the Industrial Revolution, Frijters suggests three alternatives for the paper (the first two as throwaway ideas, the third as a more serious suggestion): 1. Use the model to further dispel the myth that the Industrial Revolution had anything to do with evolutionary selection. 2. Switch from economics to something biological, such as why some animal species don’t signal status and others do. 3. Talk not of changes within a few hundred years, but instead about human sub-populations.\nOn the first suggestion, evolutionary models can be very fast. The basic model in the conspicuous consumption paper required only 10 or so generations for the take-off. In some ways, it is the Industrial Revolution like take-off in the simulations that gives the impression that we are trying to explain the Industrial Revolution. Even if we rule out conspicuous consumption as a specific trigger, there are plenty of other evolved traits worth considering.\nHowever, I agree with Frijters that looking at human sub-populations will be vital in picking apart this topic. Better demographic data is being developed and I am hopeful that the next few years will see cheaper genome sequencing driving the development of interesting longitudinal data on selection pressure. Ultimately, an evolutionary theory of the Industrial Revolution must stand or fall based on the ability to differentiate between populations that experienced rapid technological growth, and those that did not.\nOn that point, there is not much evidence of biologically mediated differences in conspicuous consumption between populations - particularly in a way that would support conspicuous consumption as being that specific Industrial Revolution trigger. Conspicuous consumption is relatively ubiquitous, with only the form varying. Cross-population analysis will be more interesting when we consider other economic traits.\nAs an aside, Frijters has a book coming out next year (with Gigi Foster) - An Economic Theory of Greed, Love, Groups and Networks - that might be worth a look."
  },
  {
    "objectID": "posts/crime-and-selection-of-aggressive-males.html",
    "href": "posts/crime-and-selection-of-aggressive-males.html",
    "title": "Crime and selection of aggressive males",
    "section": "",
    "text": "As I posted a couple of months ago, a higher level of violence in a society may lead women to prefer more masculine appearing men. In such an environment, picking the healthiest appearing male is more important than the level of parental care the woman expects the man to give.\nThe latest issue of Evolution and Human Behavior has an article examining the link between female preference and violence, with Jeffrey Snyder and colleagues examining whether a woman’s fear of crime might be a predictor of her preference for “aggressive and formidable” mates. Unlike earlier research, which focused on actual violence levels, Snyder and colleagues’ targeted their hypothesis at the woman’s perception of her vulnerability to crime. This makes sense, as the need for an aggressive man is likely to be a function of both the level of crime and the woman’s ability to defend herself. The woman’s perception of her vulnerability should capture both of these elements.\nSnyder and colleagues also framed the trade-off around outcomes to the women instead of reproductive outcomes. Instead of asking whether the aggressive man will deliver a healthy child or invest in parental care, the trade-off discussed concerned violence to the woman by the aggressive man versus the protection he can offer the woman.\nUsing three internet based studies of United States women, Snyder and colleagues supported their hypothesis through the discovery of a positive relationship between a woman’s perception of her vulnerability to crime and her preference for aggressive men. However, there was no or a very weak link between female mate preference and actual crime rates (which were determined by zip code). This is somewhat confusing, as it suggests that fear of crime may not be rationally based. Snyder and colleagues’ hypothesis would predict a weaker link between actual crime and preferences than between perceived vulnerability and preferences, but there should still be a link.\nA further issue with the results is the low-level of effect that the perception of crime has. While it comfortably passes the significance tests, fear of crime can only explain (at most) 5.5 per cent, 7 per cent and 6 per cent of the variation in preferences for aggressive men across the three studies (and that is including other variables in some of the regressions including race, age, education and inequality). This suggests that while perceived vulnerability is significant in a statistical sense, it has very little predictive power.\nHaving mulled on this study for a couple of days, I am not sure what to make of it. I find the hypothesis attractive, but the absence of a link to actual crime leaves me with a large number of questions about the survey methods used and suggestions for follow-up research - a number of which were also noted by Snyder et al.\nFirst, it would be useful to get some more variation in the sample. The study participants were highly educated, with less than ten per cent of the sample in each study having a level of education at high school level or below. As a result, there are likely to be very few in the sample who live in a violent area. This variation may be particularly important if the survey subjects are subject to levels of crime too low for protection from aggressive men to matter.\nMore variation could be introduced by including other countries or particularly high crime areas. In those countries or areas, a more masculine male may deliver much stronger benefits and be more strongly preferred.\nA related observation is that for this study’s well-educated participants, the easier way to avoid crime would be to marry a rich man. As a result, women in this sample might want to avoid aggressive men. However, that may not be a feasible choice in Sudan or for some inner-city residents. Are these preferences stronger where the best response to violence is a violent response?\nSecond, and as suggested in the paper, finding out what these women actually do as opposed to their survey responses would be useful. Apart from the obvious benefits to seeing revealed preferences, this might also help to calibrate the responses. In each survey, the women were asked their responses to characterisations such as “bad-boy”, “broad shouldered” and “strong”. If a well-educated woman thinks an accountant with a motorcycle is a bad boy, that is probably a different level of masculinity compared to someone seeking physical protection from a real risk of crime.\nI would also like to know more about the women in the survey. Are they in a partnership or married? How tall are they? Have they been a victim of crime? Does their fear of violence come from in the home or from people they know? How much property do they have which could be appropriated in a crime? This might help find some explanatory variables with some real predictive power. However, to test the basic hypothesis, we need a sample with more variation in the levels of violence and ideally, a sample in which we can observe real choices.\nI don’t believe that this story sheds much light on my earlier ruminations on violence (most recent here) and the importance of a shift away from violence to allow characteristics such as hard work, intelligence and patience to be rewarded and spread through the population. It could be argued that as the study was conducted in a developed country, and among educated women in that country, we would expect violence to be a trait associated with low fitness. You would expect women to generally favour other traits, with the aggressive characteristics to be secondary and only accepted if they do not come at the cost of economically important traits. Again, to test this idea, we require more variation in the sample. We would need some of the sample to come from populations in which crime brings benefits to its purveyors and results in reproductive success."
  },
  {
    "objectID": "posts/crime-abortion-and-genes.html",
    "href": "posts/crime-abortion-and-genes.html",
    "title": "Crime, abortion and genes",
    "section": "",
    "text": "First, from Donohue and Levitt’s The Impact of Legalized Abortion on Crime, which argued that the legalisation of abortion contributed to later declines in crime:\n\nMore interesting and important is the possibility that children born after abortion legalization may on average have lower subsequent rates of criminality for either of two reasons. First, women who have abortions are those most at risk to give birth to children who would engage in criminal activity. Teenagers, unmarried women, and the economically disadvantaged are all substantially more likely to seek abortions. Recent studies have found children born to these mothers to be at higher risk for committing crime in adolescence. Gruber, Levine, and Staiger, in the paper most similar to ours, document that the early life circumstances of those children on the margin of abortion are difficult along many dimensions: infant mortality, growing up in a single-parent family, and experiencing poverty. Second, women may use abortion to optimize the timing of child-bearing. A given woman’s ability to provide a nurturing environment to a child can fluctuate over time depending on the woman’s age, education, and income, as well as the presence of a father in the child’s life, whether the pregnancy is wanted, and any drug or alcohol abuse both in utero and after the birth. Consequently, legalized abortion provides a woman the opportunity to delay childbearing if the current conditions are suboptimal. Even if lifetime fertility remains constant for all women, children are born into better environments, and future criminality is likely to be reduced.\n\nSecond, from Bryan Caplan’s Selfish Reasons to Have More Kids:\n\nParents have little or no effect on criminal behavior. …\nIn 1984, Science published a study of almost 15,000 Danish adoptees age fifteen or older, their adoptive parents, and their birth parents. … As long as the adoptee’s biological parents were law abiding, their adoptive parents made little difference: 13.5 percent of adoptees with law-abiding biological and adoptive parents got convicted of something, versus 14.7 percent with law-abiding biological parents and criminal adoptive parents. If the adoptee’s biological parents were criminal, however, upbringing mattered: 20 percent of adoptees with law-breaking biological and law-abiding adoptive parents got convicted, versus 24.5 percent with law-breaking biological and adoptive parents. Criminal environments do bring out criminal tendencies. Still, as long as the biological parents were law abiding, family environment made little difference.\nIn 2002, a study of antisocial behavior in almost 7,000 Virginian twins born since 1918 found a small nurture effect for adult males and no nurture effect for adult females. The same year, a major review of fifty-one twin and adoption studies reported small nurture effects for antisocial attitudes and behavior. For outright criminality, however, heredity was the sole cause of family resemblance.\nThe lesson: Even if your standards are low, instilling character is hard. Genes are the main reason criminal behavior runs in families.\n\nHow much of the abortion-crime link (to the extent it is real) is driven by elimination of those more genetically predisposed to crime?"
  },
  {
    "objectID": "posts/courseras-executive-data-science-specialisation-a-review.html",
    "href": "posts/courseras-executive-data-science-specialisation-a-review.html",
    "title": "Coursera’s Executive Data Science Specialisation: A Review",
    "section": "",
    "text": "As my day job has shifted toward a statistics and data science focus, I’ve been reviewing a lot of online materials to get a feel for what is available – both for my learning and to see what might be good training for others.\nOne course I went through was Coursera’s Executive Data Science Specialisation, created by John Hopkins University. Billed as the qualification to allow you to run a data science team, it is made up of five “one week” courses covering the basics of data science, building data science teams and managing data analysis processes.\nThere are some goods parts to the courses, but unlike the tagline that you will learn what you need to know “to begin assembling and leading a data science enterprise”, it’s some way short of that benchmark. For managers who have data scientists sitting under them, or who use a data science team in their organisation, it might give them a sense of what is possible and an understanding of how data scientists think. But it is not much more than that.\nIf I were to recommend any part of the specialisation, it would be the third and fourth courses - Managing Data Analysis and Data Science in Real Life (notes below). They offer a better crash course in data science than the first unit, A Crash Course in Data Science, and might help those unfamiliar with data science processes to understand how to think about statistical problems. That said, someone doing them with zero statistical knowledge will likely find themselves lost.\nWith Coursera’s subscription option you can subscribe to the specialisation for $50 or so per month, and smash through all five units in a few days (as I did, and you could do it in one day if you had nothing else on). From that perspective, it’s not bad value – although the only material change through paying versus auditing is the ability to submit the multiple choice quizzes. Otherwise, just pick videos that look interesting.\nHere’s a few notes on the five courses:\n\nA Crash Course in Data Science: Not bad, but likely too shallow to give someone much feeling about data science. The later units provide a better crash course for managers as they focus on methodology and practice rather than techniques.\nBuilding a Data Science Team: Some interesting thoughts on the skills required in a team, but the material on managing teams and communication was generic.\nManaging Data Analysis: A good crash course in data science - better than the course with that title. Walks through the data science process.\nData Science in Real Life: Another good crash course in data science, although you will likely need some statistical background to fully benefit. A reality check on how the data science process is likely to go relative to the perfect scenario.\nExecutive Data Science Capstone: You appreciate the effort that went into producing an interactive “choose your own adventure”, but the entire effort was around half a dozen decisions in less than an hour."
  },
  {
    "objectID": "posts/could-this-critique-apply-to-economics.html",
    "href": "posts/could-this-critique-apply-to-economics.html",
    "title": "Could this critique apply to economics?",
    "section": "",
    "text": "From an excellent article in Nature News by Ed Yong on problems with replication in psychology:\n\nOne reason for the excess in positive results for psychology is an emphasis on “slightly freak-show-ish” results, says Chris Chambers, an experimental psychologist at Cardiff University, UK. “High-impact journals often regard psychology as a sort of parlour-trick area,” he says. Results need to be exciting, eye-catching, even implausible. Simmons says that the blame lies partly in the review process. “When we review papers, we’re often making authors prove that their findings are novel or interesting,” he says. “We’re not often making them prove that their findings are true.”\n\nI recommend reading the whole article."
  },
  {
    "objectID": "posts/cooperation-and-conflict-in-the-family-conference.html",
    "href": "posts/cooperation-and-conflict-in-the-family-conference.html",
    "title": "Cooperation and Conflict in the Family Conference",
    "section": "",
    "text": "I’m excited that I can finally announce the Cooperation and Conflict in the Family Conference.\n\nFirst announcement\nThe Cooperation and Conflict in the Family conference will be held at UNSW in Sydney, Australia from February 2-5 2014.\nWe will bring together leading economic and evolutionary researchers to explore the nature of conflict and cooperation between the sexes in the areas of marriage, mating and fertility.\nThe conference provides an opportunity for researchers to discuss the economic and evolutionary biology approaches to these issues, explore common ground and identify collaborative opportunities.\nCONFIRMED SPEAKERS\nDavid Barash, University of Washington Monique Borgerhoff Mulder, University of California Davis Lena Edlund, Columbia University Joe Henrich, University of British Columbia Michael Jennions, Australian National University Hillard Kaplan, University of New Mexico Hanna Kokko, Australian National University Jason Potts, Royal Melbourne Institute of Technology Paul Seabright, Toulouse School of Economics\nWe hope you will join us in beautiful Sydney for an exciting meeting of disciplines.\nConference Organisers\nJason Collins & Rob Brooks\n\nPlease spread the word."
  },
  {
    "objectID": "posts/consumption-and-fitness.html",
    "href": "posts/consumption-and-fitness.html",
    "title": "Consumption and fitness",
    "section": "",
    "text": "After posting Friday’s piece on Hansson and Stuart’s paper on natural selection and savings, I realised I had not commented on one of the most important assumptions made by the authors. To get their result that people would save such that they maximise consumption across generations, Hansson and Stuart assumed that consumption corresponded with fitness (a relative measure of reproductive success). To maximise fitness, one would need to maximise consumption.\nThis makes some sense over the very long-term, regardless of whether you consider that consumption contributes to fitness directly, by (say) improving health, or indirectly as a signal of quality. In the signalling case, it has been shown that conspicuous consumption as a handicap can be consistent with the standard utility approach to consumption used in economics.\nWhile I can hand-wave my way through that explanation, some of the attempts in economics to more explicitly consider fertility and fitness have run into a larger problem. In a presentation by Alan Grafen on Sunday (at this conference), he went through four papers on the boundary of evolutionary biology and economics, and one of those he picked was by Robert Barro and Gary Becker. In Barro and Becker’s model, they created a utility function which included both consumption and fertility. An agent in the model would be interested in increasing consumption and their number of offspring. But as Grafen asked, what is the purpose of consumption in the model from an evolutionary perspective?\nThe interesting thing about Barro and Becker’s paper is that by adding an evolutionary biology flavour to the model, they have created a trade-off between fitness and consumption. Higher consumption reduces fitness, and no hand waving like I used for Hansson and Stuart’s model can justify consumption biologically. Any defence of Barro and Becker’s model will have to look to its predictive power, with the evolutionary explanation to come later. Given that most people in developed countries consume much more than can be justified biologically and do not seem to be maximising their fitness (sperm donation anyone?), there is room to defend it on this ground (in at least the short term).\nThe other issue that I should mention about Hansson and Stuart’s paper is that they implicitly assume that if one saves, the saver has effective protection of their private property and is able to pass it down to their children without fear of appropriation. As I have posted about before, violence may be a significant obstacle to the development of behaviour such as saving. While Hansson and Stuart saw differences in saving and labour preferences arising from environmental conditions, another plausible driver of differences would be the probability of any savings being stolen."
  },
  {
    "objectID": "posts/conspicuous-consumption-and-poverty-traps.html",
    "href": "posts/conspicuous-consumption-and-poverty-traps.html",
    "title": "Conspicuous consumption and poverty traps",
    "section": "",
    "text": "Poverty is no barrier to conspicuous consumption. As Banerjee and Duflo wrote in Poor Economics:\n\nOne hidden assumption in our description of the poverty trap is that the poor eat as much as they can. …\nYet, this is not what we see. Most people living with less than 99 cents a day do not seem to act as if they are starving. If they were, surely they would put every available penny into buying more calories. But they do not. In our eighteen-country data set on the lives of the poor, food represents from 36 to 79 percent of consumption among the rural extremely poor, and 53 to 74 percent among their urban counterparts.\nIt is not because all the rest is spent on other necessities: In Udaipur, for example, we find that the typical poor household could spend up to 30 percent more on food than it actually does if it completely cut expenditures on alcohol, tobacco, and festivals.The poor seem to have many choices, and they don’t elect to spend as much as they can on food.\n\nIn a paper published earlier this year, Moav and Neeman proposed an explanation for the relatively high levels of consumption on goods other than food in poor societies. They suggested that some of this consumption is conspicuous consumption, which is driven by the difficulty in signalling status and wealth where it is not recognisable through professional titles or other markers:\n\n[W]we suggest that those with high human capital have a recognisable ability (professional titles, degree certificates etc.) and relatively little need to signal success, whereas those without certified accomplishments, such as the poor and the ‘newly rich’, have a relatively stronger motivation to impress via conspicuous consumption. As a result, the fraction of income allocated to conspicuous consumption can decline, on average, with the level of human capital, resulting in a larger share of income allocated to savings and investment in education.\n\nConspicuous consumption is only useful to the extent that someone is signalling a quality that cannot otherwise be determined. If the quality is observable, there is no need for the signal.\nTo explore this idea, Moav and Neeman developed a model in which a person’s knowledge, skills and experience (what is called human capital) is observable to others. However, a person’s income is not directly observable, although it is imperfectly correlated with their human capital. To attempt to signal the unobservable variation in income, a person can engage in conspicuous consumption.\nIn each model generation, an adult must divide their resources between normal consumption, conspicuous consumption and a bequest to their children, which determines that child’s level of human capital. The adult gets utility from the three options.\nMoav and Neeman showed that depending on the relationship between human capital and income, it was possible for the share of conspicuous consumption as a proportion of income to be decreasing with income. That is, the poor would engage in relatively more conspicuous consumption as a share of their income than the rich. The condition for this to occur is that human capital acts as a buffer to negative income shocks, meaning that variation in income is likely to be relatively less as human capital rises.\nWhen this is put into a dynamic context over generations, this situation can turn into a poverty trap. Adults with low human capital tend to invest more in conspicuous consumption, which then prevents investment in the human capital of their children. As a result, the human capital of that dynasty remains low and they stay in a poverty trap. Those with high human capital, however, invest relatively less of their income in conspicuous consumption, which leaves resources for increasing the human capital of their children. As the share of conspicuous consumption as a portion of someone’s income declines with increasing income, the high human capital dynasty ends up in a virtuous circle of increasing human capital and wealth.\nI am not convinced of the conclusion of conspicuous consumption leading to poverty traps, as human capital is not purely a function of parental investment. Further, to the extent that conspicuous consumption is a signal of resources and leads to differential reproductive success, the people in the successive generations will only be a subset of the dynasties that were initially in existence. However, as the authors note, there is potential to test this idea by exploring conspicuous consumption across societies and to see whether it varies with variation in the transparency of income.\nI would have like to have seen the model placed in an evolutionary setting, particularly given Moav’s background in the area (Moav is one of the authors of the seminal Natural Selection and the Origin of Economic Growth). It is possible to frame the model in evolutionary terms with little need for any model modification. This would answer questions such as why the people prefer to conspicuous consumption, and what is the fitness benefit that allows it to continue to occur? Evolutionary dynamics may be beyond the length of time for which Moav and Neeman seek to explore, but the evolutionary foundations of people’s actions would give robustness to the preference to conspicuously consume."
  },
  {
    "objectID": "posts/consilience-conference.html",
    "href": "posts/consilience-conference.html",
    "title": "Consilience Conference",
    "section": "",
    "text": "Have just discovered the upcoming Consilience Conference. From the blurb:\n\nSpeakers at this conference are all top researchers in biology, the social sciences, or the humanities. All the speakers know the level of consensus in their fields and can recognize major changes taking place, identify the major unsolved problems, and point toward future directions of research. They can all also discuss relations among at least two of the three areas (biology, the social sciences, and the humanities).\n\nMomentum to incorporate biology into the social sciences is growing.\nThe speakers list has me tempted to jump on a plane: Edward O Wilson, Henry Harpending, John Hawks, Christopher Boehm, Herb Gintis and Robert Frank among others. (Although a quick look at flight schedules - 24 hours there, 32 hours back…..)\nUpdate: I attended, and posted about the conference here and here."
  },
  {
    "objectID": "posts/consensus-in-biology-and-economics.html",
    "href": "posts/consensus-in-biology-and-economics.html",
    "title": "Consensus in economics and biology",
    "section": "",
    "text": "Despite the common public brawls, a paper presented at the American Economic Association annual meeting by Gordon and Dahl shows high levels of consensus between economists on most economic issues. Based on questions to a 41 economist panel established by the Chicago Booth School of Business, on average only 6 per cent disagree with the “consensus” answer to a question, with around 25 per cent uncertain. This observation holds for what might be viewed as relatively controversial questions, including the effect of stimulus on jobs (although the phrasing of the questions could be changed to increase dispute - such as asking whether the benefits of the stimulus outweighed the costs. Some of the questions seem benign). In some areas the consensus is weaker, such as on labour issues where 17 per cent oppose the consensus and 29 per cent are uncertain, leaving a 3 to 1 ratio between those in the majority and those against.\nThere has been plenty of reaction to these findings in the blogosphere (such as Noah Smith and Paul Krugman), and Justin Wolfers’s comments on the role of  ideology are worth a look. But the question in my mind is what should we consider to be a high level of consensus?\nAs a benchmark, a raft of climate and earth scientist surveys put disagreement among climate scientists on whether humans are behind climate change at six per cent or less.\nWhat if we posed some questions to biologists? Group selection seems an obvious area to generate a division, but my instinct is that most professional biologists would coalesce around the same view. Advocates of the general importance of group selection are still a small minority. If we expanded the question to include cultural group selection (and similarly increased the surveyed group), we might be closer to discovering a split. However, I suspect one of the features increasing consensus in the case presented by Gordon and Dahl is that it is a small group of high-profile economists from top institutions. We could probably also increase the measured level of dispute between economists by increasing the sample to include those less tied to the middle.\nEvolutionary psychology might be fertile ground for a dispute, with there still some reluctance to apply evolutionary theory to the human mind. However, I expect that it would be specific evolutionary psychology hypotheses that would be more controversial and not the general concepts. Other possible areas of dispute might be the rate of recent human evolution and the question of race, which sees some disagreement among biologists (16 per cent in the minority), although anthropologists seem more divided. On that note, I would expect a stronger split if I started quizzing anthropologists about issues such as human nature. I am also  relatively confident I could predict which anthropology faculties would be on each side of the debate.\nOn a historical note, would Stephen Jay Gould have been able to rally more than 10 per cent of evolutionary biologists to his side in his famous debates with Dawkins or Maynard Smith? I suspect not (is this hindsight bias from the perspective of the victors?), although he might have succeeded at exceeding that benchmark in the sociobiology debates.\nIn this light, it’s probably right to consider the level of consensus in economics as being strong, but to note that some of the areas have less consensus than we’d normally see in a field such as biology. Compare the consensus in economics to anthropology, however, and economists could appear quite unified.\nBut what of the future? Gordon and Dahl noted that consensus is generally strongest where the area in question has a larger body of literature behind it. That makes sense, but for the areas of dispute (particularly the public disputes that created the perception of a rift between economists), I don’t expect to see much short-term change. I have never been very confident on the rejection of controversial theories in economics through data and Wolfers shows evidence of ideology behind the division for some issues, so I suspect that at a minimum we may need to wait for a few funerals to occur."
  },
  {
    "objectID": "posts/concern-about-the-tyranny-of-choice-or-condescension-towards-others-preferences.html",
    "href": "posts/concern-about-the-tyranny-of-choice-or-condescension-towards-others-preferences.html",
    "title": "Concern about the “tyranny of choice”? Or condescension towards others’ preferences?",
    "section": "",
    "text": "I have been reading Robert Sugden’s book The Community of Advantage: A Behavioural Economist’s Defence of the Market in preparation for an upcoming webinar with Robert about the book, facilitated by Henry Leveson-Gower.\nThe webinar will be help at 1pm London time and 10pm Sydney time on Monday 3 September. Details about the webinar are here and you can register here. A video will be posted after.\nI’ll also post an in-depth review later, but the book is a mix of philosophy, technical economics, and critique of applied behavioural economics. The critiques are great reading, the philosophy is interesting but tougher, and the technical economic sections are for aficionados only.\nHere’s one snippet of critique as a taster:\n\nAn extreme version of the claim that choice overload is a serious problem in developed economies has been popularized by Barry Schwartz (2004) in a book whose premise is that when the number of options becomes too large, ‘choice no longer liberates, but debilitates. It may even be said to tyrannize’ (2004: 2). Researchers who investigate choice overload sometimes suggest that their findings reveal a fundamental failure of the market system—that it provides too much choice.\n…\n[T]he idea that markets offer too much choice seems to have some resonance in public debate, as evidenced by the success of Schwartz’s book and by the fame of Iyengar and Lepper’s experiment with jams. My sense is that it appeals to culturally conservative or snobbish attitudes of condescension towards some of the preferences to which markets cater. This may seem harmless fogeyism, as when Schwarz (2004: 1–2) begins his account of the tyranny of choice by complaining that Gap allows him to choose between too many different types of pairs of jeans (‘The jeans I chose turned out just fine, but it occurred to me that buying a pair of pants should not be a daylong project’). But it often reflects a misunderstanding of the facts of economic life, and a concealed interest in restricting other people’s opportunities to engage in mutually beneficial transactions.\nImagine you are asked to describe your ideal shopping environment. For many people, and I suspect for Schwartz, the description would be something like this. Your Perfect Shop is a small business, conveniently located in your own neighbourhood (perhaps just far enough away that you are not inconvenienced by other customers who might want to park their cars in front of your house). It stocks a small product range, tailored to your particular tastes and interests, but at prices that are similar to those charged by large supermarkets. There are some categories of goods (such as jeans if you are Schwartz) which you sometimes need to buy but whose detailed features do not much interest you. The Perfect Shop stocks a small but serviceable range of such items. There are other categories of goods (breakfast cereal might be an example) for which you have a strong preference for a specific brand and feel no need to try anything different; the Perfect Shop sells a limited range of this type of good, but your favourite brand is always on sale. However, there are a few categories of goods in which you are something of a connoisseur and like to experiment with different varieties. Here, the Perfect Shop offers a wide range of options, imaginatively selected to appeal to people who want to experiment in just the kinds of ways that you do. No shelf space is wasted on categories of goods which you have no desire to buy.\nCompared with such an ideal, real shopping may well seem to offer too much choice, not to mention clutter and vulgarity. But, of course, in a world in which there are economies of scale in retailing and people have different tastes and interests, the idea that each of us can have a Perfect Shop is an economic fantasy. A less fantastic possibility is that there are Perfect Shops for some people, but everyone is constrained to use them. Because these shops are well-used, prices can be kept low. But then the viability of what are some people’s Perfect Shops depends on the absence of opportunities for other people to buy what they want. Restricting other people’s opportunities to buy goods that have no appeal to you can be a way of conserving your preferred shopping environment without your having to pay for it. Describing these restrictions as defences against the tyranny of choice can be a convenient camouflage for a form of protectionism."
  },
  {
    "objectID": "posts/complexity-and-the-art-of-public-policy.html",
    "href": "posts/complexity-and-the-art-of-public-policy.html",
    "title": "Complexity and the Art of Public Policy",
    "section": "",
    "text": "The basis of David Colander and Roland Kupers’s book Complexity and the Art of Public Policy: Solving Society’s Problems from the Bottom Up is that the economy is a complex system and it should be examined through a complexity frame.\nA complex system comprises many parts that interact in a nonlinear manner. You can’t simply add the parts. While you might expect this would lead to chaos, emergent behaviour in complex systems can lead to what appears to be an order state. Outcomes in complex systems are inherently hard to predict. Small changes in initial conditions can have massive effects on outcomes. Any tweak to the system can cascade through the system in a myriad of ways. Determining the web of interactions with precision is impossible, so despite the abundance of experts making economic arguments as though they know what they’re doing, they usually don’t.\nThe complexity frame does away with a lot of the assumptions built into policy analysis, including:\n\nIt doesn’t assume people are hyper rational; it doesn’t assume system dynamics are linear; it doesn’t assume tastes are unaffected by process; it doesn’t assume government can control; it doesn’t assume the competitive market can somehow exist independently of government and other social natural systems; it doesn’t assume the institutional structure is fixed. Through what it doesn’t assume, the complexity policy frame changes the nature of the policy discussion.\n\nIn general, I am a fan of this argument. It leads to a more humble approach to policy. And despite the suggestion that prediction is hard in such systems, complexity science does provide some insight.\n\nWe are not suggesting that society should resign itself to a fatalistic relativist position by concluding that since everything is complicated; you just have to fall back to your subjective judgment. That’s not what we mean. We advocate setting the bar substantially higher, with the idea of educated common sense. Educated common sense involves an awareness of the limitations of our knowledge that is inherent in the complexity frame. A central argument of this book is that with complexity now being several decades old as a discipline (and much older as a sensibility), policy that ignores this frame fails the educated common sense standard.\n\nImportantly, the complexity frame provides a new perspective on government. Rather than the question being one of government versus markets, both top-down government solutions and bottom-up market solutions are seen as having evolved from the bottom up. The government solution is an element of the system, not outside it, with government a (crude) bottom-up solution to previous problems.\nOn that basis, the proper role of government is not to implement the government’s will, but rather the people’s will through governmental institutions built to solve collective action problems and provide the institutional environment for people to solve problems themselves. People can solve their problems in the right environment. Colander and Kupers write:\n\n[The complexity approach to policy] assumes that individuals are smart and adaptive, and if responsibility is given to them, they will use it to avoid problems for themselves and to design institutions that achieve the goals they want. But they will do so only if they are given the chance and only in the appropriate institutional environment. To “be given a chance” means that government does not “solve” the problem prematurely. In that view the complexity approach to policy is similar to the market fundamentalist view. But it differs from the market fundamentalist position in two crucial ways: First, it sees individuals as concerned with much more than material welfare, their aspirations extending to broader social welfare improvements. Second, while the market fundamentalist position assumes that the market will enable an optimal solution, the complexity frame recognizes that there may well be lock-ins, emergent collective effects, or market failures that need to be willfully overcome via collective action to allow the economy to move to a more desirable basin of attraction.\n\nAlthough I broadly agree, you can see the caricature of the ‘market fundamentalist” creeping in - the implication that ’market fundamentalists’ believe people only care about material welfare. A degree of caricature is present through much of the book. However, the second point is important. Emergent outcomes may not necessarily be optimal, although in the right institutional environment they often are.\nOne example where a successful institutional structure was established concerns an intersection in the town of Drachten in the Netherlands. This intersection has no traffic lights, no sidewalks, no stop signs and no traffic directions from police. The result is slightly faster traffic flow and fewer accidents. The appropriate institutional environment was one in which users were mixed and took care in their approach to the intersection, but were free to use judgment as to how to approach it.\nDespite being distinguished from the market fundamentalist view, there is a laissez-faire bent to the complexity approach - although as Colander and Kupers explain, their use of “laissez-faire” better matches its historical origins than some current uses. Colander and Kupers also suggest that greater direct government intervention reduces the success of the system getting the bottom-up “ecostructure” right.\nHaving said I am a fan of a complexity approach, the way Colander and Kupers make their case is repetitive. They spend the first two-thirds of the book placing the complexity frame between the opposing viewpoints of free markets and government control, and suggest that a complexity approach can rise above politics as it assumes neither government or free markets are superior (Eric Beinhocker tries the same trick in The Origin of Wealth). This claim is somewhat naïve, and I expect that adoption of the complexity frame will simply add a new tool to the old battle (as used by Paul Ormerod in Why Most Things Fail). The authors also spend a lot of time contrasting the complexity frame with what they call the standard policy frame - also somewhat repetitive - although I do suspect this is ultimately the greatest contribution that will come from complexity science.\nOne important piece of the complexity frame is norms. Norms are strong shapers of behaviour and outcomes. Turning back to the traffic example in Drachten, norms around watching out for pedestrians and the appropriate way to drive through a mixed use space are likely important. Trust is also an essential enabler. An influx of foreign drivers with different norms and values could result in the experiment breaking down. As an example of different norms, Colander and Kupers note that traffic merging varies between Germany and the Netherlands, meaning that road design might need to vary between two apparently similar cultures.\nA further twist to the complexity frame is the assumption that norms and culture are not constant, and tastes are not necessarily well formed. The activities that people undertake can feedback on their wants and shape them as a person. The complexity frame shows that what the collective wants as its taste and norms and what is has as its tastes and norms can differ, and “both society and individuals can know that some of the tastes and norms they have are not the tastes and norms they want”.\nThis mismatch leads Colander and Kupers to advocate what they call norms policy. Rather than small changes in environment to affect people’s choices (the territory of behavioural economics and ‘nudges’), they propose supernudges - institutional changes that change and feed back on people’s tastes and norms. The policy discussion should be on how tastes evolve, change and can be influenced.\nTheir advocacy of norms policy has a starting assumption that tastes are arbitrary, which gives a degree of freedom in changing them. However, that starting point seems to have the same problem as the approach they critique, unsubstantiated assumptions. An evolutionary understanding of human behaviour might suggest some norms are rather robust.\nContinuing on, they argue that as norms can be influenced by government action, government should tweak the environment so pro-social norms emerge.  One example they provide is that government should try to influence the tastes and norms relating to the materialistic nature of society. They suggest that “many would agree that in today’s Western societies material welfare is given more prevalence than most people would like” and that there is nothing normal about conspicuous consumption as a norm. However, as above, expensive signalling and status desires seem very much part of human nature.\nA specific example relates to climate change. Colander and Kupers argue that if people have climate friendly tastes, there is little cost to dealing with the problem of climate change (I’m not sure that argument strictly holds - if I have BMW friendly tastes, there may still be a substantial cost of acquiring one). As a result, government should encourage tastes less likely to create global warming than other tastes.\nTo give a practical example, they point to a publication co-authored by Kupers that suggests climate change is an economic opportunity, and that bigger emission reduction targets can result in more investment growth and employment in Europe. They do state that they don’t know if will work, but they applaud it as useful experiment. But how would you ever know it worked in a complex system? And what are the costs of failure? Further, parts of the report look like standard economics junk, complete with estimates of GDP in 2020 to the nearest billion dollars.\nBeyond this climate change example, norms policy hits me as problematic for two reasons. First, it smacks of naïvety about the nature of government (as does their suggestion that government could provide a positive role model). What tastes and norms would government like us to have (beyond voting for incumbents)? But more importantly, given that tastes and norms are part of a complex system, what confidence could government have that their actions would influence people in the right way? And how might those norms play out? Materialistic norms might just be the type of norms that allow many of our other non-materialistic preferences to be realised.\nMore broadly, they discuss the need to get the ecostructure right for positive norms or solutions to emerge. They consider this to be the area where the biggest gains from a complexity frame will be found.\nOne suggestion is facilitating the development of for-benefit firms, sparing social entrepreneurs from being shoehorned into the not-for-profit or profit categories. To the extent there are barriers from people forming firms with social objectives, it may be a useful idea. But once again, their starting position - that only for-benefit institutions can optimise overall welfare - appears somewhat strong. As a result, they worry about people using new forms of for-benefit institutions to benefit themselves, not society (is the for benefit label just a marketing tool?), which seems a misplaced concern.\nHaving said that, as they do for many of their suggestions, they acknowledge that for-benefit structures might not achieve positive outcomes. This is matched with their general (welcome) call for experimentation - although experimentation in a complex system is difficult.\nFor many  of the policy implications presented by Colander and Kupers, I was unclear how they related to the complexity frame. At times it felt as though they were attempting to claim any interesting idea as relating to complexity, be that prediction markets or prizes for innovation (see the notes at the bottom of this post for some examples). They even praise economists’ aim to find “the ideal instrumental variable and more generally torturing the data to make it tell a story,” despite the linearity embedded in most of these models. Complexity theory would suggest most of these results are useless.\nThen there is also the occasional bashing of the standard policy model straw man. They claim that the standard policy model results in people thinking of government policy by default in terms of GDP, ignoring the mass of policy concerning equity or capability. They suggest resilience plays no part in the standard frame, ignoring the massive prudential regulatory apparatus applied to the banking system. They also provide an example of a massive investment in the Netherlands in constructing a water wall. People changed behaviour and moved into the protected areas, which makes cost of failure even higher - an event they suggest the standard policy frame ignores. But this is exactly what the standard frame would predict - change incentives, change behaviour.\nColander and Kupers also praise the emergence of behavioural economics, although they do criticise attempts to put these findings into the standard policy model. Their willingness to jump on the behavioural economics bandwagon does highlight one of the shortcomings of their approach - their failure to adopt an even deeper understanding of human behaviour as provided by evolutionary theory. It would provide a better framework on which to build their understanding of norms.\nOverall, Complexity and the Art of Public Policy is a good book. However, the last third of the book did not convince me that complexity theory arms us with many new policy tools. A complexity frame punches holes in many of our methods of analysis and the policy options we put on the table using standard economic frameworks. But the very nature of complex systems makes it a challenge to propose options that we can claim with any confidence to have positive effect. Roland and Kupers have ventured into that area - which I am grateful for. I was glad to read a complexity book with an attempt to give some policy relevance without yet another description of the El Farol Bar problem. But the particular examples they provided leave me of the view that the strongest contribution of complexity theory will be to tell us when our standard economic policy tools will work, and when they won’t. That’s no small accomplishment, but in a world where we need to “do something”, it’s not an easy sell.\n\nAs I didn’t want to fill this review with minor complaints (as has been my bad habit recently), below are some additional notes that I took as I read the book.\nThrough the last third of the book I found myself constantly marking proposed applications of the complexity frame that did not seem to have much to do with complexity:\n\nColander and Kupers point to a 90 per cent reduction in plastic bag use in Ireland following introduction of a small tax as a discontinuity or tipping point, but in the presence of easy substitutes, it is not hard to explain in a standard economic framework.\nThey put granting prizes in the complexity frame, but why? It appears to involve standard incentives.\nPrediction markets are placed complexity frame, but can also be accommodated in the standard policy frame. And why doesn’t the complexity frame suggest that sometimes the prediction markets will fail?\nThey propose charging fees for quasi-public goods where beneficiaries can be identified. Economics 101.\nTheir discussion of single payer versus competitive market in healthcare looks a lot like a standard policy debate. They state that the complexity framework shows the current setup is not ideal (does any?) and point to problems separating the buyer and payer (as standard as you might get).\nThey point to shorter duration copyright being better, and suggest that extensions to keep Mickey Mouse under copyright might be considered OK under the standard policy frame. It seems ridiculous under all frames to me.\nThey propose leasing government resources to fund social goods, rather than privatising. But what does the complexity frame show about this that a standard policy frame does not?\nThey propose charging for patents and copyright, pointing out that while it reduces financial incentives, people innovate for other motivations too. An interesting idea, but why is this a complexity frame? And on the merits, can you imagine the complexity of copyright law if government directly had its fingers in that pie? You would have the mess of copyright minus the incentive it is designed to provide.\n\nOn behavioural economics, they suggest the behavioural economists’ argument of less than perfect rationality undermines the traditional economic framework and all policy conclusions that flow from it. They also suggest that you can’t simply shoehorn behavioural economics into the standard economic model as it rests on different assumptions. This claim is a bit too strong - Vernon Smith’s experiments in markets suggest this. Their willingness to adopt the findings of behavioural economics is also an interesting fit with their belief that people are smart and adaptive, which they suggest is a precondition for the emergence of bottom-up solutions.\nAn evolutionary framework could inform some of their analysis of the existence of path dependency and lock-in of income. Contrast their suggestion that changing the ecostructure could change distribution of income with the work of Gregory Clark showing the failure of policy changes to affect social mobility. Path dependency in the income of children might simply reflect the existence of the same underlying (genetic) factors."
  },
  {
    "objectID": "posts/cluelessness.html",
    "href": "posts/cluelessness.html",
    "title": "Cluelessness",
    "section": "",
    "text": "Some of the reviews of Michael Chwe’s Jane Austen, Game Theorist suggest that it is worth a read (such as Diane Coyle ). One idea in the book that I like the sound of is “cluelessness”.\nFrom Jennifer Schuessler in the NYT:\n\nMost game theory, he noted, treats players as equally “rational” parties sitting across a chessboard. But many situations, Mr. Chwe points out, involve parties with unequal levels of strategic thinking. Sometimes a party may simply lack ability. But sometimes a powerful party faced with a weaker one may not realize it even needs to think strategically.\n\nApplications of game theory to the real world often neglect the limited ability of the players. And this is not simply a case of bounded rationality or biases of the types identified in behavioural economics. It includes straight miscalculation. Some problems are hard and even those that are easy are often messed up. The reviews also hint at another important feature of cluelessness - that not all people are equal and we can expect more clueless behaviour from some people than others.\nI am a fan of the analysis of crime, cooperation, war, sports and so on from the perspective of cold rationality (an evolutionary-derived rationality of course). But acknowledging the cluelessness of the players can provide a simple explanation for a lot of apparently mysterious behaviour."
  },
  {
    "objectID": "posts/clark-on-violence.html",
    "href": "posts/clark-on-violence.html",
    "title": "Clark on violence",
    "section": "",
    "text": "In Greg Clark’s excellent book A Farewell to Alms, Clark posited that there was only one important event in human history - the Industrial Revolution. Before that time, per capita income was effectively flat, with no discernible trend. That all changed around 1800 AD with the Industrial Revolution. Clark saw the Neolithic revolution and the move to settled agriculture as simply an extension of hunting and gathering and symptomatic of the steadily improving efficiency that had occurred over the previous tens of thousands of years.\nFollowing publication of the book, a series of articles by reviewers were published in the European Review of Economic History (which are unfortunately gated for those without university access). In Clark’s response (which he has helpfully placed on his website), he concedes that there is another event to the Industrial Revolution that we should note. This event is not the Neolithic revolution but is the move to societies where violence was limited (and centralised). It is in such societies the competition for reproductive success shifts towards economic means, meaning that those traits conducive to economic growth can spread.\nClark noted the poor understanding that we have of this transition. Following from my posts of the last two days on violence (here and here), if a violent society favours genotypes which have a tendency to violence, what is the trigger to exit that violent state? The Neolithic revolution appears to be a necessary but not sufficient step.\nClark uses the example of the Huli of the southern highlands of Papua New Guinea. They had established a settled system of agriculture and men accumulated wealth through the accumulation of pigs and control over gardens. However, 20 per cent of male deaths and 6 per cent of female deaths were from violence or warfare. Rather than social status being attained through wealth, causation was reversed, with high social status leading to wealth and that status often coming from being distinguished in fighting. Success in this society was built on war and social intercourse and not skill in production."
  },
  {
    "objectID": "posts/christian-and-griffithss-algorithms-to-live-by-the-computer-science-of-human-decisions.html",
    "href": "posts/christian-and-griffithss-algorithms-to-live-by-the-computer-science-of-human-decisions.html",
    "title": "Brian Christian and Tom Griffiths’s Algorithms to Live By: The Computer Science of Human Decisions",
    "section": "",
    "text": "In a sea of books describing a competition between perfectly rational decision makers and biased humans who make systematic errors in the way they decide, Brian Christian and Tom Griffiths’s Algorithms to Live By: The Computer Science of Human Decisions provides a nice contrast.\nChristian and Griffiths’s decision-making benchmarks are the algorithms developed by mathematicians, computer scientists and their friends. In that world, decision making under uncertainty involves major trade-offs between efficiency, accuracy and the types of errors you are willing to accept. As they write:\n\nThe solutions to everyday problems that come from computer science tell a different story about the human mind. Life is full of problems that are, quite simply, hard. And the mistakes made by people often say more about the intrinsic difficulties of the problem than about the fallibility of human brains. Thinking algorithmically about the world, learning about the fundamental structures of the problems we face and about the properties of their solutions, can help us see how good we actually are, and better understand the errors that we make.\n…\nEven where perfect algorithms haven’t been found, however, the battle between generations of computer scientists and the most intractable real-world problems has yielded a series of insights. These hard-won precepts are at odds with our intuitions about rationality, and they don’t sound anything like the narrow prescriptions of a mathematician trying to force the world into clean, formal lines. They say: Don’t always consider all your options. Don’t necessarily go for the outcome that seems best every time. Make a mess on occasion. Travel light. Let things wait. Trust your instincts and don’t think too long. Relax. Toss a coin. Forgive, but don’t forget. To thine own self be true.\n\nAnd as they close:\n\nThe intuitive standard for rational decision-making is carefully considering all available options and taking the best one. At first glance, computers look like the paragons of this approach, grinding their way through complex computations for as long as it takes to get perfect answers. But as we’ve seen, that is an outdated picture of what computers do: it’s a luxury afforded by an easy problem. In the hard cases, the best algorithms are all about doing what makes the most sense in the least amount of time, which by no means involves giving careful consideration to every factor and pursuing every computation to the end. Life is just too complicated for that.\n\nHere’s a few examples.\nSuppose you face a choice between two uncertain options. Those options have an expected value - the most likely result. If your objective is to maximise your outcome, you pick the option with the highest expected value.\nBut what if you objective is to minimise regret - the feeling of pain when you look back at what you did compared to what you could have done? In that case it may be worth looking at the confidence intervals around that expected value - the plausible ranges in which the actual value could lie. Picking the option which has the highest upper confidence interval - the highest plausible value - is the rational approach, even if it has the lower expected value. It is “optimism” in the way a behavioural scientist might frame it, but for an objective of minimising regret, it is rational.\nOr consider memory. From a computer science perspective, memory is often not a question of storage but of organisation - particularly in today’s world of cheap storage. How does a computer predict which items it will want from its memory in the future such that they are accessible within a reasonable time? Faced with that problem, it makes sense to forget things. In particular, it is often useful to forget things with time - those items least recently used. The human mind mimics this strategy, as more recently used items are more likely to be used in the future. It is too expensive to maintain access to an unbounded number of items.\nOne chapter of the book covers the idea of “less is more”, which you may be familiar if you know the work of Gerd Gigerenzer and friends. The idea behind “less is more” it that it is often rational to ignore information in making decisions to prevent “overfitting”. Overfitting is an over-sensitivity to the observed data in developing a model. The inclusion of every detail helps the model match the observed data, but prevents generalisation to new situations and predictions based on new data lack reliability.\nTo avoid overfitting you might deliberately exclude certain factors, impose penalties for including factors in analysis, or stop the analysis early. These strategies are often used in both computer science or machine learning applications, and by humans, and can result in better decisions.\nChristian and Griffiths suggest that evolution tends not to overfit as it is constrained by existing infrastructure and time - features of the environment need some degree of persistence before adaptations to that environment spread, preventing major changes in response to short-term phenomena. Preventing overfitting is also a benefit of a conservative bias in society – preventing us getting caught up in the boom and bust of fads.\nThere are times in the book where Christian and Griffiths jump too far from experiment or algorithm to real world application. As an example, they suggest that analysis of a routing tells us not to try to control traffic congestion using a top down coordinator, as the selfish solution is only 33% worse than best case top down coordination. They give little thought to whether congestion has more dimensions of control than just routing. The prisoner’s dilemma chapter also seemed shallow at points - possibly reflecting that it is the area for which I already had the most understanding.\nBut those are small niggles about an otherwise excellent book."
  },
  {
    "objectID": "posts/charts-that-dont-seem-quite-right-organ-donation-edition.html",
    "href": "posts/charts-that-dont-seem-quite-right-organ-donation-edition.html",
    "title": "Charts that don’t seem quite right - organ donation edition",
    "section": "",
    "text": "Organ donation rates are an often used example of the power of defaults. Take the following passage by Dan Ariely, explaining this (also often used) chart from Johnson and Goldstein (2003) (ungated pdf):\n\nOne of my favorite graphs in all of social science is the following plot from an inspiring paper by Eric Johnson and Daniel Goldstein. This graph shows the percentage of people, across different European countries, who are willing to donate their organs after they pass away. When people see this plot and try to speculate about the cause for the differences between the countries that donate a lot (in blue) and the countries that donate little (in orange) they usually come up with “big” reasons such as religion, culture, etc.\nBut you will notice that pairs of similar countries have very different levels of organ donations. For example, take the following pairs of countries: Denmark and Sweden; the Netherlands and Belgium; Austria and Germany (and depending on your individual perspective France and the UK). These are countries that we usually think of as rather similar in terms of culture, religion, etc., yet their levels of organ donations are very different.\nSo, what could explain these differences? It turns out that it is the design of the form at the DMV. In countries where the form is set as “opt-in” (check this box if you want to participate in the organ donation program) people do not check the box and as a consequence they do not become a part of the program. In countries where the form is set as “opt-out” (check this box if you don’t want to participate in the organ donation program) people also do not check the box and are automatically enrolled in the program. In both cases large proportions of people simply adopt the default option.\n\n\nBut does this chart seem right given that story? 99.98 per cent fail to opt-out in Austria? 99.97 per cent in Hungary? It seems too many. And for Dan Ariely’s story, it is too many, because the process is not as described.\nThe hint is in the term “presumed consent” in chart description. There is actually no time where Austrians or Hungarians are presented with a form where they can simply change from the default. Instead, they are presumed to consent to organ donation. To change that presumption, they have to take steps such as contacting government authorities to submit forms stating they don’t want their organs removed. Most people probably don’t even think about it. I would feel uncomfortable calling it a “default” - and Johnson and Goldstein are clear that there are ethical questions with such “opt-out” arrangements.\nSo what does this mean in practice? Take the following from an Austrian government site:\n\nIn Austria, organs, parts of organs or tissue of potential donors may be removed if the person in question did not expressly refuse organ donation before their death.\nIn order to document such objections effectively, the Opting-out Registry of persons refusing organ donation was established. Apart from refusals documented in the Registry, also other forms of refusal of post-mortem organ donations are respected (e.g., a written explanation among the identification papers or an oral refusal witnessed by relatives).\nThe Opting-out Registry has primarily been designed for people living in Austria, and the Austrian social security number is used as the main identification tool. Persons who are staying in Austria for a short time only (for holidays, conferences, family visits) should preferably keep their written personal wishes regarding donations, among their identification papers (consent: I am willing to donate my organs; refusal: I do not want to donate my organs). Their wish is respected in the event of death. In addition, this person’s relatives are consulted.\n\nYou can download a form from that page to lodge a refusal in the registry.\nThe last sentence of the government text gives a hint to the process on the ground - the deceased’s relatives are consulted. The process effectively leaves the question of organ donation unaddressed until after death.\nI expect consultation with relatives is part of the reason behind the much smaller differences in the outcome we care about - organ donation rates. Germany at 15.3 deceased donors per million people is not far from Austria’s 18.8 and Sweden’s 15.1. Spain, which has an opt-out arrangement, is far ahead of most countries at 33.8, but the United States, an opt-in country, is also ahead of most opt-out countries with a donation rate of 26.0.\nHaving said all this, a lot of interesting options for organ donation should be explored - active choice, preferential access to organs for previously registered donors, respecting the wishes of the deceased over the preferences of relatives, or payments of some kind. But the story behind this chart is not as neat as it seems. And a lesson - if you can, read the original paper."
  },
  {
    "objectID": "posts/cass-sunstein-and-reid-hasties-wiser-getting-beyond-groupthink-to-make-groups-smarter.html",
    "href": "posts/cass-sunstein-and-reid-hasties-wiser-getting-beyond-groupthink-to-make-groups-smarter.html",
    "title": "Cass Sunstein and Reid Hastie’s Wiser: Getting Beyond Groupthink to Make Groups Smarter",
    "section": "",
    "text": "Cass Sunstein and Reid Hastie’s Wiser: Getting Beyond Groupthink to Make Groups Smarter is not an exciting read. However, it is a good catalogue of group decision-making research (leading to this post to also be somewhat of a catalogue) and worth reading for an overview.\nThe book’s theme is that group decisions are often better than individual decisions, but that groups have weaknesses that can impair outcomes. Much of the analysis of failures in group decision-making follows a similar theme to the research into individual judgement and decision-making, in that the research has generated a long list of “biases” that groups are subject to. Most of the book, however, focuses on getting better decisions, and a lot of these (thankfully) don’t rest on identification of particular biases."
  },
  {
    "objectID": "posts/cass-sunstein-and-reid-hasties-wiser-getting-beyond-groupthink-to-make-groups-smarter.html#two-types-of-groups",
    "href": "posts/cass-sunstein-and-reid-hasties-wiser-getting-beyond-groupthink-to-make-groups-smarter.html#two-types-of-groups",
    "title": "Cass Sunstein and Reid Hastie’s Wiser: Getting Beyond Groupthink to Make Groups Smarter",
    "section": "Two types of groups",
    "text": "Two types of groups\nSunstein and Hastie look at two types of groups - statistical and deliberating groups.\nIn a statistical group, members give their inputs individually. Those inputs are then aggregated. Think voting (which works well as long as the majority is right).\nThere is no shortage of material about the wisdom of statistical groups. The story of Francis Galton, where he had people estimate the weight of an ox, is a classic example. The average of the individual predictions was right on the mark.\nIn deliberating groups, individuals provide input during deliberations. Those inputs can affect and be affected by the inputs of other group members. People aim to influence others. People might change their minds.\nEven if most members of a group have the wrong answer or belief, you can picture a scenario where reason and discussion allow the right answer emerge. That is sometimes the case, but the evidence is that deliberating groups do not necessarily converge on the truth.\nIn one experiment, people answered questions individually before answering those same questions in groups. If the majority of the group knew the correct answer to a problem, the group’s decision was correct 79% of the time. (It’s impressive that the incorrect minority were able to derail the group 21% of the time.) If the majority of the group answered a question incorrectly when answering individually, the group converged on the right answer only 44% of the time. The result of this dynamic was that the average group decision was better, but only marginally so, than the average individual (66% versus 62%).\nAs a result, it may be easier to simple elicit people’s individual views and average them (or combine in some other novel way) than go through the effort of the group discussion. A statistical group may be a more efficient solution."
  },
  {
    "objectID": "posts/cass-sunstein-and-reid-hasties-wiser-getting-beyond-groupthink-to-make-groups-smarter.html#why-deliberating-groups-go-wrong-or-right",
    "href": "posts/cass-sunstein-and-reid-hasties-wiser-getting-beyond-groupthink-to-make-groups-smarter.html#why-deliberating-groups-go-wrong-or-right",
    "title": "Cass Sunstein and Reid Hastie’s Wiser: Getting Beyond Groupthink to Make Groups Smarter",
    "section": "Why deliberating groups go wrong (or right)",
    "text": "Why deliberating groups go wrong (or right)\nWhy do we get results such as this? Sunstein and Hastie describe plenty of problems that can derail deliberating groups. Group decisions can be poor due to both the rational conduct of group members and because of their “biases”. Here are a few problems that can occur for “rational” reasons:\n\nInformational signals: It is sensible to take into account what others have said in a group deliberation. If you know Jane is knowledgeable and has good judgment, hearing that she supports a project is evidence that can affect your support. But if she is wrong, she can derail the group. Seeing other people make errors can also provide “social proof” to an error.\nSelf-censorship: People tend not to give information contradicting their preferred outcome. In one study of over 500 mock jury trials, the experimenters never once observed someone giving information in this circumstance.\nReputational cascades: People might know what is right (or what they think is right), but they go along with the group or certain members of the group due to concern for their reputation or standing.\n\nThen there are the “irrational” (a lot of these points are based on single studies, so take with a grain of salt):\n\nDeliberating groups are more likely to escalate commitment to a failing course of action. They are also more susceptible to the sunk cost fallacy, the consideration of past costs that should be irrelevant to the decision about future action\nGroups can amplify the representativeness heuristic, where we judge probability based on resemblance or similarity\nPeople in deliberating groups have more unrealistic “overconfidence” (looking at the abstract of the paper cited for this point - I can’t access the full paper - I think they are talking about over-precision)\nGroups are more vulnerable to framing effects, varying their decision based on how a choice is framed (although looking at the paper Sunstein and Hastie cite, it states that there is little consistency between studies)\nGroup deliberation can make both groups and the individuals in those groups more extreme\nShared information has a disproportionate effect on group members. If information is distributed so that key material is unshared (held by only a few group members), this can cause deliberating groups to perform worse.\n\nThat said, deliberating groups can temper some biases:\n\nGroups tend to rely less on the availability heuristic - a heuristic by which we judge probability by how easily examples readily come to mind. The heuristic is tempered possibly because the group members have different memories. Across the group the available memories may be somewhat more realistic. That said, groups can be subject to availability cascades. An idea held by one person can spread through the group, eventually producing a widespread belief.\nGroups have a lower tendency to anchor, the over-reliance on the first piece of information with which they are presented (even if it is irrelevant to the decision at hand)\nGroups tend to have reduced hindsight bias, possibly because not everyone revises their views in the same way\nGroups tend to have reduced egocentric biases, the belief that others think like you. A group typically has a wider set of tastes to draw on, so you are more likely to have someone point out that your tastes are not shared."
  },
  {
    "objectID": "posts/cass-sunstein-and-reid-hasties-wiser-getting-beyond-groupthink-to-make-groups-smarter.html#improving-deliberation",
    "href": "posts/cass-sunstein-and-reid-hasties-wiser-getting-beyond-groupthink-to-make-groups-smarter.html#improving-deliberation",
    "title": "Cass Sunstein and Reid Hastie’s Wiser: Getting Beyond Groupthink to Make Groups Smarter",
    "section": "Improving deliberation",
    "text": "Improving deliberation\nThe most interesting part of the book is when Sunstein and Hastie turn to their tactics to improve group decision. There are two groups of tactics: those designed to improve deliberation, and alternative decision-making methods. A common threads to these is diversity, although this is “not necessarily along demographic lines, but in terms of ideas and perspectives.”\nThey list eight ways to avoid problems in deliberating groups: (1) inquisitive and self-silencing leaders; (2) “priming” critical thinking (although we have seen how the priming literature is holding up); (3) rewarding group success (incentives are important, particularly to counter self-censorship and reputational cascades); (4) role assignment; (5) perspective changing; (6) devil’s advocates; (7) red teams; and (8) the Delphi method. A few are worth mentioning.\nRole assignment involves giving people discrete roles, such as labelling someone as an “expert”. The purpose is to bring out unshared information by making it clear that the individual expert has a role to play.\nDevil’s advocacy involves appointing some group members to deliberately advocate against the group’s inclinations. Sunstein and Hastie suggest that the research behind devil’s advocates is mixed. There is some evidence that devil’s advocacy can be helpful and can enhance group performance. But it requires genuine dissent. If the dissent is insincere (which is often the case if the role is assigned), people discount the dissent accordingly. The advocate also has little to gain by zealously challenging the dominant view. This means it may be better for groups to encourage real dissent.\nSunstein and Hastie are more optimistic about red teaming, the creation of a team tasked with criticising or defeating the preferred solution or plan. I can see how they might be occasionally useful, such as in mock trials, but it wasn’t clear where their optimism came from as they provided little evidence in support.\nOne option I find useful is the Delphi method. You ask people to state their opinions anonymously and independently before deliberation. These opinions are then made available to others. It is effectively a secret ballot plus reasons, and provides a basis for hidden information to emerge without reputational or informational cascades. Several rounds of this process can be held as the group converges on a solution. It’s a great way to flush out doubts and dissent."
  },
  {
    "objectID": "posts/cass-sunstein-and-reid-hasties-wiser-getting-beyond-groupthink-to-make-groups-smarter.html#better-decisions-without-deliberation",
    "href": "posts/cass-sunstein-and-reid-hasties-wiser-getting-beyond-groupthink-to-make-groups-smarter.html#better-decisions-without-deliberation",
    "title": "Cass Sunstein and Reid Hastie’s Wiser: Getting Beyond Groupthink to Make Groups Smarter",
    "section": "Better decisions without deliberation",
    "text": "Better decisions without deliberation\nMuch of the book is dedicated to methods to arrive at good decisions outside of, rather than within, the deliberation process. These include design thinking (as a way of eliciting as much information and as many ideas as possible), cost-benefit analysis, asking the public (public comment or consultation), tournaments, prediction markets, and harnessing experts. Some of these are effectively statistical groups with different models for combining inputs.\nUnsurprisingly given Sunstein’s background, the authors are positive on cost benefit analysis. Having seen some cost-benefit sausages being made for government decision-making, I don’t quite share the same optimism, but can see the benefits in the right place.\nSunstein and Hastie are also boosters of use of tournaments. The dispersion of competitors leads to independence in inputs. Their winner take all nature incentivises divergent strategies. They can promote elite performance at the top of competitor’s capabilities.\nA question not addressed in the book is to what extent tournaments can be scaled and be a widely used solution. There is a waste of resources inherent in tournaments - the input of the losing teams. A Kaggle competition uses a massive amount of data science capability, far more than the “prize”. At the moment, many candidates are happy to input this effort as there are other benefits, such as reputation. Could it be the standard way of doing things? In the case of government tournaments, they would want to pick the projects of most value to avoid over-stretching the resource.\nAs a tournament example, Sunstein and Hastie were underwhelmed by the IARPA prediction tournament, where teams competed to predict political and economic events. They felt that the winning solution from the Good Judgement Project was more focused on reducing noise and bias, rather than developing game changing methods that increase signal (tough crowd). (See my post on Superforecasting for more on that tournament.) Maybe the new hybrid forecasting tournament might be more to their liking.\nThe final technique I’ll note is effective harnessing of experts. This could be using experts who use statistics to develop accurate predictions or make decisions (often in turn drawing on other sources). It could involve identifying fields where expert knowledge is genuine (as identified in the work of Gary Klein). When doing this, however, it is often best to look at statistical groups of experts, rather than to chase a single expert. The average of experts is likely the best prediction. And there is no need to weight for an expert’s confidence in developing that average - it has no correlation with their accuracy.\n\nPostscript 1: Sunstein and Hastie explore the question of collective intelligence (the “c factor”). That deserves to be the subject of another post.\nPostscript 2: Sunstein and Hastie talk of “eureka” problems, where the right answer is clear to all once announced. Groups are good at these. They give the “trivial” example of “Why are manhole covers round?” Because “if they were almost any other shape, a loose cover could shift orientation and fall through the hole, potentially causing damage and injuries.” Is that really the logic behind their design? Or is this just a benefit? (I ask not just because most manhole covers in Australia are square or rectangular, and I have never seen a cover fall through the hole.) This example is famous as being used in Microsoft job interviews, but it is a question more focused on making the interviewer feel clever than actually predicting, say, good job performance."
  },
  {
    "objectID": "posts/caplans-selfish-reasons-to-have-more-kids.html",
    "href": "posts/caplans-selfish-reasons-to-have-more-kids.html",
    "title": "Bryan Caplan’s Selfish Reasons to Have More Kids",
    "section": "",
    "text": "Bryan Caplan has a simple recommendation. Have more kids. If you have one, have another. If you have two, consider three or four. As Caplan spells out in his book, Selfish Reasons to Have More Kids, children have higher private benefits than most people think. Research shows that parents can take it easy, as there is not much they can change about their children. He also argues that there are social benefits to a higher population, with more people leading to more ideas, which are the foundation of modern economic growth.\nCaplan provides a fun, easy to read book that gives a great, swift overview of his case. This is the book I’ll be giving to parents, grandparents and friends who have heard me go on about twin studies and genetics. I particularly like it that Caplan gives some practicality to the swathes of findings about trait heritability. Caplan brings to life the arguments of the giants on whose shoulders he stands, particularly Judith Rich Harris and Julian Simon.\nI felt that the largest shortcoming of the book was that it does not address the third factor affecting outcomes for the child - non-shared environment. While heritability explains some of the variation in a child’s traits and outcomes, and nurture generally explains close to nothing, Caplan does not explore the research into non-shared environment. Instead, he puts the variation down to free will:\n\nSo far, researchers have failed to explain why identical twins - not to mention ordinary siblings - are so different. Discrediting popular explanations is easy, but finding credible alternatives is not. Personally, I doubt that scientists will ever account for my sons’ differences, because I think their primary source is free will. Despite genes, despite family, despite everything, human beings always have choices - and when we can make different choices, we often do.\n\nCaplan states that several of his friends call his belief in free will his “most absurd belief”. While I don’t know all of Caplan’s beliefs, for the moment I will agree with his friends. In Judith Rich Harris’s The Nurture Assumption: Why Children Turn Out the Way They Do, she explored what this non-shared environment might be. In her case, she argued for the effect of peers. What bothered me most with Caplan’s take on free will was not that he did not agree with Harris’s suggestion, but rather, his “it’s all too hard” approach. Unlike Caplan, I expect that over the next few years we will add even further to the explanations for how non-shared environment influences children.\nWhen Caplan came to addressing potential reasons why family size has decreased over the last 60 years, I wanted to hear his arguments in more depth. Take Caplan’s take on Gary Becker’s argument that as women now earn more, they have to give up more income to have kids:\n\nThis explanation sounds good, but it’s not as smart as it seems. Women lose more income when they take time off, but they also have a lot more income to lose. They could have worked less, earned more, and had more kids. Since men’s wages rose, too, staying home with the kids is actually more affordable for married moms than ever. If that’s too retro, women could have responded to rising wages by working more, having more kids, and using their extra riches to hire extra help.\n\nIt sounds neat, but Caplan assumes that the income effect, which would tend to increase the number of children, dominates the substitution effect, which would tend to decrease the number. It is perfectly plausible for the substitution effect to dominate and women to decide to have fewer children, but Caplan does not address this. He might be right, but as there is no depth to his discussion, it is hard to judge the strength of his argument.\nCaplan does point out that in the United States, fertility bottomed out in the 1970s. This occurred despite further increases in income and Caplan uses this as evidence against any income based hypothesis. But the people having children in the 1970s are different to the people having children now. For those women who chose to have no children in the 1970s and possibly responded most strongly to the income effect, they did not contribute to the gene pool and any heritable predisposition has disappeared with them. It is the children of larger families that are having children today. Second, the net fertility rate in the United States is substantially affected by recent immigrants.\nCaplan’s preferred view on the decline in fertility is that we have gained a small amount of foresight, allowing us to see the negative effects of early childhood, but not gained enough foresight to note the benefits of children when they are older. There might be some truth to this, but I expect that the other factors that Caplan dismisses are also relevant.\nOne point where I disagree with Caplan is around his statement that men and women see eye to eye on the number of children they wish to have. Caplan considers that this puts to bed any arguments around women having increased bargaining power. While Caplan’s statistic is true in the most basic sense, the number of children that a man or woman want are a function of a number of things. The main one of these is who the other parent will be. If a woman is paired with the man of her dreams she is likely to want more children than if she is married to a guy who showed promise but has gone nowhere. While Caplan notes that condoms have been widely available since the end of World War II, the pill gave women extra power to decide who exactly the parent is. There is some interesting scope for sexual conflict here.\nWhen it comes to policy prescriptions arising from his position, Caplan explicitly opposes natalist policies to increase birth rates. Caplan states:\n\nAfter natalists finish lamenting low birthrates, they usually get on a soapbox and demand that the government “do something about it.” There are two big reasons why I refuse to join their chorus. First, while I agree that more kids make the world a better place, I oppose social engineering - especially for such a personal decision. When people are deciding how many children to have, government ought to mind its own business.\n\nInstead, Caplan suggests that grandparents replicate the natalist incentives privately. Given this, it is interesting that Caplan drifts into supporting natalist tax credits in his recent Cato Unbound essay (as I have commented on here). I prefer his arguments for the use of private incentives from his book than his more recent encouragement of government action."
  },
  {
    "objectID": "posts/brookss-sex-genes-rock-n-roll.html",
    "href": "posts/brookss-sex-genes-rock-n-roll.html",
    "title": "Brooks’s Sex, Genes & Rock ‘n’ Roll",
    "section": "",
    "text": "Australian evolutionary biologist Rob Brooks’s book Sex, Genes & Rock ‘n’ Roll: How evolution has shaped the modern world has been released. It’s a good read - accessible and amusing. The books ranges between more serious issues such as obesity, population control and infanticide to the more light-hearted, the exploration of rock ‘n’ roll that the book title foreshadows. It is a book I’ll be adding to my list of recommendations for those who ask why evolution is relevant today.\nI think it is fair to describe Brooks as an optimist when it comes to population. He shares the common view among biologists that resource scarcity and peak oil will be troublesome (an issue that I am relatively optimistic about - as most economists seem to be) but he finds hope that population growth may be curtailed. His logic lies in the idea of sexual conflict, which occurs because men and women do not have the same interests when it comes to mating. Women must invest in pregnancy, childbirth (which risks death) and breastfeeding, so have a higher incentive to invest in the child than the father, who only needs to give a couple of minutes of his services.\nBy considering sexual conflict, Brooks adds a dimension that is often missing from discussions of population. As power shifts in a relationship to the female, an expected result will be a reduction in fertility. If you wish to cut fertility, educate women and increase their power. However, I’m less optimistic than Brooks as I’m not convinced that the fertility decline will be permanent. and don’t consider increasing population to be a one way path to success in the long-term (despite the increase in ideas that comes with more people).\nAs I read the chapters that addressed sexual conflict, I realised that I should have written a few of my posts on Bryan Caplan’s Selfish Reasons to Have More Kids differently. Brooks alerted me to more dimensions of sexual conflict than I had considered at the time. In particular, the asymmetric investment of men and women makes statements that men and women want the same number of children seem even less plausible.\nAs a libertarian leaning economist, the chapter on obesity was the one that most directly confronted my biases. Brooks’s argument is that humans have evolved to varying degrees to the high carbohydrate diet of the modern world. As populations transition from their old diet high in protein, fat and complex carbohydrates, the risk of obesity increases. Populations with the shortest exposure to modern agriculture and grains are the most vulnerable. Once you add in the subsidies given to many grain and sugar producers and the higher prices of protein and fresh fruit and vegetables, this combination directs the human diet in the wrong direction.\nThe solution proposed by Brooks is to consider subsidising protein and vegetables, and taxing simple carbohydrates such as sugar. At the least, we should remove the perverse subsidies that we offer many farmers. Although I am always supportive of removing subsidies, I’m naturally wary of any story involving taxes or subsidies to shift human behaviour. However, I want to explore this issue in some depth, so will come back to it in a later post.\nThe rock ‘n’ roll story contained in the book is one that seems obvious, but at the same time, is a story that so many resist. Why are rock stars predominantly male? What is the evolutionary rationale for their actions when it seems to increase their death rate by so much? When you look at Brian Jones of the Rolling Stones, dead at 27 but with four children to four different mothers (and I’ve seen higher estimates of his reproductive output), the evolutionary explanation seems clear. Evolution is not just about survival - it is largely about sex. And if you want to read a book about sex, Brooks’s book is as fun to read as any."
  },
  {
    "objectID": "posts/brooks-on-evolution-and-obesity.html",
    "href": "posts/brooks-on-evolution-and-obesity.html",
    "title": "Brooks on evolution and obesity",
    "section": "",
    "text": "Rob Brooks has posted an article (also published in The Conversation) outlining his argument that the relatively cheap price of carbohydrates compared to the price of protein is driving the obesity crisis. Drawing on material from his book Sex, Genes and Rock ‘n’ Roll: How Evolution Has Shaped the Modern World, Rob argues that as our recent evolutionary history involved a diet of lean meat and high fibre plant foods, modern humans are poorly evolved for the cheap simple carbohydrates that dominate many modern diets.\nI have considered Rob’s arguments and policy suggestions in an earlier post, but there is one element in Rob’s post that I would like to discuss. At the end of his post, he writes:\n\nNew York’s mayor, Michael Bloomberg, infuriated civil liberties groups and the sugar and soda lobbies last October when he asked the US Department of Agriculture to allow New York City to ban the 1.7 million citizens who receive food stamps from using them to buy soda.\nThose Americans poor enough to receive food stamps are precisely the people most at risk of obesity: people with enough access to food that they do not starve, but not enough money to eat a healthy diet with plenty of protein.\nSo did Mayor Bloomberg have the right idea? Should we tell people what they can and can’t eat or drink? Libertarians love to tell us that we have a choice, and that nobody is forced to overeat.\n\nDespite describing myself as having libertarian leanings, I don’t think this is a bad idea. Bloomberg is not telling people what to eat or drink. If the food stamp users wish to consume sugary products using their own resources, they are free to do so. Similarly, they cannot buy cigarettes with food stamps, but they are still free to smoke.\nI usually extend this idea across any provision of government services. While I would often prefer that they were not provided to begin with, placing conditions on access is not restricting liberty as long as the option to take action as a private citizen remains. If the conditions become tight enough, the government service disappears altogether (not that I am arguing that is what should be done with food stamps).\nSo, controlling use of food stamps among a group most vulnerable to obesity might be a good place to start. However, what I would like to see are some random trials of controlling food stamp expenditure. While state by state comparisons will produce some useful evidence, a random trial would give some evidential meat to the argument that restricting access to sugar will work in practice."
  },
  {
    "objectID": "posts/boyd-and-richersons-group-selection.html",
    "href": "posts/boyd-and-richersons-group-selection.html",
    "title": "Boyd and Richerson’s group selection",
    "section": "",
    "text": "In my review of Boyd and Richerson’s The Origin and Evolution of Cultures, I noted that I was not completely happy with their treatment of group selection. This post catalogues some of my thoughts.\nBoyd and Richerson open their group selection discussion by noting that selection can be broken down into between-group and within-group selection (as per the Price equation - and given this equation can be developed for multiple levels, we refer to “multi-level selection”). But in their analysis of what they call group selection, they do not practically use this framework and there are no attempts to decompose the levels of selection despite the initial framing of the problem around the ability to do this. Part of the reason for the lack of decomposition between the levels is that Boyd and Richerson generally (but not always) have another conception of group selection in mind - differential reproduction and spread of groups. But that is part of what makes the initial framing deceiving.\nThis problem becomes apparent when they start to discuss situations where it is unclear at what level the selection is occurring. Take the following:\n\nPayoff-biased imitation means people will preferentially copy individuals who get higher payoffs. The higher an individual’s payoff, the more likely that individual is to be imitated. If individuals have occasion to imitate people in neighboring groups, people from cooperative populations will be preferentially imitated by individuals in noncooperative populations because the average payoff to individuals from cooperative populations is much higher than the average payoff of individuals in noncooperative populations. Boyd and Richerson (2000) have shown that, under a wide range of conditions (and fairly quickly), this form of cultural group selection will deterministically spread group-beneficial behaviors from a single group (at a group-beneficial equilibrium) through a meta-population of other groups, which were previously stuck at a more individualistic equilibrium.\n\nSo, individuals copy people from more successful groups and the trait then spreads within those  groups. Is this actually group selection? Why does the trait spread in the new group - doesn’t this require individual advantage?\nThe paper referred to in this quote - Boyd and Richerson (2000) - is also contained in the book, and it describes a model with the spread of norms about drinking. Drinking has negative long-term consequences, but some people drink due to strong discounting. However, the presence of people with puritanical (rather than tolerant) norms can increase the cost of drinking due to social disapproval, meaning populations with puritan norms are better off as a whole than populations of tolerant people.\nAs people with tolerant and puritanical norms get on each other’s nerves, an isolated group’s members will tend to have the same norm. But given the lower number of drinkers in the puritan groups, the puritan groups will have the higher total payoff. Thus, if groups can mix, the puritan norms may spread as people copy the most successful individuals from other groups. Boyd and Richerson describe this as group selection, but the spread of the norms within groups after mixing demonstrates a degree of individual benefit. At what level are the dynamics dominant?\nIn other parts of the book, it is difficult to disentangle what the trait under group selection is. For example, when Boyd and Richerson write of the spread of ritualised cannibalism in New Guinea and the associated spread of the disease kuru, they describe the process as group selection. But is the relevant cultural trait eating kuru? Conforming to group rituals? Conforming to rituals concerning cannibalism? Which of these is being selected affects the assessment of the level of selection. Educated guesses can be made, but it is hard work.\nThese examples indicate a degree of looseness in Boyd and Richerson’s use of the term group selection. At times the term seems to be thrown at any dynamics that involve groups, with no clear definition of what group selection is and no attempt to place the observed behaviour in the context of the definition. This is, of course, a broader issue in much of the literature concerning group selection.\nHaving said this, as I mentioned in my review, I am not averse to the idea of examining cultural evolution in a group selection framework. I like many of Boyd and Richerson’s models and the descriptions of the dynamics, even if I consider the group selection label has been incorrectly applied. And it is possible that some of my complaints above could be dealt with through better explanation. But Boyd and Richerson use the term group selection so loosely that it is hard to agree with their use, particularly as I’m not sure what exactly I would be agreeing with. For the moment I prefer to describe their overall approach as “cultural group dynamics”."
  },
  {
    "objectID": "posts/books-i-read-in-2023.html",
    "href": "posts/books-i-read-in-2023.html",
    "title": "Books I read in 2023",
    "section": "",
    "text": "These are the books I enjoyed the most in 2023, although they were published in different years:\n\nEdward Abbey, Desert Solitaire: I love the sense of place that Abbey creates.\nPaige Harden, The Genetic Lottery: Harden’s politics seep through the text, and it is slightly irritating to be constantly labelled a eugenicist. However, it contains an accessible explanation of the science with many great examples.\nRay Bradbury, The Martian Chronicles: I bought it for the kids, but I suspect it’s beyond them. Wonderful stories.\n\nThe book I invested in the most was Douglas Hofstadter’s, Godel, Escher, Bach. It was a good book to work through during 12 days of surfing on an isolated island. Too much clever for the sake of being clever and an unclear thesis (even after reading the 20-year anniversary foreword that stated what the thesis was), but enough interesting ideas to keep me going.\nBelow is the list of books I read in 2023 (with a star if I have read them before). For the first time in a few years, the volume of my reading of books cover-to-cover has increased, with 36 total (23 non-fiction, 13 fiction).\nNon-Fiction\n\nEdward Abbey, Desert Solitaire\nPeter Attia, Outlive: The Science and Art of Longevity\nJordan Belfort, The Wolf of Wall Street\nDale Bredesen, The End of Alzheimer’s\nJanet Browne, Charles Darwin: Voyaging*\nJanet Browne, Charles Darwin: The Power of Place\nRoss Douthat, The Decadent Society\nDave Grohl, The Storyteller: Tales of Life and Music\nPaige Harden, The Genetic Lottery\nDouglas Hofstadter, Godel, Escher, Bach\nChuck Klosterman, The Nineties\nJon Krakuer, Into Thin Air\nMichael Lewis, Liar’s Poker*\nJohn List, The Voltage Effect\nAndrew Lo, Adaptive Markets\nGordon Menzies, Western Fundamentalism\nDouglas Murray, The War on the West\nBenjamin Newell, David Lagnado and David Shanks, Straight Choices: The Psychology of Decision Making\nJenny Odell, Saving Time\nLionel Page, Optimally Irrational\nRichard Reeves, Of Boys and Men\nRichard Thaler, The Winner’s Curse\nRichard Wrangham, Catching Fire\n\nFiction\n\nEdward Abbey, The Monkeywrench Gang\nEdward Abbey, Fire on the Mountain\nRay Bradbury, The Martian Chronicles\nDouglas Coupland, Generation X*\nAldous Huxley, Brave New World\nGeorge RR Martin, A Dance with Dragons\nGeorge RR Martin, A Feast for Crows\nBaroness Orczy, The Scarlet Pimpernel\nGeorge Orwell, 1984\nRobert Louis Stevenson, Treasure Island*\nJRR Tolkein, The Fellowship of the Ring*\nJRR Tolkein, The Two Towers*\nJRR Tolkein, The Return of the King*\n\nPrevious annual book lists: 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022"
  },
  {
    "objectID": "posts/books-i-read-in-2018.html",
    "href": "posts/books-i-read-in-2018.html",
    "title": "Books I read in 2018",
    "section": "",
    "text": "The best books I read in 2018 - generally released in other years - are below. Where I have reviewed, the link leads to that review.\n\nNick Bostrom, Superintelligence: Paths, Dangers, Strategies (2014) - Changed my mind, and gave me a framework for thinking about the problem that I didn’t have before.\nAnnie Duke, Thinking in Bets: Making Smarter Decisions When You Don’t Have All the Facts (2018) - While I have many small quibbles with the content, and it could easily have been a long-form article, I  liked the overarching approach and framing.\nGary Klein, Sources of Power: How People Make Decisions (1998) - Rightfully considered classic in decision-making. Review coming soon\nMichael Lewis’s The Undoing Project (2016) - Despite focusing on Kahneman and Tversky’s relationship, it is also one of the better introductions to their work.\nRobert Sapolsky’s Behave: The Biology of Humans at Our Best and Worst (2017) - A wonderful examination of what “causes” of our actions. Sapolsky zooms out from the almost immediate activity in our brain, to the actions of our hormones over seconds to hours, through our developmental influences, out to our evolutionary past. Review also coming soon.\nRobert Sapolsky’s Why Zebras Don’t Get Ulcers (3rd ed, 2004) - Great writing and interesting science.\nFred Schwed, Where Are the Customer’s Yachts? (1955) - Timeless commentary on the value delivered by the financial services sector\nRobert Sugden, The Community of Advantage: A Behavioural Economist’s Defence of the Market (2018) - The most compelling critique of the practical application of behavioural economics that I have read.\nJoseph Conrad, Lord Jim - I love Conrad. Nostromo is possibly my favourite book.\nDaphne Du Maurier, Rebecca\nHenry James, Turn of the Screw\n\nBelow is the full list of books that I read in 2018 (with links where reviewed and starred if a reread). Relative to previous years, I read (and reread) fewer books in total, less non-fiction, more fiction. That was largely a consequence of regularly reading my youngest to sleep.\nMy non-fiction reading through 2018 was less deliberate than I would have liked. There are fewer timeless pieces in the list than usual, with many of the choices based on whim or the particular piece of work I was doing at the time.\nNon-Fiction\n\nChris Anderson, TED Talks\nDan Ariely, Payoff: The Hidden Logic That Shapes Our Motivations\nMax Bazerman and Don Moore, Judgment in Managerial Decision Making (8th ed)\nSchlomo Bernartzi (with Jonah Lehrer), The Smarter Screen: Surprising Ways to Influence and Improve Online Behavior\nWarren Berger, A More Beautiful Question\nNick Bostrom, Superintelligence: Paths, Dangers, Strategies\nSusan Cain, Quiet:The Power of Introverts in a World That Can’t Stop Talking.\nStephen Covey, Seven Habits of Highly Effective People\nRay Dalio, Principles: Life and Work\nWilliam Deresiewicz, A Jane Austen Education\nAngela Duckworth, Grit: The Power of Passion and Perseverance\nAnnie Duke, Thinking in Bets: Making Smarter Decisions When You Don’t Have All the Facts\nCarol Dweck, Mindset: The New Psychology of Success\nRichard Feynman, What Do you Care What Other people Think?\nWilliam Finnegan, A Surfing Life*\nGerd Gigerenzer, Gut Feelings: Short Cuts to Better Decision Making\nJonathan Gottschall, The Professor in the Cage\nYuval Noah Harari, Sapiens: A Brief History of Humankind\nYuval Noah Harari, Homo Deus: A Brief History of Tomorrow\nYuval Noah Harari, 21 Lessons for the 21st Century\nDan Harris, 10% Happier: How I Tamed the Voice in My Head, Reduced Stress Without Losing My Edge, and Found Self-Help That Actually Works\nPhil Jarratt, Life of Brine\nGary Klein, Sources of Power: How People Make Decisions\nSteven Levitt and Stephen Dubner, Think like a Freak\nMichael Lewis, The Undoing Project: A Friendship That Changed the World\nMark Manson, The Subtle Art of Not Giving a Fuck\nMichael Mauboussin, Think Twice: Harnessing the Power of Counterintuition\nMichael Mauboussin, More Than You Know: Finding Financial Wisdom in Unconventional Places\nAndrew McAfee and Erik, Brynjolfsson, Machine, Platform, Crowd: Harnessing Our Digital Future\nCal Newport, Deep Work: Rules for Focused Success in a Distracted World*\nTim O’Reilly, WTF: What’s the Future and Why It’s Up to Us\nJordan Peterson, 12 Rules for Life: An Antidote to Chaos\nDaniel Pink, Drive: The Surprising Truth About What Motivates Us\nStuart Ritchie, Intelligence: All That Matters\nRobert Sapolsky, Behave: The Biology of Humans at Our Best and Worst\nRobert Sapolsky, Why Zebras Don’t Get Ulcers\nFred Schwed, Where Are the Customer’s Yachts?\nOwain Service and Rory Gallagher, Think Small: The Surprisingly Simple Ways to Reach Big Goals\nRobert Sugden, The Community of Advantage: A Behavioural Economist’s Defence of the Market\nCass Sunstein and Reid Hastie, Wiser: Getting Beyond Groupthink to Make Groups Smarter\nHenry David Thoreau, Walden\nEdward Thorpe, Beat the Dealer: A Winning Strategy for the Game of Twenty-One\nAshlee Vance, Elon Musk: How the Billionaire CEO of SpaceX and Tesla is Shaping our Future\nChris Voss, Never Split the Difference: Negotiating as if your life depended on it\nMatt Warshaw, The History of Surfing\nGeoffrey West, Scale: The Universal Laws of Life, Growth, and Death in Organisms, Cities and Companies\nEleizer Yudkowsy, Inadequate Equilibria: Where and How Civilizations Get Stuck\n\nFiction\n\nChristopher Buckley, Thank You For Smoking*\nEdgar Rice Burroughs, The Return of Tarzan\nEdgar Rice Burroughs, Tarzan of the Apes\nRay Bradbury, Farenheit 451*\nJoseph Conrad, Lord Jim\nJames Fenimoore Cooper, The Last of the Mohicans\nCharles Dickens, Bleak House\nCharles Dickens, A Christmas Carol\nCharles Dickens, A Tale of Two Cities\nCharles Dickens, Hard Times\nWilkie Collins, The Moonstone\nArthur Conan Doyle, A Study in Scarlet*\nArthur Conan Doyle, The Hound of the Baskervilles*\nArthur Conan Doyle, The Lost World\nDaphne Du Maurier, Rebecca\nGeorge Elliott, Middlemarch\nGustave Flaubert, Madame Bovary\nHenry James, Turn of the Screw\nJames Joyce, The Dubliners\nAndrew Lang, The Arabian Nights\nGeorge Bernard Shaw, Pygmalion\nUpton Sinclair, The Jungle\nRobert Louis Stevenson, Kidnapped\nBram Stoker, Dracula*\nJRR Tolkein, The Hobbit*\nMark Twain, The Adventures of Huckleberry Finn\nMark Twain, The Adventures of Tom Sawyer*\nJules Verne, Journey to the Center of the Earth\nAndy Weir, The Martian\nOscar Wilde, The Picture of Dorian Gray\nPG Wodehouse, Carry on, Jeeves\nPG Wodehouse, Meet Mr Mulliner"
  },
  {
    "objectID": "posts/better-school-performance-leads-to-more-children.html",
    "href": "posts/better-school-performance-leads-to-more-children.html",
    "title": "Better school performance leads to more children",
    "section": "",
    "text": "An article by Anna Goodman and Ilona Koupil in last month’s Evolution and Human Behavior found a link between school performance and number of children and grandchildren (in Sweden 1915-1929). This effect, as might be expected, held in males only. The number of children was linked almost entirely to whether the male married, with marriage largely a function of socioeconomic position. As most males married (around 90%), the effect of schooling performance on number of children was largely evidenced in those males at the bottom of the distribution.\nOne observation made by Goodman and Koupil was that as marriage prospects were largely mediated by socioeconomic position, there was little evidence of selection for cognitive abilities per se. It could be interpreted as a cold hearted conclusion. While money and social position are clearly going to have influence, what of the effort put into conversation, humour and other displays of intelligence. Are they all for nought?\nFor the group in this particular study, it might be  reasonable conclusion. However, given the differences are only at the low end of the school performance distribution, we might be seeing a threshold effect, whereby women are not interested in marrying someone below a certain standard. It reminds me of some of Bernard Salt’s work, whereby an eligible man is defined as someone not married, straight, not in jail and earning over $50,000 a year. They are not even in consideration.\nFrom a selection point of view, the elimination of individuals at the bottom of the distribution has somewhat different consequences to different reproductive success across the full spectrum of performance. I am going to dig some more into the data over the next few days to get a feel for what the change in distribution might look like over time."
  },
  {
    "objectID": "posts/best-books-i-read-in-2020.html",
    "href": "posts/best-books-i-read-in-2020.html",
    "title": "Best books I read in 2020",
    "section": "",
    "text": "The best books I read in 2020 - generally released in other years - were:\n\nAlbert Camus, The Plague\nTom Chivers, The AI Does Not Hate You: Superintelligence, Rationality and the Race to Save the World: Great introduction to and history of the rationalist community.\nMelanie Mitchell, Artificial Intelligence, A Guide for Thinking Humans: Mitchell is too easy on humans, but a fair examination of where we are with AI and some great explanations of various AI approaches.\nDavid Reich, Who We Are and How We Got Here: I was amazed at how far over the last decade the research into human origins had shifted. The story being told around the time of my PhD has been blown apart.\nJudea Pearl, The Book of Why: I’m still working through Pearl’s approach and how it relates to other approaches to causation, but this is possibly the best starting point on Pearl’s work.\n\nBelow is the list of books that I read in 2020 (starred if a re-read). The number of books fell yet again relative to the previous year, with 37 books (21 non-fiction, 16 fiction). I’ve left a lot of fiction read to the kids off this list - the Harry Potter series (multiple times), the full Wizard of Oz series, the Deltora Quest series, Enid Blyton, and so on - but the decline in substantive reading was driven by other factors.\nFirst, I took on a heavy teaching load in the second half of the year - teaching post-graduate economics and behavioural economics units - on top of the day job. Most of the teaching involved designing the units from scratch. I was swamped. From July to November inclusive, I completed two non-fiction books.\nSecond, and what I expect will prevent a rebound in the number of books I complete, is that my approach to reading has changed in the last couple of years. My previous rationale for reading a book was typically to hear someone’s thoughts and arguments on a topic that might be interesting. Now, I’m more likely picking up a non-fiction book to learn something specific. I’ll read as much as I need to, which is often only a small portion, in which case the book doesn’t make this list. Academic articles or web material are often better sources.\nI expect that’s a better use of my time. If you asked me to summarise a book I read 10-years ago, I can typically recall the main concepts and an example or two. But that’s about it, even if I took a lot of notes or wrote a review. It’s the concepts and associated illustrations that stick, and the most efficient way to understand and remember them is typically not reading a book.\nNon-Fiction\n\nTobias Baer, Understand, Manage and Prevent Algorithmic Bias\nPeter Burow, Behavioural Economics for Business: How the insights of behavioural economics can transform your business\nJohn Carreyrou, Bad Blood: Secrets and Lies in a Silicon Valley Startup\nTom Chivers, The AI Does Not Hate You: Superintelligence, Rationality and the Race to Save the World\nBrian Christian, The Most Human Human: What Artificial Intelligence Teaches Us About Being Alive\nJohn Cleese, So, anyway\nAndy Grove, Only the Paranoid Survive\nDavid Krakauer (ed), Worlds Hidden in Plain Sight: The Evolving Idea of Complexity at the Santa Fe Institute 1984-2019\nHoward Kunreuther, Mak Pauly and Stacey McMorrow, Insurance and Behavioural Economics\nBurton Malkiel, A Random Walk Down Wall Street\nKevin Mitchell, Innate: How the Wiring of Our Brains Shapes Who We Are\nMelanie Mitchell, Artificial Intelligence, A Guide for Thinking Humans\nHaruki Murakami, What I talk About When I Talk About Running\nJudea Pearl, The Book of Why\nDavid Reich, Who We Are and How We Got Here\nJenni Romaniuk and Byron Sharp, How Brands Grow, Part 2\nLisa Servon, The Unbanking of America: How the New Middle Class Survives\nRebecca Skloot, The Immortal Life of Henrietta Lacks\nNathalie Spencer, Good Money: Understand your choices, boost your financial wellbeing\nNassim Taleb, Skin in the Game: Hidden Asymmetries in Daily Life\nDavid Thomas and Andrew Hunt, The Pragmatic Programmer\n\nFiction\n\nDouglas Adams, The Hitch-hiker’s Guide to the Galaxy*\nMargaret Atwood, The Handmaid’s Tale\nMargaret Atwood, The Testaments\nChristopher Buckley, Little Green Men\nAlbert Camus, The Plague\nKate Chopin, The Awakening\nMichael Crichton, The Lost World\nWilliam Gibson, Neuromancer\nFrances Hodgson Burnett, The Secret Garden\nRobert Heinlein, The Moon is a Harsh Mistress\nMiles Kundera, The Unbearable Lightness of Being\nDH Lawrence, Lady Chatterley’s Lover\nHarper Lee, To Kill a Mockingbird*\nThomas Mann, Death in Venice\nHaruki Murakami, Norwegian Wood\nChuck Palahniuk, Fight Club*\nWalter Tevis, The Queen’s Gambit\n\nPrevious lists: 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019"
  },
  {
    "objectID": "posts/best-books-i-read-in-2017.html",
    "href": "posts/best-books-i-read-in-2017.html",
    "title": "Best books I read in 2017",
    "section": "",
    "text": "The best books I read in 2017 - generally released in other years - are below (in no particular order). Where I have reviewed, the link leads to that review.\n\nDon Norman’s The Design of Everyday Things (2013): In a world where so much attention is on technology, a great discussion of the need to consider the psychology of the users.\nDavid Epstein’s The Sports Gene: Inside the Science of Extraordinary Athletic Performance (2013): The best examination of nature versus nurture as it relates to performance that I have read. I will write about The Sports Gene some time in 2018.\nCathy O’Neil’s Weapons of Math Destruction: How Big Data Increases Inequality and Threatens Democracy (2016) - Although O’Neil is too quick to turn back to all-too-flawed humans as the solution to problematic algorithms, her critique has bite.\nKasparov’s Deep Thinking: Where Machine Intelligence Ends and Human Creativity Begins (2017) - Deep Thinking does not contain much deep analysis of human versus machine intelligence, but the story of Kasparov’s battle against Deep Blue is worth reading.\nGerd Gigerenzer, Peter Todd and the ABC Research Group’s Simple Heuristics That Make Us Smart (1999) - A re-read for me (and now a touch dated), but a book worth revisiting.\nPedro Domingos The Master Algorithm: How the Quest for the Ultimate Learning Machine Will Remake Our World (2015) - On the list for the five excellent chapters on the various “tribes” of machine learning. The rest is either techno-Panglossianism or beyond my domain of expertise to assess.\nBrian Christian and Tom Griffiths’s Algorithms to Live By: The Computer Science of Human Decisions (2016) - An excellent analysis of decision making, with the benchmark the solutions of computer science. As they say, “the best algorithms are all about doing what makes the most sense in the least amount of time, which by no means involves giving careful consideration to every factor and pursuing every computation to the end.”\nWilliam Finnegan’s Barbarian Days: A Surfing Life - Simply awesome, although I suspect of more interest to surfers (that said, it did win a Pulitzer). I also read a lot of great fiction during the year. Fahrenheit 451 and The Dice Man were among those I enjoyed the most."
  },
  {
    "objectID": "posts/best-books-i-read-in-2015.html",
    "href": "posts/best-books-i-read-in-2015.html",
    "title": "Best books I read in 2015",
    "section": "",
    "text": "A touch late, but as for earlier years, my list comprises the best books I read in the year, not the best of those released in the year (in fact, almost every book I read was released before 2015). It’s a short list - I read only 20 or so non-fiction book this year - but here it is:\nTriumph of the City: How Our Greatest Invention Makes Us Richer, Smarter, Greener, Healthier, and Happier by Edward Glaeser. A strong message for environmentalists.\nSpent by Geoffrey Miller. I included this book in my 2011 list, but read it again this year in preparation for a presentation I was giving. It deserves to be listed again. Evolutionary psychology at its most readable.\nUncontrolled:The Surprising Payoff of Trial-and-Error for Business, Politics, and Society by James Manzi. It should be read by every social scientist.\nI read a bunch of long classics, including Tolstoy’s War and Peace and Anna Karenina, and Conrad’s Nostromo (fantastic). One strong recommendation is Richard Flanagan’s The Narrow Road to the Deep North.\nAs an aside, I also re-read Daniel Kahneman’sThinking, Fast and Slow. I included it in my 2012 list, and my rating of it has dropped. The last four years haven’t been kind to some sections. (I posted about this here).\nI’ve got a dozen or so sets of notes from books I read last year that I hope to turn into posts in the near future - including for The Triumph of the City, Kenrick and Griskevicius’s The Rational Animal, Saad’s The Consuming Instinct, Thiel’s Zero to One, Levitin’s The Organized Mind and Taleb’s Antifragile.\nAnd although I’ve got copies, I haven’t yet read Garett Jones’s Hive Mind, Joe Henrich’s The Secret of Our Success or Greg Ip’s Foolproof (although I have been giving out the first two of these as Christmas presents).\nPast lists: 2010, 2011, 2012, 2013 and 2014."
  },
  {
    "objectID": "posts/best-books-i-read-in-2013.html",
    "href": "posts/best-books-i-read-in-2013.html",
    "title": "Best books I read in 2013",
    "section": "",
    "text": "As is my habit, each year I give a list of the best books I have read during the year. I tend not to focus on the newest releases, so most of the list was not published this year. In no particular order:\n\nPaul Frijters and Gigi Foster’s An Economic Theory of Greed, Love, Groups, and Networks (see also here): I don’t buy into many of the arguments, but the most interesting book I read all year.\nRichard Nelson and Sidney Winter’s An Evolutionary Theory of Economic Change: The book that kickstarted evolutionary economics as a serious pursuit. Although slightly dated, a great example of how to critique mainstream economics.\nSteven Pinker’s The Better Angels of Our Nature: Why Violence Has Declined (no review yet): Pinker’s case is compelling and important.\nOded Galor’s Unified Growth Theory: Another book for which I’m not completely onboard with the central arguments, but I love the ambition and ideas.\nI read a lot of classics this year. I thought Victor Hugo’s Les Miserables was great. I loved the use of language in Vladimir Nabakov’s Lolita. But ultimately, I enjoyed Jules Verne’s Twenty Thousand Leagues Under the Sea the most (an early seasteader?).\n\nBooks I read that didn’t make the list but are worth a mention include Nate Silver’s The Signal and the Noise: Why So Many Predictions Fail but Some Don’t (I thought a couple of chapters were great, but just wasn’t that excited by a lot of it) and Victor Hwang and Greg Horowitt’s The Rainforest: The Secret to Building the Next Silicon Valley."
  },
  {
    "objectID": "posts/benartzi-and-lehrers-the-smarter-screen-surprising-ways-to-influence-and-improve-online-behaviour.html",
    "href": "posts/benartzi-and-lehrers-the-smarter-screen-surprising-ways-to-influence-and-improve-online-behaviour.html",
    "title": "Benartzi (and Lehrer’s) The Smarter Screen: Surprising Ways to Influence and Improve Online Behaviour",
    "section": "",
    "text": "The replication crisis has ruined my ability to relax while reading a book built on social psychology foundations. The rolling sequence of interesting but small sample and possibly not replicable findings leaves me somewhat on edge. Shlomo Benartzi’s (with Jonah Lehrer) The Smarter Screen: Surprising Ways to Influence and Improve Online Behavior (2015) is one such case.\nSure, I accept there is a non-zero probability that a 30 millisecond exposure to the Apple logo could make someone more creative than exposure to the IBM logo. Closing a menu after making my choice might make me more satisfied by giving me closure. Reading something in Comic Sans might lead me to think about it in a different way. But on net, most of these interesting results won’t hold up. Which? I don’t know.\nThat said, like a Malcolm Gladwell book, The Smarter Screen does have some interesting points and directed me to plenty of interesting material elsewhere. Just don’t bet your house on the parade of results being right.\nThe central thesis in The Smarter Screen is that since so many of our decisions are now made on screens, we should invest more time in designing these screens for better decision making. Agreed.\nI saw Benartzi present about screen decision-making a few years ago, when he highlighted how some biases play out differently on screens compared to other mediums. For example, he suggested that defaults were less sticky on screens (we are quick to un-check the pre-checked box). While that particular example didn’t appear in The Smarter Screen, other examples followed a similar theme.\nAs a start, we read much faster on screens. Benartzi gives the example of a test with a written instruction at the front of the test to not answer the following questions. Experimental subjects suffered double rate of failure when on a computer - up from around 20% to 46% - skipping over the instruction and answering questions they should not have answered.\nPeople are also more truthful on screens. For instance, people report more health problems and drug use to screens. Men report less sexual partners, women more. We order pizza closer to our preferences (no embarrassment about those idiosyncratic tastes).\nScreens can also exacerbate biases as the digital format allows for more extreme environments, such as massive ranges of products. The thousands of each type of pen on Amazon or the maze of healthcare plans on HealthCare.gov are typically not seen in stores or in hard copy.\nThe choice overload experienced on screens is a theme through the book, with many of Benartzi’s suggestions focused on making the choice manageable. Use categories to break up the choice. Use tournaments where small sets of comparisons are presented and the winners face off against each other (do you need to assume transitivity of preferences for this to work?). All sound suggestions worth trying.\nOne interesting complaint of Benartzi’s is about Amazon’s massive range. They have over 1,000 black roller-ball pens! An academic critiquing one of the world’s largest companies built on offering massive choice (and with a reputation for A/B testing) is somewhat circumspect. Maybe Amazon could be even bigger? (Interestingly, after critiquing Amazon for not allowing “closure” and reducing satisfaction by suggesting similar products after purchase, Benartzi suggests Amazon already knows this issue).\nThe material on choice overload reflects Benartzi’s habit through the book of giving a relatively uncritical discussion of his preferred underlying literature. Common examples such as the jam experiment are trotted out, with no mention of the failed replications or the meta-analysis showing a mean effect of changing the number of choices of zero. Benartzi’s message that we need to test these ideas covers him to a degree, but a more sceptical reporting of the literature would have been helpful.\nSome other sections have a similar shallowness. The material on subliminal advertising ignores the debates around it. Some of the cited studies have all the hallmarks of a spurious result, with multiple comparisons and effects only under specific conditions. For example, people are more likely to buy Mountain Dew if the Mountain Dew ad played at 10 times speed is preceded by an ad for a dissimilar product like a Honda. There is no effect when an ad for a (similar) Hummer is played first. Really?\nOr take disfluency and the study by Adam Alter and friends. Forty students were exposed to two versions of the cognitive reflection task. A typical question in the cognitive reflection task is the following:\n\nA bat and a ball cost $1.10 in total. The bat costs $1.00 more than the ball. How much does the ball cost?\n\nThe two versions differed in that one used a small light grey font that made the questions hard to read. Those exposed to the harder to read questions achieved higher scores. Exciting stuff\nBut 16 replications involving a total of around 7,000 people found nothing (Terry Burnham discusses these replications in more detail here). Here’s how Benartzi deals with the replications:\n\nIt’s worth pointing out, however, that not every study looking at disfluent fonts gets similar results. For reasons that remain unclear, many experiments have found little to no effect when counterintuitive math problems, such as those in the CRT, are printed in hard-to-read letters. While people take longer to answer the questions, this extra time doesn’t lead to higher scores. Clearly, more research is needed.\n\nWhat is Benartzi’s benchmark for accepting that a cute experimental result hasn’t stood up to further examination and that we can move on to more prospective research? Sixteen studies involving 7,000 people in total showing no effect, one study with 40 people showing a result. The jury is still out?\nOne feeling I had at the end of the book was that the proposed solutions were “small”. Behavioural scientists are often criticised for proposing small solutions, which is generally unfair given the low cost of many of the interventions. The return on investment can be massive. But the absence of new big ideas at the close of the book raised the question (at least for me) of where the next big result can be.\nBenartzi was, of course, at the centre of one of the greatest triumphs in the application of behavioural science - the Save More Tomorrow plan he developed with Richard Thaler. Many of the other large successful applications of behavioural science rely on the same mechanism, defaults.\nSo when Benartzi’s closing idea is to create an app for smartphones to increase retirement saving, it feels slightly underwhelming. The app would digitally alter portraits of the user to make them look old and help relate them to their future self. The app would make saving effortless through pre-filled information and the like. Just click a button. But you first have to get people to download it. What is the marginal effect on these people already motivated enough to download the app? (Although here is some tentative evidence that at least among certain cohorts this effect is above zero.)\nOther random thoughts:\n\nOne important thread through the book is the gap between identifying behaviours we want to change and changing them. Feedback is simply not enough. Think of a bathroom scale. It is cheap, available, accurate, and most people have a good idea of their weight. Bathroom scales haven’t stopped the increase in obesity.\nBenartzi discusses the potential of query theory, which proposes that people arrive at decisions by asking themselves a series of internal questions. How can we shape decisions by posing the questions externally?\nBenartzi references a study in which 255 students received an annual corporate report. One report was aesthetically pleasing, the other less attractive. Despite both reports containing the same information, the students gave a higher valuation for the company with the attractive report (more than double). Bernartzi suggests the valuations should have been the same, but I am not sure. In the same way that wasteful advertising can be a signal that the brand has money and will stick around, the attractive report provides a signal about the company. If a company doesn’t have the resources to make its report look decent, how much should you trust the data and claims in it?\nDoes The Smarter Screen capture a short period where screens have their current level of importance? Think of ordering a pizza. Ten years ago we might have phoned, been given an estimated time of delivery and then waited. Today we can order our pizza on our smartphone, then watch it move through the process of construction, cooking and delivery. Shortly (if you’re not already doing this), you’ll simply order your pizza through your Alexa.\nBenartzi discusses how we could test people through a series of gambles to determine their loss aversion score. When people later face decisions, an app with knowledge of their level of loss aversion could help guide their decision. I have a lot of doubt about the ability to get a specific, stable and useful measure of loss aversion for a particular person, and am a fan of the approach of Greg Davies to the bigger question of how we should consider attitudes to risk and short-term behavioural responses.\nIn the pointers at the end of one of the chapters, Benartzi asks “Are you trusting my advice too much? While there is a lot of research to back up my recommendations, it is equally important to test the actual user experience and choice quality and adjust the design accordingly.” Fair point!"
  },
  {
    "objectID": "posts/behavioural-economics-versus-behavioural-science.html",
    "href": "posts/behavioural-economics-versus-behavioural-science.html",
    "title": "“Behavioural economics” versus “behavioural science”",
    "section": "",
    "text": "In the comments, Rory Sutherland writes:\n\nOne favour to ask. I completely agree with you that Behavioural Economics should be called Behavioural Science. But\n\nWe don’t really to decide what things are called. Darwin only used the word “evolution” a handful of times.\nIt is a very valuable term as a Trojan Horse. If I want to get people studying for MBAs, say, or people in finance, to take behavioural science seriously, anything with the word “economics” in it will get their attention: anything with the word “psychology” in it, by contrast, will probably make them think of couches and hypnosis.\nSince economics has become a dominant ideology in business and policy-making, and since one pressing job for behavioural science is to encourage people in such positions of influence to incorporate behavioural science into their thinking, any name which suggests that their pre-existing model (in which they have already invested an immense amount of thought and effort) needs be improved will be more successful than any head-on assault which suggests their model is wrong and needs to be replaced. After all, loss aversion and the endowment effect apply to ideas as well as things. There is hence nothing wrong with sugaring this pill if it helps people swallow it. It is simply easier to switch from “economics” to “behavioural economics” than it is to give up “economics” entirely – just as it is easier to switch from cigarettes to e-cigarettes than it is to give up smoking completely.\n\nSo, As a definition, “behavioural economics” is rather dodgy; but as a rebranding effort, it is genius.\nI work in advertising not academia. For a mixture of principled and self-interested reasons I would like behavioural science to have an influence commensurate with its importance. If that means it’s sometimes called something different, it’s a trade-off I can easily accept. But it is monstrously unfair to those people in psychology and behavioural science who must sometimes feel the baton has being snatched from them within sight of the finishing line.\n\nThat’s a fairly important “but”. How much of what I do is enabled in economics faculties because there is a body of work labelled “behavioural economics”?"
  },
  {
    "objectID": "posts/behavioral-scientist-is-live.html",
    "href": "posts/behavioral-scientist-is-live.html",
    "title": "Behavioral Scientist is live",
    "section": "",
    "text": "The folks at ideas42, the Center for Decision Research, and the Behavioral Science and Policy Association have kicked off a new online magazine, The Behavioral Scientist.\nI am one of the founding columnists, and it looks like I am part of a pretty good line up. My first column should appear in late July.\nYou can sign up to the Behavioral Scientist email edition on the homepage, or follow on twitter.\nAs an aside, I’ve been quiet on the blogging front recently, but contemplating article ideas for The Behavioral Scientist has reminded me how important blogging is for the development of my thinking. So expect to see a higher frequency of posts over the next few months."
  },
  {
    "objectID": "posts/beauty-as-a-fitness-indicator.html",
    "href": "posts/beauty-as-a-fitness-indicator.html",
    "title": "Beauty as a fitness indicator",
    "section": "",
    "text": "A study by Berri and colleagues on quarterback performance and their attractiveness has gained some attention over the last couple of months:\n\nWe show that attractiveness, as measured by facial symmetry, leads to greater rewards in professional sports. National Football League quarterbacks who are more attractive are paid greater salaries and this premium persists after controlling for player performance.\n\nThis is relatively consistent with the picture painted in Dan Hamermesh’s Beauty Pays: Why Attractive People Are More Successful. There is a premium to beauty. But as I noted in my review, we should not ignore the correlation between beauty and other positive traits. An earlier study by Williams and colleagues puts the first study in perspective:\n\nResults from a preliminary study showed a positive correlation between 30 NFL QBs’ passer ratings and their facial attractiveness as rated by 30 women. In a further study, a different group of 30 women rated a different cohort of 58 NFL QBs. The results showed that the QBs’ mean attractiveness ratings were positively correlated with their passer ratings, which was found to be independent of players’ age, ethnicity, height, weight, or facial expression.\n\nThese two studies can be easily reconciled - attractive quarterbacks are (on average) higher performers, but they also gain a premium above their performance. Whether that beauty premium comes from intangible elements such as self-confidence and leadership, or from bias by team owners, is unclear. One possibility is that, as football is a spectator sport and income depends on sponsorship and media, attractiveness has a financial value in itself.\nIt is also unclear what mechanism underlies the higher performance by attractive athletes that Williams and colleagues discovered. Is the mechanism directly beauty related, in that improved communication with teammates improves their own performance? Or, as I would argue, do positive traits tend to cluster? Beauty is a fitness indicator, so a lower mutation or parasite load, which would affect athletic performance, might also affect beauty. Further, assortive mating tends to match smart, beautiful and athletic partners, making their children smarter, more beautiful and more athletic than average."
  },
  {
    "objectID": "posts/barry-schwartzs-the-paradox-of-choice-why-more-is-less.html",
    "href": "posts/barry-schwartzs-the-paradox-of-choice-why-more-is-less.html",
    "title": "Barry Schwartz’s The Paradox of Choice: Why More Is Less",
    "section": "",
    "text": "I typically find the argument that increased choice in the modern world is “tyrannising” us to be less than compelling. On this blog, I have approvingly quoted Jim Manzi’s warning against extrapolating the results of an experiment on two Saturdays in a particular store - the famous jam experiment - into “grandiose claims about the benefits of choice to society.” I recently excerpted a section from Bob Sugden’s excellent The Community of Advantage: A Behavioural Economist’s Defence of the Market on the idea that choice restriction “appeals to culturally conservative or snobbish attitudes of condescension towards some of the preferences to which markets cater.”\nDespite this, I liked a lot of Barry Schwartz’s The Paradox of Choice: Why More Is Less. I still disagree with some of Schwartz’s recommendations, his view that the “free market” undermines our well-being, and that areas such as “education, meaningful work, social relations, medical care” should not be addressed through markets. I believe he shows a degree of condescension toward other people’s preferences. However, I found that for much of the diagnosis of the problem I agreed with Schwartz, even if that doesn’t always extend to recommending the same treatment.\nSchwartz’s basic argument is that increased choice can negatively affect our wellbeing. It can damage the quality of our decisions. We often regret our decisions when we see the trade-offs involved in our choice, with those trade-offs often multiplying with increased choice. We adapt to the consequences of our choices, meaning that the high search costs of search may not be recovered.\nThe result is that we are not satisfied with our choices. Schwartz argues that once our basic needs are met, much of what we are trying to achieve is satisfaction. So if the new car, phone or brand of salad dressing don’t deliver satisfaction, are we worse off?\nThe power of Schwartz’s argument varies with the domain. When he deals with shopping, it is easy to see that the choices would be overwhelming to someone who wanted to examine all of the options (do we need all 175 salad dressings that are on display?). People report that they are enjoying shopping less, despite shopping more. But it is hard to feel that a decline in our enjoyment of shopping or the confusion we face looking at a sea of salad dressings is a serious problem.\nSchwartz spends little time examining the benefits of increased consumer choice for individuals whose preferences are met, or the effect of the accompanying competition on price and quality. Schwartz has another book in which he tackles the problems with markets, so having not read it I can’t say he doesn’t have a case. But that case is absent from The Paradox of Choice.\nIn fairness to Schwartz, he does state that it is big jump to extrapolate the increased complexity of shopping into claims that too much choice can “tyrannise”. Schwartz even notes that we do OK with many consumer choices. We implicitly execute strategies such as picking the same product each time.\nSchwartz’s argument is more compelling when we move beyond consumer goods into important high-stakes decisions such as those about our health, retirement or work. A poor choice there can have large effects on both outcomes and satisfaction. These choices are of a scale that genuinely challenges our wellbeing.\nThe experimental evidence that we struggle with high-stakes choices is more persuasive evidence of a problem than experiments involving people having difficulty choosing jam. For instance, when confronted with a multitude of retirement plans, people tend to simply split between them rather than consider the merits or appropriate allocation. Tweak the options presented to them and you can markedly change the allocations. When faced with too many choices, they may simply not choose.\nSchwartz’s argument about our failures when choosing draws heavily from the heuristics and biases literature, and a relatively solid part of the literature at that: impatience and inter-temporal inconsistency, anchoring and adjustment, availability, framing and so on. But in some ways, this isn’t the heart of Schwartz’s case. People are susceptible to error even when there are few choices, which is the typical scenario in the experiments in which these findings are made. And much of Schwartz’s case would hold even if we were not susceptible to these biases.\nRather, much of the problem that Schwartz identifies comes when we approach choices as maximisers instead of satisficers. Maximisation is the path to disappointment in a world of massive choice, as you will almost certainly not select the best option. Maximisers may not even make a choice as they are not comfortable with compromises and will tend to want to keep looking.\nSchwartz and some colleagues created a maximisation scale, where survey respondents rate themselves against statements such as “I never settle for second best.” Those who rated high on the maximisation were less happy with life, less optimistic, more depressed and score high on regret. Why this correlation? Schwartz believes there is a causal role and that learning how to satisfice could increase happiness.\nWhat makes this result interesting is that maximisers make better decisions when assessed objectively. Is objective or subjective success more important? Schwartz considers that once we have met our basic needs, what matters most is how we feel. Subjective satisfaction is the most important criteria.\nI am not convinced that the story of satisfaction from particular choices flows into overall satisfaction. Take a particular decision and satisfice, and perhaps satisfaction for that particular decision is higher. Satisfice for every life decision, and what does your record of accomplishment look like? What is your assessment of satisfaction then? At the end of the book, Schwartz does suggest that we need to “choose when to choose”, and leave maximisation for the important decisions, so it seems he feels maximisation is important on some questions.\nI also wonder about the second order effects. If everyone satisficed to achieve higher personal satisfaction, what would we lose? How much do we benefit from the refusal of maximisers such as Steve Jobs or Elon Musk to settle for second best. Would a more satisfied world have less of the amazing accomplishments that give us so much value? Even if there were a personal gain to taking the foot off the pedal, would this involve broader cost?\nAn interesting thread relating to maximisation concerns opportunity costs. Any economist will tell you that opportunity cost - the opportunity you forgo by choosing an option - is the benchmark against which options should be assessed. But Schwartz argues that assessing opportunity costs has costs in itself. Being forced to make decisions with trade-offs makes people unhappy, and considering the opportunity costs makes those trade-offs salient.\nThe experimental evidence on considering trade-offs is interesting. For instance, in one experiment a groups of doctors were given a case history and a choice between sending the patient to a specialist or trying one other medication first. 75% choose the medication. Give the same choice to another group of doctors, but with the addition of a second medication option, and this time only 50% chose medication. Choosing the specialist is a way of avoiding a decision between the two medications. When there are trade-offs, all options can begin to look unappealing.\nAnother problem lurking for maximisers is regret, as the only way to avoid regret is to make the best possible decision. People will often avoid decisions if they could cause regret, or they aim for the regret minimising decision (which might be considered a form of satisficing).\nThere are some problems that arise even without the maximisation mindset.  One is that expectations may increase with choice. Higher expectations create a higher benchmark to achieve satisfaction, and Schwartz argues that these expectations may lead to an inability to cope rather than more control. High expectations create the burden of meeting them. For example, job options are endless. You can live anywhere in the world. The nature of your relationships - such as decisions about marriage - have a flexibility far above that of our recent past. For many, this creates expectations that are unlikely to be met. Schwartz does note the benefits of these options, but the presence of a psychological cost means the consequences are not purely positive.\nThen there is unanticipated adaptation. People tend to predict bigger hypothetical changes in their satisfaction than that reported by those who experienced the events. Schwartz draws on the often misinterpreted paper that compares the happiness of lottery winners with para- and quadriplegics. He notes that the long-term difference in happiness between the two groups is smaller than you would expect (although I am not sure what you would expect on a 5-point scale). The problem with unanticipated adaptation is that the cost of search does not get balanced by the subjective benefit that the chooser was anticipating.\nSo what should we do? Schwartz offers eleven steps to reduce the burden of choosing. Possibly the most important is the need to choose when to choose. Often it is not that any particular choice is problematic (although some experiments suggest they are). Rather, it is the cumulative effect that is most damaging. Schwartz suggests picking those decisions that you want to invest effort in. Choosing when to choose allows adequate time and attention when we really want to choose. I personally do this: a wardrobe full of identical work shirts (although this involved a material initial search cost), a regular lunch spot, and many other routines.\nSchwartz also argues that we should consider the opportunity costs of considering opportunity costs. Being aware of all the possible trade-offs, particularly when no option can dominate on all dimensions, is a recipe for disappointment. Schwartz suggests being a satisficer and only consider other options when you need to.\nThe final recommendation I will note is the need to anticipate adaptation. I personally find this a useful tool. Whenever I am making a new purchase I tend to recall a paragraph in Geoffrey Miller’s Spent, which often changes my view on a purchase:\n\nYou anticipate the minor mall adventure: the hunt for the right retail environment playing cohort-appropriate nostalgic pop, the perky submissiveness of sales staff, the quest for the virgin product, the self-restraint you show in resisting frivolous upgrades and accessories, the universe’s warm hug of validation when the debit card machine says “Approved,” and the masterly fulfillment of getting it home, turned on, and doing one’s bidding. The problem is, you’ve experienced all this hundreds of times before with other products, and millions of other people will experience it with the same product. The retail adventure seems unique in prospect but generic in retrospect. In a week, it won’t be worth talking about.\n\nMiller’s point in that paragraph was about the signalling benefits of consumerism, but I find a similar mindset useful when thinking about the adaptation that will occur."
  },
  {
    "objectID": "posts/bankers-are-more-honest-than-the-rest-of-us.html",
    "href": "posts/bankers-are-more-honest-than-the-rest-of-us.html",
    "title": "Bankers are more honest than the rest of us",
    "section": "",
    "text": "In an oft-quoted and cited Nature paper, Business culture and dishonesty in the banking industry, Cohn and colleagues argue that the culture in banking weakens and undermines the honesty norm. In the abstract they state:\n\n[W]e show that employees of a large, international bank behave, on average, honestly in a control condition. However, when their professional identity as bank employees is rendered salient, a significant proportion of them become dishonest. … Our results thus suggest that the prevailing business culture in the banking industry weakens and undermines the honesty norm, implying that measures to re-establish an honest culture are very important.\n\nThere was no shortage of media spouting the news. Altmetric finds 83 stories from 74 outlets (as of the date of this post). Here’s one example from the ABC:\n\nA Swiss study has set out to establish once and for all whether bankers are scheming, untrustworthy scoundrels.\nThe study of more than 200 international bankers put their honesty to the test and found them to be fundamentally decent human beings, until they were reminded about what they did for a living.\nAt that point, the research team discovered they began cheating on their tests.\n\nI am going to take their experiment and provide a different interpretation: that outside of their employment, bankers are more honest than the rest of us.\nI don’t believe my alternative interpretation. Both my and the authors’ interpretations are exercises in chasing noise. This study won’t replicate (and in at least two instances, didn’t - more on that below).\nBut, this examination of the claims on face value illustrates the forking paths that an analysis can take to create a newsworthy headline.\nThe experiment\nCohn and friends recruited 128 bank employees and randomly split them into two groups, the treatment and control. (First warning sign: the sample size.) Before undertaking the experimental task, the treatment group was “primed” with a series of questions that reminded them that they were a bank employee (e.g. At which bank are you presently employed?). The control group were asked questions unrelated to their professional identity.\nThe experimenters then asked each member of these two groups to flip a coin 10 times, reporting the result via a computer. No-one else could see what they had flipped. The experimenters paid them $20 per flip if they equalled or outperformed a randomly selected colleague. Ten correct flips and you could have $200 coming your way.\nSo how can we know if any particular person is telling the truth? You can’t. But across a decent sized group you expect, on average, 50% heads and 50% tails (or more precisely, a binomial distribution with a mean of 0.5). Someone getting 10 heads is a 1 in a thousand event. By comparing the distribution of the results to what you would expect, you can infer the level of cheating.\nSo, how did the bankers go? In the control group, 51.6% of coin flips were successful. It’s slightly more than 50%, but within the realms of chance for a group of honest coin flippers. The bankers primed with their professional identity reported 58.2% successful flips, 6.6 percentage points more than the control group. The dishonest bandits.\nBut how do we know that this result is particular to bankers? What if we primed other professionals with their profession? What if we took a group with no connection to the banking industry and primed them with banking concepts?\nCohn and friends answered these questions directly. When they primed a group of non-banking professionals with their professional identity, they reported 3 percentage points fewer successful coin flips than those in a control condition. Students primed with banking concepts also reported fewer successes, around 1.5%. These differences weren’t statistically significant and could have happened by chance.\n\nThese experimental outcomes are the centrepiece behind the conclusion that the prevailing culture in banking weakens and undermines the honesty norm for bankers.\nBut now let’s go to the supplementary materials and learn a bit more about these non-banking professionals and students.\nAn alternative interpretation\nFor the non-banking professionals and students, I have only reported the differences in successful coin flips above - as did the authors in the main paper. So how many successes did these non-banking professionals and students have?\nIn the control condition, the non-banking professionals reported 59.8% successful flips. This dropped to 55.8% when primed with their professional identity. The students were also dishonest bandits, reporting 57.9% successful flips in the control condition, and 56.4% in the banking prime condition.\nSo looking across the three groups (bankers, non-banking professionals and students), the only honest group we have come across are the bankers in the control condition.\nWe see a similar effect in an additional sample of 80 bankers reported in the supplementary material. There, the professional prime made bankers more dishonest (although the authors claim victory with p=0.069), but bankers themselves are more honest than the non-bankers to begin with.\nThis raises the question of what the appropriate reference point for this analysis is. Should we be asking if banking primes induce banker dishonesty? Or should we be asking whether the control primes - which were designed to be innocuous - can induce honesty? To accept that the banking prime induces bankers to cheat more, we also need to have a starting point that bankers, on the whole, cheat less.\nI don’t see a great deal of value in trying to interpret this result and determine which frame is correct. The result is just noise. But once you look at these numbers, the interpretation by Cohn and friends appears little more than an overly keen attempt to get the results to fit a story.\nReplications\nI noted above that I don’t believe this result will replicate, and there are now at least two studies making that argument: one a direct replication, and the other conceptual.\nThe direct replication is by Rahwan and friends. They increased the sample size to 768 bankers, plus over 500 non-bankers. The headline is that the result did not replicate in the Asia Pacific or Middle East banker populations used in the study. The two blue bars in the below chart represent the original Cohn and friends study, plus the additional 80 banker sample I referred to above.\n\nWhat is perhaps more interesting than this replication, however, is the response. Cohn and friends were given the opportunity to publish a response to the replications. Given the original submission date by Rahwan and friends of 6 September 2016 and an acceptance date of 26 August 2019, I can only imagine the wrangling that went on behind the scenes.\nCohn and friends raise several objections to the replication. The main one is that their original study is famous and received wide media coverage. A third of the replication participants reported being aware of the original study. This could affect which banks agreed to participate in the study (most declined) and the behaviour of the participants.\nI give this critique some weight, but don’t feel great about it. The original authors conduct a low sample size study of a noisy phenomena, overhype their result and then complain about a replication because everyone has heard about their overhyped result. It’s as though the publicity creates a shield to replication.\nIf we can’t take the replication on face value, this isn’t grounds for retaining a belief in the original experiment. Personally, I’ll revert to my default belief that the original result is spurious. I’ll give weight to it when someone finds a away to get around the publicity problem and demonstrate the result in a pre-registered replication.\nAnother criticism of the replication by Cohn and friends is whether the replication result is generalisable. They write:\n\nRahwan et al. acknowledge that there are important differences between the groups used in their study and our study. For example, both of their experiments were confined to employees of commercial banks who provide basic retail services. By contrast, the participants in our study work in banks who also mostly operate in trading, asset management, investment banking and private banking. It is possible that commercial banks have a different business culture and attract different types of employees than banks that offer the full range of financial services.\nIn summary, these issues suggest that the study by Rahwan et al. cannot address the question of whether our results generally apply to other institutions and countries.\n\nI must have missed the section in the original paper suggesting that it might not apply outside of their particular banking context.\nThat said, I’m not convinced Cohn and friends’ original study tells us anything about the institution they studied, nor its country.\nThe other replication that I’ll briefly mention was a conceptual replication by Huber and friends. This paper was interesting in that it found the opposite result: financial professionals behaved more honestly in a financial context. If you were to differentiate it from Cohn and friends, you would say that Cohn and friends was about identify, whereas Huber and friends was about the decision making context. If I were to extrapolate either study to real-world implications, I’d lean to the former, but I can’t say I would confidently generalise either of the experiments.\nOther critiques\nThere is a longer line-up of critiques of the original banker dishonesty paper.\nVranka and Houdek, in their paper Many faces of bankers’ identity: how (not) to study dishonesty, suggest that there are many other ways to interpret the results. I agree with that overarching premise that the interpretation that was picked up was one of many (as in my own illustration). But I am less convinced by some of their suggested alternatives, such as the presence of stereotype or money primes. Those primes seem as robust as this banking prime is likely to be. They also pick on the point that bankers are more honest in the control condition.\nJean-Michel Hupé critiques the statistical approach, with which I also have some sympathy, but I haven’t spent enough time thinking about it to agree with his suggested alternative approach.\nFinally, Thomas Stöckl argues there is an alternative explanation to business culture. Rather, the behaviour might simply reflect the professional requirements of the banking industry. This feels like splitting hairs but is an interesting point. As the payoff is competitive ($20 per flip if you beat your colleague), there is a strong risk-reward tradeoff. If the bankers are particularly skilled at this - it is a basic competence of the industry - the prime could lead them to exercise this skill (which occurs at the expense of the honesty norm).\nAn afterthought\nThat this experimental result is bunk is not a reason to dismiss the idea that banking culture is poor or that exposure to that culture increases dishonesty. The general problem with the priming literature is that it attempts to elicit effects through primes that are insignificant relative to the actual environments people face.\nFor example, there is a large difference between answering a few questions about banking and working in a bank. In the latter, you are surrounded by other people, interacting with them daily, seeing what they do. Just because a few questions do not produce an effect doesn’t mean that months of exposure to your work environment won’t change behaviour. Unfortunately, experiments such as this add approximately zero useful information as to whether this is actually the case."
  },
  {
    "objectID": "posts/bad-nudges-organ-donation-edition.html",
    "href": "posts/bad-nudges-organ-donation-edition.html",
    "title": "Bad nudges - organ donation edition",
    "section": "",
    "text": "It’s a favourite behavioural science story. Countries that have opt-in organ donation have lower rates of organ donation than countries where you have to opt out of being an organ donor. If we change the way the choice is framed from opt in to opt out, we can dramatically increase the rate of organ donation.\nExcept, it’s not that simple. For countries where there is an opt-out system, there is no simple point where the choice is presented to them as the default and they can easily tick the box to be removed from the organ donation register. Instead, they need to find and fill out forms or call various government agencies to overcome a presumed consent.\nFurther, although opt-out countries tend to have higher donation rates, some countries with opt-in systems have higher donation rates than those with opt-out. The high numbers that get reported - 99.98% of Austrians are organ donors - are the numbers who haven’t opted out, not the number who donate. When it comes to the point of donation, other issues become more relevant, including the wishes of the family. It’s no surprise that the family might wish to intervene, particularly given that in an opt-out system there is no moment where the donor is required to express their wish.\nIt seems to have been on the cards for a while now, but I’ve just realised Wales is shifting to being an opt-out country. The rolling out of this nudge is lazy.\nInstead of trying to design a system where they actually determine the wishes of the person and their family, they rely on the inconvenience of having to opt out to boost registered numbers. At the time of death, the family will have no sense of what the deceased wished for- after all, the number of people who choose to opt out is far less than the number who than indicate in surveys that they do not wish to donate.\nThere is no shortage of alternative approaches that don’t rely on granting (or attempting to grant) the right to organs through omission. Active choice is one option - when people get a driver’s licence or tax file number, ask them to choose whether they wish to be an organ donor or not. It will likely increase the number of registered organ donors over an opt-in system, while proving some indication of people’s wishes. An active choice also indicates these wishes to the family, with those wishes one of the major factors in families agreeing to donation.\nThen there are the more interesting options that could be tested - markets for organs, preferential treatment for those on registers (as occurs in Israel), or systems to match donors. These options are often prohibited.\nInstead of increasing the freedom to develop new solutions, we have a misunderstood story about a nudge gaining momentum, inadvertently placing the burden on family members at the time of death.\nAs an aside, I’m still hunting for examples of nudges introduced explicitly to increase freedom by allowing a hard requirement to be relaxed. Current count remains at zero.\n*Searching around for information on the changes in Wales, I came across another perspective from Eric Crampton."
  },
  {
    "objectID": "posts/ayn-rand-and-altruism.html",
    "href": "posts/ayn-rand-and-altruism.html",
    "title": "Ayn Rand and altruism",
    "section": "",
    "text": "While I find the occasional Ayn Rand (or Ayn Rand fan) bashing amusing, critics of Rand typically mis-characterise her writings (as many of her ardent fans also do). A Slate article by Eric Michael Johnson continues this tradition, where Johnson sets Rand up as the representative of selfish individualists against the altruists of hunter-gather tribes.\nJohnson’s altruistic case study is the Mbuti hunters of the Congo:\n\nThe Mbuti employed long nets of twined liana bark to catch their prey, sometimes stretching the nets for 300 feet. Once the nets were hung, women and children began shouting, yelling, and beating the ground to frighten animals toward the trap. As Turnbull came to understand, Mbuti hunts were collective efforts in which each hunter’s success belonged to everybody else. But one man, a rugged individualist named Cephu, had other ideas. When no one was looking, Cephu slipped away to set up his own net in front of the others. “In this way he caught the first of the animals fleeing from the beaters,” explained Turnbull in his book The Forest People, “but he had not been able to retreat before he was discovered.” …\nFaced with banishment, a punishment nearly equivalent to a death sentence, Cephu relented. “He apologized profusely,” Turnbull wrote, “and said that in any case he would hand over all the meat.” … Cephu was bound to support the tribe whether he chose to or not.\n\nThe “altruistic” behaviour of the Mbuti in conducting their hunt is only one example of wider altruism in hunter-gatherer societies. In research by Christopher Boehm, he found that sharing and cooperation are the most commonly named moral values in these societies.\nJohnson contrasts this tribal collectivism with Ayn Rand’s worship of the individual:\n\n“Collectivism,” Rand wrote in Capitalism: The Unknown Ideal, “is the tribal premise of primordial savages who, unable to conceive of individual rights, believed that the tribe is a supreme, omnipotent ruler, that it owns the lives of its members and may sacrifice them whenever it pleases.” An objective understanding of “man’s nature and man’s relationship to existence” should inoculate society from the disease of altruistic morality and economic redistribution.”\n\nThe problem with the dichotomy set up by Johnson is that, at least as it relates to his Mbuti example, he has it backwards. Rand rails against people who did not pull their weight and who loot rather than relying on their own productive efforts. In Johnson’s example, it was Cephu who was the looter who sought to take advantage of the hard work of others. If someone turned up in John Galt’s town and sought to skim off the rewards of his effort, Galt would withdraw his cooperation with them. And that is the beauty of the picture painted by Johnson - the association was voluntary (it may not classify as euvoluntary, however). Cephu was offered the choice to stay and cooperate, or he could leave. Tribe members are free to cooperate with each other and reap the rewards of that cooperation as they see fit.\nOf course, Rand’s philosophy was not to ignore others. If helping them or providing them services is valued by you, go ahead and do it. The heroes of Atlas Shrugged did not seek to become self-sufficient. They sought to succeed by providing goods and services valued by others. Dagney Taggart’s trains ran on steel provided by Hank Reardon. Hank Reardon used coal provided by Ken Dannager. The gains from specialisation and trade make it in your interest to care about the welfare of others.\nIf Rand makes an error, it may be her understanding of primordial savages. She saw the tribe as dominating the lives of its members, when it is closer to a cooperative pact. Johnson’s story also suggests that Rand may have been too pessimistic. Rand saw a world where the looters were winning, where second-class talent prospered at the expense of the truly talented and where the wealthy maintain their wealth through political patronage. In contrast, Johnson paints a picture of the looter getting his due, so at least in this small part of the world, Rand’s nightmare has not come true.\nJohnson goes as far as noting the benefits that come from “altruistic” behaviour (hence my use of inverted commas around “altruism” during this post). Gossip is a primary form of communication within tribes. Reputations rapidly spread and those with bad reputations can be quickly marginalised. Further, altruistic behaviour is a signal to the opposite sex as it may be a reliable signal of your quality. Is something still altruistic when it allows the continued propagation of your genes?"
  },
  {
    "objectID": "posts/arielys-upside-irrationality.html",
    "href": "posts/arielys-upside-irrationality.html",
    "title": "Dan Ariely’s The Upside of Irrationality",
    "section": "",
    "text": "I rate Dan Ariely’s The Upside of Irrationality: The Unexpected Benefits of Defying Logic lower than Predictably Irrational. Like Predictably Irrational, The Upside of Irrationality is based largely on Ariely’s own work (a good thing). But where Ariely had 15 years of experiments to call on for his first book, for this one he seems limited to a couple of years of newer experiments and the experiments in the reject pile from the first. We then get a lot of filler where Ariely riffs on the theme of the experiment rather than reporting experimental results. It causes this book to feel lightweight in comparison to the first.\nThat said, I enjoyed the first couple of chapters about work incentives. Ariely and his colleagues ran experiments where they offered incentives to experimental subjects for the performance of mentally taxing tasks, such as remembering a sequence of numbers or hitting a target. But rather than incentives improving performance, high incentives (in the order of several months pay) caused the participants to choke and perform worse than those who were moderately incentivized. This contrasts with experiments that required purely mechanical activities for bonuses, with larger bonuses generally increasing performance.\nAriely related a story about telling a group of bankers about these experimental results, with the bankers suggesting this incentive problem did not apply to them. In some senses, I agree with the bankers, but likely for different reasons. The crumbling in performance witnessed by Ariely and his colleagues was for short-term mentally taxing tasks. In contrast, most bankers, consultants, lawyers and the like are receiving bonuses for a year of effort, making the bonus less salient at any time. But more importantly, the bonuses are heavily tied to the mechanical part of the job - putting in or billing a massive number of hours. I expect there are not many split second decisions that are required to be made with the bonus in mind, and Ariely is overestimating the short-term creativity required.\nI find these experimental outcomes somewhat perplexing from an evolutionary perspective. What is the benefit to choking when the stakes are high? One explanation might be that in the environment of evolutionary adaptedness, high-stakes games often ended in death, with choking a signal for the person to get out of there. Another might be that most high-stakes events in that environment simply needed a fight or flight response, not the maintenance of mental coordination.\nIt was when Ariely got into areas such as dating that the experiments seemed thinner and Ariely was forced to fill more space with his personal views on the subject. I don’t mind a bit of speculation, but Ariely spent a lot of time extending his discussion beyond the experimental context than in Predictably Irrational.\nFor example, Ariely reported the results of an experiment where, before a speed dating event, participants engaged in a virtual online date where they explored a virtual space together. Those who had earlier participated in the virtual date with their later speed dating partner liked them more. Ariely suggests this indicates a flaw in speed dating setups. But what is the objective of speed dating? Does the increased probability of liking someone due to an earlier virtual date, even though the characteristics of that person have not changed, lead to achievement of the goal of a long-term partner? Or is that familiarity leading them to ignore more suitable people in the room during the speed dating?\nAriely also looked at online dating, and noted that huge amounts of time are expended online relative to the time spent on dates. People do not rate the experience as enjoyable. He saw this as a general indication of the failure of the dating market. I won’t claim that dating markets are perfectly efficient, but again, what is the objective of online dating? I expect it is not enjoyment. If the purpose of online dating is to create a large pool from which the dross can be weeded out, the counterfactual for comparison is going on dates from a smaller pool without that filtering mechanism. Which is the better option? I don’t know the answer to these questions, but I’m not convinced Ariely did either. But as there seemed to be a need for filler around the experimental results, we got a lot of Ariely’s thoughts on these subjects.\nI absorbed The Upside of Irrationality in the right way - through an audiobook on my way to and from work. It’s an easy read/listen, has some interesting ideas (particularly early in the book) but seems light compared to Predictably Irrational. I’ll keep Ariely’s next book, The Honest Truth About Dishonesty: How We Lie to Everyone–Especially Ourselves, on my reading list, but I hope it is more dense with research results than riffs on the theme than is the case for The Upside of Irrationality."
  },
  {
    "objectID": "posts/arielys-predictably-irrational.html",
    "href": "posts/arielys-predictably-irrational.html",
    "title": "Ariely’s Predictably Irrational",
    "section": "",
    "text": "After sitting in my reading pile for the best part of three years, I have finally read (or more accurately, listened to) Dan Ariely’s Predictably Irrational. One of the most commonly referenced popular books on behavioural science, it describes Ariely’s experiments in the areas of cheating, procrastination, social norms, hot decision-making and so on.\nOne nice element of the book is that Ariely describes his own experiments. In the behavioural science literature, you often come across the same experiments over and over. The field sometimes doesn’t feel very deep. But it is easier to deal with this when the person is describing their own experiments (you can’t hold it against an author that others keeps referring to their work) and they are able to give the experiments some colour beyond the content of the published papers. Then again, if you have listened to a lot of Ariely’s lectures via podcast (as I have), much of the book will be familiar territory.\nRather than review the book, I thought I’d point out a couple of interesting ideas that Ariely presents.\nFirst was his framing of how the link between price and demand flows in two directions. People do not simply demand a certain quantity at a certain price, as the price may feedback about how desirable a product is. Ariely opens that section with the story of how James Assael created demand for black pearls. After initially failing in his marketing attempts, he placed the pearls in the store of a gemstone dealer friend with an outrageously high price relative to the previous prices at which he had failed to sell any of the pearls. Accompanied by an advertising campaign, the demand was born.\nAriely’s explanation of this feedback between price and demand relates to our need for arbitrary coherence. Since we do not know what many things are worth, we will seek an anchor - say a previous price or the last two digits on our social security card. Once that anchor is created, this is the benchmark against which value is measured. Placing the pearls in the gemstone store and advertisements created an anchor that still exists.\nA second interesting thread was the desire for individuality. Ariely relates an experiment where he offered a selection of beers to pub patrons as free samples. Where people ordered in sequence and aloud so that each member of a group heard what the others ordered, they tended to order different beers. With private ordering they tended to cluster more. This occurred because those who ordered last avoided the beers that people in their group had ordered before them. So, if you feel a need to express individuality through your meal or drink order, get in first.\nFinally, Ariely’s work on hot decision-making, where his team provided computers loaded with arousing pictures to experimental subjects, is truly amusing.\nAll up, Predictably Irrational is not a bad book for a sample of behavioural economics if you are new to the area. And when you are done with it, I suggest balancing it with some Gigerenzer."
  },
  {
    "objectID": "posts/are-children-normal-goods.html",
    "href": "posts/are-children-normal-goods.html",
    "title": "Are children normal goods?",
    "section": "",
    "text": "I finished a post last week with the question of whether children are normal goods. Below I want to lay out some economic arguments on this, before putting in an evolutionary twist that raises the question of whether we can rely on any of the economic analysis.\nA normal good is a good for which demand increases with income. Cars, holidays and jewellery are examples of normal goods. The opposite is an inferior good, with demand decreasing as income rises. An example of an inferior good might be hamburger mince (as people move to steak) or other low-quality products. This change in demand for a good in response to changing income is known as an income effect.\nThere has been some debate over the years about whether children are normal goods, particularly given the ubiquitous pattern of fertility declining as a country’s residents get richer. However, determining whether children are a normal good is complicated, as we do not get to witness a simple increase in income without other conflating factors.\nFirst, income is not the major input into children. Rather, time is the scarce resource, with that time balanced (crudely) between work, leisure and children. Gary Becker argued back in 1965 that using time was the right way to frame the problem.\nConsider a sudden increase in your wage. This increases the relative price of leisure and children and would result in someone wanting to work more and to demand less children and leisure. This is known as a substitution effect. The income effect means that the worker does not need to work as much for the same income. This increase in income increases the time effectively available, and if children are a normal good, a person will have more of them. However, the substitution and income effects operate in opposite directions, making it difficult to determine whether that person will actually have more children. If we see them reducing the number of children they have, we cannot determine the direction of the income effect and whether children are a normal good.\nAccordingly, the decline in fertility with wealth witnessed at a country level suggests that the substitution effect is strong and that the income effect, which would be operating in the opposite direction if children are a normal good, does not outweigh it. However, it also leaves open the possibility that children are an inferior good, with both the substitution and income effects contributing to the decline.\nBetsey Stevenson made a similar point concerning the need to consider the income and substitution effects in Cato Unbound last year when discussing a drop in the price of children. Stevenson argued that children might be a Giffen good, a type of inferior good for which the income effect is so strong that not only does it counteract the substitution effect, but actually completely outweighs it. If children were a Giffen good, a decrease in the price of children (through decreasing time requirements to raise them) would lead you to consume less.\nAs a Giffen good must be an inferior good, and not a normal good, Stevenson’s question turned into a debate on whether children are normal goods. Bryan Caplan ran some regressions on General Social Survey data from the United States, and found that once you control for IQ and education, the number of children increases with income. That pattern holds for both men and women. As the price of children and leisure goes up through an increase in income, there is a substitution effect away from children and leisure and towards working. If children are normal goods, there is an income effect to spend more on children. As income has a positive effect on the number of children in Caplan’s analysis, children must be normal goods.\nJustin Wolfers came back with an argument that children were inferior goods, largely based on the observation of cross-country fertility declining with income, but it misses the combination of the income and substitution effects discussed above.\nA major difference between Caplan and Wolfer’s analyses is that Caplan controls for education. That complicates matters further, and will be a subject of a separate post. There is also the question of whether children are a Veblen good, which means that as the price of children rises, the preference for children also increases as children become a status signal. If the fundamental nature of the product changes, it is even more difficult to unravel. Another post to come for that too.\nNow for the evolutionary spanner in the works. Children are a decision of two people. A man must convince a woman, and vice versa, to have a child. Parental investment theory tells us that the man will be doing more of the convincing, but both sides are not free of constraints.\nIf a man has more children as his income increases, it may be a result of a change in his demand for children as his income increases. Alternatively, it may be due to a change in the constraint that he faces. In other words, suppose men all want the same number of children regardless of income, but women will only mate with higher income men. As a result, we will see a positive relationship between income and children. This is even though we assumed that children are not a normal good.\nAs women are less constrained, their changing behaviour may be more representative of their preferences, but they are not completely independent. Woman are constrained in the quality (and income) of men that they can attract, and that may affect their willingness to have children. To the extent that their income is representative of traits desired by men, increasing children with female income (as in Caplan’s regressions) may be evidence of lower constraints rather than a response to their own income.\nI should note that there are plenty of studies seeking to assess whether children are normal goods. They use all sorts of external shocks, such as the man losing his job or shocks to male income through a resources boom. The evolutionary problem remains. If a man loses his job, he is more likely to run into the female constraint. Unemployment is a strong predictor of divorce.\nIs there an experiment in which you can separate male preferences from the constraint?"
  },
  {
    "objectID": "posts/angela-duckworths-grit-the-power-of-passion-and-perseverance.html",
    "href": "posts/angela-duckworths-grit-the-power-of-passion-and-perseverance.html",
    "title": "Angela Duckworth’s Grit: The Power of Passion and Perseverance",
    "section": "",
    "text": "In Grit: The Power of Passion and Perseverance, Angela Duckworth argues that outstanding achievement comes from a combination of passion - a focused approach to something you deeply care about - and perseverance - a resilience and desire to work hard. Duckworth calls this combination of passion and perseverance “grit”.\nFor Duckworth, grit is important as focused effort is required to both build skill and turn that skill into achievement. Talent plus effort leads to skill. Skill plus effort leads to achievement. Effort appears twice in the equation. If one expends that effort across too many domains (no focus through lack of passion), the necessary skills will not be developed and those skills won’t be translated into achievement.\nWhile sounding almost obvious written this way, Duckworth’s claims go deeper. She argues that in many domains grit is more important than “talent” or intelligence. And she argues that we can increase people’s grit through the way we parent, educate, coach and manage.\nThree articles from 2016 (in Slate, The New Yorker and npr) critiquing Grit and the associated research make a lot of the points that I would. But before turning to those articles and my thoughts, I will say that Duckworth appears to be one of the most open recipients of criticism in academia that I have come across. She readily concedes good arguments, and appears caught between her knowledge of the limitations of the research and the need to write or speak in a strong enough manner to sell a book or make a TED talk.\nThat said, I am sympathetic with the Slate and npr critiques. Grit is not the best predictor of success. To the extent there is a difference between “grit” and the big five trait of conscientiousness, it is minor (making grit largely an old idea rebranded with a funkier name). A meta-analysis (working paper) by Marcus Credé, Michael Tynan and Peter Harms makes this case (and forms the basis of the npr piece).\nAlso critiqued in the npr article is Duckworth’s example of grittier cadets being more likely to make it through the seven-week West Point training program Beast Barracks, which features in the book’s opening. As she states, “Grit turned out to be an astoundingly reliable predictor of who made it through and who did not.”\nThe West Point research comes from two papers by Duckworth and colleagues from 2007 (pdf) and 2009 (pdf). The difference in drop out rate is framed as a rather large in the 2009 article:\n\n“Cadets who scored a standard deviation higher deviation higher than average on the Grit-S were 99% more likely to complete summer training”\n\nBut to report the results another way, 95% of all cadets made it through. 98% of the top quartile in grit stayed. As Marcus Credé states in the npr article, there is only a three percentage point difference between the average drop out rate and that of the grittiest cadets. Alternatively, you can consider that 88% of the bottom quartile made it through. That appears a decent success rate for these low grit cadets. (The number reported in the paper references the change in odds, which is not the way most people would interpret that sentence. But on Duckworth being a great recipient of criticism, she concedes in the npr article she should have put it another way.)\nHaving said this, I am sympathetic to the argument that there is something here that West Point could benefit from. If low grit were the underlying cause of cadet drop-outs, reducing the drop out rate of the least gritty half to that of the top half could cut the drop out rate by more than 50%. If they found a way of doing this (which I am more sceptical about), it could be a worthwhile investment.\nOne thing that I haven’t been able to determine from the two papers with the West Point analysis is the distribution of grit scores for the West Point cadets. Are they gritty relative to the rest of the population? In Duckworth’s other grit studies, the already high achievers (spelling bee contestants, Stanford students, etc.) look a lot like the rest of us. Why does it take no grit to enter into domains which many people would already consider to be success? Is this the same for West Point?\nPossibly the biggest question I have about the West Point study is why people drop out. As Duckworth talks about later in the book (repeatedly), there is a need to engage in search to find the thing you are passionate about. Detours are to be expected. When setting top-level goals, don’t be afraid to erase an answer that isn’t working out. Finishing what you begin could be a way to miss opportunities. Be consistent over time, but first find a thing to be consistent with. If your mid-level goals are not aligned with your top level objective, abandon them. And so on. Many of the “grit paragons” that Duckworth interviewed for her book explored many different avenues before settling on the one that consumes them.\nSo, are the West Point drop-outs leaving because of low grit, or are they are shifting to the next phase of their search? If we find them later in their life (at a point of success), will they then score higher on grit as they have found something they are passionate about that they wish to persevere with? How much of the high grit score of the paragons is because they have succeeded in their search? To what extent is grit simply a reflection of current circumstances?\nOne of the more interesting sections of the book addresses whether there are limits to what we can achieve due to talent. Duckworth’s major point is that we are so far from whatever limits we have that they are irrelevant.\nOn the one hand, that is clearly right - in almost every domain people could improve through persistent effort (and deliberate practice). But another consideration is where their personal limits lie relative to the degree of skill required to successfully achieve a person’s goals. I am a long way from my limits as a tennis player, but my limits are well short of that required to ever make a living from it.\nFollowing from this, Duckworth is of the view that people should follow their passion and argues against the common advice that following your passion is the path to poverty. I’m with Cal Newport on this one, and think that “follow your passion” is horrible advice. If you don’t have anything of value to offer related to your passion, you likely won’t succeed.\nDuckworth’s evidence behind her argument is mixed. She notes that people are more satisfied with jobs when they follow a personal interest, but this is not evidence that people who want to find a job that matches their interest are more satisfied. Where are those who failed? Duckworth also notes that these people perform better, but again, what is the aggregate outcome of all the people who started out with this goal?\nOne chapter concerns parenting. Duckworth concedes the evidence here is thin, incomplete and that there are no randomised controlled trials. But she then suggests that she doesn’t have time to wait for the data come in (which I suppose you don’t if you are already raising children).\nShe cites research on supportive versus demanding parenting, derived from measures such as surveys of students. These demonstrate that students with more demanding parents have higher grades. Similarly, research on world-class performers shows that their parents are models of work ethic. The next chapter reports on the positive relationship between extracurricular activities while at school and job outcomes, particularly where they stick with the same activity for two or more years (i.e. consistent parents).\nBut Duckworth does not address the typical problem of studies in this domain - they all ignore biology. Do the students receive higher grades because their parents are more demanding, or because they are the genetic descendants of two demanding people? Are they world-class performers because their parents model a work ethic, or because they have inherited a work ethic? Are they consistent with their extracurricular activities because their parents consistently keep them at it, or because they are the type of people likely to be consistent?\nThese questions might appear speculation in themselves, but the large catalogue of twin, adoption and now genetic studies points to the answers. To the degree children resemble their parents, this is largely genetic. The effect of the shared environment - i.e. parenting - is low (and in many studies zero). That is not say interventions cannot be developed. But they are not reflected in the variation in parenting the subject of these studies.\nDuckworth does briefly turn to genetics when making her case for the ability to change someone’s grit. Like a lot of other behavioural traits, the heritability of grit is moderate: 37% for perseverance, 20% for passion (the study referenced is here). Grit is not set in stone, so Duckworth takes this as a case for the effect of environment.\nHowever, a heritability less than one provides little evidence that deliberate changes in environment can change a trait. The same study finding moderate heritability also found no effect of shared environment (e.g. parenting). The evidence of influence is thin.\nFinally, Duckworth cites the Flynn effect as evidence of the malleability of IQ - and how similar effects could play out with grit - but she does not reference the extended trail of failed interventions designed to increase IQ (although a recent meta-analyses show some effect of education). I can understand Duckworth’s aims, but feel that the literature in support of them is somewhat thin.\nOther random points or thoughts:\n\nAs for any book that contain colourful stories of success linked to the recipe it is selling, the stories of the grit paragons smack of survivorship bias. Maybe the coach of the Seattle Seahawks pushes toward a gritty culture, but I’m not sure the other NFL teams go and get ice-cream every time training gets tough. Jamie Dimon, CEO of JP Morgan, is praised for the $5 billion profit JP Morgan gained through the GFC (let’s skate over the $13 billion in fines). How would another CEO have gone?\nDo those with higher grit display a higher level of sunk cost fallacy, being unwilling to let go?\nInteresting study - Tsay and Banaji, Naturals and strivers: Preferences and beliefs about sources of achievement. The abstract:\n\n\nTo understand how talent and achievement are perceived, three experiments compared the assessments of “naturals” and “strivers.” Professional musicians learned about two pianists, equal in achievement but who varied in the source of achievement: the “natural” with early evidence of high innate ability, versus the “striver” with early evidence of high motivation and perseverance (Experiment 1). Although musicians reported the strong belief that strivers will achieve over naturals, their preferences and beliefs showed the reverse pattern: they judged the natural performer to be more talented, more likely to succeed, and more hirable than the striver. In Experiment 2, this “naturalness bias” was observed again in experts but not in nonexperts, and replicated in a between-subjects design in Experiment 3. Together, these experiments show a bias favoring naturals over strivers even when the achievement is equal, and a dissociation between stated beliefs about achievement and actual choices in expert decision-makers.”\n\n\nA follow up study generalised the naturals and strivers research over some other domains.\nDuckworth reports on the genius research of Catharine Cox, in which Cox looked at 300 eminent people and attempted to determine what it was that makes them a genius. All 300 had an IQ above 100. The average of the top 10 was 146. The average of the bottom 10 was 143. Duckworth points to the trivial link between IQ and ranking within that 300, with the substantive differentiator being level of persistence. But note those average IQ scores…"
  },
  {
    "objectID": "posts/an-msix-reading-list.html",
    "href": "posts/an-msix-reading-list.html",
    "title": "An MSiX reading list",
    "section": "",
    "text": "Yesterday was day one of the Marketing Science Ideas Xchange (MSiX). As I mentioned in a previous post, it has been an interesting opportunity to see behavioural science outside of the academic and economics environments I am used to. There were a lot of interesting presentations, and a lot of good books were mentioned along the way.\nFirst, a couple of blasts from the past: Claude Hopkins’s Scientific Advertising (if the one dollar Amazon price is prohibitive, it doesn’t take much searching to find some free pdf versions) and Vance Packard’s The Hidden Persuaders. The idea of injecting more science into advertising is not new.\nThe usual behavioural science texts got plenty of mentions, particularly Daniel Kahneman’s Thinking, Fast and Slow. System 1 and System 2 thinking were regular frames for the speakers (and in today’s workshops). Richard Thaler and Cass Sunstein’s Nudge and Dan Ariely’s Predictably Irrational also got the expected mentions.\nThe first three speakers had an evolutionary thread in parts of their talks (nice to see), so naturally a few books I have plugged before came up. Rory Sutherland put up his reading list from Verge, which includes Paul Seabright’s The Company of Strangers, Jonathan Haidt’s The Righteous Mind, Robert Kurzban’s Why Everyone (Else) Is a Hypocrite: Evolution and the Modular Mind and Robert Frank’s The Darwin Economy. All highly recommended, as is the rest of Sutherland’s reading pile, although I haven’t read Stuart Sutherland’s Irrationality: the enemy within, which I suppose will get added to my list.\nAnother book I had not come across before was Iain McGilchrist’s The Master and His Emissary: The Divided Brain and the Making of the Western World, which looks interesting.\nUri Gneezy and John List got a solid mention from the last presenter, Liam Smith from Monash University’s BehaviourWorks Australia. Gneezy and List’s new book The Why Axis: Hidden Motives and the Undiscovered Economics of Everyday Life is also sitting on my reading pile.\nOutside of the presentations, a few other interesting books came up in conversation. They included Jim Manzi’s Uncontrolled: The Surprising Payoff of Trial-and-Error for Business, Politics, and Society, which should be on your reading list. One of my favourite books, Christopher Buckley’s Thank You for Smoking was also mentioned, which was unsurprising considering the potential sin industry clients of many of the conference attendees – and if you do read it, rip out the last couple of pages. While Barry Schwartz’s book The Paradox of Choice was not specifically mentioned, the phrase was regularly used.\nFinally, Adam Ferrier, the conference curator, has a book out - The Advertising Effect: How to Change Behaviour. After organising a conference myself earlier in the year, I feel for him - many rewards but so much effort."
  },
  {
    "objectID": "posts/an-evolutionary-occupy.html",
    "href": "posts/an-evolutionary-occupy.html",
    "title": "An evolutionary Occupy",
    "section": "",
    "text": "In an evolutionary sense, resource inequality affects survival and access to mates. While the current “Occupy” debates about growing inequality and the power of the 1 per cent are very much focused on the resource issue, the underlying reason people have an innate aversion to the unequal distribution of power (or, more particularly, their being at the wrong end of that distribution) comes back to these evolutionary factors. But to what extent are survival and reproduction actually affected by the inequality being protested?\nIn the case of survival in a developed country, the “we are the 99 per cent” is a useful benchmark, but in this case it is the 1 per cent at the bottom compared to the 99 per cent above. Only those at the very bottom likely to have their survival through their reproductive years threatened. Survival is not at the core of the debate.\nWhen we consider the issue from a reproductive perspective, the issue becomes more interesting. I would expect that the increased share of income to the top 1 per cent increases their ability to attract mates. This might be through the ability to support more mistresses and to engage in serial monogamy, with the increased wealth allowing second and third wives to be obtained more easily despite advancing years. There would also be some effect on mate quality.\nHowever, I am not sure that the next 10 per cent of the income or wealth distribution are significantly harmed by this, except for the possibility of a small decline in quality. They still have the means to attract mates – and will successfully do so. It is not until we get to men in the bottom, say, 20 or 30 per cent of the income or wealth distributions that we find a group that is suffering.\nInequality in income growth must be one factor in this, but the increase in the status and income of women over the last 50 years (a reduction in inequality), together with growth in the welfare state, has also reduced the benefit for many women of pairing with low-income men. However, I am not sure that low-income men form a significant part of the Occupy movement (or even support it). Maybe they should be the ones protesting in the street?\nOr maybe they already are protesting. High levels of single men without access to mates is linked to crime, gambling, risk taking and other social problems – the protest takes a different form. We might be about to see this happen in China.\nAs a last note, I recently pulled out an essay by Napoleon Chagnon titled “Is Reproductive Success Equal in Egalitarian Societies?” (from a 1979 volume edited by Chagnon and Irons) in which Chagnon nicely captures this issue across time:\n\nPolygyny is widespread in the tribal world and has probably characterised human mating and reproduction for the greater fraction of our species’ history. Given that natural selection by definition entails the differential reproduction and survival of individuals, this fact of life - this inequality - is of considerable importance. This raises the question of the utility of viewing human status differentials largely, if not exclusively, in terms of material resources and the relationships that individuals in different societies have to such resources. That the relationship between people and control over strategic resources is central to understanding status differences in our own highly industrialised, materialist culture is insufficient reason to project these relationships back in evolutionary time and to suggest that all human status systems derive from struggles over the means and ends of production. Struggles in the Stone Age were more likely over the means and ends of reproduction.\n\nAlthough not as obvious, today’s struggles are over those same ends and means."
  },
  {
    "objectID": "posts/an-economics-and-evolutionary-biology-reading-list.html",
    "href": "posts/an-economics-and-evolutionary-biology-reading-list.html",
    "title": "An economics and evolutionary biology reading list",
    "section": "",
    "text": "I have added a new page with a suggested reading list for those interested in the intersection of economics and evolutionary biology. It is here, and you can see it in the menu bar across the top of the page.\nThe list is a work is progress, and I plan to update it as new sources emerge or are suggested (or when I realise what oversights I have made). I also intend to constrain it to the best sources, rather than being a complete list on every thought on the topic.\nSo if you have any suggestions, please let me know. Comments can be made at the bottom of the reading list page."
  },
  {
    "objectID": "posts/altruists-and-the-knowledge-problem.html",
    "href": "posts/altruists-and-the-knowledge-problem.html",
    "title": "Altruists and the knowledge problem",
    "section": "",
    "text": "I have posted before about Gary Becker’s argument that the evolution of altruism can be explained by a version of his rotten kid theorem. In short, if an altruist cares about other people’s welfare in addition to their own and is willing to transfer their resources to others, an egoist’s action to harm the altruist may also harm the egoist as the amount that the altruist would be willing to transfer to the egoist will be reduced. As a result, the egoist will refrain from hurting the altruist, making the altruist better off than if they were an egoist.\nAs I raised in my post, Becker’s argument can run into corner solutions, whereby the scale of the gain to the egoist and damage to the altruist are such that the egoist is willing to harm the altruist. However, I only recently realised that the year after publication of Becker’s article, the Journal of Economic Literature published responses to Becker’s paper by Jack Hirshleifer and Gordon Tullock, along with a reply by Becker.\nHirshleifer’s critique focuses on the order in which the altruist and egoist’s actions occur. If the egoist has the last word, they will likely take advantage of the altruist at the end, meaning that the altruist should not be as altruistic to begin with. In the case of altruism between a parent and child, the child will normally have the last word due to differences in ages. Tools such as a legal system that allows the making of wills are required for the altruist to have the last word.\nHirshleifer then goes on to show that Becker’s analysis is powerful where the altruist can keep the last word, and an altruist may be selfishly better off than if they were planning an egoistic action. Their altruism restrains the behaviour of the egoist.\nIn reply, Becker called Hirshleifer’s comments perceptive, with the note that his scenario can only work among a small number of relatives or neighbours (so Becker had already dealt with part of my criticism).\nAfter quibbling with the definition of altruism, Tullock’s criticism focuses on an altruist’s ability to know the preference ordering of the recipient of the altruism. Tullock suggests judgments of this type are almost impossible. The history of charitable administration and its attempts to prevent recipients from taking advantage of gifts suggests a knowledge problem.\nTullock argues that in this case, the egoist can abuse the situation. For example, they could stop working, which reduces their income considerably but their utility only slightly as they no longer have to work. If the altruist only looks at the slacker’s loss of income, the altruist may effectively overcompensate the egoist.\nTullock also talks of the potential for corner solutions based on different orderings of the size of the gain of the egoist, damage to the altruist and transfer from the altruist. He notes that Becker’s scenario can only occur where the damage inflicted on the donor is greater in size than the gift that would be given to the egoist in the absence of damage, which is greater than the gain to the egoist from damaging the altruist. This is only one of six possible orderings (although two of the others involving no or almost no harm to the altruist are the most common).\nBecker’s reply to Tullock is sharp. “Although Tullock’s comment is much longer than Hirshleifer’s, it is less focused and less useful, and my response shall be brief.” His dismissal of Tullock’s argument is largely on the basis that Becker is not seeking to explain altruism to people 1,000 miles away, but rather to kin and close neighbours about whom the altruist will have more knowledge. Further, while this is a restrictive class, Becker (rightfully) considers it an important one.\nBecker’s point on knowledge of kin and neighbours does not completely nullify Tullock’s, as parents have imperfect knowledge of their children. That same criticism could be applied to kin selection, which requires some degree of understanding of what actions will benefit kin. However, Tullock may be comfortable that the criticism would also apply to kin selection given that he prefers group selection based explanations of altruism and cooperation."
  },
  {
    "objectID": "posts/ai-in-medicine-outperforming-humans-since-the-1970s.html",
    "href": "posts/ai-in-medicine-outperforming-humans-since-the-1970s.html",
    "title": "AI in medicine: Outperforming humans since the 1970s",
    "section": "",
    "text": "From an interesting a16z podcast episode Putting AI in Medicine, in Practice (I hope I got the correct names against who is saying what):\n\nMintu Turakhia (cardiologist at Stanford and Director of the Centre for Digital Health): AI is not new to medicine. Automated systems in healthcare have been described since the 1960s. And they went through various iterations of expert systems and neural networks and called many different things.\nHanne Tidnam: In what way would those show up in the 60s and 70s.\nMintu Turakhia: So at that time there was no high resolution, there weren’t too many sensors, and it was about a synthetic brain that could take what a patient describes as the inputs and what a doctor finds on the exam as the inputs.\nHanne Tidnam: Using verbal descriptions?\nMintu Turakhia: Yeah, basically words. People created, you know, what are called ontologies and classification structures. But you put in the ten things you felt and a computer would spit out the top 10 diagnoses in order of probability and even back then, they were outperforming sort of average physicians. So this is not a new concept.\n\nThis point about “average physicians” is interesting. In some circumstances you might be able to find someone who outperforms the AI. The truly extraordinary doctor. But most people are not treated by that star.\nThey continue:\n\nBrandon Ballinger (CEO and founder of Cardiogram): So an interesting case study is the Mycin system which is from 1978 I believe. And so, this was an expert system trained at Stanford. It would take inputs that were just typed in manually and it would essentially try to predict what a pathologist would show. And it was put to the test against five pathologists. And it beat all five of them.\nHanne Tidnam: And it was already outperforming.\nBrandon Ballinger: And it was already outperforming doctors, but when you go to the hospital they don’t use Mycin or anything similar. And I think this illustrates that sometimes the challenge isn’t just the technical aspects or the accuracy. It’s the deployment path, and so some of the issues around there are, OK, is there convenient way to deploy this to actual physicians. Who takes the risk? What’s the financial model for reimbursement? And so if you look at the way the financial incentives work there are some things that are backwards, right. For example, if you think about kindof a hospital from the CFO’s perspective, misdiagnosis actually earns them more money because when you misdiagnose you do follow up tests, right, and those, and our billing system is fee for service, so every little test that’s done is billed for.\nHanne Tidnam: But nobody wants to be giving out wrong diagnoses. So where is the incentive. The incentive is just in the system, the money that results from it.\nBrandon Ballinger: No-one wants to give incorrect diagnosis. On the other hand there’s no budget to invest in making better diagnosis. And so I think that’s been part of the problem. And things like fee for value are interesting because now you’re paying people for, say, an accurate diagnosis, or for a reduction in hospitalisations, depending on the exact system, so I think that’s a case where accuracy is rewarded with greater payment, which sets up the incentives so that AI can actually win in this circumstance.\nVijay Pande (a16z General Partner): Where I think AI has come back at us with a force is it came to healthcare as as a hammer looking for a nail. What we’re trying to figure out is where you can implement it easily and safely with not too much friction and with not a lot of physicians going crazy, and where it’s going to be very very hard.\n\nFor better diagnoses, I’d be willing to drive a few physicians crazy.\nThe section on the types of error was also interesting:\n\nMintu Turakhia: There may be a point that it truly outperforms the cognitive abilities of physicians, and we have seen that with imaging so far. And some of the most promising aspects of the imaging studies and the EKG studies are that the confusion matrices, the way humans misclassify things, is recapitulated by the convolutional neural networks. …\nA confusion matrix is a way to graph the errors and which directions they go. And so for rhythms on an EKG, a rhythm that’s truly atrial fibrillation could get classified as normal sinus rhythm, or atrial tachycardia, or super-ventricular tachycardia, the names are not important. What’s important is that the algorithms are making the same type of mistakes that humans are doing. It’s not that its making a mistake that’s necessarily more lethal, and just nonsensical so to speak. It recapitulates humans. And to me that’s the core thesis of AI in medicine, because if you can show that you are recapitulating human error, you’re not going to make it perfect, but that tells you that, in check and with control, you can allow this to scale safely since its liable to do what humans do. ….\nHanne Tidnam: And so you’re just saying it doesn’t have to be better. It just has to be making the same kinds of mistakes to feel that you can trust the decision maker.\nMintu Turakhia: Right. And you dip your toe in the water by having it be assistive. And then at some point we as a society will decide if it can go fully auto, right, fully autonomous without a doctor in the loop. That’s a societal issue. That’s not a technical hurdle at this point.\n\nCertainly a heavy bias to the status quo. I’d certainly prefer something with better net performance even if some of the mistakes are different."
  },
  {
    "objectID": "posts/age-dependent-evolution.html",
    "href": "posts/age-dependent-evolution.html",
    "title": "Age-dependent evolution",
    "section": "",
    "text": "At the Consilience Conference earlier this year I bumped into evolutionary biologist Michael Rose, whose research interests include examining ageing through the lens of evolutionary theory. In our brief conversation, Rose mentioned that he had laid out much of his thinking on the topic in 55 Theses, where Rose describes how to use the insights of evolutionary biology to improve your health.\nReading through the archives of the Social Evolution Forum over the weekend (worth adding to your feed), I came across a post by Peter Turchin recounting a conversation he had with Rose at the same conference. Turchin writes on one element of Rose’s thinking, which concerns age-dependent traits:\n\nWe think of people having ‘traits,’ but actually we change quite dramatically as we age. … As an extreme example, consider reproductive ability, something of great interest to evolution. Humans do not reproduce until they reach a fairly advanced age of maturation (puberty). Young adults are not very good mothers or fathers, but they improve with age during their twenties. After that reproductive ability declines and eventually disappears. …\nAbility to digest certain foods can also be age-dependent. I have already mentioned the ability to digest lactose, the sugar present in milk. Before we domesticated animals such as cows and sheep, only very young humans had this ability. Natural selection turned this ability off in adults because they never needed it (and it would be wasteful to continue producing the enzyme lactase that aids in the digestion of milk sugar). …\nBecause abilities to do something at the age of 10, 30, 50, etc. are separate (even if correlated) traits, they evolve relatively independently of each other. When grains became a large part of the diet, the ability of children to digest them (and detoxify the chemical compounds plants put into seeds to protect them against predators such as us) became critical. If you don’t have genes to help you deal with this new diet, you don’t survive to adulthood and don’t leave descendants. In other words, evolution worked very hard to adapt the young to the new diet. On the other hand, the intensity of selection on the old (e.g., 55 years old) was much less – in large part, because most people did not live to the age of 55 until very recently. …\nThe striking conclusion from this argument is that older people, even those coming from populations that have practiced agriculture for millennia, may suffer adverse health effects from the agricultural diet, despite having no problems when they were younger.\n\nThe conclusion that Rose draws is that young people descended from populations with a substantial history of agriculture can probably cope with an agricultural diet. However, as they age, may need to revert to a Paleo diet. Those without that agricultural history should get on a Paleo diet from the start. It is an interesting twist on the usual Paleo diet story."
  },
  {
    "objectID": "posts/accelerating-adaptive-evolution-in-humans.html",
    "href": "posts/accelerating-adaptive-evolution-in-humans.html",
    "title": "Accelerating adaptive evolution in humans",
    "section": "",
    "text": "In my last post, I noted R.A. Fisher’s argument that a larger population leads to more mutations and greater potential for adaptive evolution. As human populations have undergone massive growth over recent tens of thousands of years, we would expect the evidence of this population growth to show in our genomes. In this post, I point to a couple of papers that look at this evidence.\nIn the first, Recent acceleration of human adaptive evolution, John Hawks and colleagues examined the age distribution of positively selected gene variants in a 3.9 million SNP dataset. To determine the variant (allele) ages, they examined “linkage blocks”. When an SNP is selected, it tends to carry with it other polymorphisms on the chromosome around it. If a variant tends to have a similar set of polymorphisms around it, while another variant at that same point does not, this suggests that the first is under positive selection, dragging along its neighbouring polymorphisms with it (the linkage block). The older original variant will have a bigger mix of polymorphisms around it through reshuffling over time.\nThis paper has two great charts that illustrate the findings. The first compares the age distribution of the variants in African and European populations. In both populations, the peak in variants is observed in recent history, which matches a theory of mutations increasing with population. Interestingly, we see an earlier peak for the African population. Africa had a larger late Pleistocene population, while population in Europe and West Asia took off after the Neolithic revolution.\n\n\nHawks et al (2007) Fig 1\n\n\n\nThe second chart plots the observed variant ages against two models: one with constant mutation rates, and a second where mutations rates increase with population size. The second model provides a far better (but not perfect) fit to the observed data.\n\n\nHawks et al (2007) Fig 3\n\n\n\nYou can also read more on this paper (written around the time it came out) on John Hawks’s blog and at Gene Expression.\nThe second paper by Fu and colleagues, published in Nature Genetics, contained analysis of the age of over one million single nucleotide variants across 6,500 people. Fu and colleagues found that most of the variants that they analysed were relatively young, with the majority arising in the last 5,000 to 10,000 years (as would be expected if mutations increase with population). The age of the variants is best shown in the following chart which is a heat map of the variants in African and European populations before and after recent accelerated population growth (i.e. before or within the last 5,000 years).\n\n\nFu et al (2007) Fig 3a\n\n\n\nWhile Fu and colleagues don’t transfer this story of the growth in rare variants to one of adaptive evolution, you can see how it fits with the first paper by Hawks and colleagues. These variants, even though most may be deleterious, provides the store of variation on which evolution can act.\nI posted a round-up of some other blog posts on this second article at the end of last year when the article underwent initial electronic release. The post by John Hawks is very good.\n*The reason I’m covering this territory is to lay the groundwork for a post on my latest working paper: Population, Technological Progress and the Evolution of Innovative Potential. You can now find that post here."
  },
  {
    "objectID": "posts/a-week-of-links.html",
    "href": "posts/a-week-of-links.html",
    "title": "A week of links",
    "section": "",
    "text": "This week we have paleo for dogs, irrational consumers and the question of what is the right size:\n\nJohn Hawks posts on a new Nature paper that provides a reason for “why we feed dogs kibble instead of raw beef” - well, at least those who don’t have their dogs on the paleo diet. Like humans with a long agricultural history, dogs have extensive duplication of the amylase gene, which assists in the digestion of starch. Razib asks “Is Paleolithic man to Holocene man what the wolf is to the dog?”\nThe Atlantic covers a new paper by Daniel McFadden that summarises the case against the neoclassical economic view of the consumer. From a quick skim, there’s not too many surprises but it’s a nice summary.\nNick Gruen tweeted a link to this 1928 classic by J.B.S. Haldane, On Being the Right Size. Andy Haldane of the Bank of England borrowed it for a speech on bank size. I suggest going back to the original."
  },
  {
    "objectID": "posts/a-week-of-links-98.html",
    "href": "posts/a-week-of-links-98.html",
    "title": "A week of links",
    "section": "",
    "text": "Links this week (or more like two weeks):\n\nThe problem with satisfied patients.\nHappiness inequality.\nExplaining the growth mindset.\nGender-blind economists.\nLogical versus ecological rationality.\nSlaughter scientific peer review. HT: Christopher Snowdon.\nPoor children have smaller brains.\n\nAnd if you missed them, my posts from the last two weeks:\n\nUnemployment and self control.\nUncertainty and understanding behaviour.\n\nLife is busy at the moment, so posting will continue to be sparse over the next month."
  },
  {
    "objectID": "posts/a-week-of-links-96.html",
    "href": "posts/a-week-of-links-96.html",
    "title": "A week of links",
    "section": "",
    "text": "Links this week:\n\nMisrepresentation of Asch’s studies.  HT: Stuart Ritchie\nDon’t pretend sports events have big economic effects. And this holds for the Olympics too.\nGet rid of all the ordinary accidents, and you are left with the weird. HT: Tyler Cowen\nIf told obesity is a disease, obese people are less worried about their weight.\nJerry Coyne on the new paper arguing kin selection is a factor in eusociality.\nYes to marine reserves.\n\nAnd if you missed them, my posts this week:\n\nThe gender reading gap and love of learning.\nThe law of law’s leverage."
  },
  {
    "objectID": "posts/a-week-of-links-94.html",
    "href": "posts/a-week-of-links-94.html",
    "title": "A week of links",
    "section": "",
    "text": "Links this week:\n\nPsychological research sucks.\nAre humans getting cleverer?\nWhy the $10,000 Apple Watch is a good thing, especially for people who can’t afford it.\n[O]ur findings suggest that correlations observed in affluent, developed countries between (i) wealth and health or (ii) parental income and children’s outcomes do not reflect a causal effect of wealth.\n\nAnd no posts this week, although with Robert Frank joining Twitter, here are three book reviews from the vault:\n\nThe Darwin Economy\nPassions Within Reason\nLuxury Fever."
  },
  {
    "objectID": "posts/a-week-of-links-92.html",
    "href": "posts/a-week-of-links-92.html",
    "title": "A week of links",
    "section": "",
    "text": "Links this week:\n\nI have an article at ABC’s The Drum on the Australian Government’s Intergenerational Report - “It’s time to end the demographic pessimism”.\n“The myriad processes that bring in food, convert it to metabolic fuel, and burn this fuel in our cells act in concert to keep our energy budget – the daily paper – essentially fixed in size.” HT: Melissa McEwen\nAn education intervention that might have worked.\nWhy are some demographic groups doing better than others?\n What makes the current era feel so deprived?\nA surprisingly good article on the whether science supports a paleo diet.\nThe global flight from the family. And miserable 19th century marriages.\nA short history of  iterated prisoner’s dilemma tournaments.\nSo much for peak oil.\n\nAnd if you missed them, my posts this week:\n\nIntroducing Evonomics.\nCan government policy overcome implicit bias?"
  },
  {
    "objectID": "posts/a-week-of-links-90.html",
    "href": "posts/a-week-of-links-90.html",
    "title": "A week of links",
    "section": "",
    "text": "Links this week (a slightly sparse list as I didn’t find much time to see what was out there):\n\nThe number of never married in the US continues to grow. HT: Arnold Kling\nModerate drinking is still good for you. Christopher Snowdon delves into the details.\nIt’s no surprise that Uber and friends want to be regulated.\nThere is plenty of signalling in years K-12."
  },
  {
    "objectID": "posts/a-week-of-links-89.html",
    "href": "posts/a-week-of-links-89.html",
    "title": "A week of links",
    "section": "",
    "text": "Links this week:\n\nA few years old, but good - a story from a Blue Zone “where people forget to die”. HT: Razib Khan\nAnother critique of modern dietary guidelines. Weight gain after a fecal transplant. And the US Government is about to drop warnings about cholesterol.\nImproving ‘Neoclassical man’ with a gaze heuristic.\nBigger data sets are uncovering the genetic underpinnings of intelligence.\nA take on the Peter Principle.\nTo what extent will new birth control options be another fertility shock?\nYet more on lead and crime.\nGreg Clark on social mobility. And my review of his book.\n\nAnd if you missed them, my posts this week:\n\nWill defaults lose their power in the digital age. Or, as put by Eric Crampton, Will nudges survive the Lucs Critique?\nI recommend using a story other than organ donation rates to support defaults."
  },
  {
    "objectID": "posts/a-week-of-links-87.html",
    "href": "posts/a-week-of-links-87.html",
    "title": "A week of links",
    "section": "",
    "text": "Links this week:\n\nWe see skill where none exists and are happy to pay for transparently useless advice.\nNo evidence of the effect of parenting on criminal behaviour.\nDoug Kenrick on testosterone and the rationality of taking risks.\nPulling apart the recent paper on perceptions of ability and the gender gap.\nThe human guinea pig.\nDistrust of vaccines not a left wing issue."
  },
  {
    "objectID": "posts/a-week-of-links-85.html",
    "href": "posts/a-week-of-links-85.html",
    "title": "A week of links",
    "section": "",
    "text": "Links this week:\n\nSkip your annual physical.\nThe phrase “Statistical significance is not the same as practical significance” is leading us astray.\nThe ineffectiveness of food and soft drink taxes (although not all calories are the same). The latest extension of the nanny state - banning junk food from playgrounds. And a new book worth looking at - Government Paternalism: Nanny State or Helpful Friend? (HT: Diane Coyle)\nMarijuana in Colorado - no surprise that the most grandiose claims of both sides haven’t come to fruition.\nEven more on lead and crime."
  },
  {
    "objectID": "posts/a-week-of-links-83.html",
    "href": "posts/a-week-of-links-83.html",
    "title": "A week of links",
    "section": "",
    "text": "Links this week:\n\nIn praise of complexity economics.\nBooks coming out in 2015.\nTwo links from the world of intellectual property madness - what would have entered the public domain on 1 January under the old copyright regime (HT: John Bergmayer), and Uber seeks to patent the idea of pricing based on supply and demand (HT: Ben Walsh).\nTest social programs so we know that they work. But also make sure that you use decent sample sizes, don’t over-generalise the results and replicate.\nMissing heritability - more evidence that the gap will be filled.\nParticipation in warfare leads to reproductive success (HT: Razib Khan).\nApplying statistical thinking to education."
  },
  {
    "objectID": "posts/a-week-of-links-81.html",
    "href": "posts/a-week-of-links-81.html",
    "title": "A week of links",
    "section": "",
    "text": "Links this week:\n\n“I worry that most smart people have not learned that a list of dozens of studies, several meta-analyses, hundreds of experts, and expert surveys showing almost all academics support your thesis – can still be bullshit.” Awesome.\nI have only just realised that Gary Klein blogs at Psychology Today. A relatively recent post - The Insight Test.\nBad statistics - same sex marriage edition.\nHow to doctor a cost-benefit analysis.\nReplicating epigenetic claims."
  },
  {
    "objectID": "posts/a-week-of-links-8.html",
    "href": "posts/a-week-of-links-8.html",
    "title": "A week of links",
    "section": "",
    "text": "Links this week:\n\nRazib Kahn encourages academics to get on the blogging bandwagon (and, my post on my experience).\nDavid Barash on “evolutionary existentialism”, my favourite piece this week.\nPaleo or not, we all get heart disease. Also on the paleo front, Peter Turchin reviews Paul and Shou-Ching Jaminet’s Perfect Health Diet; and John Hawks reviews Marlene Zuk’s Paleofantasy.\nI’ve spent a lot of time over the last year or two reading material on Nowak, Tarnita and Wilson’s paper The Evolution of Eusociality, the trigger for the latest round of the group selection wars. It might be a couple of years old, but this excellent discussion by Chris Jensen is the most useful I have found. Jensen also blogs on Nowak’s work here."
  },
  {
    "objectID": "posts/a-week-of-links-78.html",
    "href": "posts/a-week-of-links-78.html",
    "title": "A week of links",
    "section": "",
    "text": "Links this week:\n\nWhere is the literature on behavioural political economy?\nBashing a paper that claims people search for meaning as they approach a new decade (i.e. at 29, 39, 49 etc.)\nVernon Smith on Adam Smith.\nA bucketload of Charles Darwin’s papers are now available online."
  },
  {
    "objectID": "posts/a-week-of-links-76.html",
    "href": "posts/a-week-of-links-76.html",
    "title": "A week of links",
    "section": "",
    "text": "Links this week:\n\nW. Brian Arthur on economic complexity.\nA great article on humans as imitators.\nHigher latitudes have colder weather which leads to larger people which causes lower population and higher investment in children which triggers economic growth.\nAn epidemic of over-diagnosis.\nFinancial price data are converted into music, the music is played to a rat, then the rat guesses whether the price will fall or rise.\nIs being good at science a matter of nature?\nWomen earn less even when they set the pay.\nSocial and cognitive skills are complements."
  },
  {
    "objectID": "posts/a-week-of-links-74.html",
    "href": "posts/a-week-of-links-74.html",
    "title": "A week of links",
    "section": "",
    "text": "Links this week:\n\nA good Jared Diamond interview.\nThe 10,000 hours rule - the best you can do is find the peak of your own ability.\nTinder works because a picture is “worth that fabled thousand words, but your actual words are worth… almost nothing”. (HT: Razib)\nDumb incentives, although economists would be the first to point out a lot of the unintended consequences.\nNo evidence for the benefits of expertise for fund managers.\nDrunks are more utilitarian. And maybe you should do that drinking on an empty stomach.\nAre social psychologists biased against Republicans?"
  },
  {
    "objectID": "posts/a-week-of-links-72.html",
    "href": "posts/a-week-of-links-72.html",
    "title": "A week of links",
    "section": "",
    "text": "Links this week:\n\nWas the paper proposing that mice can pass their fears onto their offspring and grandchildren via epigenetic mechanisms too good to be true? Neuroskeptic comments (and read the comments to Neuroskeptic’s post).  And my favourite epigenetics statement of the week: “Women too can succeed in business. Because epigenetics.”\nWhat are agent based models?\nGenetic engineering will create the smartest humans who have ever lived.\nIs low fertility really a problem?"
  },
  {
    "objectID": "posts/a-week-of-links-70.html",
    "href": "posts/a-week-of-links-70.html",
    "title": "A week of links",
    "section": "",
    "text": "Links this week:\n\nThe workplace is needed to overcome our lack of self control.\nResisting instant gratification - the FT explores Walter Mischel’s the Marshmallow Test.\nMost critiques of twin studies recycle the same discredited 40-year-old arguments. Here’s another paper pulling them apart.\nThe college educated are still getting married, just later. The same can’t be said for everyone else.\nA critique of the 10,000 hour rule. But people should not feel obliged to follow every mention of the role of genetics with an apology for war, slavery and genocide (surprised eugenics didn’t get a mention).\nPeople respond to prices on medical procedures. Demand curves still slope down.\nYour baby looks like your ex. Slightly disturbing.\nA collection of W. Brian Arthur papers on complexity in economics is due out at the end of the month.\nMatt Ridley reviews Steven Johnson’s book How We Got To Now.\nNeuroskeptic skewers claims that “this will change your brain”.\nYoung, middle-aged and old men all want women in their 20s. But they’re realistic about what they can get.\nAcademics writing stinks.\nFrench regulators are mad. Or at least a touch madder than the rest."
  },
  {
    "objectID": "posts/a-week-of-links-69.html",
    "href": "posts/a-week-of-links-69.html",
    "title": "A week of links",
    "section": "",
    "text": "Links this week:\n\nIs your body mostly microbes? A nice take-down of  another trail of citations dead-ends. One thing I have learnt from my PhD research is that when it comes to citations, academics are lazy.\nA good piece on Peter Thiel. Curing death is on the agenda.\nFixing gender bias in research subjects. HT: John Durant\nUsing data in determining punishment - some interesting implications. HT: Marginal Revolution\nDeadweight loss in pointless research.\nThree complexity MOOCs about to kick off from the Santa Fe Institute.\nA not-so-convincing attempt to take down the philosophy behind nudging.\nDon’t expect praise if you state your motivations behind hiring women.\nPredicting when bubbles will pop from MRIs."
  },
  {
    "objectID": "posts/a-week-of-links-67.html",
    "href": "posts/a-week-of-links-67.html",
    "title": "A week of links",
    "section": "",
    "text": "Links this week:\n\nAn excellent Econtalk podcast with Jonathan Haidt. Just don’t buy his lines about group selection - my reasons here.\nSteven Pinker’s amusing article on the Ivy League.\nGreg Clark applies his work on social mobility to immigration. Reihan Salam comments.\nA great swipe at “talent deniers”.\nTracking supercentenarians.\nThe agricultural origins of time preference - I’ll blog about this once I digest. HT: Tyler Cowen\nThe cognitive gains from Head Start fade out by elementary school.\nI’m back into my habit of linking to Andrew Gelman articles every week - this time a great rant about expected utility titled “It’s as if you went into a bathroom in a bar and saw a guy pissing on his shoes, and instead of thinking he has some problem with his aim, you suppose he has a positive utility for getting his shoes wet”"
  },
  {
    "objectID": "posts/a-week-of-links-65.html",
    "href": "posts/a-week-of-links-65.html",
    "title": "A week of links",
    "section": "",
    "text": "Links this week:\n\nDaniel McFadden on how people make choices.\nNot that new but only spotted this week - Gerd Gigerenzer has a great rants on statistics. (HT: Noah Smith)\nForty per cent of modern Chinese are patrilineal descendants of only three super-grandfathers from 6,000 years ago. (HT: Carl Zimmer)\nAnti-marijuana advocates funded by drug companies.\n“There were no associations between childhood family income and subsequent violent criminality and substance misuse once we had adjusted for unobserved familial risk factors.”"
  },
  {
    "objectID": "posts/a-week-of-links-63.html",
    "href": "posts/a-week-of-links-63.html",
    "title": "A week of links",
    "section": "",
    "text": "Links this week:\n\nAn awesome looking conference - Complexity and Evolution: A New Synthesis for Economics\nHybridisation - what does it mean for conversation and the future of species?\nPicking up in a Lamborghini\nStatus wars\nHow far are we from designer babies?\nNeuroscience and marketing"
  },
  {
    "objectID": "posts/a-week-of-links-61.html",
    "href": "posts/a-week-of-links-61.html",
    "title": "A week of links",
    "section": "",
    "text": "Links this week:\n\nSome gold from Robert Sapolsky - what is going on in teenage brains? Plus a bonus interview.\nThe latest issue of Nautilus (the source of the Sapolsky material) contains a lot of other good material - fruit and vegetables trying to kill you and chaos in the brain among them. I recommend scanning the table of contents.\nThe changing dynamics of marriage inequality.\nAndrea Castillo with an introduction to the neoreaction (including some “homebrewed evolutionists”).\nGeoffrey Miller has teamed up with Tucker Max and is offering dating advice informed by evolution."
  },
  {
    "objectID": "posts/a-week-of-links-6.html",
    "href": "posts/a-week-of-links-6.html",
    "title": "A week of links",
    "section": "",
    "text": "Links this week:\n\nA good profile of Joe Henrich and WEIRD people.\nNassim Taleb and Daniel Kahneman discuss antifragility.\nA gene for cooperation in response to punishment.\nI’ve uploaded an updated version of my working paper Conspicuous Consumption, Sexual Selection and Economic Growth. There are no earth-shattering changes, but we’ve simplified the second model and generally sharpened it up."
  },
  {
    "objectID": "posts/a-week-of-links-58.html",
    "href": "posts/a-week-of-links-58.html",
    "title": "A week of links",
    "section": "",
    "text": "Links this week (or closer to a month):\n\nIt’s reigning men. How our convict past explains our glass ceiling.\nRory Sutherland on measurebation.\nThe genetics of investment biases (ungated version). HT: Tyler Cowen.  Basically another confirmation of the three laws of behaviour genetics.\nRats regret bad decisions.\nMatt Ridley on fat.\nPulling apart the research on the destructiveness of female hurricanes - Paul Frijters and Andrew Gelman.\nSendhil Mullainathan on the limits to big data."
  },
  {
    "objectID": "posts/a-week-of-links-56.html",
    "href": "posts/a-week-of-links-56.html",
    "title": "A week of links",
    "section": "",
    "text": "Links this week:\n\nA conversation with Rory Sutherland. Many good pieces, but my favourite line:\n\n\nCertainly there’s a problem with numbers in that there are sophisticated things in life that we all understand perfectly well when verbally described. Should psychology be constrained by math? I mean, who has the better understanding of human behavior—Shakespeare or Eugene Fama? If you make mathematical expression a barrier to entry, to any kind of theory, you are undoubtedly limiting yourselves.\n\n\nAnother article taking a shot at the idea that “all calories are equal”.\nA not so glowing review of the latest book in the Freakonomics franchise - Think Like a Freak. Noah Smith also takes a shot.\nTwo good pieces on the mess that is copyright protection - an extended riff on barriers to innovation and the ridiculous length of copyright terms."
  },
  {
    "objectID": "posts/a-week-of-links-54.html",
    "href": "posts/a-week-of-links-54.html",
    "title": "A week of links",
    "section": "",
    "text": "Links this week:\n\nThis piece on big data is one of the best I have read.\nNo surprise here - single CEOs take bigger risks.\nA conference on hype in science.\nCameron Murray on post-crash economics and the Lucas critique."
  },
  {
    "objectID": "posts/a-week-of-links-51.html",
    "href": "posts/a-week-of-links-51.html",
    "title": "A week of links",
    "section": "",
    "text": "More like a month of links, but here goes:\n\nWe’re going to be hearing a lot about Greg Clark’s new book on social mobility - The Son Also Rises: Surnames and the History of Social Mobility. Clark gives a synopsis in the NYT. In short, Clark and his colleagues estimate “that 50 to 60 percent of variation in overall status is determined by your lineage.”\nKolk and colleagues present a paper in The Proceedings of the Royal Society B presents a model in which intergenerational fertility correlations drive a long-term fertility increase. They cite my working paper a source for genetic correlations driving fertility up. When (if) I get that paper published, do I now cite back?\nThe germs made you do it. (A good long read)\nAre the hot hand deniers, so desperate to demonstrate cognitive biases, falling to biases of their own?\nDon’t hire like Google.\nBehavioural economics versus behavioural finance. On House’s question of where the behavioural economics folk are, I suggest the supply responded to the demand."
  },
  {
    "objectID": "posts/a-week-of-links-5.html",
    "href": "posts/a-week-of-links-5.html",
    "title": "A week of links",
    "section": "",
    "text": "Links this week:\n\nA week ago I posted about a piece in the Economist on Greg Clark’s work using surnames to estimate social mobility. A debate followed the Economist article, with contributions by Miles Corak, Fransicso Ferreira, Greg Clark (and again), and Jason Long. All are worth reading.\nJonathan Last notes three issues that he would include in a second edition of What to Expect When No One’s Expecting: America’s Coming Demographic Disaster. The third relates to my recent working paper with Oliver Richards on natural selection driving an increase in fertility.\nThe anthropology wars are in full swing at the moment, moving from Diamond to Chagnon. The two most interesting articles I read this week were by Alice Dreger in the Atlantic and John Horgan in Scientific American. I’d be interested in hearing a response from the gang of five to Horgan’s claims.\nHalf a year late, I have finally read this article by Dierdre McCloskey on the measurement of happiness. Its full of straw men, but many passages are fantastic."
  },
  {
    "objectID": "posts/a-week-of-links-48.html",
    "href": "posts/a-week-of-links-48.html",
    "title": "A week of links",
    "section": "",
    "text": "Links this week:\n\nRobert Shiller on the rationality debate.\nMore macro wars.\n“Adam” wasn’t necessarily human.\nEric Crampton on free-range kids.\nAnd finally, one of my favourite surf clips from last year - I’d love to see someone paddling this wave:\n\n[vimeo http://vimeo.com/73838157]"
  },
  {
    "objectID": "posts/a-week-of-links-46.html",
    "href": "posts/a-week-of-links-46.html",
    "title": "A week of links",
    "section": "",
    "text": "Links this week:\n\nRobert Kurzban wonders why priming works.\nA disturbing way of maximising fitness. A fertility clinic worker may be the father of a lot of children.\nSome chaff in with the wheat, but this article on Social Darwinism reports some interesting research.\nThe Santa Fe Institute’s MOOC Introduction to Dynamical Systems and Chaos has kicked off.\nA new paper in JEBO. People cooperate because they are selfish. (ungated pdf)\nThe world is complicated.\n\nAnd to close, my twitter and blog feeds contain an inordinate amount of baseball content. I don’t understand why economists are so interested in baseball, despite the fact they can use their statistical skills to re-live the jock versus nerd battles of their childhood (In the same way, I don’t understand my countrymen’s infatuation with cricket - adults chasing balls?). Surely there are more interesting statistics.\nSo, to get some real sport into your feeds (this being the only sport in which I can bring myself to watch), I’m introducing a semi-regular surf link or clip to my week of links posts. Today, some awesome Pipeline footage (using drones, another area that economists seem to be infatuated with). I love how you can see the reef, the holes in it, and how the water depth changes so suddenly at its edge. Other highlights - Kelly Slater at 1:05 catching the wave that won him the recent Pipe Masters, and the crowd all paddling for the horizon at 2:54 when they see some sets starting to rear up on third reef. (As an aside, surfing could use some numerate economists - from the almost award of the Eddie Aikau to Tony Ray to the premature crowning of Kelly Slater as world champion, the surfing hierarchy could benefit from the ability to add.)\n[vimeo http://vimeo.com/83187924 w=800&h=450]"
  },
  {
    "objectID": "posts/a-week-of-links-44.html",
    "href": "posts/a-week-of-links-44.html",
    "title": "A week of links",
    "section": "",
    "text": "Links this week:\n\nMatt Ridley on intelligence and social mobility.\nThere is a lot to like about this paper: Cultural transmission and the evolution of human behaviour: a general approach based on the Price equation. I’ll post more about it later.\nAndrew Gelman on the backlash against replication.\nThe 15 best behavioural science graphs of 2010 to 2013."
  },
  {
    "objectID": "posts/a-week-of-links-42.html",
    "href": "posts/a-week-of-links-42.html",
    "title": "A week of links",
    "section": "",
    "text": "Links this week:\n\nDavid Dobbs takes on the Selfish Gene. Dawkins responds (as does Jerry Coyne, three times). I’m generally with Dawkins and Coyne on the theme (if not every detail) of this argument. I’d go as far to say that I wish the selfish gene concept would make a comeback in some areas.\nMartin Nowak and E.O Wilson, this time with Ben Allen, have launched another broadside at inclusive fitness. Allen posts on the paper (and if anyone can point me to any commentary on the paper, it would be appreciated - it seems a bit quiet compared to Nowak and Wilson’s paper with Tarnita).\nA new journal - Cosmos & Taxis: Studies in Emergent Order and Organization - has launched. The first issue is available.\n\nOtherwise, I’m now back from holidays and almost over some tropical fever I managed to pick up, so regular blogging should recommence in the next few days."
  },
  {
    "objectID": "posts/a-week-of-links-40.html",
    "href": "posts/a-week-of-links-40.html",
    "title": "A week of links",
    "section": "",
    "text": "Links this week:\n\nGary Marcus takes on John Horgan over the achievements of science.\nThe medicalisation of normality.\nOn neuroeconomics- we need to understand the processes underlying decisions to get microeconomics right.\nAndrew Gelman on statistical significance: “A serious researcher can easily get statistical significance when nothing is going on at all …. And this can happen without the researcher even trying, just from doing an analysis that seems reasonable for the data at hand…So, given all this, the focus on p=.04 or .06 or .10 seems to be beside the point. It’s worth looking at … but what it’s focusing on is the least of our problems.”\nA piece by Robert Shiller where, among other things, he defends the use of mathematics in economics.\nA few more pieces of criticising economics - Chris Dillow, Alex Marsh and Diane Coyle."
  },
  {
    "objectID": "posts/a-week-of-links-39.html",
    "href": "posts/a-week-of-links-39.html",
    "title": "A week of links",
    "section": "",
    "text": "Links this week:\n\nDavid Dobbs on progress in genetics.\nSteve Horwitz on what makes libertarians cry.\nThe uncertain biological basis of morality.\nEugene Fama on his Nobel.\nA great story of an amateur pulling apart a psychological theory (an exception to the psychology equivalent of Sign 1 of a good critique).\nThe great interviews on the origin of the Human Behavior and Evolution Society continue. This time, Napoleon Chagnon:\n\nhttp://youtu.be/X51rX7zES1w"
  },
  {
    "objectID": "posts/a-week-of-links-37.html",
    "href": "posts/a-week-of-links-37.html",
    "title": "A week of links",
    "section": "",
    "text": "Links this week:\n\nDavid Barash reviews some books taking an evolutionary perspective on reproduction. (HT: Andrea Castillo)\nA cut from the Econtalk interview with Tyler Cowen. Conscientiousness will become more important in response to increasing opportunities for self-improvement.\nBehavioural economists did not discover irrationality. And while we’re at it, let’s start calling it behavioural science.\nHow much species diversity was there at the time of Home erectus? I love it that such a simple yet obvious consideration can completely change the way of looking at something. (HT: Daniel Lende)"
  },
  {
    "objectID": "posts/a-week-of-links-35.html",
    "href": "posts/a-week-of-links-35.html",
    "title": "A week of links",
    "section": "",
    "text": "Links this week:\n\nEnrico Spolaore and Romain Wacziarg write on genetic barriers to growth. I have previously posted on their work here, here and here.\nDoug Kenrick on Free Market Psychology. There needs to be a rule that only those who have read Adam Smith are allowed to describe Adam Smith’s take on the invisible hand\nAlso at Psychology Today, Matthew Rossano on human and Neanderthal rituals. The difference - cost. (HT: OneNakedApe)\nMalcolm Gladwell is interviewed in The Telegraph. Christopher Chabris reviews Gladwell’s latest book.\nMore Gladwell - on genes and fairness in sport. Despite Gladwell’s flaws (see my thoughts on Outliers), I need to find some time to write a contrarian “In praise of Malcolm Gladwell” piece during this Gladwell bashing season (which it must be based on my twitter feed).\nFinally, the Human Fertility Collection has been launched. It is a supplement to the Human Fertility Database, and contains additional data that doesn’t meet HFD standards."
  },
  {
    "objectID": "posts/a-week-of-links-33.html",
    "href": "posts/a-week-of-links-33.html",
    "title": "A week of links",
    "section": "",
    "text": "Links this week:\n\nDavid Dobbs on the social life of genes. Full of interesting ideas, although I’d love to see someone who knows more about this area critique it.\nTim Harford reviews Scarcity.\nA good review of some new books on neuroscience.\nDave Nussbaum on why you are working too hard.\nIan Rickard pulls apart David Attenborough’s suggestion that humans are no longer evolving.\nAnd below, the latest two videos in the Evolution: This View of Life series On The Origin Of Human Behavior And Evolution Society: Doug Kenrick and John Tooby.\n\nhttp://youtu.be/esSrCSyXH9k\nhttp://youtu.be/mOkw713BFSA"
  },
  {
    "objectID": "posts/a-week-of-links-31.html",
    "href": "posts/a-week-of-links-31.html",
    "title": "A week of links",
    "section": "",
    "text": "Links this week:\n\nNoah Smith on maths in economics. Responses from Paul Krugman, Arnold Kling, Robin Hanson and Bryan Caplan.\nMalcolm Gladwell defends the 10,000 hour rule.\nGary Becker and Richard Posner discuss population.\nAnd finally, an interview with Martin Daly as part of the “On the Origin of HBES: An Oral History Project”, from Evolution, This View of Life.\n\nhttp://youtu.be/u-MTe0UNoa8"
  },
  {
    "objectID": "posts/a-week-of-links-3.html",
    "href": "posts/a-week-of-links-3.html",
    "title": "A week of links",
    "section": "",
    "text": "Four links this week:\n\nPaleofantasy: What Evolution Really Tells Us about Sex, Diet, and How We Live by Marlene Zuk. (HT: Evolvify)\nIgnorance, the Ultimate Asset has some good bits. Some interesting lines: Taleb, however, is so consumed with the downsides of a complex world and advises strategies so “hyperconservative,” that he often ends up preaching futility and paralysis. The Santa Fe Institute, likewise, has for years looked to complexity as the next economic paradigm but has mostly emphasized the chaotic nature of the economy and has thus run into explanatory dead ends.\nOn a similar theme, Ronald Coase laments the disconnect between economics and entrepreneurship.\nThe latest fertility crisis piece that manages to ignore that fertility has increased in most developing countries over the last decade."
  },
  {
    "objectID": "posts/a-week-of-links-28.html",
    "href": "posts/a-week-of-links-28.html",
    "title": "A week of links",
    "section": "",
    "text": "Links this week:\n\nRobert Kurzban takes on PZ Myers’s views on evolutionary psychology.\nThe evolution of lactase persistence (HT: John Hawks).\nThe madness of crowds (of ants).\nThe Nature and Nurture of High IQ (HT: Tyler Cowen).\nBeware Star Academia."
  },
  {
    "objectID": "posts/a-week-of-links-26.html",
    "href": "posts/a-week-of-links-26.html",
    "title": "A week of links",
    "section": "",
    "text": "Links this week:\n\nA few articles on the recent JEBO special issue on economics and evolution - Jag Bhalla in Scientific American, commented on by Mark Thoma, commented on by Mark Buchanan.\nLarry Arnhart has posted about the recent Mont Pelerin Society meeting in the Galapagos - Evolution, the Human Sciences, and Liberty. Here are posts on presentations by Robert Boyd and Robin Dunbar. I expect more posts will follow. You can also download copies of the presentations here.\nWill dementia decline due to the Flynn effect?\nA great Econtalk on charities.\nSexual Economics and the Forgotten Men by Andrea Castillo"
  },
  {
    "objectID": "posts/a-week-of-links-24.html",
    "href": "posts/a-week-of-links-24.html",
    "title": "A week of links",
    "section": "",
    "text": "Links this week:\n\nThe Journal of Economic Behavior and Organization has a special issue out “Evolution as a General Theoretical Framework for Economics and Public Policy”. You can also access the papers through the Evolution Institute website and there is a series of summary articles in Evolution: This View of Life. Many look worth a read, and I’ll post about them over coming weeks/months.\nDavid Sloan Wilson (one of the editors and authors in the JEBO special issue above) has an article in aeon magazinecritiquing economics from an evolutionary angle.\nAlso in aeon magazine, an interesting take on obesity (HT: John Hawks). It’s fair to say that a simple “calorie in-calorie out” analysis doesn’t cut the mustard anymore. (But I don’t buy the bit about lab animal food staying the same).\nAnother article from Evolution: This View of Life that is worth a look - Daniel Hruschka on collectivism versus individualism.\nSupport for Gregory Clark’s argument that analysis of social mobility over a single generation overestimates its extent - a child’s socioeconomic position is determined by their grandparents, not just their parents (blog post on this article to come soon).\nBritain is undergoing a baby boom. I expect this will be a common developed country observation over the next decade."
  },
  {
    "objectID": "posts/a-week-of-links-22.html",
    "href": "posts/a-week-of-links-22.html",
    "title": "A week of links",
    "section": "",
    "text": "Links this week:\n\nMalcolm Gladwell on Albert O. Hirschman. One good paragraph:\n\n\nPeople don’t seek out challenges, he went on. They are “apt to take on and plunge into new tasks because of the erroneously presumed absence of a challenge—because the task looks easier and more manageable than it will turn out to be.” This was the Hiding Hand principle—a play on Adam Smith’s Invisible Hand. The entrepreneur takes risks but does not see himself as a risk-taker, because he operates under the useful delusion that what he’s attempting is not risky.\n\n\nThis list of book recommendations from Nassim Taleb is from early last year, but is definitely worth a read.\nDoes The Hunter-Gatherer Style Of Education Work?\nI came across the idea of pathological altruism when Barbara Oakley presented at last year’s Consilience Conference. I don’t like the idea, and neither does Robert Kurzban."
  },
  {
    "objectID": "posts/a-week-of-links-20.html",
    "href": "posts/a-week-of-links-20.html",
    "title": "A week of links",
    "section": "",
    "text": "Links this week:\n\nPinker, Wrangham, Dennett, Haig, Seabright and friends on Napoleon Chagnon (including interviews with Chagnon).\nAnother Andrew Gelman swipe at those “Psychological Science”papers.\nFrom late last year, but worth a read - John S Wilkins takes a look at sociobiology (Parts 1, 2, 3 and 4 follow) [HT: Åse Kvist Innes-Ker].\nLike Zuk’s book, an interesting take on the paleo diet, but beating a straw man.\nRob Brooks and I have an article up at Evolution: This View of Life on the rise of “breadwinner moms”."
  },
  {
    "objectID": "posts/a-week-of-links-19.html",
    "href": "posts/a-week-of-links-19.html",
    "title": "A week of links",
    "section": "",
    "text": "Links this week:\n\nAndrew Gelman critiques the recent paper on bicep size and political preferences.\nOur food is evolving. John Hawks comments.\nA nice article on growing evidence that a bit of extra weight later in life is not a bad thing.\nIf you’re not subscribed to the a replicated typo feed, get on it. More on memes."
  },
  {
    "objectID": "posts/a-week-of-links-17.html",
    "href": "posts/a-week-of-links-17.html",
    "title": "A week of links",
    "section": "",
    "text": "Were the Victorian’s cleverer than us? Patrick Rabbit pulls it apart.\nKevin Mitchell has a shot at the new eugenics. Razib responds. Read the comments on both.\nPeter Singer critiques conspicuous consumption. The example is similar to one Robert Frank uses in Luxury Fever, but I still like it.\nGender identity and relative income within households (pdf). I haven’t read it yet, but some interesting results.\nMaybe I’d have more time to read if I cut out the distractions. Avoid news (HTRyan Murphy). Actually, I’ve almost completely cut news from my diet, but blogs and the twitterverse are still very distracting.\nDo markets erode moral values? I’ll post about the paper when I’ve digested it some more, but I don’t think it’s the “market” eroding the morals."
  },
  {
    "objectID": "posts/a-week-of-links-15.html",
    "href": "posts/a-week-of-links-15.html",
    "title": "A week of links",
    "section": "",
    "text": "Links this week:\n1. [Gender language and economic power - another economic paper with a spurious correlation](http://www.replicatedtypo.com/gender-language-and-economic-power-another-spurious-correlation/6227.html)?\n\n\n2. [Culture and economic development](http://noahpinionblog.blogspot.com.au/2013/04/can-culture-predict-economic-development.html).\n\n\n3. [We haven't yet reached our satiation point](http://www.brookings.edu/~/media/research/files/papers/2013/04/subjective%20well%20being%20income/subjective%20well%20being%20income.pdf). [A summary](http://www.brookings.edu/research/papers/2013/04/subjective-well-being-income).\n\n\n4. It's been a few weeks since the last, but here's [another Chagnon versus the anthropologists article](http://www.slate.com/articles/health_and_science/science/2013/05/napoleon_chagnon_controversy_anthropologists_battle_over_the_nature_of_fierceness.single.html).\n\n\n5. [How many priming studies are safe to cite](http://www.scientificamerican.com/article.cfm?id=disputed-results-a-fresh-blow-for-social-psychology)?"
  },
  {
    "objectID": "posts/a-week-of-links-13.html",
    "href": "posts/a-week-of-links-13.html",
    "title": "A week of links",
    "section": "",
    "text": "Links this week:\n\nFrom The Umlaut (worth adding to your feed) - Why Choosing to Make Less Money Is Easier Than Ever. The return of conspicuous leisure?\nA new book Forecast: What Physics, Meteorology, and the Natural Sciences Can Teach Us About Economics may be worth a look (HT: Diane Coyle).\nShould human genes be patented? I wonder how much of this debate will be surpassed by technological progress. Cheap genomes and the ability to upload to free or near free websites for analysis will make attempts to prevent unauthorised testing of a patented gene impossible.\nI’ve updated my economics and evolutionary biology reading list, including adding a couple more articles on the evolution of preferences and genoeconomics."
  },
  {
    "objectID": "posts/a-week-of-links-11.html",
    "href": "posts/a-week-of-links-11.html",
    "title": "A week of links",
    "section": "",
    "text": "Links this week:\n\nThere are plenty of good looking MOOCs popping up. John Hawks has announced his MOOC Human Evolution: Past and Future, starting early 2014. Dan Ariely’s behavioural economics course kicked off a couple of weeks ago, and this course on social and economic networks by Matthew Jackson looks worth a look. I seem to be adopting a speed learning strategy for MOOCs - wait until all the material is up and then binge.\nHow will the Flynn effect and the bright tax balance out? We’re looking OK until 2042.\nSlavery and capitalism."
  },
  {
    "objectID": "posts/a-week-of-links-106.html",
    "href": "posts/a-week-of-links-106.html",
    "title": "A week of links",
    "section": "",
    "text": "Links this week:\n\nAmerican hippopotamus. HT: Scott Alexander.\nA walk in the park increases poor research practices and decreases reviewer critical thinking.\nEncourage more students to study science and put their future employment at greater risk.\nBehavioural economics and savings.\nThe economic future for men.\nWhy twitter is terrible. I don’t spend much time there any more.\nThe mainstream may be getting dumber by the day, but we are living in what looks like a golden age of publishing for, of all people, the university presses.\n\nAnd if you missed them, my posts from the last week:\n\nSam Bowles on the death of Homo Economicus.\nA grumpy rant on behavioural economics."
  },
  {
    "objectID": "posts/a-week-of-links-104.html",
    "href": "posts/a-week-of-links-104.html",
    "title": "A week of links",
    "section": "",
    "text": "Links this week:\n\nStorytelling about famous experiments tends to go a bit askew.\nNoah Smith takes on Deirdre McCloskey.\nChimps on the drink.\nA review of Richard Thaler’s ‘Misbehaving: The Making of Behavioural Economics’.\nThe gender gap in tech.\n\nAnd if you missed them, my posts from the last week:\n\nMSiX 2015 is on July 30 in Sydney, and features yours truly.\nHumans cause accidents."
  },
  {
    "objectID": "posts/a-week-of-links-102.html",
    "href": "posts/a-week-of-links-102.html",
    "title": "A week of links",
    "section": "",
    "text": "Links this week:\n\nTwo perspectives on the chocolate study - Scott Alexander and Andrew Gelman. I would say that the chocolate study didn’t tell us anything that we didn’t already know.\nSelf-deluded leaders.\nThe education myth.\nIf only chimpanzees had ovens. HT: Tyler Cowen\n\nAnd if you missed them, my posts from the last week:\n\nRation information. Avoid news.\nMeasurement error on 23andme.\nMerton on retirement incomes."
  },
  {
    "objectID": "posts/a-week-of-links-100.html",
    "href": "posts/a-week-of-links-100.html",
    "title": "A week of links",
    "section": "",
    "text": "Links this week (or more like two weeks):\n\nAnother favourite behavioural science story bites the dust.\nThree schools of thought on decision making.\nBetter teachers receive worse evaluations.\nAn attempt to reduce bias backfires.\nBiased scientists.\nHayek and business management.\nMore highly educated women are having children.\n\nLife continues to be busy, so posting will continue to be sparse for at least another couple of weeks."
  },
  {
    "objectID": "posts/a-wasteful-christmas.html",
    "href": "posts/a-wasteful-christmas.html",
    "title": "A wasteful Christmas",
    "section": "",
    "text": "As noted by Chris Berg, the Australia Institute has rolled out its annual reminder of Joel Waldfogel’s almost 20-year old observation about the wastefulness of Christmas giving. As the gift giver has less than complete knowledge of the preferences of the recipient, the recipient tends to value the gift at less than the amount that the giver paid. The disparity in value represents a deadweight loss.\nAs for most of the annual responses to the Australia Institute’s release (with a special mention for Steven Kirchner’s 2009 comparison of Christmas gift giving with government spending), Berg suggests that such spending may not be wasteful. Christmas gifts signal to loved ones the strength of the relationship. To the giver, the money is not wasted. Further, the narrow definition of efficiency fails to capture benefits such as the effect of the gift on the strength of the relationship.\nWhile I consider Berg’s analysis to be on the mark, I would not characterise the situation as one without “waste”. In fact, waste can be a vital element of the signal. For a signal to be reliable, there should a cost to the signaller. Otherwise, the signal may be faked. If gifts had no cost, people would give them to a far wider set of recipients, destroying the value of the signal to the receiver as to who cares about them. Receivers of the signal would then start to ignore the signals as they can’t tell the good from the bad. This makes gift giving useless to the signaller and destroys their incentive to make the signal in the first place.\nThis scenario creates an incentive for the giver to increase the cost (including price, inconvenience, degree of thought) of the gift to a size that cheaters will not match it. This might result in continually increasing gift size as givers seek to make sure that the gift is large enough.\nGiven the above, it is worth asking whether gifts are the most efficient way to make this signal or whether there is a way for all parties to agree to restrain the costs that they will incur. On the first point, the evidence of continued gift giving would suggest that the givers consider this an efficient way for them to signal. However, this is an indicator of private efficiency and it may not be the socially optimal method.\nOn the second point of restraint, one possibility could be  along the lines of Robert Frank’s suggestion of progressive consumption taxation. If the government taxed consumption at a progressively higher rate, there would be less incentive to engage in competitive conspicuous consumption or gift giving. However, we then end up back where we started. As Kirchner asked, if we put this money into the hands of government result in the purchase of goods with an even lower value to the recipients? Personally, I would rather trust my loved ones."
  },
  {
    "objectID": "posts/a-science-of-intentional-change.html",
    "href": "posts/a-science-of-intentional-change.html",
    "title": "A science of intentional change",
    "section": "",
    "text": "If nothing else, David Sloan Wilson is ambitious. He’s been pushing the multilevel selection wheelbarrow with not much support for close to forty years (although support seems to be growing in some circles). And over the last couple of years, he’s been increasingly promoting the idea of a evolution-centred “science of intentional change” that will allow us to change our behavioural and cultural practices. And as a simple step on the way there, all we have to do is conceptually unify the behavioural sciences.\nThis program for a science of intentional change is now sketched out in an article for Behavioral and Brain Sciences (in press), co-authored with Steven Hayes, Anthony Biglan and Dennis Embry. The interesting thing about BBS articles of this nature, as explained in a companion post by Wilson at Evolution: This View of Life, is that they are followed by commentary of peers, with submissions for commentary due by 28 May. Given some of the criticisms in the article, I hope that the right respondents will be flushed out.\nOn a first read, I’m not sold on the concept. At times it feels as though the authors are swiping at a paper tiger. Parts of the paper feel like evolutionary overreach (with evolution encompassing the four prongs of genetics, epigenetics, learning and symbolic thought), and I struggled to understand what evolution added to the understanding of many of the examples. And the discussion of group dynamics is laced with Wilson’s approach to multi-level selection, which raises questions about how informative the evolutionary approach is if you disagree with Wilson’s starting point.\nStill, there are plenty of good and interesting ideas in the article and maybe I’ll be more on side after I give it some more thought and (hopefully) see some good critiques. In the meantime, here are some interesting passages (minus references):\n\n[A]n enormous amount of integration must occur before a science of human behavioral and cultural change can center on evolution. This integration needs to be a two-way street, involving not only contributions of evolutionary theory to the human- related disciplines, but also the reverse. For example, core evolutionary theory needs to expand beyond genetics to include other inheritance systems, such as environmentally induced changes in gene expression (epigenetics), mechanisms of social learning found in many species, and the human capacity for symbolic thought that results in an almost unlimited variety of cognitive constructions, each motivating a suite of behaviors subject to selection.\nWe will argue that the first steps toward integration, represented by a configuration of ideas that most people associate with the term evolutionary psychology (EP), was only the beginning and in some ways led in the wrong direction. In particular, the distinction between EP and the standard social science model (SSSM) was a wrong turn we must correct. A mature EP needs to include elements of the SSSM associated with major thinkers such as Emile Durkheim, B.F. Skinner, and Clifford Geertz. Only when we depolarize the distinction between EP and the SSSM can a science of change occur.\n\nDo evolutionists rely upon their own “blank slate” assumption”? I hope Steven Pinker takes the time to respond to this piece.\n\nIronically, although Tooby and Cosmides praised genetic evolution as a domain-general process, capable of adapting organisms to virtually any environment, they failed to generalize this insight to include other evolutionary processes. If they had, their critique of the “blank slate” traditions in the human behavioral sciences would have appeared in a new light.\nEvolutionists routinely rely upon a “blank slate” assumption of their own when they reason about adaptation and natural selection. They predict the adaptations that would evolve by natural selection, given heritable variation and a sufficient number of generations. For example, they confidently predict that many species inhabiting desert environments will evolve to be sandy colored to conceal themselves from their predators and prey. This prediction can be made without any knowledge of the genes or physical composition of the species. Insofar as the physical makeup of organisms results in heritable variation, that is the extent to which it can be ignored in predicting the molding action of natural selection. The phenotypic properties of organisms are caused by selection and merely permitted by heritable variation.\n\nEvolutionists have focused on only one evolutionary process, missing out on much of the power of the evolutionary approach.\n\nEvolutionists do not have an already perfected framework to offer other disciplines. They have concentrated almost entirely on genetic evolution and paid scant attention to evolutionary processes that rely upon other mechanisms of inheritance. The dominant heuristic in narrow-school EP, when trying to explain a particular trait, is to assume that it is genetically determined, ask how it evolved by genetic evolution in the distant past, and then ask how it functions in the current environment. For traits associated with parental neglect, the heuristic has led to valid insights concerning the importance of such things as genetic relatedness or availability of resources. Yet it missed the fast-paced process of selection by consequences, resulting in behavioral strategies in parents and offspring that are adaptive in the context of the immediate family environment but profoundly maladaptive over the long term. These are the practices that are most amenable to change after identifying and understanding the contingencies. Evolutionists therefore have much to learn from branches of the human behavioral sciences where learning as a variation-and- selection process has occupied center stage for decades.\n\nIf only we consider all forms of evolution, seemingly insoluble problems may seem tractable.\n\nA believer in Jesus sees the world differently than a follower of Ayn Rand, and seeing differently results in acting differently. This is true not only for religions and political ideologies, but also for scientific theories, as Tooby and Cosmides correctly note. Consider the possibility that severe personal and societal dysfunctions, which have defied solutions for decades, can sometimes be relieved by interventions that require just a handful of hours. Against the background of an evolutionary theory confined to genetic evolution, this claim seems too good to be true. Against the background of an evolutionary approach that actively manages a symbotype-phenotype relationship, the possibility begins to make more sense. If we expect artificial selection, genetic engineering, and gene therapy to provide new solutions, then why not expect the same from their counterparts in learning and symbolic systems? In this fashion, expanding core evolutionary theory beyond genetic evolution results in new possibilities for action that were previously invisible. Indeed, as the behavioral and symbolic impact on epigenetic processes becomes better understood, this expansion promises to alter our perspective on the role of genetic evolution itself.\n\nIf you have a bit of time up your sleeve, give the paper a read."
  },
  {
    "objectID": "posts/a-problem-in-the-world-or-a-problem-in-the-model.html",
    "href": "posts/a-problem-in-the-world-or-a-problem-in-the-model.html",
    "title": "A problem in the world or a problem in the model",
    "section": "",
    "text": "In reviewing Michael Lewis’s The Undoing Project, John Kay writes:\n\nSince Paul Samuelson’s Foundations of Economic Analysis, published in 1947, mainstream economics has focused on an axiomatic approach to rational behaviour. The overriding requirement is for consistency of choice: if A is chosen when B is available, B will never be selected when A is available. If choices are consistent in this sense, their outcomes can be described as the result of optimisation in the light of a well-defined preference ordering.\nIn an impressive feat of marketing, economists appropriated the term “rationality” to describe conformity with these axioms. Such consistency is not, however, the everyday meaning of rationality; it is not rational, though it is consistent, to maintain the belief that there are fairies at the bottom of the garden in spite of all evidence to the contrary. …\n… In the 1970s, however, Kahneman and Tversky began research that documented extensive inconsistency with those rational choice axioms.\n…\nWhat they did, as is common practice in experimental psychology, was to set puzzles to small groups of students. The students often came up with what the economics of rational choice would describe as the “wrong” answer. These failures of the predictions of the theory clearly demand an explanation. But Lewis—like many others who have written about behavioural economics—does not progress far beyond compiling a list of these so-called “irrationalities.”\nThis taxonomic approach fails to address crucial issues. Is rational choice theory intended to be positive—a description of how people do in fact behave—or normative—a recommendation as to how they should behave? Since few people would wish to be labelled irrational, the appropriation of the term “rationality” conflates these perspectives from the outset. Do the observations of allegedly persistent irrationality represent a wide-ranging attack on the quality of human decision-making—or a critique of the economist’s concept of rationality? The normal assumption of economists is the former; the failure of observation to correspond with theory identifies a problem in the world, not a problem in the model. Kahneman and Tversky broadly subscribe to that position; their claim is that people—persistently—make stupid mistakes.\n\nI have seen many presentations with an opening line of “economists assume we are rational”, quickly followed by conclusions about poor human decision-making, the two being conflated. More often than not, it’s better to ignore economics as a starting point and to simply examine the evidence for poor decision making. That evidence is, of course, much richer - and debatable - than a simple refutation of the basic economics axioms.\nOne of those debates concerns the Linda problem. Kay continues:\n\nTake, for example, the famous “Linda Problem.” As Kahneman frames it: “Linda is 31 years old, single, outspoken, and very bright. She majored in philosophy. As a student, she was deeply concerned with issues of discrimination and social justice, and also participated in anti-nuclear demonstrations. Which of the following is more likely? ‘Linda is a bank teller,’ ‘Linda is a bank teller and is active in the feminist movement.’”\nThe common answer is that the second alternative—that Linda is more likely to be a feminist bank teller than a bank teller—is plainly wrong, because the rules of probability state that a compound probability of two events cannot exceed the probability of either single event. But to the horror of Kahneman and his colleagues, many people continue to assert that the second description is the more likely even after their “error” is pointed out.\nBut it does not require knowledge of the philosopher Paul Grice’s maxims of conversation—although perhaps it helps—to understand what is going on here. The meaning of discourse depends not just on the words and phrases used, but on their context. The description that begins with Linda’s biography and ends with “Linda is a bank teller” is not, without more information, a satisfactory account. Faced with such a narrative in real life, one would seek further explanation to resolve the apparent incongruity and, absent of such explanation, be reluctant to believe, far less act on, the information presented.\nKahneman and Tversky recognised that we prefer to tell stories than to think in terms of probability. But this should not be assumed to represent a cognitive failure. Storytelling is how we make sense of a complex world of which we often know, and understand, little.\n…\nSo we should be wary in our interpretation of the findings of behavioural economists. The environment in which these experiments are conducted is highly artificial. A well-defined problem with an identifiable “right” answer is framed in a manner specifically designed to elucidate the “irrationality” of behaviour that the experimenter triumphantly identifies. This is a very different exercise from one which demonstrates that people make persistently bad decisions in real-world situations, where the issues are typically imperfectly defined and where it is often not clear even after the event what the best course of action would have been.\n\nKay also touches on the more general criticisms:\n\nLewis’s uncritical adulation of Kahneman and Tversky gives no credit to either of the main strands of criticism of their work. Many mainstream economists would acknowledge that people do sometimes behave irrationally, but contend that even if such irrationalities are common in the basements of psychology labs, they are sufficiently unimportant in practice to matter for the purposes of economic analysis. At worst, a few tweaks to the standard theory can restore its validity.\nFrom another perspective, it may be argued that persistent irrationalities are perhaps not irrational at all. We cope with an uncertain world, not by attempting to describe it with models whose parameters and relevance we do not know, but by employing practical rules and procedures which seem to work well enough most of the time. The most effective writer in this camp has been the German evolutionary psychologist Gerd Gigerenzer, and the title of one of his books, Simple Heuristics That Make Us Smart, conveys the flavour of his argument. The discovery that these practical rules fail in some stylised experiments tells us little, if anything, about the overall utility of Gigerenzer’s “fast and frugal” rules of behaviour.\n…\nPerhaps it is significant that I have heard some mainstream economists dismiss the work of Kahneman in terms not very different from those in which Kahneman reportedly dismisses the work of Gigerenzer. An economic mainstream has come into being in which rational choice modelling has become an ideology rather than an empirical claim about the best ways of explaining the world, and those who dissent are considered not just wrong, but ignorant or malign. An outcome in which people shout at each other from inside their own self-referential communities is not conducive to constructive discourse."
  },
  {
    "objectID": "posts/a-nobel-prize-for-biology.html",
    "href": "posts/a-nobel-prize-for-biology.html",
    "title": "A Nobel Prize for biology",
    "section": "",
    "text": "At the beginning of a lecture by Robert Trivers at the London School of Economics on his book The Folly of Fools: The Logic of Deceit and Self-Deception in Human Life, Helena Cronin notes the absence of a Nobel prize in biology. The closest substitute, the Crafoord Prize in Biosciences, is awarded once every four years, with the winner chosen by the Royal Swedish Academy of Sciences. The Academy also participates in deciding the Nobel prizes for Chemistry and Physics and Nobel Memorial Prize in Economic Sciences. Trivers was awarded the Crafoord prize in 2007.\nLooking down the list of earlier Crafoord prize winners, I was struck by how many of them were influential to me as an economist. How many of them could be, or should be, candidates for the economics Nobel prize?\nWhile Edward O. Wilson won the prize for his work on island biogeography and species developing in isolation, his work in sociobiology and later application to humans should be a lynchpin of economic thinking. Bill Hamilton, were he still alive, could be considered for his work on altruism and kin selection. Robert Trivers might also be a worthy winner for his contributions in the area of cooperation. Robert May’s paper on the complex behaviour of simple systems is at the foundation of my understanding of complexity. The work of John Maynard Smith is peppered through any book on game theory (although he is also deceased).\nBiology has not yet been adopted in economics to an extent that would make it likely that any of these candidates will receive the economics Nobel prize. However, as the award to Daniel Kahneman showed, the Nobel Committee is not averse to looking outside the narrow group labeled as economists."
  },
  {
    "objectID": "posts/a-model-of-the-quantity-quality-trade-off.html",
    "href": "posts/a-model-of-the-quantity-quality-trade-off.html",
    "title": "A model of the quantity-quality trade-off",
    "section": "",
    "text": "Following my last post suggesting that there was no quality-quantity trade-off in modern societies (at least to an extent that mattered), I wanted to point to a nice model that makes this argument. The interesting thing about the model is that the purpose of the authors in developing it was not primarily to make that point.\nThe model is by Oded Galor and Omer Moav model in their article Natural Selection and the Origin of Economic Growth. I’ve posted on the paper before and my first working paper simulated this model (a post about that paper is here).\nThe basic idea of the model is as follows. In the population there are two genotypes - those who prefer higher quality children, and those who prefer a greater quantity of children. Those with a preference for higher quality invest more in educating their children.\nIn the Malthusian state before the Industrial Revolution, those who invest in their children’s quality have a fitness advantage. Their children are better able to take advantage of the scarce resources. As a result, the quality-preferring types increase in prevalence, increasing the average level of education in the population. Technological progress depends on the level of education in the population, so this also increases.\nThis process creates a feedback loop. Faster technological progress increases the return to human capital, so the quality-preferring types further increase their children’s education. Eventually the returns to human capital become so large that even the quantity-preferring types invest in some education. As this point technological progress greatly accelerates and the economy enters into a new modern growth state.\nHowever, this changes the dynamics of investment in education. With technological progress so high, the quality-preferring types over-invest in educating their children and their prevalence starts to decline. Those with a lower investment in child quality now have the evolutionary advantage.\nThe optimal strategy in this new plentiful environment is to put all the resources into having more children, not increasing their quality. Resources are now so plentiful that all children can survive and reproduce in the next generation regardless of the level of investment. In my working paper, we showed that the less an agent invested in quality in the modern growth state, the greater fitness advantage they had over the other types in the population.\nThis particular model implies that, depending on the triggers for investment in children in modern economies, many people may be over-investing, at least if you consider evolutionary success the benchmark. Those types who cut the investment in quality will grow in number.\nTogether with evidence that the costs to child quality of having another sibling are small, this model suggests that, from an evolutionary perspective, the trade-off is not one of quantity versus quality. Rather, the trade-off is between quantity and investment in quality. That investment in quality simply does not generate the evolutionary returns that it once did."
  },
  {
    "objectID": "posts/a-flood-of-new-genetic-variation.html",
    "href": "posts/a-flood-of-new-genetic-variation.html",
    "title": "A flood of new genetic variation",
    "section": "",
    "text": "A new Nature paper by Fu and colleagues has been the subject of a few good write-ups. First, from Brandon Keim at Wired:\n\nIn the most massive study of genetic variation yet, researchers estimated the age of more than one million variants, or changes to our DNA code, found across human populations. The vast majority proved to be quite young. The chronologies tell a story of evolutionary dynamics in recent human history, a period characterized by both narrow reproductive bottlenecks and sudden, enormous population growth.\nThe evolutionary dynamics of these features resulted in a flood of new genetic variation, accumulating so fast that natural selection hasn’t caught up yet. As a species, we are freshly bursting with the raw material of evolution.\n“Most of the mutations that we found arose in the last 200 generations or so. There hasn’t been much time for random change or deterministic change through natural selection,” said geneticist Joshua Akey of the University of Washington, co-author of the Nov. 28 Nature study. “We have a repository of all this new variation for humanity to use as a substrate. In a way, we’re more evolvable now than at any time in our history.” …\n\nPut simply, more people means more mutations, and in a growing population, there is less chance that those mutations will be lost through drift.\nDespite their young age, these new variants are relevant for our recent evolutionary history. If you couple this explosion in new variants with the large changes in selection pressures associated with the shift to agriculture, you would see adaptive evolution in humans speed up.\nThe different population dynamics associated with African and European populations is also interesting. More from Keim:\n\nAlso playing a role are the dynamics of bottlenecks, or periods when populations are reduced to a small number. The out-of-Africa migration represents one such bottleneck, and others have occurred during times of geographic and cultural isolation. Scientists have shown that when populations are small, natural selection actually becomes weaker, and the effects of randomness grow more powerful. …\nThe result, calculated Akey, is that people of European descent have five times as many gene variants as they would if population growth had been slow and steady. People of African descent, whose ancestors didn’t go through that original bottleneck, have somewhat less new variation, but it’s still a large amount: three times more variation than would have accumulated under slow-growth conditions.\n\nThis finding is an interesting fit with the argument of Ashraf and Galor linking genetic diversity and economic development. As genetic diversity results in a wider spectrum of traits in the population, it is more likely that there will be traits complementary to technological advance. The measure of diversity used by Ashraf and Galor, expected heterozygosity, is based on the probability that two randomly selected people will differ with respect to a certain gene, averaged over all measured genes. As people migrated from Africa through Europe and beyond, they migrated out with only a subset of the available genetic variation in the population, and therefore, expected heterozygosity declines with distance from Africa. But as the paper by Fu and colleagues shows, the prevalence rare variants does not decline with distance from Africa. Could these additional rare variants in some populations increase the probability that there will be traits complementary to technological advance?\nAlso worth reading is the post from John Hawks. Hawks raises some questions around the estimated age of the variants (are they older than they seem?) and asks whether some of these rare variants came from Neanderthals."
  },
  {
    "objectID": "posts/a-critique-of-behavioura-economics-from-1988.html",
    "href": "posts/a-critique-of-behavioura-economics-from-1988.html",
    "title": "A critique of behavioural economics from 1988",
    "section": "",
    "text": "In the closing section to Robert Frank’s Passions Within Reason: The Strategic Role of the Emotions, Frank writes of the failure of critics of the self-interest model of human action to gain traction in their critique.\n\nThe difficulty confronting critics is that they have failed to come forward with an alternative theory. None of the evidence they cite against the self-interest model is really new. The experiments with prisoner’s dilemmas date from the 1950s, those with honesty and victims-in-distress from the 1960s. Even Kagan’s recent work on the role of emotional competencies merely provides modern scientific footing for beliefs that were all but universal during the nineteenth century. But as the philosopher Thomas Kuhn has stressed, a reigning theory is almost never displaced by mere contradictions in the data. If it is to be challenged at all, it must be by an alternative theory that better fits the facts.\n\nTwenty four years on, it is still a fair assessment."
  },
  {
    "objectID": "posts/a-bunch-of-links.html",
    "href": "posts/a-bunch-of-links.html",
    "title": "A bunch of links",
    "section": "",
    "text": "A past regular feature of this blog was “A week of links’. Primarily, it was a useful way to aggregate interesting articles - I often search my blog posts for material (they are a collection of text files on my computer).\nBut, the regularity of the feature drove my behaviour: I started looking for links for the post. So, I killed it.\nNow for the partial resurrection, with links posts to be delivered at random intervals to share articles or ideas that are worth a read. Here’s the first.\n\nUnlearning Economics unloads in a set of tweets about empirical economics research. I have a draft post on this idea, but this thread hits most my points plus more. In my view, the robustness of some subfields of empirical economics research is on par with early 2000s social psychology research. The problem, however, is that you can’t run a replication of most of this empirical work, meaning it doesn’t suffer the same public fate.\n\n\n;document.getElementById(\"tweet-68431\").innerHTML = tweet[\"html\"];\nBehavioural and Brain Sciences has a target article on The generalizability crisis by Tal Yarkoni (ungated pdf here). Many responses worth reading, including this one from Gerd Gigerenzer (can’t find an ungated version of the Gigerenzer response to share). Daniel Lakens comments.\nYou can now access a free pdf version of Gelman, Hill and Vehtari’s Regression and Other Stories.\nHolden Karnofsky on reading books. I think about this a lot: for the average book I simply “read”, my recall a few years later is close to nothing. Pair with Andy Matuschak’s Why books don’t work.\nIf you pressed me to name the most important areas of behavioural science research, near the top of the list is human-algorithm interaction, and in particular, how people respond to advice from machines. Jennifer Logg has a book chapter on the subject. From the abstract:\n\nData analytics needs psychology. Organizations cannot realize the full potential of algorithms until they address the last mile problem and consider how people respond to algorithmic advice. Algorithms have the potential to greatly improve human judgment and decision making, as they generally outperform the accuracy of experts when the two are directly compared. But people can only leverage the accuracy of algorithms if they are willing to listen. Should they ignore algorithmic advice, the resources invested into data analytics, both within academia and industry, will go to waste. While the field of data analytics (the systematic computation of data, most commonly using algorithms) continues to evolve at a rapid rate, most overlook the important connection between producing and utilizing insights.\n\nIn developing my teaching materials, I always spend a lot of time seeing what else is out there. Here’s one the sets of resources I found most useful. Gilad Feldman, if I’m ever in your neck of the woods, I owe you a beer. [And I have every intention of sharing in the same way.]\nI’ve been thinking about meta-analysis recently on the back of the recently published meta-analysis on choice architecture. This old Slate Star Codex post is never far from my mind."
  },
  {
    "objectID": "posts/a-bunch-of-links-2.html",
    "href": "posts/a-bunch-of-links-2.html",
    "title": "A bunch of links",
    "section": "",
    "text": "Social science a mess, journals no good, the meaningless of the label “misinformation”, Flipper Zero, and cleaning up the list of named “biases”:\n\nSocial science is a mess and it’s not getting better. A few years old, but a lot of gold. A few nuggets:\n\n\nEconomics topped the charts in terms of expectations, and it was by far the strongest field. There are certainly large improvements to be made — a 2/3 replication rate is not something to be proud of. But reading their papers you get the sense that at least they’re trying, which is more than can be said of some other fields. … A unique weakness of economics is the frequent use of absurd instrumental variables. I doubt there’s anyone (including the authors) who is convinced by that stuff, so let’s cut it out.\n\n\nGoing into this, my view of evolutionary psychology was shaped by people like Cosmides, Tooby, DeVore, Boehm, and so on. You know, evolutionary psychology! But the studies I skimmed from evopsych journals were mostly just weak social psychology papers with an infinitesimally thin layer of evolutionary paint on top. Few people seem to take the “evolutionary” aspect really seriously.\n\n\nPNAS is Not a Good Journal. The 69 Nature branded journals aren’t any good either.\nThe misinformation label has become almost meaningless. Fact checking is often more about “vibe” than whether a claim is factual. Dan Williams makes the case that any attempt to divide content into misleading and non-misleading buckets is too subjective.\nI’ve recently come across Flipper Zero. I’ve spent a lot of time trying to understand the implications of generative AI and other tech while sitting in front of my laptop, but there’s something to be said about tinkering in the physical world.\nA step toward cleaning up the huge list of named “biases” floating around: Toward Parsimony in Bias Research: A Proposed Common Framework of Belief-Consistent Information Processing for a Set of Biases. Some of the clean-up probably doesn’t even need a theoretical framework, just the simple act of recognising that people have named the same phenomena multiple times."
  },
  {
    "objectID": "posts/a-critical-behavioural-economics-and-behavioural-science-reading-list.html",
    "href": "posts/a-critical-behavioural-economics-and-behavioural-science-reading-list.html",
    "title": "A critical behavioural economics and behavioural science reading list",
    "section": "",
    "text": "This reading list is a balance to the one-dimensional view in many popular books, TED talks, conferences, academic press releases and consultancy sales pitches. For those who feel they have a good understanding of the literature after reading Thinking Fast and Slow, Predictably Irrational and Nudge, this is for you. [In the time since I drafted the first version of this list in 2017, it’s fair to say that the balance has swung a bit.]\nThe purpose of this reading list is not to imply that all behavioural economics or behavioural science is bunk (it’s not). That said, I did not design the list to be balanced; you can combine this list with plenty of reading lists from elsewhere for that.\nPlease let me know if there are any other books or articles I should add, or if there are any particularly good replies to what I have listed. I am sure I have missed some good ones. I have set a mild quality bar on what I have included. I don’t agree with all the arguments, but everything on the list has at least one interesting idea."
  },
  {
    "objectID": "posts/a-critical-behavioural-economics-and-behavioural-science-reading-list.html#books",
    "href": "posts/a-critical-behavioural-economics-and-behavioural-science-reading-list.html#books",
    "title": "A critical behavioural economics and behavioural science reading list",
    "section": "1. Books",
    "text": "1. Books\nGerd Gigerenzer, Peter Todd and the ABC Research Group, Simple Heuristics That Make Us Smart: Simple heuristics can be both fast and accurate, particularly when we assess real-life performance rather than conformity with the principles of rationality.\nDoug Kenrick and Vlad Griskevicius, The Rational Animal: How Evolution Made Us Smarter Than We Think: A good introduction to the idea that evolutionary psychology could add a lot of value to behavioural economics, but has the occasional straw man discussion of economics and a heavy reliance on priming research (and you will see below how that is panning out).\nDavid Levine, Is Behavioural Economics Doomed?: A good but slightly frustrating read. I agree with Levine’s central argument that rationality is underweighted, but the book is littered with straw man arguments.\nLionel Page, Optimally irrational: The Good Reasons We Behave the Way We Do: We should invest more in understanding why people behave the way they do.\nMario J. Rizzo and Glen Whitman, Escaping Paternalism: Rationality, Behavioral Economics, and Public Policy: An excellent critique of the traditional behavioural economists’ arguments for paternalism.\nPhil Rosenzweig, Left Brain, Right Stuff: How Leaders Make Winning Decisions: An entertaining examination of how behavioural economics findings hold up for real world decision-making.\nGilles Saint-Paul, The Tyranny of Utility: Behavioral Social Science and the Rise of Paternalism: Sometimes hard to share Saint-Paul’s anger, but some important underlying points.\nHugo Mercier, Not Born Yesterday: The Science of Who We Trust and What We Believe: A strong argument that we are not gullible and easily manipulated, but rather skeptical and rational in the way we filter information.\nRobert Sugden, The Community of Advantage: A Behavioural Economist’s Defence of the Market: A well balanced critique from someone who has worked in the field for decades.\nMark D. White, The Manipulation of Choice: Ethics, Libertarianism and Paternalism: Some great chapters on what policy makers can know about people’s preferences, but for misses the mark is his critique on what behavioural economics can bundle under the “preferences” label."
  },
  {
    "objectID": "posts/a-critical-behavioural-economics-and-behavioural-science-reading-list.html#general-and-methodological-critiques",
    "href": "posts/a-critical-behavioural-economics-and-behavioural-science-reading-list.html#general-and-methodological-critiques",
    "title": "A critical behavioural economics and behavioural science reading list",
    "section": "2. General and methodological critiques",
    "text": "2. General and methodological critiques\nApplied behavioural economics: In The death of behavioral economics, Jason Hreha argues that applied behavioural economics is on the way out. Scott Alexander responds.\nAre we biased?: Gerd Gigerenzer debates Daniel Kahneman and Amos Tversky. Gigerenzer tees off (pdf). Kahneman and Tversky respond (pdf - this pdf also includes a rejoinder to Gigerenzer’s later piece). Gigerenzer returns (pdf). I’m a fan of a lot of Gigerenzer’s work, but his strength has never been the direct attack. Kahneman and Tversky get the better of this exchange. My post here.\nAs-if models: Nathan Berg and Gerd Gigerenzer note that behavioral economics is neoclassical economics in disguise (pdf of working paper). They write that “‘As-if’ arguments are frequently put forward in behavioral economics to justify ‘psychological’ models that add new parameters to fit decision outcome data rather than specifying more realistic or empirically supported psychological processes that genuinely explain these data.” Includes a critique of prospect theory’s lack of realism as a decision-making process.\nCritiquing economics I: Ken Binmore argues (pdf) that the claim “economic man” is a failure can be both attacking a position not held by economics and ignoring the experimental evidence of people behaving like “economic man”.\nCritiquing economics II: Pete Lunn and Tim Harford debate whether “the idea that the very foundations of economics are being undermined is absurd.”\nThe effectiveness of nudging: Mertens et al “found” that choice architecture interventions promote behavior change with a small to medium effect size. Andrew Gelman responds. Three articles in reply argue that most of the pooled effects in Mertens et al. are overestimated and hence unrepresentative, there is no evidence for nudging after correcting for publication bias, and there is no reason to expect large and consistent effects of nudge interventions. Some of the garbage in the meta-analysis led to a correction, although the papers from Brian Wansink remained (more on Wansink below).\nEvolutionary theory I: Owen Jones proposes that “… Behavioral Economics, and those who rely on it, are falling behind with respect to new developments in other disciplines that also bear directly on the very same mysteries of human decision-making.”\nEvolutionary theory II: Douglas Kenrick and colleagues argue that many of our biases are in fact deeply rational. (My post).\nErgodicity: Ole Peters proposes The ergodicity problem in economics. “[B]y carefully addressing the question of ergodicity, many puzzles besetting the current economic formalism are resolved in a natural and empirically testable way.” See also David Meder and friends’ Ergodicity-breaking reveals time optimal economic behavior in humans. My posts here, here and here.\nHumility: In Aren’t we smart, fellow behavioural scientists, I suggest that “As applied behavioural scientists, we need to inject some humility into our assessment of other people’s decisions. … We need to stop making glib assumptions about what other people want and how they can best achieve their objectives.”\nLab experiments 1: Ken Binmore and Avner Shaked urge experimentalists to “join the rest of the scientific community in adopting a more skeptical attitude when far-reaching claims about human behavior are extrapolated from very slender data”. Fehr and Schmidt respond, as do Eckel and Gintis. Binmore and Shaked wrote a rejoinder.\nLab experiments 2: Steven Levitt and John List note that, while economic models can benefit from incorporating insights from psychology, “behavior in the lab might be a poor guide to real-world behavior.” (pdf).\nLab experiments 3: Steven Levitt and John List suggest that caution is required when attempting to generalise lab results out of sample.\nMany co-authors: Emerging from the Francesca Gino frauds (see below) was the Many Co-authors project. For all studies in which Gino was involved, was Gino involved in data collection? The truly underwhelming element of this project is how rarely data has been made publicly available. Further, it once again highlights that shenanigans by people like Gino are only the tip of the iceberg. Here’s one outcome, a retraction of Don’t stop believing: Rituals improve performance by decreasing anxiety for which Gino was a co-author but not involved in data collection for most of the studies. Missing data and questionable data management all round. It’s best to retain the default of disbelief.\nMegastudies: Do megastudies improve the impact of applied behavioural science? Katherine Milkman and friends argue so. My initial take and a later reflection suggest there are trade-offs and problems in execution.\nPreferences: Gerardo Infante, Guilhem Lecouteux and Robert Sugden argue that Behavioural welfare economics does not model human psychology as it really is, but rather as “faulty Econs” (pdf). Daniel Hausman responds. Infante and friends provide a rejoinder (working paper pdf).\nPre-registration: Protzko and friends argue that rigour-enhancing practices such as confirmatory tests, large sample sizes, preregistration and methodological transparency increase replication rates. The problem: they didn’t preregister their own analysis. Jessica Hullman discusses here and here. My two cents. Andrew Gelman provides a nice summary following the retraction.\nThe need for theory I: David Levine and Jie Zheng propose that (pdf) Economic theory makes strong predictions about many situations and is generally quite accurate in predicting behavior in the laboratory. “In situations where the theory is thought to fail, the failure is in the application of theory rather than the theory failing to explain the evidence.”\nThe need for theory II: Michael Muthukrishna and Joseph Henrich argue that the replication crisis in the psychological sciences is a problem of lack of theory.\nReplication: The Open Science Collaboration found that Thirty-six percent of psychology replications had significant results (pdf). Effect sizes were halved in magnitude. Social psychology fares particularly poorly.\nSelf criticism: Ariel Rubinstein notes that “[f]or Behavioral Economics to be a revolutionary program of research rather than a passing episode, it must become more open-minded and much more critical of itself.”\nToo many biases: I argue that instead of building a messier and messier picture of human behavior, we need a new model.\nWEIRD people: Joseph Henrich, Steven Heine and Ara Norenzayan propose that “we need to be less cavalier in addressing questions of human nature on the basis of data drawn from this particularly thin, and rather unusual, slice of humanity.” But see Cremieux on weirdness and two papers in response (1, 2)."
  },
  {
    "objectID": "posts/a-critical-behavioural-economics-and-behavioural-science-reading-list.html#counterpoints-to-famous-biases-effects-and-stories",
    "href": "posts/a-critical-behavioural-economics-and-behavioural-science-reading-list.html#counterpoints-to-famous-biases-effects-and-stories",
    "title": "A critical behavioural economics and behavioural science reading list",
    "section": "3. Counterpoints to famous biases, effects and stories",
    "text": "3. Counterpoints to famous biases, effects and stories\nThe backfire effect: Daniel Engber reviews the evidence. I first saw doubts about the effect on WNYC.\nChoice overload: Mark Lepper and Sheena Iyengar’s famous jam study (pdf). A meta-analysis by Benjamin Scheibehenne and friends (pdf) - the mean effect size of changing the number of choices across the studies was virtually zero (although note the Brian Wansink studies in the meta-analysis!). Other studies point to conditions where it might occur, such as Chernev and friends who identify some factors that facilitate choice overload.\nDepletion of willpower: Daniel Engber summarises the state of affairs. The meta-analysis referred to by Engber. And the failed replication that triggered the article.\nDisfluency: The original N=40 paper (pdf). The N=7000 replication (pdf). Terry Burnham tells the story. (And interestingly, Adam Alter, author of the first paper, suggests that the law of small numbers should be more widely known).\nDishonest bankers: Cohn and colleagues argue that “When their professional identity as bank employees is rendered salient, a significant proportion of them become dishonest”. But look at the data more closely, and primed bankers cheat no more than the student controls. See also Rahwan and friends for a failed replication.\nThe Florida effect: The poster child for the replication crisis. Ed Yong catalogues the story nicely.\nGrit: Daniel Engber reviews Angela Duckworth’s book. I review. (I like the way Angela Duckworth deals with criticism. Also listen to this Econtalk episode.)\nGrowth mindset: The Wikipedia summary. Scott Alexander’s initial exploration and clarification. A pre-registered study and meta-analysis both showing a tiny but apparently real effect. A more recent meta-analysis concludes that “Across all studies, we observed a small overall effect … which was nonsignificant after correcting for potential publication bias. … We conclude that apparent effects of growth mindset interventions on academic achievement are likely attributable to inadequate study design, reporting flaws, and bias.”\nThe hot hand illusion: The original Thomas Gilovich, Robert Vallone and Amos Tversky paper arguing people are seeing a hot hand in basketball when none exists. Work by Joshua Miller and Adam Sanjurjo (working paper pdf) shows the original argument was based on a statistical mistake. The hot hand does exist in basketball. (Although I will say that there is plenty of evidence of people seeing patterns where they don’t exist.) ESPN explores. My post here.\nHungry judges: Shai Danziger and friends find that favourable rulings by Israeli parole boards plunge in the lead up to meal breaks (from 65% to near 0). Andreas Glockner suggests this might be a statistical artefact. Keren Weinshall-Margela and John Shapard point out that the hearing order is not random (Danziger and friends respond). And Daniel Lakens suggests we should dismiss the finding as simply being impossible. My post here. A similar analysis of judges during Ramadan (working paper pdf) finds the opposite effect - they are more lenient when hungry.\nHyperbolic discounting: Ariel Rubinstein points out that (pdf) “the same type of evidence, which rejects the standard constant discount utility functions, can just as easily reject hyperbolic discounting as well.”\nIllusion of control: Francesca Gino, Zachariah Sharek and Don Moore note that illusion of control experimental results can be statistical artefacts (pdf). “[B]y focusing on situations marked by low control, prior research has created the illusion that people systematically overestimate their level of control.” My post here.\nLoss aversion I: David Gal and Derek Rucker claim that (working paper) “current evidence does not support that losses, on balance, tend to be any more impactful than gains.” E. Tory Higgins and Nira Liberman respond, as do Itamar Simonson and Ran Kivetz. Gal and Rucker rejoinder (working paper pdf). My post here. Mrkva and friends also add to the debate.\nLoss aversion II: Eldad Yechiam makes a related argument in Acceptable losses: the debatable origins of loss aversion (pdf). My post here. Also see Scott Alexander.\nMoney priming: Doug Rohrer, Harold Pashler and Christine Harris find that subtle reminders of money don’t change people’s political views (pdf). Kathleen Vohs fights back (pdf). Miguel Vadillo, Tom Hardwicke and David R. Shanks respond. Analysis of the broader literature on money priming suggests, among other things, massive publication bias.\nMoral reminders: The original (N = 229) paper co-authored by Nina Mazar, On Amir and Dan Ariely (pdf). The (N=5,786) multi-lab replication by Verschuere and friends: “This small effect was numerically in the opposite direction of the original study.” More recently, an investigation into the data provenance has led to an Expression of Concern. Relatedly, here and here are posts analysing the “shredders” used in some of Ariely’s honesty experiments.\nOrgan donation: Does Austria have a 99.94% organ donation rate because of the design of their driver’s licence application? No.\nOverconfidence: Don Moore and Paul Healy address the many concepts tangled up in the word “overconfidence”” (pdf). [My post]/overconfident-about-overconfidence/).\nPower pose: Jesse Singal on Dana Carney’s shift from author of the classic power pose paper (pdf) to skeptic. Carney’s posted a document about her shift on her website.\nPriming mating motives: Shanks and friends on Romance, risk, and replication: Can consumer choices and risk-taking be primed by mating motives? (pdf): A failed replication, plus “a meta-analysis of this literature reveals strong evidence of either publication bias or p-hacking.” (I have cited some of these studies approvingly in published work - a mistake.)\nProspect theory: The prospect theory model, the centrepiece of behavioural economics, has us as loss averse and risk seeking when facing losses, and risk averse when considering gains. Ryan Oprea proposes that most of the evidence underlying theories of risk, such as prospect theory, actually reflect mistakes under complexity.\nSafety signs kill motorists: Hall and Madsen proposed that dynamic signs that reported Texas road fatalities - “1669 deaths this year on Texas roads” - caused more accidents and fatalities. I argue that we shouldn’t take too much from this single paper.\nScarcity: My review of the book. Reanalysis of the original scarcity paper (pdf) without dichotomising income eliminated the effect. The original authors managed to resurrect the effect (pdf) by combining the data from three experiments, but once you are at this point, you have well and truly entered the garden of forking paths. Leandro Carvalho and friends found that “participants surveyed before and after payday performed similarly on a number of cognitive function tasks.” Then, in a replication of scarcity papers by O’Donnell and friends: “Of the 20 studies that were significant in the original, four of our replication efforts yielded significant results.”\nSigning at the top, part I: Lisa Shu and friends report in PNAS that “signing before—rather than after—the opportunity to cheat makes ethics salient when they are needed most and significantly reduces dishonesty.” Ariella Kristal, Ashley Whillans and the authors of the original paper report a failed replication. A discussion of what this means in Scientific American. That, of course, is only the beginning of the story (see the fraud story below)."
  },
  {
    "objectID": "posts/a-critical-behavioural-economics-and-behavioural-science-reading-list.html#fraud-and-misconduct",
    "href": "posts/a-critical-behavioural-economics-and-behavioural-science-reading-list.html#fraud-and-misconduct",
    "title": "A critical behavioural economics and behavioural science reading list",
    "section": "4. Fraud and misconduct",
    "text": "4. Fraud and misconduct\nThe Cornell Food and Brand Lab’s catalogue of eating biases (led by Brian Wansink): Jesse Singal catalogues the events. Stephanie Lee’s reviews emails from the lab. Corrections and retractions are flowing. It’s fair to say that we shouldn’t place any weight on results out of that lab. (Although somewhat amazingly, Wansink’s experiment with a bottomless soup bowl replicated! I didn’t believe the original experiment ever existed - and am still doubtful that it did.)\nDiederik Stapel: For a long-time, the most salient fraud in social science. The NYT tells the story. My favourite (now retracted) study of his was on how trash-filled environments make people racist. For a long time I thought of Stapel as an extreme but rare case of fraud. I now believe fraud is common, but most people don’t leave such a trail.\nFrancesca Gino: In a series of four posts (1, 2, 3, 4), the Data Colada team document a series of frauds in Francesca Gino’s work. Failing to recall Barbara Streisand’s experience, Gino sued the Data Colada team. The lawsuit was later dismissed (although as at the time of writing, Gino’s claim against Harvard remains ongoing). Fortunately, the Harvard investigation was made public as a result of the court proceedings, allowing even more analysis by the Data Colada team into how the fraud was perpetrated.\nSigning at the top, part II: The field trial data from the signing at the top study (noted above) was completely made up. This led to the paper being retracted and an investigation into Ariely (that ultimately reached no adverse findings). That, of course, was only one of two frauds in this paper. The other, also uncovered by the Data Colada team, was that the data in experiment 1 had been manipulated. Absent the manipulation, there was no effect."
  },
  {
    "objectID": "posts/a-critical-behavioural-economics-and-behavioural-science-reading-list.html#applications-of-behavioural-economics-and-nudging",
    "href": "posts/a-critical-behavioural-economics-and-behavioural-science-reading-list.html#applications-of-behavioural-economics-and-nudging",
    "title": "A critical behavioural economics and behavioural science reading list",
    "section": "5. Applications of behavioural economics (and nudging)",
    "text": "5. Applications of behavioural economics (and nudging)\nGovernment failure I: In Homo economicus or homo paleas?, John Cochrane states that “The case for the free market is not that each individual’s choices are perfect. The case for the free market is long and sorry experience that government bureaucracies are pretty awful at making choices for people.” Noah Smith responds.\nGovernment failure II: Ted Gayer writes that “the main failure of rationality is not with the energy-using consumers and firms, but instead the main failure of rationality is with the regulators themselves.” Two related papers by Gayer and W. Kip Viscusi are Overriding Consumer Preferences With Energy Regulations (pdf) and Behavioral Public Choice: The Behavioral Paradox of Government Policy (pdf)\nImplementation: DellaVigna, Kim and Linos find that a nudge trial with a negative result is almost as likely to be implemented as a positive result.\nA manifesto for applying behavioural science: Michael Hallsworth writes a manifesto for applying behavioural science (longer and ungated BIT version here). A few observations from me.\nMore than nudging I: Reuben Finighan looks beyond nudging (pdf), stating that “Policymakers often mistakenly see behavioural policy as synonymous with”nudging”. Yet nudges are only one part of the value of the behavioural revolution—and not even the lion’s share”\nMore than nudging II: George Loewenstein and Nick Chater put nudges in perspective, writing that “This paper aims to remind policy-makers that behavioural economics can influence policy in a variety of ways, of which nudges are the most prominent but not necessarily the most powerful.” Richard Thaler responds. Chater and Loewenstein later took this critique further, arguing that the belief that society’s problems can be addressed cheaply and effectively at the level of the individual, without modifying the system in which the individual operates, is a mistake.\nPaternalism: Robert Sugden writes that (pdf) “The claim that the paternalist is merely implementing what the individual would have chosen for herself under ideal conditions is a common theme in paternalistic arguments, but should always be viewed with scepticism.” Also see Sugden’s Do people really want to be nudged towards healthy lifestyles?, Sunstein’s response (pdf) and Sugden’s rejoinder.\nPolicy failure I: Philip Booth notes that “We seem to have gone … to a situation where we have regulators who use economics 101 supplemented with behavioural economics to try to bring perfection to markets that simply cannot be perfected and perhaps cannot be improved.”\nPolicy failure II: Tim Harford writes that “The appeal of a behavioural approach is not that it is more effective but that it is less unpopular.” (Google the article and go through that link if you hit the paywall.)\nPolicy failure III: George Loewenstein and Peter Ubel argue that “behavioral economics is being used as a political expedient, allowing policymakers to avoid painful but more effective solutions rooted in traditional economics.”\nPolicy failure IV: In a Behavioural and Brain Sciences target article, George Loewenstein and Nick Chater argue that focussing on interventions at the individual level is inadvertently preventing systemic change. There are many responses, but I’ll highlight those by Michael Hallsworth, David Gal and Derek Rucker, Cass Sunstein, Richard Thaler and Ralph Hertwig."
  },
  {
    "objectID": "posts/a-critical-behavioural-economics-and-behavioural-science-reading-list.html#if-you-want-some-background",
    "href": "posts/a-critical-behavioural-economics-and-behavioural-science-reading-list.html#if-you-want-some-background",
    "title": "A critical behavioural economics and behavioural science reading list",
    "section": "6. If you want some background",
    "text": "6. If you want some background\nI know this list is of critiques, but here are a few books I would recommend if you want a basic background.\nDaniel Kahneman’s Thinking, Fast and Slow is still the best popular overview of behavioural science. However, it is not standing the test of time particularly well. Here is a fantastic analysis of the priming chapter, and Kahneman’s response to that review in the comments. A review of the estimated replicability of all the chapters is similarly damming. It’s unfortunate that something better hasn’t yet emerged. Just pair it with this reading list!\nErik Angner’s A Course in Behavioral Economics is a good and readable academic presentation of the core principles of behavioural economics.\nCass Sunstein and Richard Thaler’s Nudge: The Final Edition is not my favourite book, but it’s a useful to understand the mindset of many nudge proponents.\nRichard Thaler’s Misbehaving is a pretty good (although very US-centric) history of behavioural economics.\nMichael Lewis’s The Undoing Project is an accessible overview of Kahneman and Tversky’s work.\nMichael Hallsworth and Elspeth Kirkman’s Behavioral Insights is a solid book on translating behavioural science into applied public policy."
  },
  {
    "objectID": "posts/a-default-of-disbelief.html",
    "href": "posts/a-default-of-disbelief.html",
    "title": "A default of disbelief",
    "section": "",
    "text": "It was easy to see why many behavioural practitioners loved the idea that you could induce honesty by getting someone to sign a form at the top, not the bottom. It was practical. It was cheap to implement. It involved more than the common “send them a reminder” or “chuck a social norm on it” that comprises much of the applied behavioural science canon. (That said, don’t underestimate reminders.) It provided an unintuitive proposal to improve business outcomes that wasn’t likely to come from any other source.\nThe fraudulent data in the paper that first proposed this idea has attracted a lot of recent attention (plus a retraction). But apart from the pall cast over the field and Dan Ariely’s work, the exposure of the fraud has also had the unfortunate effect of distracting from what I think is a more important story arising from this study.\nSo, for a moment, let’s ignore the fraud and look at the state of play before its exposure.\nIn 2012, Lisa Shu and friends published a paper in PNAS in which they reported that “signing before—rather than after—the opportunity to cheat makes ethics salient when they are needed most and significantly reduces dishonesty.” The paper reported the results of two lab experiments and one field experiment, all pointing in the right direction.\nHere is how the authors reported the field experiment:\n\nPartnering with an automobile insurance company in the southeastern United States, we manipulated the policy review form, which asked customers to report the current odometer mileage of all cars insured by the company. Customers were randomly assigned to one of two forms, both of which required their signature following the statement: “I promise that the information I am providing is true.” Half the customers received the original forms used by the insurance company, where their signature was required at the end of the form; the other half received our treatment forms, where they were required to sign at the beginning. The forms were identical in every other respect. Reporting lower odometer mileage indicated less driving, lower risk of accident occurrence, and therefore lower insurance premiums. …\n… Customers who signed at the beginning on average revealed higher use (M = 26,098.4, SD = 12,253.4) than those who signed at the end [M = 23,670.6, SD = 12,621.4; F(1, 13,485) = 128.63, P &lt; 0.001]. The difference was 2,427.8 miles per car. That is, asking customers to sign at the beginning of the form led to a 10.25% increase in implied miles driven (based on reported odometer readings) over the current practice of asking for a signature at the end.\n\nBut then in 2020 came a new paper in PNAS by Ariella Kristal, Ashley Whillans and the authors of the 2012 paper. The abstract says it all:\n\nIn 2012, five of the current authors published a paper in PNAS showing that people are more honest when they are asked to sign a veracity statement at the beginning instead of at the end of a tax or insurance audit form. In a recent investigation, across five related experiments we failed to find an effect of signing at the beginning on dishonesty. Following up on these studies, we conducted one preregistered, high-powered direct replication of experiment 1 of the PNAS paper, in which we failed to replicate the original result. The current paper updates the scientific record by showing that signing at the beginning is unlikely to be a simple solution for increasing honest reporting.\n\nThese authors discussed what we should learn from these two papers in an article in Scientific American. It was pointedly titled “When We’re Wrong, It’s Our Responsibility as Scientists to Say So”.\nThe title relates to the point where I depart from their interpretation. They suggest scientists should say when they are wrong. But the question is, what exactly went wrong?\nHere’s the authors’ take:\n\nWe also hope that this collaboration serves as a positive example, whereby upon learning that something they had been promoting for nearly a decade may not be true, the original authors confronted the issue directly by running new and more rigorous studies, and the original journal was open to publishing a new peer-reviewed article documenting the correction.\n…\nWhile we may have lost confidence in the effect of signing first (versus last) being the simple fix that we thought we found, this collaboration has strengthened our belief that psychology, like all good science, is continuously updating and self-correcting and that it is up to all of us to maintain this growing positive trend.\n\nThis is underselling the nature of what went wrong and the correction that is required.\nThe problem is not simply that a single study about signing a form failed to replicate. The authors are hardly Robinson Crusoe in failing to have their work replicate. This isn’t even the first failed replication involving Dan Ariely or Nina Mazar on this topic of inducing honesty.\nRather, the problem is that there is a huge swathe of published literature in the behavioural sciences that, to use Andrew Gelman’s framing, suffers from the garden of forking paths, publication bias and the like. It is built on noisy experiments. The PNAS paper is a typical example, not an exception. Much of this literature won’t replicate. And despite this, people are still taking that work as true absent a failed replication.\nSo rather than saying they “lost confidence in the effect of signing first”, a better calibrated self-correction would have been to say “we should not have trusted this potentially spurious result absent rigourous replication, and we should be applying similar scepticism to more of the literature.”\nTo put it another way, the default position should not be to take all published results in the behavioural science literature as “the record”. Instead, we should treat many of these published results as exploratory or untested hypotheses. Start from a position near “unlikely to be true”, and update to a stronger belief in the presence of replications or other supporting evidence.\nAbsent that approach, the story repeats. Well-meaning practitioners will continue to pick up ideas from the literature, waste time and effort until (if they are lucky) someone gets around to killing off the original idea in a replication. And as highlighted in the Scientific American article, this is costly:\n\nThis matters because governments worldwide have spent considerable money and time trying to put this intervention into practice with limited success. In particular, failed attempts have been reported in several countries, and thousands of dollars were spent when one of us (Whillans) was working with a government in Canada to attempt to change their tax forms.\n\nAs a final aside, I’m not as optimistic that behavioural science is continuously updating and self-correcting, at least not yet in a way that matches the scale of the challenge. Responses to failed replications, such as the other failed replication of an intervention designed to induce honesty that I mentioned above, are typically far more defensive. Then there is the ongoing quantum of the flow of papers with questionable claims. They still outnumber robust replications.\nMaybe the distinguishing point for these “sign at the top” experiments is that at least one incentive pointed in the right direction. By joining the replication effort and applying a less defensive approach to their work, the original authors got the great value of two PNAS papers for the price of one."
  },
  {
    "objectID": "posts/a-grumpy-take-on-behavioural-economics.html",
    "href": "posts/a-grumpy-take-on-behavioural-economics.html",
    "title": "A grumpy take on behavioural economics",
    "section": "",
    "text": "I missed this when it was first posted, but John Cochrane has posted a great rant (not that I agree with it all) in response to a couple of articles on Richard Thaler’s new book Misbehaving: The Making of Behavioral Economics (HT: Diane Coyle).\nA couple of excerpts:\n\nWhen it gets to economics, though – market outcomes, not individual decisions –  a common complaint is that “behavioral” approaches study small-potatoes effects. OK, some asset might have a price 10 basis points off. OK, Dick knows how to rebase exams to get a bit better teaching ratings. OK, so your non-economist spouse wants roses on Valentine’s day. But really, in the big picture of growth, unemployment, inequality, climate – you name it – has this risen past cuteonomics? How do I use psychology to study the practical problems of everyday economics, say How much does progressive taxation hinder innovation and growth; How do I separate the risk premium from expected inflation in reading long-term bonds; How much carbon would a tax reduce, and so on?\nThat’s an interesting debate. We could have it. We should have it. There are good points on both sides. Too bad Dick chose not to address it at all.\n\nOn libertarian paternalism:\n\nThe case for the free market is not that each individual’s choices are perfect. The case for the free market is long and sorry experience that government bureuacracies are pretty awful at making choices for people. “Empirically demonstrating” that some people do silly things does not empirically demonstrate that other people, organized into the US regulatory agencies, can make better choices for them. This is another simple failure of basic logic.\nAnd psychological, social-psychological, sociological, anthropological, and sociological study of bureaucracies and regulatory agencies, trying to understand their manifest “irrationality,” rather than just bemoan it as libertarians tend to do, ought to be a tremendously interesting inquiry. Where is behavioral public choice? (More in a previous post.)\n\nAnd on being an outcast:\n\nMost of the Wall Street Journal review passes along Thaler’s of complaining about how people resisted his early ideas. Really, now, complaining about being ignored and mistreated is a bit unseemly for a Distinguished Service professor with a multiple-group low-teaching appointment at the very University of Chicago he derides, partner in an asset management company running $3 billion dollars, recipient of numerous awards including AEA vice president, and so on.\n\nThaler is now AEA president.\nRead the full post. A response by Noah Smith is here."
  },
  {
    "objectID": "posts/a-new-useless-class.html",
    "href": "posts/a-new-useless-class.html",
    "title": "A New Useless Class?",
    "section": "",
    "text": "Yuval Noah Harari writes:\n\nFears of machines pushing people out of the job market are, of course, nothing new, and in the past such fears proved to be unfounded. But artificial intelligence is different from the old machines. In the past, machines competed with humans mainly in manual skills. Now they are beginning to compete with us in cognitive skills. And we don’t know of any third kind of skill—beyond the manual and the cognitive—in which humans will always have an edge.\nAt least for a few more decades, human intelligence is likely to far exceed computer intelligence in numerous fields. Hence as computers take over more routine cognitive jobs, new creative jobs for humans will continue to appear. Many of these new jobs will probably depend on cooperation rather than competition between humans and AI. Human-AI teams will likely prove superior not just to humans, but also to computers working on their own.\nHowever, most of the new jobs will presumably demand high levels of expertise and ingenuity, and therefore may not provide an answer to the problem of unemployed unskilled laborers, or workers employable only at extremely low wages. Moreover, as AI continues to improve, even jobs that demand high intelligence and creativity might gradually disappear. The world of chess serves as an example of where things might be heading. For several years after IBM’s computer Deep Blue defeated Garry Kasparov in 1997, human chess players still flourished; AI was used to train human prodigies, and teams composed of humans plus computers proved superior to computers playing alone.\n\nI have written previously that it is overly simplistic to extrapolate from the “freestyle chess” example to a statement that the future is human-machine combinations. This has to be true in some form, even if the sole human role is designer. But when we look at the level of individual decisions, the evidence in support of human-machine combinations appears somewhat weak.\nFirst, the idea that we can work in effective teams of this type overestimates the capabilities of most humans. Garry Kasparov may not have been defeated by a machine until 1997, but most humans had been inferior to chess computers for decades earlier. Most people should not interfere with their chess playing computer, suggesting difficulty in implementing these models at scale. As Harari notes above, this option may not be available to the unskilled.\nSecond, these successful pairings appear the exception, rather than the rule. Most of the (admittedly underdeveloped) evidence in this area suggests that when you put an algorithm in the hands of a human, the human is more likely to degrade its performance than if the algorithm was left alone.\nBut finally, even where the rare skilled human forges a partnership, how long does it remain superior? Harari continues:\n\nYet in recent years, computers have become so good at playing chess that their human collaborators have lost their value and might soon become entirely irrelevant. On December 6, 2017, another crucial milestone was reached when Google’s AlphaZero program defeated the Stockfish 8 program. Stockfish 8 had won a world computer chess championship in 2016. It had access to centuries of accumulated human experience in chess, as well as decades of computer experience. By contrast, AlphaZero had not been taught any chess strategies by its human creators—not even standard openings. Rather, it used the latest machine-learning principles to teach itself chess by playing against itself. Nevertheless, out of 100 games that the novice AlphaZero played against Stockfish 8, AlphaZero won 28 and tied 72—it didn’t lose once. Since AlphaZero had learned nothing from any human, many of its winning moves and strategies seemed unconventional to the human eye. They could be described as creative, if not downright genius.\nCan you guess how long AlphaZero spent learning chess from scratch, preparing for the match against Stockfish 8, and developing its genius instincts? Four hours. For centuries, chess was considered one of the crowning glories of human intelligence. AlphaZero went from utter ignorance to creative mastery in four hours, without the help of any human guide.\nAlphaZero is not the only imaginative software out there. One of the ways to catch cheaters in chess tournaments today is to monitor the level of originality that players exhibit. If they play an exceptionally creative move, the judges will often suspect that it could not possibly be a human move—it must be a computer move. At least in chess, creativity is already considered to be the trademark of computers rather than humans! So if chess is our canary in the coal mine, we have been duly warned that the canary is dying. What is happening today to human-AI teams in chess might happen down the road to human-AI teams in policing, medicine, banking, and many other fields.\n\nHarari also argues that even if the computer is not superior by itself, its connectivity and updatability might still mean that it is sensible to replace all the humans.\n\nWhat’s more, AI enjoys uniquely nonhuman abilities, which makes the difference between AI and a human worker one of kind rather than merely of degree. Two particularly important nonhuman abilities that AI possesses are connectivity and updatability.\nFor example, many drivers are unfamiliar with all the changing traffic regulations on the roads they drive, and they often violate them. In addition, since every driver is a singular entity, when two vehicles approach the same intersection, the drivers sometimes miscommunicate their intentions and collide. Self-driving cars, by contrast, will know all the traffic regulations and never disobey them on purpose, and they could all be connected to one another. When two such vehicles approach the same junction, they won’t really be two separate entities, but part of a single algorithm. The chances that they might miscommunicate and collide will therefore be far smaller.\nSimilarly, if the World Health Organization identifies a new disease, or if a laboratory produces a new medicine, it can’t immediately update all the human doctors in the world. Yet even if you had billions of AI doctors in the world—each monitoring the health of a single human being—you could still update all of them within a split second, and they could all communicate to one another their assessments of the new disease or medicine. These potential advantages of connectivity and updatability are so huge that at least in some lines of work, it might make sense to replace_ all_ humans with computers, even if individually some humans still do a better job than the machines.\n\nHarari’s article expands to discuss the broader question of whether AI will lead to tyranny. I recommend reading the full piece."
  },
  {
    "objectID": "posts/a-passion-for-equality.html",
    "href": "posts/a-passion-for-equality.html",
    "title": "A passion for equality?",
    "section": "",
    "text": "In Benoit Debreuil’s Human Evolution and the Origins of Hierarchies, the opening chapter contains the interesting argument that egalitarian hunter-gatherer societies were not built on a wish for equality. Dubreuil writes:\n\nEgalitarian social arrangements must build on what Boehm (1999: 66) called an “egalitarian ethos,” which is culturally constructed and transmitted and does not straightforwardly result from our passion for equality. This does not mean that equality does not matter per se. People arguably have a certain preference for equality, which, at the level of cultural evolution, might create a bias in favor of more egalitarian social organizations, albeit indirectly by making cooperation among equals more efficient. However, because our egalitarian motives are limited in intensity, action often comes under the influence of more powerful passions: lust, greed, fear, or hatred. Consequently, the emergence of egalitarian social outcomes depends on other factors that are not directly connected with equality, such as the various motivational and cognitive mechanisms related to social norm and sanction.\n\nDubreuil’s conclusion comes from a review of the literature in experimental economics (which forms the basis of the first chapter of the book), where he examines what motivations drive people to punish behaviour by other players. Many of these experiments suggest that norms about fairness are a stronger driver of punishment that equality. For example, punishment for unequal distribution of resources was much increased where the distributor stated their desire for an unequal distribution.\nDubreuil does not suggest that inequality aversion is completely irrelevant, and points to experiments where players are willing to cut the income of top earners and augment that of bottom earners. However, arguments can be crafted to suggest even these actions are driven by other motives. The punishment of top earners might be driven by envy, while augmentation of bottom earners might be based on the desire to encourage cooperation and build alliances.\nDespite arguing that inequality aversion is not the main driver of our actions, Dubreuil does consider that inequality can have harmful effects. It can reduce cooperation and trust. It can change people’s sensitivity to sanction by others. And in many ways, Dubreuil’s arguments match my own intuitions about inequality. Even if concern about inequality is not the primary driver of our actions (such as the Occupy movement), it affects important elements of human interaction such as trust and cooperation."
  },
  {
    "objectID": "posts/a-review-of-2018-and-some-thoughts-on-2019.html",
    "href": "posts/a-review-of-2018-and-some-thoughts-on-2019.html",
    "title": "A review of 2018 and some thoughts on 2019",
    "section": "",
    "text": "As a record largely for myself, below are some notes in review of 2018 and a few thoughts about 2019.\nWriting: I started 2018 intending to post to this blog at least once a week, which I did. I set this objective as I had several long stretches in 2017 where I dropped the writing habit.\nI write posts in batches and schedule in advance, so the weekly target did not require a weekly focus. However, at times I wrote shorter posts that I might not have otherwise written to make sure there was a sufficient pipeline. Traffic for the blog was similar to the previous year, with around 100,000 visitors, although unlike previous years there was no runaway post with tens of thousands of views. Three of the 10 most popular posts were written during the year.\nIn 2019, I am relaxing my intention to post on the blog every week (although that will largely still happen). I will prioritise writing on what I want to think about, rather than achieving a consistent flow of posts.\nI wrote three articles for Behavioral Scientist during the year. I plan to increase my output for external forums such as Behavioural Scientist in 2019. My major rationale for blogging is that I think (and learn) about issues better when I write for public consumption, and forums outside of the blog escalate that learning experience.\nI also had a paper published in Evolution & Human Behavior (largely written in 2017). For the immediate future, I plan to stop writing academic articles unless I come up with a cracker of an idea. Having another academic publication provides little career value, and the costs of the academic publication process outweigh the limited benefit that comes from the generally limited readership.\nFor some time I have had two book ideas that I would love to attack, but I did not progress in 2018. One traces back to my earlier interest and writings on the link between economics and evolutionary biology. The other is an attempt to distil the good from the bad in behavioural economics - a sceptical take if you like. Given what else is on my plate (particularly a new job), I’d need a strong external stimulus to progress these in 2019, but I wouldn’t rule out dabbling with one.\nReading: I read 79 books in 2018 (47 non-fiction, 32 fiction). I read fewer books than a typical year, largely due to having three children four and under. My non-fiction selection was less deliberate than I would have liked and included fewer classics than I planned. In 2019 I plan to read more classics and more books that directly relate to what I am doing or want to learn, and picking up fewer books on whim.\nI’m not sure how many academic articles I read, but I read at least part of an article most days.\nFocus: I felt the first half of 2018 was more focused and productive than the second. For various reasons, I got sucked into a few news cycles late in the year, with almost zero benefit. I continued to use most of the productivity hacks described in my post on how I focus (and live) - one of the most popular posts this year, and continue to struggle with the distraction of email.\nI am meditating less than when I wrote that post (then daily), but still do meditate a couple of times a week for 10 to 20 minutes when I am able to get out for a walk at lunch. I use 10% Happier for this. I find meditation most useful as a way to refocus, as opposed to silencing or controlling the voices in my head.\nHealth: I continue to eat well (three parts Paleo, one part early agriculturalist), excepting the Christmas break where I relax all rules (I like to think of myself as a Hadza tribesman discovering a bunch of bee hives, although it’s more a case of me simply eating like the typical Australian for a week or two).\nI surf at least once most weeks. My gym attendance waxed and waned based on various injuries (wrist, back), so my strength and fitness is below the average level of the last five years, although not by a great amount.\nWith all of the chaps generally sleeping through the night, I had the best year of sleep I have had in three years.\nWork: I lined up a new role to start in late January this year. For almost three years I have been building the data science capability in my organisation, and have recruited a team that is largely technically stronger than me and can survive (thrive) without me. I’m shifting back into a behavioural science role (although similarly building a capability), which is closer to my interests and skillset. I’m also looking forward to shifting back into the private sector.\nI plan to use the change in work environment to reset some work habits, including batching email and entering each day with a better plan on how I will tackle the most important (as opposed to the most urgent) tasks.\nLife: Otherwise, I had few major life events. I bought my first house (settlement coming early this year). It meets the major goals of being five minutes walk from a surfable beach, next to a good school, and sufficient to cater to our needs for at least the next ten years.\nAnother event that had a large effect on me was an attempt to save a drowning swimmer while surfing at my local beach (some news on it here and here). It reinforced something that I broadly knew about myself - that I feel calm and focused in a crisis, but typically dwell heavily on it in the aftermath. My attention was shot for a couple of weeks after. It was also somewhat of a learning experience of how difficult a water rescue is and how different CPR is on a human compared to a training dummy. My thinking about this day has brought a lot of focus onto what I want to do this year."
  },
  {
    "objectID": "posts/a-unified-behavioural-theory-of-economic-activity.html",
    "href": "posts/a-unified-behavioural-theory-of-economic-activity.html",
    "title": "A unified behavioural theory of economic activity",
    "section": "",
    "text": "John Brockman has wheeled out another good bunch of experts for the newest Edge question “What’s the question about your field that you dread being asked?”\nOne response by Richard Thaler is particularly interesting, who fears being asked “When will there be a single unified ‘behavioral’ theory of economic activity?” For those who know Thaler’s work in behavioural economics, his reason might be surprising:\n\nIf you want a single, unified theory of economic behavior we already have the best one available, the selfish, rational agent model. For simplicity and elegance this cannot be beat. Expected utility theory is a great example of such a theory. von Neumann was no dummy! And if you want to teach someone how to make good decisions under uncertainty, you should teach them to maximize expected utility.\n\nObviously, Thaler knows that this model is not perfect:\n\nThe problem comes if, instead of trying to advise them how to make decisions, you are trying to predict what they will actually do. Expected utility theory is not as good for this task.\n\nHowever, Thaler is not convinced that alternatives such as prospect theory are up for the task, and he suggests that there will ultimately be a multitude of theories:\n\nJust as psychology has no unified theory but rather a multitude of findings and theories, so behavioral economics will have a multitude of theories and variations on those theories. You need to know both physics and engineering to be able to build a structurally sound bridge, and as far as I know there is no general theory of structural engineering. But (most) bridges are still standing. As economics becomes more like engineering, it will become more useful, but it will not have a unified theory.\n\nThaler is being overly pessimistic - and I’m not sure that there are many theories of bridge building that can ignore the unifying framework of physics. He is right that the rational agent model is simple, elegant and powerful. The problem is that while behavioural economics can pick holes in the model on the basis of predicting how people make decisions, there has been limited attempt to generate a unified theory. Prospect theory is a useful tool for predicting behaviour, but the question that is rarely asked is why people act in that way.\nI am optimistic about the role that evolutionary biology will play in filling this gap. Evolution is the ultimate rationality machine, and any actions that are not rational will be ruthlessly eliminated. This is what lies behind the power of the rational agent model. But evolution can only work with the material at hand, leading to a constrained rationality. Heuristics that use less energy and time can be favoured. Many adaptations are path dependent (Robert Frank’s Passions Within Reason gives one excellent account of how path dependence might have shaped human emotions). A changed environment can result in decisions that were once rational no longer being optimal.\nThaler points to the multitude of theories in psychology as an example, but psychology is now being reconstructed by evolutionary psychology, with many of the available theories unable to withstand the light of evolutionary theory. Economics, and more particularly behavioural economics, is slowly being examined using evolutionary theory and the unifying basis of human decision-making as an evolved trait. Those theories inconsistent with our evolved past will be discarded, and the commonality between those that remain will provide considerable unification across the field."
  },
  {
    "objectID": "posts/a-week-of-links-10.html",
    "href": "posts/a-week-of-links-10.html",
    "title": "A week of links",
    "section": "",
    "text": "Links this week:\n\nAn excellent review of Paleofantasy.\nRoss Douthat in the New York Times on the marriage premium, and Bryan Caplan’s thoughts. When the debate popped up a year ago, I wrote this piece.\nA few months ago, Andrew Gelman wrote a blog poston Ashraf and Galor’s paper on genetic diversity and economic development(the comments are worth reading). Gelman has extended that post for publication in Chance.\nThe Society for the Evolutionary Analysis in Law (SEAL) has a few interesting speakers lined up for their annual conference next week."
  },
  {
    "objectID": "posts/a-week-of-links-101.html",
    "href": "posts/a-week-of-links-101.html",
    "title": "A week of links",
    "section": "",
    "text": "Links this week:\n\nEugenics, ready or not. A good long read.\nTort reform preventing people from suing for “weight related harms” may increase attempts to lose weight. HT: Ryan Murphy\nWhat does behavioural economics mean for income distribution? The argument ignores most the interesting subtleties, as there are questions around what the reference point is, how you could redistribute while avoiding loss frames etc., but the idea is still worth considering.\nCholesterol is OK.\nThe number of childless women in their 40s is falling, particularly among the most educated.\nIf you’re in Sydney on June 17, Rob Brooks is presenting on the price of sex.\nSome coverage of my new paper in the Daily Mail and (for those who can get through the paywall) The Times.\n\nAnd if you missed them, my posts this week:\n\nThe thinking behind my newly published paper.\nFifty years of twin studies."
  },
  {
    "objectID": "posts/a-week-of-links-103.html",
    "href": "posts/a-week-of-links-103.html",
    "title": "A week of links",
    "section": "",
    "text": "Links this week:\n\nNobody is doing more to save the NHS than the “drinkers, smokers or fatties”.\nSome bashing of the benefits of education: Did schooling drive the industrial revolution? Against tulip (education) subsidies.\nIs war on the wane?\nThe Dead Sea lives.\n\nAnd if you missed them, my posts from the last week:\n\nWhy family friendly policies backfire.\nThe winner effect in humans."
  },
  {
    "objectID": "posts/a-week-of-links-105.html",
    "href": "posts/a-week-of-links-105.html",
    "title": "A week of links",
    "section": "",
    "text": "Links this week:\n\nHighly rated doctors may not be that good. HT: Scott Alexander\nA Nobel prize for the inventor of vaping?\nVaccinated people can still spread whooping cough.\nMore complex products require elaborate networks of teamwork, and only a few places manage the trick.\nIntelligence and criminal behaviour by Joseph Schwartz and friends. No surprises here.\nSelf control and political ideology. HT: Tyler Cowen\nWhy the US can’t copy Sweden.\nNot enough studies involve blinding.\n\nAnd if you missed them, my posts from the last week:\n\nThe Evolutionary Foundations of Economics.\nPlease experiment on us.\nThe more we can send the message we have no idea, the better."
  },
  {
    "objectID": "posts/a-week-of-links-107.html",
    "href": "posts/a-week-of-links-107.html",
    "title": "A week of links",
    "section": "",
    "text": "This week, language, language models and replication:\n\nHow To Become A Mechanistic Interpretability Researcher: So much great material in here, even if you’re just interested in getting across LLM foundations.\nFrom that list, ARENA’s AI Safety course is fantastic - again, even if you are just interested in LLM foundations.\nDo Machine Learning Models Memorize or Generalize?: A great explainer on grokking.\nAfter hearing it mentioned on Dwarkesh’s podcast episode with Sholto Douglas and Trenton Bricken, I’ve been reading The Symbolic Species by Terrence Deacon. I’ve learnt a lot about language, although I must admit that my eyes glaze over (as always) during the extended discussion of brain parts. Definitely worth the read (and listen to the podcast episode too).\nOn the process and value of direct close replications: A rejoinder to Shafir and Cheek’s (2024) commentary on Chandrashekar et al. (2021): So often failures to replicate experimental results are met with a load of waffle about context, precise experimental conduct and the like. Shafir and Cheek provided one such example. Chandrashekar and Gilad Feldman provide a fantastic response."
  },
  {
    "objectID": "posts/a-week-of-links-12.html",
    "href": "posts/a-week-of-links-12.html",
    "title": "A week of links",
    "section": "",
    "text": "Links this week:\n\nE.O. Wilson’s argument that great scientists don’t need math has already received plenty of responses. Wilson’s argument reminded me of one of Paul Krugman’s critiques of Stephen Jay Gould’s popular work, which were “literary confection” as they lacked math.\nA free webinar with Geoffrey Miller and others on What Every Marketer Should Know About the Nonconscious.\nDiane Coyle posts on a paper by Sergio Da Silva in which he argues that “economics fails to ground itself in the underlying knowledge provided by biology”.\nWhile researching a paper, I came across this 20 year-old piece by Jared Diamond on the isolation and technological regress of the Tasmanian aboriginals."
  },
  {
    "objectID": "posts/a-week-of-links-14.html",
    "href": "posts/a-week-of-links-14.html",
    "title": "A week of links",
    "section": "",
    "text": "Links this week:\n\nHodgson and Knudsen have set up a reading group for their book Darwin’s Conjecture: The Search for General Principles of Social and Economic Evolution. Chapter one has already kicked off.\nAnother from The Umlaut - Conspicuous Frugality.\nFlip-flopping selection pressure in a modern population.\nFollowing from my post on Douglas Kenrick and colleagues’ theory of Deep Rationality, below are a couple of short videos - one on How Mating and Self-Protection Motives Alter Loss Aversion, and the other on the upcoming book The Rational Animal: How Evolution Made Us Smarter Than We Think by Kenrick and Vlad Griskevicius, which also looks like it covers similar territory."
  },
  {
    "objectID": "posts/a-week-of-links-16.html",
    "href": "posts/a-week-of-links-16.html",
    "title": "A week of links",
    "section": "",
    "text": "Links this week:\n\nMarshall Sahlins says goodbye to the NAS.\nA review of Jared Diamond’s The World Until Yesterday that is well worth reading (HT for these first two links: Andrew Badenoch)\nDan Dennet and memes."
  },
  {
    "objectID": "posts/a-week-of-links-18.html",
    "href": "posts/a-week-of-links-18.html",
    "title": "A week of links",
    "section": "",
    "text": "Links this week:\n\nRobert Kurzban reports on a very cool dictator game experiment. Do people only give money in experimental conditions?\nNicholas Gruen digs up a nice Alfred Marshall quote.\nPeter Turchin calls on cultural group selection to explain the transition to farming. Or maybe it’s because “people like to own stuff”.\nA take-down of the paper suggesting the Victorians were smarter than us.\nThe French fertility transition."
  },
  {
    "objectID": "posts/a-week-of-links-2.html",
    "href": "posts/a-week-of-links-2.html",
    "title": "A week of links",
    "section": "",
    "text": "Four links this week:\n\nQuamrul Ashraf and Oded Galor’s paper on genetic diversity and economic growth has been formally published in February 2013 edition of the American Economic Review (ungated working paper version here). When its release was foreshadowed a few months ago, it generated some interesting debate (such as here and here, particularly in the comments). Starting next week I’m going to write series of posts examining the threads of Ashraf and Galor’s argument.\nThe gorilla is invisible, even to radiologists.\nWas Herbert Spencer a Social Darwinist? (From late 2011, but a good read)\nStephen Corry of Survival International lays into Diamond and Pinker. (HT: Evolvify)"
  },
  {
    "objectID": "posts/a-week-of-links-21.html",
    "href": "posts/a-week-of-links-21.html",
    "title": "A week of links",
    "section": "",
    "text": "Links this week:\n\nThere have been some strong reactions to the Edge piece on Napoleon Chagnon (I pointed it out last week, and if you haven’t yet, give it a look). Some are entertaining, although most are not particularly enlightening. Jason Anstrosio goes the epigenetics maneuver and Stephen Corry pins death rates among ancient hunters on hunting accidents.\nIf you’re risk averse, don’t do science - become an actuary.\nThe Faroe Islands Health Ministry plans to genetically sequence everyone on the island who wants it.\nOverview of the Flynn effect (unfortunately gated if you don’t have journal access).\nGeorge Monbiot on rewilding (audio download)"
  },
  {
    "objectID": "posts/a-week-of-links-23.html",
    "href": "posts/a-week-of-links-23.html",
    "title": "A week of links",
    "section": "",
    "text": "Links this week:\n\nIf you read one thing this week, read this article by Enrico Spolaore and Romain Wacziarg: How Deep Are the Roots of Economic Development? (ungated version here). I’ll post in more detail on the article in a couple of weeks.\nNoah Smith provides a list of essential papers in behavioural finance.\nCannabis and IQ.\nMy newest working paper has just gone online: Population, Technological Progress and the Evolution of Innovative Potential. I’ll post about it sometime next week. [Post is now up here]"
  },
  {
    "objectID": "posts/a-week-of-links-25.html",
    "href": "posts/a-week-of-links-25.html",
    "title": "A week of links",
    "section": "",
    "text": "Links this week:\n\nEric Falkenstein on Stevenson and Wolfers’s happiness research. “When an economist tells you a symmetric ovoid contains a highly significant trend via the power of statistics, don’t believe them”.\nAustralia’s Productivity Commission has fingered genetics as a cause of differences in educational attainment. It’s healthy that ideas such as this are starting to be mentioned in serious discussion (although the Commission still treats the issue as though it is walking on eggshells).\nRory Sutherland suggests we need more Darwinists, fewer economists. Read the last paragraph.\nDiane Coyle reviews An Economic Theory of Greed, Love, Groups and Networks. I’ve started reading it and will review myself in the next few weeks.\nThe UK’s National Institute for Health and Care Excellence recommends adjusting BMI recommendations by ethnicity."
  },
  {
    "objectID": "posts/a-week-of-links-27.html",
    "href": "posts/a-week-of-links-27.html",
    "title": "A week of links",
    "section": "",
    "text": "Links this week:\n\nBrandon Keim discusses a new Science paper by Douglas Fry and Patrik Söderberg questioning how warlike human nature is. My two cents: a war to personal violence ratio is a poor way to look at this. If we interpret the ratio in the other direction, we could say that human nature inclines us to high rates of interpersonal violence. I’d prefer examination of baseline rates.\nLarry Arnhart has continued his series of posts on the Mont Pelerin Society Meeting in the Galápagos. Two posts of note: Leda Cosmides and John Tooby on liberalism and mismatch; and Richard Wrangham on the evolution of war (the Wrangham post directly addresses the Science paper linked above).\nNicholas Christakis proposes a shake up of the social sciences. Andrew Gelman responds, Christakis comments and Gelman responds again.\nPaul Frijters goes on a rant on magical explanations for the rise of obesity.\nBaba Brinkman schools Jeremy Yoder."
  },
  {
    "objectID": "posts/a-week-of-links-29.html",
    "href": "posts/a-week-of-links-29.html",
    "title": "A week of links",
    "section": "",
    "text": "Links this week:\n\nAaron Sell rants about some recent papers on whether there are sex differences in the willingness to have casual sex.\nRobert Kurzban (who I seem to be linking to a lot recently) posts on one of my favourite Gerd Gigerenzer papers.\nAlex Tabarrok comments on fat animals.\n“Derek” fathers over 500 children filling in for shellshocked husbands.\nDo we need more pharmaceutical advertising to enhance the placebo effect?\nFinally, after catching some tweets about Iain Couzin’s talk at Behaviour 2013, I was perusing his website and found a lot of interesting bits and pieces, including the below video. I’m convinced that work on herding and swarming has a lot to offer economics (as Andrew Oswald argues).\n\nhttp://video.wnyc.org/radiolab/thegreenespace20100414_RadiolabIC.flv"
  },
  {
    "objectID": "posts/a-week-of-links-30.html",
    "href": "posts/a-week-of-links-30.html",
    "title": "A week of links",
    "section": "",
    "text": "Links this week:\n\nSean Roberts on spurious correlations over at Replicated Typo.\nSome babble on the neuroscience bubble.\nWhy do Jews succeed? Noah Smith throws some reasons out there, but I expect Smith’s explanations would do a poor job of explaining ultra-success in the form of, say, Nobel prizes. And here’s Noah Millman’s take.\nAnd finally, below, an interview with Leda Cosmides (HT: Douglas Kenrick).\n\nhttp://youtu.be/UpIlIkzoZCo"
  },
  {
    "objectID": "posts/a-week-of-links-32.html",
    "href": "posts/a-week-of-links-32.html",
    "title": "A week of links",
    "section": "",
    "text": "Links this week:\n\nPeter Turchin on Ibn Khaldun and the rise and decline of corporate empires.\nAnother critique of p-values.\nJohn McNamara proposes that we need to move to a richer evolutionary game theory. The article focuses on biology, but many of the same comments could be made of economics. I’ll post on the article in coming weeks.\nTyler Cowen on scarcity of mental bandwidth. And another take by Emily Badger.\nThe ongoing Evolution: This View of Life series of interviews On the Origins of Human Behavior And Evolution Society moves to William Irons.\n\nhttp://youtu.be/HgVHdu546AY"
  },
  {
    "objectID": "posts/a-week-of-links-34.html",
    "href": "posts/a-week-of-links-34.html",
    "title": "A week of links",
    "section": "",
    "text": "Links this week:\n\nPaul Frijters on race and IQ (also read the comments). And if you want some perspective on the epigenetic undertone to that post, Kevin Mitchell’s piece from the beginning of the year is worth a read.\nAn article that got plenty of press - how heritable is IQ for people of low SES? The Wall Street Journal comments. There is an excellent discussion of the paper in the comments (including from one of the paper’s authors) over at Information Processing.\nPeter Turchin and friends have a new paper out in PNAS on the transition from small-scale societies to today’s massive, complex and largely anonymous societies. Turchin blogs on the paper (and provides a Q&A).\nIf you are overwhelmed by the book offerings during peak-PaleoTM, Daniel Lieberman’s forthcoming book, The Story of the Human Body: Evolution, Health, and Disease is probably a good one to put towards the top of your reading pile. The Guardian asks him a couple of questions."
  },
  {
    "objectID": "posts/a-week-of-links-36.html",
    "href": "posts/a-week-of-links-36.html",
    "title": "A week of links",
    "section": "",
    "text": "Links this week:\n\nTalks for the Darwinian Business conference have been posted online.\nA great interview with Richard Thaler.\nTaboo genetics.\nRobert Kurzban and Rob Brooks on religion and cooperation.\nThe Gladwell debates continue.Chabris writes a second piece. Gladwell responds. Andrew Gelman weighs in."
  },
  {
    "objectID": "posts/a-week-of-links-38.html",
    "href": "posts/a-week-of-links-38.html",
    "title": "A week of links",
    "section": "",
    "text": "Links this week:\n\nCarl Zimmer and Razib Khan on sequencing your kids.\nJohn Hawks on the Dmanisi skull.\nThe Economist asks whether science is self-correcting.\nDo hunter-gatherers exhibit an endowment effect?\nA review of Worldly Philosopher: The Odyssey of Albert O. Hirschman. (HT: Diane Coyle)"
  },
  {
    "objectID": "posts/a-week-of-links-4.html",
    "href": "posts/a-week-of-links-4.html",
    "title": "A week of links",
    "section": "",
    "text": "Links this week:\n\nMatt Zimmerman of Biased Transmission reviews Ashraf and Galor’s theory of genetic diversity and economic development. He points out that the hypothesis can explain any observed pattern in the data. I recommend subscribing to Matt’s feed.\nJason Antrosio takes on Jared Diamond’s argumentsabout violence in hunter-gatherer societies. An excellent read.\nDavid Sloan Wilson provides another critique of a straw man version of the invisible hand. The interesting aspect of this critique, as for many other of Wilson’s takes on economics, is that the group selection framework he wants to bring into economics doesn’t even have the evolutionary biologists onside.\nGeoffrey Miller presents on sexual selection and runway consumerism.\nThe Santa Fe Institute MOOC on complexity has kicked off."
  },
  {
    "objectID": "posts/a-week-of-links-41.html",
    "href": "posts/a-week-of-links-41.html",
    "title": "A week of links",
    "section": "",
    "text": "Links this week:\n\nBiology, behaviour and obesity.\nEric Crampton on the heritability of political preferences.\nPolywater (HT: Joe Pickrell).\nMatt Zwolinski defends the morality of markets.\nJason Potts on funding the arts.\n\nI’m going to be away in the Malay Archipelago the next two weeks. I’ve scheduled some old posts (with accompanying promotional tweet) from my early blogging days for while I am away, but otherwise, it will be electronic silence from me."
  },
  {
    "objectID": "posts/a-week-of-links-43.html",
    "href": "posts/a-week-of-links-43.html",
    "title": "A week of links",
    "section": "",
    "text": "Links this week:\n\nBarry Kuhle posts about On the origin of Human Behavior & Evolution Society, an oral history project. Part I and Part II. Lots of great interviews in each post.\nDavid Dobbs tweaks his critique of the selfish gene.\nMark Buchanan takes on the idea of “learning” in the economics literature."
  },
  {
    "objectID": "posts/a-week-of-links-45.html",
    "href": "posts/a-week-of-links-45.html",
    "title": "A week of links",
    "section": "",
    "text": "Links this week (or two):\n\nThis Is Your Brain on Gluten. A good paleo related read.\nEpigenetics and the root of aggressive behaviour.\nA new book coming from Gregory Clark - The Son Also Rises: Surnames and the History of Social Mobility (And another Hemingway pun title).\nThe Evolution Institute’s annual report. Full of interesting links."
  },
  {
    "objectID": "posts/a-week-of-links-47.html",
    "href": "posts/a-week-of-links-47.html",
    "title": "A week of links",
    "section": "",
    "text": "Links this week:\n\nA cool idea - people who are genealogical ancestors of everyone alive but genetic ancestors of none (HT: Joe Pickrell).\nPhilip Ball on how the apparently irrational can be rational.\nJohn Kay on the economic approach - there is no such thing.\nGenetically modified chickens.\nThe annual Edge question is out, this time “What Scientific Idea is Ready for Retirement?” Of those answers written about areas I am familiar with or interested in, the question has effectively been “What Scientific Idea Don’t You Like?” As a result, we get the latest play in old debates on race, IQ, the limits to growth (Ridley, Hidalgo and  Obrist), inclusive fitness, gene-environment interactions (Pinker and Sapolsky among others ), epigenetics, rationality, homo economicus and culture (Betzig, Richerson and Tooby). Some are framed in interesting ways (I like Sapolsky’s approach), but there are few surprises. I found more value in the answers that addressed approaches to science (such as Richard Thaler, Nicholas Christakis and Samuel Arbesman).\nAnd finally, surfing the US-Mexico border fence (HT: Michael Clemens)."
  },
  {
    "objectID": "posts/a-week-of-links-49.html",
    "href": "posts/a-week-of-links-49.html",
    "title": "A week of links",
    "section": "",
    "text": "Links this week:\n\nGreg Clark on long-term social mobility.\nCameron Murray on his reading list - Tribes, Gods, Indeterminancy, Property, Capitalism\nAssortative mating drives income inequality.\nThe behavioural science of sleep.\nThe Cooperation and Conflict in the Family Conference kicks off today. Rob Brooks posts."
  },
  {
    "objectID": "posts/a-week-of-links-50.html",
    "href": "posts/a-week-of-links-50.html",
    "title": "A week of links",
    "section": "",
    "text": "Links this week:\n\nMatt Ridley on inequality.\nInequality and assortative mating.\nThe sloth’s algae farm.\nThe work of Suzanne Scotchmer (also here).\nAnd last, the Volcom Pipe Pro is consistently the best contest each year outside the World Tour. This year’s final day highlights:"
  },
  {
    "objectID": "posts/a-week-of-links-52.html",
    "href": "posts/a-week-of-links-52.html",
    "title": "A week of links",
    "section": "",
    "text": "Again, closer to a month of links:\n\nA great set of essays triggered by David Dobbs’s assault on the selfish gene.\nTim Harford on big data. His piece on behavioural economics is also worth reading. Take the hype with a grain of salt.\nThe Greg Clark show continues - an interview on Social Science bites and a presentation at the RSA.\nA good long-read on de-extinction.\nFree-range kids."
  },
  {
    "objectID": "posts/a-week-of-links-55.html",
    "href": "posts/a-week-of-links-55.html",
    "title": "A week of links",
    "section": "",
    "text": "Links this week:\n\nThere are plenty of reviews of Nicholas Wade’s new book, A Troublesome Inheritance: Genes, Race and Human History. Robert VerBruggen’s is one of the more interesting. We’ll be throwing a lot of social science under the bus if we apply Andrew Gelman’s filter more generally.\nGelman again, this time on poor research in evolutionary psychology. I agree with both him and Pinker here.\nA review of Think Like a Freak. From the excerpts I have seen, it seems that Levitt and Dubner are running into the same problem as a lot of the behavioural economics literature - there are only so many “funky” stories to go around.\nCan a single gene explain 3 per cent of the variation in IQ? I don’t think so, but interesting none the less."
  },
  {
    "objectID": "posts/a-week-of-links-57.html",
    "href": "posts/a-week-of-links-57.html",
    "title": "A week of links",
    "section": "",
    "text": "Links this week:\n\nSome economist bashing - first from Mark Buchanan, who wonders why economists wheel out “whacky” versions of what they know in public debate, and second, from Tim Harford, on an astonishing record of complete failure.\nHerb Gintis reviews Complexity and the Art of Public Policy: Solving Society’s Problems from the Bottom Up (HT: Arnold Kling)\nWe can’t ignore the evidence: genes affect social mobility.\nThe costs of climate change - some corrections.\nSteven Pinker on writing."
  },
  {
    "objectID": "posts/a-week-of-links-59.html",
    "href": "posts/a-week-of-links-59.html",
    "title": "A week of links",
    "section": "",
    "text": "Links this week:\n\nWhy idiots succeed.\nRory Sutherland on social norms.\nEconomics incentives versus nudge (pdf). Don’t forget that basic economic mechanisms can work.\nWe’re related to our friends.\nAre there really trillion dollar bills on the sidewalk?\nA bash of the Myers-Briggs test. Personally, I’m a fan of the big five plus g. On g, the heritability of chimp IQ.\nTalent versus practice. Talent wins this one.\nThrowing away money on brain science."
  },
  {
    "objectID": "posts/a-week-of-links-60.html",
    "href": "posts/a-week-of-links-60.html",
    "title": "A week of links",
    "section": "",
    "text": "Links this week:\n\nDetecting irrational exuberance in the brain - neuroeconomists confirm Warren Buffett’s wisdom (original article here).\nSpouses are more genetically similar than people chosen at random, but they are far more similar in education (ungated pdf).\nA well established fact, but further evidence that impatient adolescents do worse later in life.\nHomo Oeconomicus Versus Homo Socialis - an interesting looking conference at ETHZ.\nEvolution in the social sciences - a special issue of PNAS."
  },
  {
    "objectID": "posts/a-week-of-links-62.html",
    "href": "posts/a-week-of-links-62.html",
    "title": "A week of links",
    "section": "",
    "text": "Links this week:\n\nAcademic urban legends spreading through sloppy citation. In PhD land, I have constantly found myself following citation chains that don’t lead to what they claim.\nSome progress in the replication wars. I’ll post about some of the specific examples over coming months.\nThe evolutionary emergence of property rights (ungated working paper). HT: Ben Southwood\nAttribute substitution in charities - the evaluability bias. HT: Alex Gyani\nPeter Turchin reviews Richard Wrangham’s Catching Fire: How Cooking Made Us Human.\nArnold Kling on Nicholas Wade. Comments and pointer from here.\nPolygenic modelling in cattle breeding. Humans next.\nAn interesting debate on Cato Unbound this month - the libertarian case for a basic income guarantee."
  },
  {
    "objectID": "posts/a-week-of-links-64.html",
    "href": "posts/a-week-of-links-64.html",
    "title": "A week of links",
    "section": "",
    "text": "Links this week:\n\nSide effect warnings increase sales by building trust.Similar effects for disclosing conflicts of interest (ungated pdf).\nAbsorbing information on paper versus kindle. Even without digital search, I often find it easier to find favourite passages in the physical form.\nHumans aren’t the only ones fighting wars.\nI pointed out a couple of weeks ago that Geoffrey Miller had joined forces with Tucker Max to give sex and dating advice. Their reading list is very good, even if you’re not after any advice. Their suggestions as to which movies might provide insight is quite amusing.\nTwin research."
  },
  {
    "objectID": "posts/a-week-of-links-66.html",
    "href": "posts/a-week-of-links-66.html",
    "title": "A week of links",
    "section": "",
    "text": "Links this week:\n\nTwo pieces on diet. First, an excellent article on how the poisons in vegetables might be making you stronger. Second, a new study in the fat-carb wars.\nAndrew Gelman on the strength of statistical evidence.\nTwo excellent podcasts. Gregory Clark on social mobility (and the genetics behind it) and Paul Sabin on the Simon-Ehrlich bet. Some of my thoughts on Julian Simon are here and here.\nEconomists are happier. The reasons? More cash and religion."
  },
  {
    "objectID": "posts/a-week-of-links-68.html",
    "href": "posts/a-week-of-links-68.html",
    "title": "A week of links",
    "section": "",
    "text": "Links this week:\n\nThe Genetic Genealogist responds to Vox’s tabloid piece on genetic testing.\nAttempts to correct false claims often entrench them - the backfire effect. But telling politicians they will be fact checked still reduced their number of lies. [Update: There is probably no backfire effect.]\nTyler Cowen suggests the gender gap will close. I’m not so sure.\nViolence in chimps an evolutionary adaptation.\nAnother study showing low social mobility.\nA great collection of papers on altruism, reciprocity and the glucose model of self control."
  },
  {
    "objectID": "posts/a-week-of-links-7.html",
    "href": "posts/a-week-of-links-7.html",
    "title": "A week of links",
    "section": "",
    "text": "Links this week:\n\nAndrew Berry guest blogs at Why Evolution is True on Alfred Russel Wallace’s unfortunate end to his Amazon expedition. The equivalent of accidentally erasing your PhD thesis the day before submission?\nI am enjoying some of the press coming out about Marlene Zuk’s new book Paleofantasy: What Evolution Really Tells Us about Sex, Diet, and How We Live (and the reactions to it); this week an interview in the Los Angeles Review of Books.\nAn interview with Gary Becker on rationality, behavioural economics and the use of mathematics in economics.\nA profile of Ingela Alger, another economist looking to bring some evolutionary biology into the picture."
  },
  {
    "objectID": "posts/a-week-of-links-71.html",
    "href": "posts/a-week-of-links-71.html",
    "title": "A week of links",
    "section": "",
    "text": "Links this week:\n\nPlenty of press and interesting articles sparked by Peter Thiel’s new book. First, he has a swipe at business schools. And some great one-liners. But is he wrong about the future?\nAnother tech-billionaire - Elon Musk wants to put people of Mars.\nEric Crampton has some great posts this week on public health. First, where should the money be going?  Some thoughts on soda taxes and fat taxes. And drinking when pregnant.\nCameron Murray risks walking onto Steven Landsburg’s lawn.\nRajiv Sethi defends agent based models from Chris House. House tends to overreach when he strolls into the unfamiliar and attacks the heterodox rather than his standard (and also not overly convincing) defense of the orthodox.\nPut your laptops away kids.\nThe missing heritability puzzle is slowing being chipped away. But the genetic post-modernists continue their losing battle.\nThe heritability of educational attainment reflects many genetically influenced traits, not just intelligence. A Science Daily summary. Plus, emotional intelligence is overrated (HT: Stuart Ritchie). Intelligence is important, and to the extent other traits matter, they are heritable too.\nDoes evolutionary theory need a rethink? No.\nDoctor decision fatigue - more unnecessary antibiotics in the afternoon."
  },
  {
    "objectID": "posts/a-week-of-links-73.html",
    "href": "posts/a-week-of-links-73.html",
    "title": "A week of links",
    "section": "",
    "text": "Links this week:\n\nCooperation in humans versus apes.\nIn praise of pilots.\nAre women better decision makers?\nAmazon is doing us a favour. Goodbye book publishers.\nThe logic of failure.\nThe Behavioural Insights Team has lunch with Walter Mischel. Mischel’s work is fantastic and his new book is on my reading list, but the mention of brain plasticity and epigenetics (in the same sentence!) has reduced my expectations."
  },
  {
    "objectID": "posts/a-week-of-links-75.html",
    "href": "posts/a-week-of-links-75.html",
    "title": "A week of links",
    "section": "",
    "text": "Links this week:\n\nThe freedom to pursue informed self-harm has a long and noble tradition.\nWhat happens when behavioural economics is used to explain rational behaviour.\nA great summary of some of Gordon Tullock’s work. HT: Garett Jones\nAnother study on the limited effect of parenting on IQ. HT: Billarevia Stuart Ritchie\nWhat Hayek might say to Republicans.\nThe long shadow of history on the distribution of human capital in Europe. HT: Ben Southwood\nOpposition to urban development by “environmentalists” is among my bigger gripes. Left-leaning cities are less affordable.\nI have only just come across Dominic Cummings. Some interesting thoughts. Check out his blog.\nHow your brain decides without you."
  },
  {
    "objectID": "posts/a-week-of-links-77.html",
    "href": "posts/a-week-of-links-77.html",
    "title": "A week of links",
    "section": "",
    "text": "Links this week:\n\nBig ideas are destroying international development. Dream smaller.\nAppealing to my biases - the skeptics guide to institutions Part 1 and Part 2.\nMost published results in finance are false.\nBe mean, look smarter.\nConstructing illusions.\nPredicting complex genotypes from genomic data - for those who confuse these two statements:\n\n\n\n“The brain is complex and nonlinear and many genes interact in its construction and operation.”\n\n\n“Differences in brain performance between two individuals of the same species must be due to nonlinear (non-additive) effects of genes.”"
  },
  {
    "objectID": "posts/a-week-of-links-79.html",
    "href": "posts/a-week-of-links-79.html",
    "title": "A week of links",
    "section": "",
    "text": "Links this week:\n\nThe case against early cancer detection. The charts on mammogram and PSA testing effectiveness are just as Gerd Gigerenzer would have us present the statistics.\nThe case for business experimentation.\nAirline inequality.\nWhile arguments continue about the predictive power of genetic testing, entrepreneurs are already using it. HT: Steve Hsu\nHowever, there are still plenty of average ‘gene for’ studies being produced. HT: Tim Frayling"
  },
  {
    "objectID": "posts/a-week-of-links-80.html",
    "href": "posts/a-week-of-links-80.html",
    "title": "A week of links",
    "section": "",
    "text": "Links this week:\n\nWhy progress has ground to a halt.\nDevelopment professionals are biased like their subjects.\nAre schools failing?\nCato Unbound - a libertarian perspective on extraterrestrial life."
  },
  {
    "objectID": "posts/a-week-of-links-82.html",
    "href": "posts/a-week-of-links-82.html",
    "title": "A week of links",
    "section": "",
    "text": "Links this week:\n\nLectures on Human Capital by Gary Becker. (HT: Eric Crampton)\nWe learn more from success than failure. A bit semantic, and where is this government agency with a learn-from-failure culture? But worth the read.\nThe 2014 Nanny State awards.\nRobert Sapolsky on the Christmas truce of 1914.\nPeople like gifts that they want.\nGreater contact between racial groups increases bias? (HT: Razib Khan)\nAnother arena where a bit more science might help - the justice system.\nThe Accidental Lobster Farmers (HT: Tyler Cowen).\nNothing like an article confirming my priors - LaTeX is a productivity sink (HT: Alex Tabarrok). A common comment I receive from economists on my papers is “Did you do this in Word?” The funny thing with LaTeX is that most people don’t change the default font - if you did that, people might not know it was prepared with LaTeX."
  },
  {
    "objectID": "posts/a-week-of-links-84.html",
    "href": "posts/a-week-of-links-84.html",
    "title": "A week of links",
    "section": "",
    "text": "Links this week:\n\nArnold Kling’s review of Complexity and the Art of Public Policy.\nAre some diets mass murder? HT: Eric Crampton\n“Social conservatism correlates with lower cognitive ability test scores, but economic conservatism correlates with higher scores.”\nMore on lead and crime.\nA risk averse culture. HT: Eric Crampton\nWelfare conditional on birth control.\n“We may regret the eclipse of a world where 6,000 different languages were spoken as opposed to just 600, but there is a silver lining in the fact that ever more people will be able to communicate in one language that they use alongside their native one.” HT: Steve Stewart Williams\nIf you want to feel older, read this. HT: Rory Sutherland"
  },
  {
    "objectID": "posts/a-week-of-links-86.html",
    "href": "posts/a-week-of-links-86.html",
    "title": "A week of links",
    "section": "",
    "text": "Links this week:\n\nDoes public policy promote obesity? This month’s Cato Unbound on whether public policy can stop obesity could be interesting when the discussion begins, but the response essays so far have generally talked past each other.\nThree links via Tyler Cowen. New cars fake their engine noise. People turn down high-cost low-value treatments when they can pocket part of the savings. The right won the economics debate; left and right are just haggling over details.\nThe Dunning-Kruger Peak of Advertising.\nChickens prefer beautiful humans. So much for the subjectivity of beauty.\nDefault retirement savings in Illinois."
  },
  {
    "objectID": "posts/a-week-of-links-88.html",
    "href": "posts/a-week-of-links-88.html",
    "title": "A week of links",
    "section": "",
    "text": "Links this week:\n\nEach of us descends many times over from a great many sexual despots.\nIn every generation, we forget how much poorer we used to be.\nRegression and other related non-experimental pattern-finding methods of this type can sound hyper-technical and very gee-whiz (“support vector machines” – cool!), and they can serve various useful purposes. … But they are simply not fit for the task of making reliable, non-obvious predictions for the effects of most contested policy interventions.\nReinterpreting Stanley Milgram’s famous experiment.\nEndogenous preferences.\nCognitive vs. behavioral in psychology, economics, and political science."
  },
  {
    "objectID": "posts/a-week-of-links-9.html",
    "href": "posts/a-week-of-links-9.html",
    "title": "A week of links",
    "section": "",
    "text": "Links this week:\n\nEd Yong on Swarms.\nThe New York Times on whether the decline of two-parent households has caused the decline in male incomes. It still astounds me that some academics can discuss this without mentioning selection effects.\nA critical review by Miki Ben-Dor of Marlene Zuk’s Paleofantasy. On the flipside, Christina Warinner “Debunking the paleo diet”."
  },
  {
    "objectID": "posts/a-week-of-links-91.html",
    "href": "posts/a-week-of-links-91.html",
    "title": "A week of links",
    "section": "",
    "text": "Links this week:\n\nThe Lancet’s obesity predictions.\nDesign things to be difficult. HT: Rory Sutherland\nIs there any known safe level of government funding?\nIncreasing diversity by hiring groups, not individuals.\nPlenty of critiques of nudge-style interventions popping up, although they are rarely done well. Here’s another. And what is a nudge?\nA perspective on consumer genomics.\nWealth heritability.\nEdging toward the right answer.\nWhy it is so much easier to data crunch sport than economics.\n\nAnd if you missed them, my posts this week:\n\nTolstoy, behavioural scientist.\nThe left and heritability."
  },
  {
    "objectID": "posts/a-week-of-links-93.html",
    "href": "posts/a-week-of-links-93.html",
    "title": "A week of links",
    "section": "",
    "text": "Links this week:\n\nA review of Baumeister and Tierney’s Willpower by Scott Alexander (I don’t buy the comparison between exercise, money and willpower - this comment sums up my view).\nWhen we act as though all opinions are equal.\nSome evidence on whether we should be inducing the best and brightest into teaching. (HT: Arnold Kling)\nOur tolerance of inequality is reference dependent.\nDeath penalty eugenics.\n\nAnd if you missed them, my posts this week:\n\nBoys are falling behind girls in school.\nResearch on the heritability of savings behaviour."
  },
  {
    "objectID": "posts/a-week-of-links-95.html",
    "href": "posts/a-week-of-links-95.html",
    "title": "A week of links",
    "section": "",
    "text": "Links this week:\n\n“If compulsory voting were to help Democrats at all, it would probably help the bad Democrats. The Democrats would end up running and electing more intolerant, innumerate, hawkish candidates.”\nThe management / bureaucratic speak of World Bank reports. It’s worth clicking through to the full article.\nPaul Meehl was talking about today’s problems in psychology 30 years ago.\nThe problems of financially strapped Americans are not caused by private jets and billionaires buying islands.\nDoes adoption increase IQ?\nAn attempt to pull apart the recent breastfeeding study, but still no mention of genetics.\nAccessing doctor or lawyer track records.\nMight GMO labelling backfire? HT: Ryan Murphy. My guess is that if people avoided foods containing GMOs, their health would improve in the short term by decreasing their consumption of processed foods. In the longer-term, as GM fruit and vegetables start to become prevalent, I am not so sure.\nRelatednesss and eusociality. HT: Stuart West\n\nAnd if you missed them, my posts this week:\n\nThe Gell-Mann amnesia effect\nGiving behavioural economics an evolutionary perspective."
  },
  {
    "objectID": "posts/a-week-of-links-97.html",
    "href": "posts/a-week-of-links-97.html",
    "title": "A week of links",
    "section": "",
    "text": "Links this week:\n\nBehavioral Public Choice: The Behavioral Paradox of Government Policy. HT: Ryan Murphy\nHappiness and growth.\nThe genetic component of sex offending.\n“[I]is growth mindset the one concept in psychology which throws up gigantic effect sizes and always works? Or did Carol Dweck really, honest-to-goodness, make a pact with the Devil in which she offered her eternal soul in exchange for spectacular study results?”\nThe weird belief that people follow dietary guidelines. A question - to what extent do food manufacturers respond to the guidelines, especially to earn “heart smart” certifications and the like?\nEconomics melts the brain. One alternative which I’ve often seen is, because the model assumptions simply don’t work, they throw every bit of economics they’ve ever learnt out the window and revert to storytelling.\n\nAnd if you missed it, my one post this week:\n\nPredicting replication.\n\nAnd a blast from the past: Why isn’t economics evolutionary?"
  },
  {
    "objectID": "posts/a-week-of-links-99.html",
    "href": "posts/a-week-of-links-99.html",
    "title": "A week of links",
    "section": "",
    "text": "Links this week:\n\nOn the misplaced politics of behavioral policy interventions. And hawkish biases.\nNoah Smith v Bryan Caplan on education signalling - 1, 2 and 3. I believe signalling is an important part of the education story, but Smith’s argument about costly signalling is on point.\n\nAnd if you missed it, my one post this last week:\n\nBad nudges toward organ donation.\n\nLife continues to be busy, so posting will continue to be sparse."
  },
  {
    "objectID": "posts/absolute-improvement.html",
    "href": "posts/absolute-improvement.html",
    "title": "Absolute improvement",
    "section": "",
    "text": "Fernando Teson writes:\n\nYet, outside the rarified circles of political philosophy journals, I haven’t heard many folks ask two other important questions about the President’s approach.  Yet these questions are, to me, obvious.\nFirst, why should reducing income equality be a worthy goal? If we are concerned with the poor, then we should focus (as Rawls famously does) in improving their lot in absolute terms, regardless of the effect of such improvement on the gap between them and the rich. Again, this is common currency in academic circles, but I don’t hear anyone in our public debate making the point.\n\nOne reason why the focus is not on improving the absolute income of the poor is that income is not the sole objective of those at the bottom.\nTo make a point I have often made before, if someone’s aim is to attract a mate, a slight increase in absolute income will not assist low-status males in achieving this objective if massive increases at the top allow high-status males to dominate the mating market (or to cause high-status women to price themselves too high).\nIf we accept that people have objectives beyond absolute income, the range of policy considerations becomes more interesting. Take prohibitions against polygamy, whose primary beneficiary (particularly before the emergence of the welfare state) was low-status males. Compulsory child-support payments reduce the benefits for a woman of partnering with a low-status male. The terms of the policy debate about inequality do not have to be primarily about income.\nThat is not to say that increasing absolute income does not matter. It does. It is just that income inequality plays out in other spheres that people care about - and increasing absolute income provides no guarantee of addressing those."
  },
  {
    "objectID": "posts/accepting-heritability.html",
    "href": "posts/accepting-heritability.html",
    "title": "Accepting heritability",
    "section": "",
    "text": "At Stumbling and Mumbling, Chris Dillow writes:\n\n[M]aybe some lefties do reject the heritability of IQ on ideological grounds. I want to make another point - that there’s no need for them to do so. You can accept that IQ (or ability generally) is heritable and still be a strong egalitarian.\nI say this because of a simple principle: luck egalitarianism. This says that inequalities are unjust if they are due to circumstances beyond one’s control. If we grant that ability is inherited, then differences in ability are obviously a matter of luck. Insofar as these give rise to inequalities of income, a luck egalitarian can thus claim they are unjust.\n…\nThat said, there is a sort of leftie who would be discombobulated by the heritability of ability. I’m thinking of that sort, like Tessa Jowell, who - in their optimism about the malleability of humankind - think that education can significantly reduce inequality.\nBut that leftism isn’t mine. I agree with Ed Smith that social mobility - even if it could be achieved - is an unattractive ideal. It’s no substitute for a just society.\n\nPeter Singer made a related argument in A Darwinian Left: Politics, Evolution, and Cooperation, suggesting that the left needs to incorporate an updated understanding of the malleability of human nature into its framework - although Singer’s arguments focused on our tendency to cooperate.\nArnold Kling suggests the discombobulation of some on the left comes from the need to maintain a narrative:\n\nIn the three-axes model, progressives want to squeeze every issue into an oppressor-oppressed narrative. To suggest that ethnic groups differ in average income for reasons other than oppression would be to weaken that narrative. So even if from a policy perspective a belief in heritability is tolerable, from a narrative perspective a book like The Bell Curve represents a huge threat.\nMy sense is that this produces a great deal of cognitive dissonance on the left. I have many friends on the left, and I do not know a single one who would instinctively deny the heritability of intelligence. On the other hand, they have been instructed to regard Murray and Herrnstein as vile racists.\n\nMy own experience is that plenty of people are willing to argue whether behavioural traits are heritable. I sense Kling’s narrative story is part of the reason, but I also suggest that it comes from a general unwillingness of people to concede any points in a debate. (Does this “bias” have a name - or is this just a manifestation of confirmation bias or a desire to reduce cognitive dissonance?)\nTake arguments about climate change. Many libertarians or conservatives fight at every step of the way - the earth is not warming, the warming is not caused by human activity, the warming will be mild, the warming will be beneficial - all this before they get to arguments about the costs and benefits of different policy responses. Yet, whether warming is occurring or harmful would not seem to be a core part of the libertarian philosophy. Debates about heritability have a similar character."
  },
  {
    "objectID": "posts/agriculture-and-population-growth.html",
    "href": "posts/agriculture-and-population-growth.html",
    "title": "Agriculture and population growth",
    "section": "",
    "text": "Over the last few months, I have heard the phrase “agriculture creates excess population” or other words to that effect from several sources. The latest is at Evolvify, where Andrew references Richard Manning and writes:\n\nAgriculture creates excess population. The argument that we need more agriculture to support higher population fails to recognize its inherently circular nature.\n\nWhile I have some sympathy to the argument about the destructiveness of agriculture for the ecosystems it supplants, I would prefer to frame the argument differently.\nIf we take the Malthusian model as a description of human history, for most of that history populations were at subsistence level. The constraint on population was the level of technology. Improved technology did not increase living standards as population would simply increase to match the rise in technology (making population density a crude measure of technology). Some populations managed to briefly have higher living standards by imposing society-wide checks on fertility, or through higher death rates due to disease or violence, but subsistence was the norm.\nThus, in hunter-gatherer societies, population was constrained by the technologies available to them. A technology that allowed more game to be caught may briefly raise living standards, but population would soon increase to take advantage of the additional resources. Population could also increase where new land was entered, such as the entry of humans into the Americas 12,000 years ago.\nWith the advent of agriculture, the new technology allowed even higher populations. However, up to the 18th century, population generally grew in line with technology and most of the population remained at subsistence levels. New land would at times be opened up to agriculture with accompanying population growth, such as with the European settlement of the Americas, but the Malthusian constraint remained.\nThus, it is not agriculture in itself that creates excess population. The very nature of the Malthusian state - which was the state of human populations for most of their history - is excess population.\nThen, around the time of the Industrial Revolution, incomes started to grow faster than population. Populations where this occurred were now able to obtain incomes above subsistence. The twist in the tail was that those with higher incomes lowered their fertility, allowing per person income to grow even faster. So, although population has grown quickly since the Industrial Revolution and on the back of agriculture, it has not grown as fast as the loosened Malthusian constraints would allow. In that sense, there is not overpopulation. We could even argue by this measure that many parts of the world have never been less crowded.\nOne obvious response is to ask whether the current use of land for agriculture is destroying future capital. Is agricultural productivity ephemeral, as today’s income is coming at the cost of income at the future? In that scenario, it could be argued that there is excess population, but the current population is able to temporarily ward off the Malthusian constraint at the cost of future populations. Even if this were the case, however, I would prefer to frame that argument in terms of the nature of the technology than in terms of “excess population”. A state of excess population is the norm, not a particular result of agriculture or any other technology of the day."
  },
  {
    "objectID": "posts/alters-irresistible-why-we-cant-stop-checking-scrolling-clicking-and-watching.html",
    "href": "posts/alters-irresistible-why-we-cant-stop-checking-scrolling-clicking-and-watching.html",
    "title": "Adam Alter’s Irresistible: Why We Can’t Stop Checking, Scrolling, Clicking and Watching",
    "section": "",
    "text": "I have a lot of sympathy for Adam Alter’s case in Irresistible: Why We Can’t Stop Checking, Scrolling, Clicking and Watching. Despite the abundant benefits of being online, the hours I have burnt over the last 20 years through aimless internet wandering and social media engagement could easily have delivered a book or another PhD.\nIt’s unsurprising that we are surrounded by addictive tech. Game, website and app designers are all designing their products to gain and hold our attention. In particular, the tools at the disposal of modern developers are fantastic at introducing what Alter describes as the six ingredients of behavioural addition:\n\n[C]ompelling goals that are just beyond reach; irresistible and unpredictable positive feedback; a sense of incremental progress and improvement; tasks that become slowly more difficult over time; unresolved tensions that demand resolution; and strong social connections.\n\nBehavioural addictions have a lot of similarity with substance addictions (some people question whether we should distinguish between them at all). They activate the same brain regions. They are fueled by some of the same human needs, such as the need for social engagement and support, mental stimulation and a sense of effectiveness. [Parts of the book seem to be a good primer on addiction, although see my endnote.]\nBased on one survey of the literature, as many as 41 per cent of the population may have suffered a behavioural addiction in the past month. While having so many people classified as addicts dilutes the concept of “addiction”, it does not seem unrealistic given the way many people use tech.\nAs might be expected given the challenge, Alter’s solutions on how we can manage addiction in the modern world fall somewhat short of providing a fix. For one, Alter suggests we need to start training the young when they are first exposed to technology. However, it is likely that the traps present in later life will be much different from those present when young. After all, most of Alter’s examples of addicts were born well before the advent of World of Warcraft, the iPhone or the iPad that derailed them.\nFurther, the ability of tech to capture our attention is only in its infancy. It is not hard to imagine the eventual creation of immersive virtual worlds so attractive that some people will never want to leave.\nAlter’s chapter on gamification is interesting. Gamification is the idea of turning a non-game experience into a game. One of the more inane but common examples of gamification is turning a set of stairs into a piano to encourage people to take those stairs in preference to the neighbouring escalator (see on YouTube). People get more exercise as a result.\nThe flip side is that gamification is part of the problem itself (unsurprising given the theme of Alter’s book). For example, exercise addicts using wearables can lose sight of why they are exercising. They push on for their gamified goals despite injuries and other costs. One critic introduced by Alter is particularly scathing:\n\nBogost suggested that gamification “was invented by consultants as a means to capture the wild, coveted beast that is video games and to domesticate it.” Bogost criticized gamification because it undermined the “gamer’s” well-being. At best, it was indifferent to his well-being, pushing an agenda that he had little choice but to pursue. Such is the power of game design: a well-designed game fuels behavioral addiction. …\nBut Bogost makes an important point when he says that not everything should be a game. Take the case of a young child who prefers not to eat. One option is to turn eating into a game—to fly the food into his mouth like an airplane. That makes sense right now, maybe, but in the long run the child sees eating as a game. It takes on the properties of games: it must be fun and engaging and interesting, or else it isn’t worth doing. Instead of developing the motivation to eat because food is sustaining and nourishing, he learns that eating is a game.\n\nTaking this critique further, Alter notes that “[c]ute gamified interventions like the piano stairs are charming, but they’re unlikely to change how people approach exercise tomorrow, next week, or next year.” [Also read this story about Bogost and his game Cow Clicker.]\nThere are plenty of other interesting snippets in the book. Here’s one on uncertainty of reward:\n\nEach one [pigeon] waddled up to a small button and pecked persistently, hoping that it would release a tray of Purina pigeon pellets. … During some trials, Zeiler would program the button so it delivered food every time the pigeons pecked; during others, he programmed the button so it delivered food only some of the time. Sometimes the pigeons would peck in vain, the button would turn red, and they’d receive nothing but frustration.\nWhen I first learned about Zeiler’s work, I expected the consistent schedule to work best. If the button doesn’t predict the arrival of food perfectly, the pigeon’s motivation to peck should decline, just as a factory worker’s motivation would decline if you only paid him for some of the gadgets he assembled. But that’s not what happened at all. Like tiny feathered gamblers, the pigeons pecked at the button more feverishly when it released food 50–70 percent of the time. (When Zeiler set the button to produce food only once in every ten pecks, the disheartened pigeons stopped responding altogether.) The results weren’t even close: they pecked almost twice as often when the reward wasn’t guaranteed. Their brains, it turned out, were releasing far more dopamine when the reward was unexpected than when it was predictable.\n\nI have often wondered to what extent surfing is attractive due to the uncertain arrival of waves during a session, or the inconsistency in swell from day-to-day.\n———\nNow for a closing gripe. Alter tells the following story:\n\nWhen young adults begin driving, they’re asked to decide whether to become organ donors. Psychologists Eric Johnson and Dan Goldstein noticed that organ donations rates in Europe varied dramatically from country to country. Even countries with overlapping cultures differed. In Denmark the donation rate was 4 percent; in Sweden it was 86 percent. In Germany the rate was 12 percent; in Austria it was nearly 100 percent. In the Netherlands, 28 percent were donors, while in Belgium the rate was 98 percent. Not even a huge educational campaign in the Netherlands managed to raise the donation rate. So if culture and education weren’t responsible, why were some countries more willing to donate than others?\nThe answer had everything to do with a simple tweak in wording. Some countries asked drivers to opt in by checking a box:\nIf you are willing to donate your organs, please check this box: □\nChecking a box doesn’t seem like a major hurdle, but even small hurdles loom large when people are trying to decide how their organs should be used when they die. That’s not the sort of question we know how to answer without help, so many of us take the path of least resistance by not checking the box, and moving on with our lives. That’s exactly how countries like Denmark, Germany, and the Netherlands asked the question—and they all had very low donation rates.\nCountries like Sweden, Austria, and Belgium have for many years asked young drivers to opt out of donating their organs by checking a box:\nIf you are NOT willing to donate your organs, please check this box: □\nThe only difference here is that people are donors by default. They have to actively check a box to remove themselves from the donor list. It’s still a big decision, and people still routinely prefer not to check the box. But this explains why some countries enjoy donation rates of 99 percent, while others lag far behind with donation rates of just 4 percent.\n\nThis story is rubbish, as I have posted about here, here, here and here. This difference has nothing to do with ticking boxes on driver’s licence forms. In Austria they are never even asked. 99 per cent of Austrians aren’t organ donors in the way anyone would normally define it. 99% are presumed to consent, and if they happen to die their organs might not be taken because the family objects (or whatever other obstacle gets in the way) in the absence of any understanding of the actual intentions of the deceased.\nTo top it off, Alter embellishes the incorrect version of the story as told by Daniel Kahneman or Dan Ariely with phrasing from driver’s licence forms that simply don’t exist. Did he even read the Johnson and Goldstein paper (ungated copy)?\nAfter reading a well-written and entertaining book about a subject I don’t know much about, I’m left questioning whether this is a single slip or Alter’s general approach to his writing and research. How many other factoids from the book simply won’t hold up once I go to the original source?"
  },
  {
    "objectID": "posts/an-economic-theory-of-greed-love-groups-and-networks.html",
    "href": "posts/an-economic-theory-of-greed-love-groups-and-networks.html",
    "title": "An Economic Theory of Greed, Love, Groups, and Networks",
    "section": "",
    "text": "My assessment of An Economic Theory of Greed, Love, Groups, and Networks by Paul Frijters with Gigi Foster varies with the objective I assess it against. On the one hand, Frijters and Foster seek to supplement what they call the “mainstream economic” view to give an enhanced perspective of how society works. Although they sometimes talk this objective down, it is inherently ambitious and encompasses a significant expansion of core economic theory. Against this benchmark, I am not convinced they have succeeded.\nOn the other hand, if you removed the paragraphs that hint at the authors’ loftier intentions and simply took the book as an interesting analysis of three areas - love, groups and networks - that many economic questions could benefit from incorporating in their analysis, the book is clearly a success. Although not the easiest read, I enjoyed it. It is dense with ideas and is one of the more stimulating books I have read for a while.\nFrijters and Foster’s basic argument is that while “mainstream economics” has great value in many spheres of analysis, it requires supplementation with perspectives on love, groups and networks to provide an explanation for a range of phenomena that mainstream economics doesn’t quite capture. They structure the book around this approach, with an overview of the mainstream economic foundation of “greed” in the opening chapter, followed by chapters on love, groups (and power) and networks. They close by discussing some applications of this enhanced framework, and if you are feeling ambitious, a theoretical appendix containing models on some of the core concepts.\nThe love chapter is based on what they call “the love principle”, which they suggest is the one new theoretical contribution of the book (the novelty of the book otherwise being the way they bring together many other strands of thought). They state the love principle as follows:\n\nLove derives from the attempt of the unconscious mind to bargain with something that is believed to be capable of fulfilling desires and that is perceived to be too powerful to be possessed by direct means.\n\nLove (or loyalty) is the result of power relations. When someone has a desire, they assess whether someone (who may not necessarily by a physical entity) can provide it and whether it can be obtained from that entity by dominating it (greed). If not, this is where Frijters and Foster suggest love comes in. Love is a submission strategy, whereby the person unconsciously makes a “love bargain” and the individual starts to recategorise themselves to become one with the entity that they have submitted to.\nThis love principle is a tough concept, and each time I think about it, I am not sure if I fully understand it. To the extent I do, I am not a fan of the idea. The difficulty in grasping the concept is illustrated by the general absence of a clear description of the love principle in the book’s promotional material, including interviews with Foster and Frijters. It is not soundbite material. To give it the time it deserves, I have written a separate post pulling the love principle apart.\nThe chapter on groups was, to my reading, the most important chapter and the concept upon which Frijters and Foster build most of their applications. They describe five types of groups - small hierarchies, small circles of reciprocity, large hierarchies, large circles of reciprocity, and networks. The smaller groups are nested within larger groups, with most people being members of many different types of groups. For example, a large company is a large hierarchy, but it sits within a large circle of reciprocity (a nation) and within it has many small hierarchies and small circles of reciprocity. When we are analysing an economic question, knowing which group a person is in and how they respond to the norms within the group (and how others enforce those norms) provides a layer of analysis\nI enjoyed the analysis of groups, although there were points I kept thinking “what about [X]?”. One that kept popping into my mind was reverse-dominance hierarchies - although this point comes back to love and dominance, so I’ll explore it in more depth in my next post.\nThe chapter on networks was interesting, being a topic I know little about (and hence I have little to add here), although I enjoyed it as the chapter appealed to my preference for thinking about the interactions in an economy as occurring between heterogeneous agents. The basic idea in the chapter is that an economy consists of a series of contacts between people. New technologies or shocks to the system (such as the collapse of communism) destroy many of the contacts, with the recovery from that shock reflecting the number of contacts destroyed and how quickly new contacts can be established.\nThe last chapter consists of a series of examples of where these perspectives on love, groups and networks can add to the mainstream view. Issues they cover include the high level of tax compliance despite the low chance of being caught cheating, voting behaviour where a single vote will almost never count, symbolic expenses such as gifts to a god, and the way competition regulation occurs. This was the point where I expected to see more tying together of the main concepts, or a stronger demonstration of their power. But, as they point out, the analysis is often not particularly different from the mainstream economic view. Where it was different, it strongly hinged on the concepts of groups, whereas love (apart from symbolic expenses) and networks had smaller roles. In many ways it felt like a sophisticated public choice type analysis, examining all the players and looking at how the nature of the groups they are part of shape their incentives. I like that way of thinking, although it usually feels as though greed and self interest are the most important factors within those groups.\nOne issue that led me to struggle to buy into Frijters and Foster’s broader objective was the manner of presenting many of the arguments. At  beginning of the book, we are told that the concepts of love, groups and networks were chosen by a process of trial and error, and that there is no point in “regurgitating all of the mental activity that led to this book”. That may be fair enough, but it makes it difficult for the reader to take the intellectual journey that the authors have. Similarly, most sections ended with what were called “out-of-sample” predictions, but given the way that the authors developed their arguments, they are only out-of-sample based on the authors’ out-of-sight thought processes. It’s hard to buy into this framework when you are only shown the result, not the process.\nIn another year or so I’ll give the book a re-read and see if I respond differently having have stewed on the components a bit longer. But even if I don’t buy into the broader framework, there is more than enough interesting material for a second visit."
  },
  {
    "objectID": "posts/an-evolutionary-explanation-of-consumption.html",
    "href": "posts/an-evolutionary-explanation-of-consumption.html",
    "title": "An evolutionary explanation of consumption",
    "section": "",
    "text": "Since Thorstein Veblen’s 1899 book Theory of the Leisure Class, the economics profession has taken a somewhat mixed approach to consumption. In areas such signalling theory, Veblen’s argument that conspicuous consumption must be wasteful and expensive to be a reliable signal of wealth is well recognised. Conspicuous consumption has a purpose as a signal. However, the typical economic model is built on the simple concept that more consumption brings more utility. There is no benefit beyond consumption itself.\nThe absence of a rationale for consumption appears even less satisfactory when considered from an evolutionary perspective. If people trade consumption against the use of resources for survival or reproduction, why does a trait which involves excessive consumption exist in the population? An individual could boost their fitness if they reallocated resources to increasing their fertility.\nIn this light, Gianni De Fraja’s explanation (ungated working paper here) of conspicuous consumption through an evolutionary lens is a useful addition to the literature. Using a modified version of Grafen’s model on the use of biological signals as handicaps (see also), De Fraja showed that under certain conditions conspicuous consumption could be explained as a signal to the opposite sex. De Fraja further described how utility maximisation (as used by economists) is formally equivalent to the maximisation of fitness through signalling. This provides a biological basis for economists to include consumption in utility functions.\nDe Fraja’s model incorporated two sexes that mate during a mating season that consists of two “periods” (although the result could be extrapolated to more periods). De Fraja assumed that men make no investment in offspring, so they are free to mate in both periods, while if a woman mates in either period, they are removed from the mating population for the rest of the season. On this basis, men are willing to mate with any woman they are paired with, while women are choosy.\nThe choosiness of women is with good reason, as men vary in quality. With a mate of higher quality, the female can expect more of her children to survive to adulthood. Females do not vary in quality, but they face a chance of death in each period. In the first period, men and women are matched one-to-one. Given the varying quality of the men, the women need to decide whether the man they are paired with is of high enough quality to mate with, or whether they should take their chances and wait until the next period in the hope of finding a better mate. If their chance of death is high, the woman may drop her standards.\nThis choice is complicated, however, as male quality is not directly observable. What women can see is the man’s level of conspicuous consumption. Putting this in terms of choices we face today, and ignoring the possible approach of bringing your bank statement or pay slip to the dinner date, total wealth is unobservable. Instead it is conspicuous consumption on the car you drive to the date, your clothes, your watch and the cost of the restaurant that will show one’s wealth. The question the woman must address is whether the signal from the man as to his wealth is reliable. Has he arrived in a BMW that he will also have to sleep in tonight as he has no resources left for accommodation? Or is he actually wealthy?\nTo make this choice, the woman needs to infer the man’s level of quality. In De Fraja’s model, the strategy employed by the women is heritable. In equilibrium, all women would adopt the same strategy (a function of the perceived quality of the male and their chance of death), as no alternative strategy would be able to increase the female’s fertility.\nThe choice faced by men is how divide resources between conspicuous consumption and survival activities, which reduce the male’s chance of death. De Fraja assumes that investment in survival activities is unobservable, leaving consumption as the only feature that the female can see. A higher quality male will have more resources to allocate between consumption and investment in survival activities. In the model, the way men allocate resources to consumption (their signalling strategy) is genetically inherited. Quality itself is not inherited but randomly allocated to each new generation.\nSo, how does this work out? De Fraja did not study the dynamics of the model but, as is the case of most consideration of preferences in economics, the model was solved for the steady state population equilibrium. In the first period of the steady state, a female will agree to mate with a man only if they above a perceived quality, with that threshold level of quality decreasing as the woman’s chance of death increases. In the second period, the women will mate with whoever they are matched with as there are no further breeding opportunities.\nFor the men, De Fraja found that for certain combinations of environmental constraints, men would split into a separating equilibrium, whereby men of above a certain quality would signal that they are of high quality (those who meet this threshold all signal at the same level of consumption). Those below that level do not signal. Put simply, the lower quality men will sacrifice too much investment in survival if they tried to match the high quality male’s level of conspicuous consumption. As a result, low quality men do not engage in conspicuous consumption and focus on surviving. If the BMW will have low quality men starving and sleeping on the streets, a low quality male will not buy it and ownership of a BMW will be a signal that women can rely on.\nIn this equilibrium, the strategy by women of believing the signal, and by men of signalling true quality (that is, low or high quality) was found to be stable as neither the men nor women can use an alternative strategy and increase their level of fitness.\nI would like to say more about the conditions under which this separating equilibrium holds, but as De Fraja notes, the mathematical proof of the separating equilibrium is not readily interpretable. Even after solving through the equations, it is not clear to me how feasible the required conditions are.\nOnce De Fraja establishes his separating equilibrium result and provides an evolutionary basis for conspicuous consumption, he moves to explaining whether this result is consistent with the utility maximising approach of economics. Is the maximisation of utility subject to a budget constraint equivalent to maximising fitness subject to environmental constraints? Under conditions of similar mathematical opacity to those for the separating equilibrium, De Fraja showed that they could be equivalent. The common strategy of all men could be thought of as common indifference curves, which in economics are the bundles of goods (in this case, consumption and survival activities) between which the man is indifferent. What determines where the man is on the indifference curves (his choice of consumption and survival activities) varies according to his quality. As a result, and assuming the conditions held, a model which involves a basic utility function that has utility increasing with consumption could be said to be biologically sound.\nDe Fraja’s paper left me with a number of questions. The most obvious one was why no-one had done this before. This issue had been known at least since the time of Veblen, and Grafen had laid the mathematical framework in 1990. I can only suggest that most economists are not overly concerned about the biological basis of their models, particularly if they have reasonable predictive power.\nThe second question relates to the range of conditions under which a separating equilibrium can arise and whether these are broad enough to be realistic. Having not got to the bottom of De Fraja’s mathematics, I am not sure of whether an alternative mathematical approach might yield more intuitive and easier to interpret results. Perhaps simulation could be used as a starting point to get a feel for how specific these conditions are.\nA further issue relates to dynamics. While De Fraja’s work should allow economists who use consumption in utility functions to argue that their approach is biologically consistent, this is restricted to static situations. Can we learn anything further from the dynamic processes that lead to equilibrium (if a dynamic process would lead to equilibrium)? Take Galor and Moav’s argument of natural selection being a trigger for economic growth. While natural selection is at the core of their model, the model’s agents’ desire to consume above subsistence levels is not subject to selection and has no biological justification. This leaves some scope for extensions to the model, or indeed any other long-term growth model that represents a period sufficiently long for selection to occur.\nThat links to the question I always ask when I see a static model which explains the equilibrium of preferences shaped by natural or sexual selection. What are the macroeconomic effects of the move to equilibrium? For example, suppose there was initially no conspicuous consumption but the separating equilibrium proposed by De Fraja evolved over a few thousand (or tens of thousands of) years. Does a preference for conspicuous consumption drive us to gather more resources, which in turn increases in economic growth? This is nothing but speculation (at this stage), but it is a question that could yield interesting answers. (Since I first wrote this post, I have developed a dynamic scenario building of De Fraja’s work.)"
  },
  {
    "objectID": "posts/an-evolutionary-perspective-on-behavioural-economics.html",
    "href": "posts/an-evolutionary-perspective-on-behavioural-economics.html",
    "title": "An evolutionary perspective on behavioural economics",
    "section": "",
    "text": "I often complain that behavioural economics (behavioural science) often appears to be no more than a loosely connected set of heuristics and biases, crying out for theoretical unification. Evolutionary biology is likely the source of that unification.\nOver the last few years, I’ve spotted the occasional attempt to analyse a bias through an evolutionary lens. But late last year, I came across Owen D Jones, a professor of law and professor of biological sciences at Vanderbilt University. At the time, I posted on his forthcoming book chapter Why Behavioral Economics Isn’t Better, and How it Could Be, but since then have been working through his impressive back catalogue (his SSRN page is here). For around 15 years Jones has published on the link between behavioural economics (or in his case, behavioural law and economics) and evolutionary biology, but this work has barely carried across from the law to the economics literature.\nI plan to post on a few of his papers, and I’ll start with a 2000 article Time-Shifted Rationality and the Law of Law’s Leverage: Behavioral Economics Meets Behavioral Biology. As in the chapter I linked above, Jones starts by critiquing the lack of theoretical background in behavioural economics, a claim that is still fair today:\n\nBLE [behavioural law and economics] scholars stand accused, for example, of merely organizing anecdotes, and of confusing counterstories for theories. This should not, of course, be construed as automatically damning. After all, unexpected empirical facts can, in sufficient number, warrant changes in legal strategies for pursuing existing goals, even absent convincing explanations for their patterned occurrence. And a number of BLE scholars have succeeded in making convincing cases for legal reform, based on empirical data about irrationalities alone, irrespective of causes.\nNevertheless, in the absence of buttressing theory such efforts represent isolated successes, rather than promisingly synergistic ones that would signal a broad, systematic approach. For it is quite clear in the end that BLE shows neither a present and satisfactory account of the origins and patterns of identified irrationalities, nor signs of making quick progress toward developing one. Constructing the theoretical foundation of these phenomena will ultimately be necessary if BLE is to achieve its potential and be as useful, persuasive, and important to law as its proponents now hope.\n\nJones argues that an evolutionary analysis can provide that theoretical foundation, primarily through distinguishing proximate from ultimate causes. Proximate causes relate to the internal mechanisms or physical processes that underlie behaviour. Ultimate causes are the evolutionary processes by which a behaviour came to be commonly observable in a species. Jones argues that there is a general failure to analyse the biases through the lens of ultimate causation, which would allow us to understand the patterns of biases and why some biases are so widespread.\nI am tempted to go further and would say that often there is not even an analysis of the proximate causes of biases. Gerd Gigerenzer tends to operate in this territory, looking to understand what decision rules are being exercised in particular environments, which allows you to understand the ecological rationality of the decision. A lot of behavioural economics research simply finds a deviation from what they consider a rational decision and moves on - with no thought as to how the decision making process led to the decision. Prospect theory, for instance, bears practically no resemblance to mechanisms or processes by which people actually make decisions.\nBack to Jones, he argues that under the lens of ultimate causation, many biases turn out to be features, not bugs:\n\n[S]ome behaviors currently ascribed to cognitive limitations reflect not defect, but rather finely tuned features of brain design. If so, we may gain important insights into the patterns of human irrationality by combining our proximate causation analysis with our ultimate causation analysis to yield a comprehensive evolutionary analysis.\n…\nA biologically informed view of the brain makes clear that substantive irrationalities are probably not just about physical, temporal, and informational limits. They are also, in some circumstances, likely to be about specific, narrowly tailored, efficiently operating features of brain design. My argument here is that the traditional approach to bounded rationality and decision-making is, in many cases, both descriptively wrong and materially misleading.  It is descriptively wrong in the same way that it would be wrong to say that a Porsche Boxster is “defective” when it fails to climb logs and ford streams off road, or that a moth’s brain is “defective” when the moth flies into an artificial light source. It is materially misleading because to the extent that irrationalities are considered to be the result of defects, rather than design features, their specific content is assumed to be, though patterned ex post, unpredictable, unsystematized, and random ex ante—rather than predictable, interrelated, and content-specific. Put another way, turning old cognitive tools to entirely new uses introduces changed circumstances, not defects. And the inappropriateness of old tools to new uses does not mean those tools lack specialized design and function. Understanding what the tools were designed to do provides significant purchase on explaining and predicting how they will function when applied in novel contexts.\n\nToday, we tend to put old cognitive tools to new uses in environments that don’t reflect those of our evolutionary past. Jones calls this “time-shifted rationality” (I think I prefer to just call it mismatch), which relates to the use of a once-successful tool in new, possibly inappropriate circumstances.\n\n[T]here will be times when a perfectly functioning brain—functioning precisely as it was designed to function— will incline us toward behavior that, viewed only in the present tense and measured only by outcomes in current environments, will appear to be substantively irrational. This is simply because the brain was designed to process information in ways tending to yield behaviors that were substantively rational in different environments than the ones in which we now find ourselves.\n…\nSpecifically, time-shifted rationality describes any trait resulting from the operation of evolutionary processes on brains that, while increasing the probability of behavior that was adaptive in the relevant environment of evolutionary adaptation in the ancestral past, leads to substantively irrational or maladaptive behavior in the present environment. In other words, poor behavior choices sometimes derive not from brain defects, per se, by rather from the brain’s deployment of old, once-successful techniques in the face of new problems. So before judging the brain’s abilities, we need to consider the effects of its choices in the environments for which the brain is principally adapted.\n\nHere’s one example of this analysis at work (although I don’t agree with the point about increases in life expectancy as an explanation):\n\nResearchers have noted not only that people often prefer to receive a smaller good now over a disproportionately greater good later, but also that people reverse this preference as the delay for receiving either good increases in equal amounts. This seems irrational. For example, the fact that a majority of adults would rather receive $50 now than $100 in two years—at the same time that virtually no one prefers $50 in four years to $100 in six years—is seen as clear evidence of “anomalies in the utilitarian reasoning of the normal human adult.” …\nIt is likely a mistake to conclude that seemingly irrationally discounted futures are necessarily the function of calculating errors. Evolutionary analysis suggests an ultimate cause explanation. Hyperbolic discounting may reflect another time-shifted rationality. How might modern environmental features differ from features of the environment of evolutionary adaptation in ways that render once-adaptive predispositions maladaptive? First, average life expectancy has skyrocketed. And high discount rates make sense when life expectancy is short. Second, for nearly all of the roughly seventy million years of primate evolution, there was no such thing as a reliable future, let alone a reliable future payoff. Even under the most generous definition of investment, investment horizons were short. Third, a “right” to receive something in the future is a trivially recent invention of modern humanity.\nSince long lives, reliable futures, and reliable rights to future payoffs were not part of the environment in which the modern brain was slowly built, it is not particularly surprising that the modern brain tends to steeply discount the value of a future benefit compared to an immediate one, and is not particularly well equipped to reach the outcome currently deemed most rational. Rather than assume that people will be rational discounters, we should, logically, expect and assume the opposite: most often people will be hyperbolic discounters. In the EEA, the environment of evolutionary adaptation, the kind of hyperbolic discounting that humans now so regularly exhibit often would have led to more substantively rational results than the alternative.\nPut another way, at almost no time in human evolutionary history could there have been a selection pressure that regularly favored the kind of coolly calculated and deferred gratification now deemed to be so reasonable.\n\nThe other major area that Jones covers in the article is what he calls the law of law’s leverage, which deserves a future post of its own."
  },
  {
    "objectID": "posts/an-updated-economics-and-evolutionary-biology-reading-list-and-a-collection-of-book-reviews.html",
    "href": "posts/an-updated-economics-and-evolutionary-biology-reading-list-and-a-collection-of-book-reviews.html",
    "title": "An updated economics and evolutionary biology reading list and a collection of book reviews",
    "section": "",
    "text": "I have updated my economics and evolutionary biology reading list, with a few new additions including John Coates’s The Hour Between Dog and Wolf, Gregory Clark’s new book on social mobility and Jonathan Haidt’s The Righteous Mind. As before, I have been selective, adding only the best books (or articles) in the area. That said, I am always open for suggestions or comment.\nWhen updating the list, I realised I have written a lot of book reviews over the last few years. I have collected most of them together on one page, which you can find here. It includes a lot of good books that aren’t on the reading list as they are not on topic. It also contains a few books that are on topic but haven’t made the cut."
  },
  {
    "objectID": "posts/another-msix-reading-list.html",
    "href": "posts/another-msix-reading-list.html",
    "title": "Another #MSiX reading list",
    "section": "",
    "text": "Yesterday was the second edition of the Marketing Science Ideas Xchange (MSiX). It was a more eclectic set of speakers than last year, extending from the first year’s behavioural economics focus to include neuroscience and “big data”. In my mind, the increased variety worked well.\nIf you saw yesterday’s post, I spoke at the event and provided a reading list which included Matt Ridley’s The Red Queen, Geoffrey Miller’s Spent, Gad Saad’s The Evolutionary Bases of Consumption and The Consuming Instinct, Amotz Zahavi’s The Handicap Principle, Robert Frank’sLuxury Fever, Gerd Gigerenzer’s Rationality for Mortals and Douglas Kendrick and Vlad Griskevicus’s The Rational Animal.\nFrom the other presentations, I didn’t record all the books as they were mentioned, but here are the main titles that I can recall:\nMichael Norton’s Happy Money: The Science of Smarter Spending\nSuzanne Mettler’s The Submerged State: How Invisible Government Policies Undermine American Democracy\nRobert Cialdini’s Influence: The Psychology of Persuasion\nClayton Christensen’s The Innovator’s Dilemma: The Revolutionary Book That Will Change the Way You Do Business\nAndrew Leigh’s The Economics of Just About Everything\nByron Sharp’s How Brands Grow: What Marketers Don’t Know\nAdam Ferrier’s The Advertising Effect: How to Change Behaviour\nRichard Thaler and Cass Sunstein’s Nudge: Improving Decisions About Health, Wealth, and Happiness\nJames Hurman’s The Case for Creativity (which seems to have drifted out of print)"
  },
  {
    "objectID": "posts/arent-we-smart-fellow-behavioural-scientists.html",
    "href": "posts/arent-we-smart-fellow-behavioural-scientists.html",
    "title": "Aren’t we smart, fellow behavioural scientists",
    "section": "",
    "text": "Below is the text of my presentation at Nudgsestock on 12 June 2020."
  },
  {
    "objectID": "posts/arent-we-smart-fellow-behavioural-scientists.html#intro",
    "href": "posts/arent-we-smart-fellow-behavioural-scientists.html#intro",
    "title": "Aren’t we smart, fellow behavioural scientists",
    "section": "Intro",
    "text": "Intro\nOver the past decade or two, behavioural scientists have had a great ride. There have been bestselling books and Nobel Memorial Prizes. Every second government department and corporate has set up a team.\nBut recently, the wind seems to have changed. We’re told that behavioural economics is itself biased.\n\n“Don’t trust the psychologists on coronavirus - Many of the responses to Covid-19 come from a deeply-flawed discipline”.\n\n“Nudgeboy” has become a pejorative.\n\nI believe this challenge is deserved.\nFor too long we have been opining about people’s irrationality - that is, the irrationality of others - and that if only we designed the world more intelligently, people would make better decisions.\nWe often make these judgements based on narrow lab experiments that we generalise to the outside world. But as we well know, sometimes those experiments don’t replicate in that narrow lab environment. And even of those that replicate in the lab, many become an ineffective, or even dangerous tool when we try to apply them in the complex outside world.\nLet me tell you a story to illustrate."
  },
  {
    "objectID": "posts/arent-we-smart-fellow-behavioural-scientists.html#the-hot-hand-fallacy",
    "href": "posts/arent-we-smart-fellow-behavioural-scientists.html#the-hot-hand-fallacy",
    "title": "Aren’t we smart, fellow behavioural scientists",
    "section": "The hot hand fallacy",
    "text": "The hot hand fallacy\nThis story comes from great work by Joshua Miller and Adam Sanjurjo. It stands as one of the starkest examples of where I have been forced to change my beliefs.\nThere is strong evidence from the lab that people have misperceptions about what randomness looks like. When a person is asked to generate a series that approximates the flipping of a coin, they will alternate between heads and tails too often, and balance the frequencies of heads and tails over too short a sequence. When people are asked to judge which of two different sequences of coin flips are more likely, they tend to pick sequences with more alternation, despite their probability being the same.\nWhat happens when we look for a failure to perceive randomness in the outside world? Out of the lab?\nWhen people watch basketball, they often see a hot hand. They will describe players as “hot” and “in form”. Their belief is that the person who has just hit a shot or a series of shots is more likely to hit their next one.\nBut is this belief in the “hot hand” a rational belief? Or is the hot hand an illusion, whereby, just like they do with coins, they are seeing streaks in what is actually randomness?\nIn a famous examination of this question, Thomas Gilovich, Robert Vallone and Amos Tversky took shot data from a variety of sources, including the Philadelphia 76ers and Boston Celtics, and examined it for evidence of a hot hand.\nWhat did they find? The hot hand was an illusion. As Daniel Kahneman wrote in Thinking, Fast and Slow when describing this research:\n\nThe hot hand is entirely in the eye of the beholders, who are consistently too quick to perceive order and causality in randomness. The hot hand is a massive and widespread cognitive illusion.\n\nPossibly even more interesting was the reaction to the findings from those in the sporting world. Despite the analysis, many sports figures denied that it could be true. Red Auerbach, who coached the Boston Celtics to nine NBA championships, said “Who is this guy? So he makes a study. I couldn’t care less.”\nThis provides another insight, about which Gilovich wrote:\n\n[T]he story of our research on the hot hand is only partly about the misperception of random events. It is also about how tenaciously people cling to their beliefs even in the face of hostile evidence.\n\nSo, this isn’t just about the misperception of the hot hand, but also about the failure of people to see their error when presented with evidence about it.\nLet’s delve into how Gilovich, Vallone and Tversky showed the absence of a hot hand.\nImagine a person who took ten shots in a basketball game. A ball is a hit, an X is a miss.\nWhat would count as evidence of a hot hand? What we can do is look at shots following a previous hit. For instance, in this sequence of shots there are 6 occasions where we have a shot following a previous hit. Five of those shots, such as the seventh here, are followed by another hit.\n\nWe can then compare their normal shooting percentage with the proportion of shots they hit if the shot immediately before was a hit. If their hit rate after a hit is higher than their normal shot probability, then we might say they get a hot hand.\nThis is effectively how Gilovich, Vallone and Tversky examined the hot hand in coming to their conclusion that it doesn’t exist. They also looked at whether there was a hit or miss after longer streaks of hits or misses, but this captures the basic methodology. It seems sensible.\nBut let me take a detour that involves flipping a coin.\nSuppose you flip a coin three times. Here are the eight possible sequences of heads and tails. Each sequence has an equal probability of occurring. What if I asked you: if you were to flip a coin three times, and there is a heads followed by another flip in that sequence, what is the expected probability that another heads will follow that heads?\nHere is the proportion of heads following a previous flip of heads for each sequence. In the first row of the table, the first flip is a head. That first flip is followed by another head. After the second flip, a head, we also have a head. There is no flip after the third head. 100% of the heads in that sequence followed by another flip are followed by a head.\nIn the second row of the table, 50% of the heads are followed by a head. In the last two rows, there are no heads followed by another flip.\nNow, back to our question: if you were to flip a coin three times, and there is a heads followed by another flip in that sequence, what is the expected probability that another heads will follow that heads? It turns out it is 42%, which I can get by averaging those proportions.\n8 possible combinations of heads and tails across three flips\n\n\n\nFlips\np(Ht+1|Ht)\n\n\n\n\nHHH\n100%\n\n\nHHT\n50%\n\n\nHTH\n0%\n\n\nHTT\n0%\n\n\nTHH\n100%\n\n\nTHT\n0%\n\n\nTTH\n-\n\n\nTTT\n-\n\n\nExpected value\n42%\n\n\n\nThat doesn’t seem right. If we count across all the sequences, we see that there are 8 flips of heads that are followed by another flip. Of the subsequent flips, 4 are heads and 4 are tails, spot on the 50% you expect.\nWhat is going on in that second column? By looking at these short sequences, we are introducing a bias. The cases of heads following heads tend to cluster together, such as in the first sequence which has two cases of a heads following a heads. Yet the sequence THT, which has only one shot occurring after a heads, is equally likely to occur. The reason a tails appears more likely to follow a heads is because of this bias whereby the streaks tend to cluster together. The expected value I get when taking a series of three flips is 42%, when in fact the actual probability of a heads following a heads is 50%. As the sequence of flips gets longer, the size of the bias is reduced, although it is increased if we examine longer streaks, such as the probability of a heads after three previous heads.\nWhy have I bothered with this counterintuitive story about coin flipping?\nBecause this bias is present in the methodology of the papers that purportedly demonstrated that there was no hot hand in basketball. Because of this bias, the proportion of hits following a hit or sequence of hits is biased downwards. Like our calculation using coins, the expected proportion of hits following a hit in a sequence is lower than the actual probability of hitting a shot.\nConversely the hot hand pushes the probability of hitting a shot after a previous hit up. Together, the downward bias and the hot hand roughly cancelled each other out, leading to the conclusion by researchers that each shot is independent of the last.\nThe result is, that when you correct for the bias, you can see that there actually is a hot hand in basketball.\nWhen Miller and Sanjurjo crunched the numbers for one of the studies in the Gilovich and friends paper, they found that the probability of hitting a shot following a sequence of three previous hits is 13 percentage points higher than after a sequence of three misses. There truly is a hot hand. If Red Auerbach had coached as though there were no hot hand, what would his record have looked like?\nI should say, this point does not debunk the earlier point about people misperceiving randomness. The lab evidence is strong. People tend to see the hot hand when people flip coins. It is possible that people overestimate the strength of the hot hand in the wild, although that is hard to show. But the hot hand exists.\nLet’s turn back to one of the quotes I showed earlier.\n\n[T]he story of our research on the hot hand is only partly about the misperception of random events. It is also about how tenaciously people cling to their beliefs even in the face of hostile evidence.\n\nThe researchers expanded the original hot hand research from a story about people misperceiving randomness, to one of them continuing to do so even when presented with evidence that they were making an error.\nBut, as we can now see, their belief in the hot hand was not an error. The punters in the stands were right. Their accumulated experience had given them the answer. The researchers were wrong. Rather than the researchers asking whether they themselves were making an error when people refused to believe their research, they double downed and identified a second failure of human reasoning. The blunt dismissal of people’s beliefs led behavioural scientists to hold an untrue belief for over thirty years\nThis is a persistent characteristic of much applied behavioural science. It was an error I made many times when I first came to the discipline. We spend too little time questioning our understanding of the decisions or observations other people make. If we believe they are in error, we should first question whether the error is ours."
  },
  {
    "objectID": "posts/arent-we-smart-fellow-behavioural-scientists.html#priming",
    "href": "posts/arent-we-smart-fellow-behavioural-scientists.html#priming",
    "title": "Aren’t we smart, fellow behavioural scientists",
    "section": "Priming",
    "text": "Priming\nHere’s another example. There is a body of behavioural science research known as priming, that suggests that even slight cues in the environment can change our actions. A lot of priming research has bitten the dust through the replication crisis - ideas such as words associated with old people slow our walking pace, known as the Florida effect, or that images of money make us selfish, or that priming us with the ten commandments can make us more honest. It simply hasn’t stood the test of time.\nYet here’s a passage from Daniel Kahneman’s Thinking, Fast and Slow:\n\nWhen I describe priming studies to audiences, the reaction is often disbelief. …\nThe idea you should focus on, however, is that disbelief is not an option. The results are not made up, nor are they statistical flukes. You have no choice but to accept that the major conclusions of these studies are true.\n\nNo. Again, it turns out the doubt of these audiences was justified.\nThere is an interesting intersection between this priming research and the hot hand. Much behavioural science research (including priming) is built on the concept that subtle, often ignored features of our environment can have marked effects on our decisions and performance. Yet why didn’t the hot hand researchers consider that a basketball player would be influenced by their earlier shots, surely a highly salient part of the environment and influence on their mental state? But, alas, the desire to show one bias allowed us to overlook another."
  },
  {
    "objectID": "posts/arent-we-smart-fellow-behavioural-scientists.html#probability-neglect",
    "href": "posts/arent-we-smart-fellow-behavioural-scientists.html#probability-neglect",
    "title": "Aren’t we smart, fellow behavioural scientists",
    "section": "Probability neglect",
    "text": "Probability neglect\nNow to a more recent story, which involves a concept called probability neglect.\nThe idea behind probability neglect is that when we consider a small risk, we tend to either ignore the risk or give it too much weight. We give disproportionate weight to the difference between zero and one percent relative to the difference between one and 99 percent probability.\nThere’s good evidence from the lab that we suffer from probability neglect - in the same way there is solid lab evidence about our misperceptions of randomness. But once again, the danger emerges when we take this finding and use it to assess the decisions of people in the outside world.\nHere’s a recent example by Nudge author Cass Sunstein that hasn’t aged particularly well: The Cognitive Bias That Makes Us Panic About Coronavirus, with the subtitle: Feeling anxious? Blame “probability neglect.”\n\nThe opening paragraph of the article reads:\n\nAt this stage, no one can specify the magnitude of the threat from the coronavirus. But one thing is clear: A lot of people are more scared than they have any reason to be. They have an exaggerated sense of their own personal risk.\n\nIf you can’t specify a magnitude, it’s somewhat hard to claim that a response is exaggerated. But beyond that, here we see a set of findings from the lab - Sunstein later in the article describes one of the lab experiments - extrapolated to the real world, with little time spent asking whether an experiment in the lab can capture the more complex dynamics around people’s response to the coronavirus. In the lab, we know the probabilities. We have set them. Outside, in a case such as coronavirus, we don’t have any benchmark against which to assess people’s responses. As Sunstein notes, we also don’t know the magnitude.\nSunstein should have asked: Some people are reacting more than I think they should. Is there something about their response to risk that I should pay attention to? Why am I right and they wrong?\nIn fact, even when they are likely wrong - perhaps those panicking are like the broken clock that is right twice a day - we should ask whether there is wisdom in their actions. What if there is an asymmetry in the potential costs and benefits of overreacting versus under-reacting? Is it better to be typically wrong on probability - always assume there is a tiger in the grass - than to be largely right but occasionally experience ruin?\nSunstein, of course, was not exactly Robinson Crusoe in claiming that we were overreacting in late February. In fact, even now it’s not entirely clear what the appropriate response was for many people, regions or countries.\nBut by late March, without skipping a beat, he was noting that “This Time the Numbers Show We Can’t Be Too Careful”. No mention of the allegation of a misperception of risk less than four weeks earlier.\n\nOf course, one of the weaknesses of applied behavioural science is that you can tell a story no matter what the observed behaviour. Six weeks later Sunstein was writing “How to Make Coronavirus Restrictions Easier to Swallow”, giving guidance on how to stop an under-reaction.\n\nAs Sunstein wrote:\n\nTo address the coronavirus pandemic, it’s essential to influence human behavior; to promote social distancing, to get people to wear masks, to encourage people to stay home. Many nations have imposed mandates as well. But to enforce the mandates and to promote safer choices as the mandates wind down, people have to be nudged.\n\nSo now it’s all about trying to get people to stay home, because they, err, are underestimating the risk? Maybe it’s better to be right twice a day than to be the clock that is always two hours too slow."
  },
  {
    "objectID": "posts/arent-we-smart-fellow-behavioural-scientists.html#getting-the-right-objective",
    "href": "posts/arent-we-smart-fellow-behavioural-scientists.html#getting-the-right-objective",
    "title": "Aren’t we smart, fellow behavioural scientists",
    "section": "Getting the right objective",
    "text": "Getting the right objective\nThese two stories - about the hot hand and coronavirus - illustrate the danger of taking lab experiments into a far more complex environment, the outside world. You can already see some of the reasons why this can cause problems. We may not have the full set of information held by the decision maker. We might simply stuff up our analysis of the problem. It’s complex.\nIn closing, I want to suggest another problem with judging other people’s decisions, and that is that we can mistake (or give insufficient consideration of) what a person’s objective actually is and how they can best achieve that objective.\nBehavioural scientists have a better insight into this than many. We know that people aren’t just selfishly trying to maximise their income, wealth or consumption.\nYet, despite this, when we assess people’s behaviour in the wild, we often assess the rationality of their behaviours against a rather narrow set of outcomes, such as how their decisions benefit their finances or health in the long-term. We then try to nudge them in that direction.\nYet, that’s often not what people want.\nMy PhD combined evolutionary biology with economics, so I often think about our objectives with an evolutionary lens. Our mind was selected to have preferences that would tend to result in survival and reproduction in the environment in which it evolved.\nOf course, most of us don’t specifically plot to maximise our reproductive output. Rather, evolution shapes our preferences so that we seek proximate objectives.\nWhen we examine objectives from an evolutionary biology perspective, what appears irrational can simply be a misunderstanding on our part of what someone’s objectives are. The type of behaviour to, say, attract a partner, is going to look somewhat different to that of someone simply maximising financial resources. In fact, someone might effectively burn financial resources as part of their rational course of action.\nOne reason for this is that a core part of the evolved toolkit is our use of signals. We want to signal our traits or resources to others, including allies, enemies and potential reproductive partners.\nYet a problem with signals is that our interests are often not aligned with the recipient of our signal. We have an incentive to be dishonest, and the recipient knows this.\nAs a result, we need our signal to be reliable. One such way is that the signal imposes a cost on the signaller - and not just any cost - an actual handicap that someone without the trait or resources could not fake. The now almost cliched example of this is the peacock’s tail. It is a reliable signal of male health as only a male in good condition can maintain the unwieldy tail without falling prey to predators.\nIn the same way, one of the best ways to signal wealth is to burn money. Health can be signalled by unhealthy behaviours that would fry someone with a lesser constitution. An applied behavioural scientist assessing these behaviours from the perspective of the effect on long-term health or retirement savings is going to be somewhat confused. Yet when you see the objective, the behaviour has a purpose.\nOf course, it does not immediately follow that understanding a person’s evolutionary objectives will rationalise their behaviour. As our taste for sweet and fatty foods implies, our preferences evolved in a world much different to ours. But it does suggest that we need to be wary in judging people’s actions as their objective may not be what we think it is."
  },
  {
    "objectID": "posts/arent-we-smart-fellow-behavioural-scientists.html#reducing-power-use",
    "href": "posts/arent-we-smart-fellow-behavioural-scientists.html#reducing-power-use",
    "title": "Aren’t we smart, fellow behavioural scientists",
    "section": "Reducing power use",
    "text": "Reducing power use\nThis possible misunderstanding of people’s objectives can also arise simply as a practical issue. Let me give an example which, although not evolutionary, I think about a lot.\nPower companies often want to limit their customers’ electricity demand for environmental reasons, or to reduce peak demand.\nOne of the favourite tricks to do this is to provide a comparison of that person or household’s power consumption with their neighbours. People have a desire to conform, and look to cues to inform their decisions. If shown that their power usage is above their neighbours, they tend to reduce their use.\n\nSource: Opower, Allcott and Kessler (2019) AEJ: Applied Economics\nWhat is this person or household’s objective? If it were purely financial, success! They have saved on their power bill. Their reduction in use also happens to align with the environmental or peak demand reduction objectives of the nudger.\nYet is their objective that simple? What if it is, say, happiness or satisfaction in life? What of factors such as their comfort?\nAnd then what of their self-image? You’ve just compared them negatively with their neighbour. Is it likely to increase their happiness to see that they compare poorly? Does it increase mental stress? As applied behavioural scientists, we spend decidedly little time thinking about the breadth of the possible objectives someone may have and the effect of our nudge on them.\nThat is not to say that you cannot find examples where behavioural scientists have gone the next step to do the welfare calculus. This example of a comparison I have shown here comes from a paper by Hunt Allcott and Judd Kessler describing work with Opower. And what did they find? Although they argued that the net social welfare of the nudge was positive, a failure to consider these other objectives markedly overstates the benefits. Plus, about one third of the recipients would be willing to pay to not receive the nudge."
  },
  {
    "objectID": "posts/arent-we-smart-fellow-behavioural-scientists.html#close",
    "href": "posts/arent-we-smart-fellow-behavioural-scientists.html#close",
    "title": "Aren’t we smart, fellow behavioural scientists",
    "section": "Close",
    "text": "Close\nAnd now I will close, with a plea. As applied behavioural scientists, we need to inject some humility into our assessment of other people’s decisions. We need stop underestimating the intelligence of other people. We need to tone down the glee we have in communicating sexy, counterintuitive experimental findings that demonstrate errors by others. We need to stop making glib assumptions about what other people want and how they can best achieve their objectives. And importantly, we need to stop being lazy storytellers who don’t subject ourselves to the same critique that we would apply to someone else."
  },
  {
    "objectID": "posts/arielys-the-honest-truth-about-dishonesty.html",
    "href": "posts/arielys-the-honest-truth-about-dishonesty.html",
    "title": "Ariely’s The Honest Truth About Dishonesty",
    "section": "",
    "text": "I rate the third of Dan Ariely’s books, The Honest Truth About Dishonesty: How We Lie to Everyone - Especially Ourselves, somewhere between his first two books.\nOne of the strengths of Ariely’s books is that he is largely writing about his own experiments, and not simply scraping through the same barrel as every other pop behavioural science author. The Honest Truth has a smaller back catalogue of experiments to draw from than Predictably Irrational, so it sometimes meanders in the same way as The Upside of Irrationality. But the thread that ties The Honest Truth together - how and why we cheat - and Ariely’s investigations into it gave those extended riffs more substance than the story telling that filled some parts of The Upside.\nThe basic story of the book is that we like to see ourselves as honest, but are quite willing and able to indulge in a small amount of cheating where we can rationalise it. This amount of cheating is quite flexible based on situational factors, such as what other people are doing, and is not purely the result of a cost-benefit calculation.\nThe experiment that crops up again and again through the book is a task to find numbers in a series of matrices. People then shred the answers before collecting payment based on how many the completed. Most people cheat a little, possibly because they can rationalise that they could have solved more, or had almost completed the next one. Few cheat to the maximum, even when it is clear they have the opportunity to do so.\nFor much of the first part of the book, Ariely frames his research against the Simple Model of Rational Crime (or ‘SMORC’) - where people do a rational cost-benefit analysis as to whether to commit the crime. He shows experiments where people don’t cheat to the maximum amount when they have no chance of being caught - almost no-one says that they solved all the puzzles (amusingly, a few say they solved 20 out of 20, but no-one says 18 or 19). And most people do not increase their level of cheating when the potential gains increase.\nAs Ariely works through the various experiments attempting to isolate parts of the SMORC and show they don’t hold, I never felt fully satisfied. It is always possible to see how people might rationally respond in a way that thwarts the experimental design.\nFor example, Ariely found that changes in the stake with no change in enforcement did not result in an increase in cheating. But if I am in an environment with more money, I might assume there is more monitoring and enforcement, even if I can’t see it. However, I believe Ariely is right in arguing that the decision is not a pure cost-benefit analysis.\nOne of the more interesting parts of the book concerned how increasing the degrees of separation from the monetary outcome increases cheating. Having people collect tokens, which could be later exchanged for cash, increased cheating. In that light, a decision to cheat in an area such as financial services, where the ultimate cost is cash but there are many degrees of separation (e.g. manipulating an interest rate benchmark which changes the price I get on a trade which affects my profit and loss which affects the size of my bonus), might not feel like cheating at all.\nAs is the case when I read any behavioural science book, the part that leaves me slightly cold is that I’m not sure I can trust some of the results. The recent replication failures involving priming and ego depletion - and both phenomena feature in the book - resulted in me taking some of the results with a grain of salt. How many will stand the test of time? [Update: not so well. See here and here for examples.]"
  },
  {
    "objectID": "posts/avoiding-trite-lists-of-biases-and-pictures-of-human-brains-on-powerpoint-slides.html",
    "href": "posts/avoiding-trite-lists-of-biases-and-pictures-of-human-brains-on-powerpoint-slides.html",
    "title": "Avoiding trite lists of biases and pictures of human brains on PowerPoint slides",
    "section": "",
    "text": "From a book chapter by Greg Davies and Peter Brooks, Practical Challenges of Implementing Behavioral Finance: Reflections from the Field (quotes taken from a pre-print):\n\nTaken in isolation, the ideas and concepts that comprise the field of behavioral finance are of very little practical use. Indeed, many of the attempts to apply these ideas amount to little more than a trite list of biases and pictures of human brains on PowerPoint slides. Talking a good game in the arena of behavioral finance is easy, which often leads to the misperception that it is superficial. Yet, making behavioral finance work in practice is much more challenging: it requires integrating these ideas with working models, information technology (IT) systems, business processes, and organizational culture.\n\nSubstitute the word “behavioural finance” with “behavioural economics” and its kin, and the message reads the same.\nOn the “bias” bias:\n\nToday, extremely long lists of biases are available, which do little to convey the underlying sophistication, complexity, and thoroughness of more than half a century of highly robust experimental and theoretical work. These lists provide no real framework for potential practitioners to deploy when approaching a tangible problem. And many of these biases appear to overlap or conflict with each other, which can make behavioral finance appear either very superficial or highly confused.\nThe easily accessible examples that academics have used to illustrate these biases to wide audiences have sometimes led to the impression that behavioral economics is an easy field to master. This misrepresentation leads to inevitable disappointment when categorizing biases proves not to be an easy panacea. A perception of the field as “just anecdotes and parlor games” reduces the willingness of the commercial world to put substantial investments of time and resource into building applications grounded on the underlying ideas. Building behavioral finance ideas into commercial applications requires both depth and breadth of understanding of the theory and, in many cases, large resource commitments.\n\nOn whether there is a grand unified theory:\n\nA commonly expressed concern, at least in the mainstream press, is that there exists no grand unified theory of behavioral economics, and that the field is thus merely a chaotic collection of unconnected and often contradictory findings. For the purpose of practical implementation, the notion that this is, or needs to be, a clearly defined field should be eliminated, reducing the desire to erode it with arbitrary labels and definitions. Human behavior operates at multiple levels from the neurological to complex social interactions. Any quest for a grand unified theory to mirror that of physical sciences may well be entirely misguided, together with the notion that such a theory is necessary for the broad field to be useful. Much more effective is an approach of treating the full range of behavioral findings as a rich toolbox that can be applied to, and tested on, a range of practical concerns.\n\nOn the superficial application:\n\nThe first major challenge is that behavioral finance is not particularly effective if applied superficially. Yet, superficial attempts are commonplace. Some seek to do little more than offer a checklist of biases, hoping that informing people of poor decision-making can solve the problem. Instead, a central theme of decision science is the consistent finding that merely informing people of their adverse behavioral proclivities is very seldom effective in combating them.\nBecause behavioral finance is both topical and fascinating to many people, it attracts ‘hobbyists’ who can readily recite a number of biases, but who neither have the depth of knowledge of the field overall, nor a solid grasp of the theoretical underpinnings of the more technical aspects of the field. …\nThis chapter is not an attempt to erect barriers to entry amongst behavioral practitioners and claim that only those with advanced degrees in the field should be taken seriously. On the contrary, the effect of greater academic training can cause its beneficiaries to hold on too closely to narrow and technical interpretations of the field to make them effective practitioners. Indeed, some of the most effective practitioners do not have an extensive academic background in the field. However, they have invested considerable time and effort getting to know and deeply understand the breadth and depth of the field.\n\nAnd on naive buyers:\n\nLimited study of behavioral finance through reading the popular books on the topic may equip one to sound knowledgeable and appear convincing. However, as a relatively new field, the purchasers of behavioral expertise are seldom equipped to know the difference and may be unable to tell a superficially convincing approach from approaches that embody true understanding. This leaves the field open to consultants peddling ‘behavioral expertise’ but having in their toolkit little more than a list of biases that they apply sequentially and with little variation to each problem encountered. Warning flags should go up whenever the proposal rests heavily on catalogues of behavioral biases or contains a preponderance of pictures of brains."
  },
  {
    "objectID": "posts/bad-behavioural-science-failures-bias-and-fairy-tales.html",
    "href": "posts/bad-behavioural-science-failures-bias-and-fairy-tales.html",
    "title": "Bad Behavioural Science: Failures, bias and fairy tales",
    "section": "",
    "text": "Below is the text of my presentation to the Sydney Behavioural Economics and Behavioural Science Meetup on 11 May 2016. The talk is aimed at an intelligent non-specialist audience. I expect the behavioural science knowledge of most attendees is drawn from popular behavioural science books and meetups such as this.\nIntro\nThe typical behavioural science or behavioural economics event is a love-in. We all get together to laugh at people’s irrationality - that is, the irrationality of others - and opine that if only we designed the world more intelligently, people would make better decisions.\nWe can point to a vast literature – described in books such as Dan Ariely’sPredictably Irrational, Daniel Kahneman’sThinking, Fast and Slow, and Richard Thaler and Cass Sunstein’sNudge – all demonstrating the fallibility of humans, the vast array of biases we exhibit in our everyday decision making, and how we can help to overcome these problems.\nToday I want to muddy the waters. Not only is the “we can save the world” TED talk angle that tends to accompany behavioural science stories boring, but this angle also ignores the problems and debates in the field.\nI am going to tell you four stories – stories that many of you will have heard before. Then I am going to look at the foundations of each of these stories and show that the conclusions you should draw from each are not as clear as you might have been told.\nI will say at the outset that the message of this talk is not that all behavioural science is bunk. Rather, you need to critically assess what you hear.\nI should also point out that I am only covering one of the possible angles of critique. There are plenty of others.\nFor those who want to capture what I say, at 7pm tonight (AEST) the script of what I propose to talk about and the important images from my slides will be posted on my blog at jasoncollins.blog. That post will include links to all the studies I refer to.\nStory one – the Florida effect\nJohn Bargh and friends asked two groups of 30 psychology students to rearrange scrambled words into a sentence that made sense. Students in each of these groups were randomly assigned into one of two conditions. Some students received scrambled sentences with words that relate to elderly stereotypes, such as worried, Florida, old, lonely, grey, wrinkle, and so on. The other students were given sentences with non-age-specific words.\n\n\nBargh et al (1996)\n\n\n\nAfter completing this exercise the participants were debriefed and thanked. They then exited the laboratory by walking down a corridor.\nNow for the punch line. The experimenters timed the participants as they walked down the corridor. Those who had rearranged the sentences with non-age specific words walked down the corridor in a touch over seven seconds. Those who had rearranged the sentences with the elderly “primes” walked more slowly – down the corridor in a bit over eight seconds. A very cool result that has become known as the Florida Effect.\nExcept……the study doesn’t seem to replicate. In 2012 a paper was published in PLOS One where Stephen Doyen and friends used a laser timer to time how long people took to walk down the corridor after rearranging their scrambled sentences. The presence of the elderly words did not change their walking speed (unless the experimenters knew about the treatment - but that’s another story). There’s another failed replication on PsychFileDrawer.\nWhat was most striking about this failed replication – apart from putting a big question mark next to the result - was the way the lead researcher John Bargh attacked the PLOS One paper in a post on his Psychology Today blog (his blog post appears to have been deleted, but you can see a description of the content in Ed Yong’s article). Apart from calling the post “Nothing in their heads” and describing the researchers as incompetent, he desperately tried to differentiate the results - such as by arguing there were differences in methodology (which in some cases did not actually exist) - and by suggesting that the replication team used too many primes.\nI don’t want to pick on this particular study alone (although I’m happy to pick on the reaction). After all, failure to replicate is not proof that the effect does not exist. But failure to replicate is a systematic problem in the behavioural sciences (in fact, many sciences). A study by Brian Nosek and friends published in Science examined 100 cognitive and social psychology studies published in several major psychology journals. They subjectively rated 39% of the studies they attempted to replicate as having replicated. Only 25% of social psychology studies in that study met that mark. The size of the effect in these studies was also around half of that in the originals - as shown in this plot of original versus replication effect sizes. The Florida effect is just the tip of the iceberg.\n\n\nNosek et al (2015)\n\n\n\nPriming studies seem to be particularly problematic. Another priming area in trouble is “money priming”, where exposure to images of money or the concept of money make people less willing to help others or more likely to endorse a free market economy. As an example, one set of replications of the effect of money primes on political views by Rohrer and friends - as shown in these four charts - found no effect (ungated pdf). Analysis of the broader literature on money priming suggests, among other things, massive publication bias.\n\n\nRohrer et al (2015)\n\n\n\nAs a non-priming example, those of you who have read Daniel Kahneman’s Thinking, Fast and Slow or Malcolm Gladwell’s David and Goliath might recall a study by Adam Alter and friends. In that study, 40 students were exposed to two versions of the cognitive reflection task. One of the typical questions in the cognitive reflection task is the following classic:\n\nA bat and a ball cost $1.10 in total. The bat costs $1.00 more than the ball. How much does the ball cost?\n\nThe two versions differed in that one used small light grey font that made the questions hard to read. Those exposed to the harder to read questions achieved higher scores.\nIt all sounds very cool. Slowing people down made them do better. But while the original study with 40 subjects found a large effect, replications involving thousands of people found nothing (Terry Burnham discusses this paper in more detail here). As you can see in the chart, the positive result is a small sample outlier.\n\n\nMeyer et al. (2015)\n\n\n\nThen there is ego depletion - the idea that we have a fixed stock of willpower that becomes depleted through use. If we have to use our willpower in one setting, we’re more likely to crumble later on as our ego is depleted.\nNow, this theory doesn’t rest on one study - a 2010 meta-analysis examined 83 studies with 198 experiments in concluding there was an ego depletion effect. But that meta-analysis had a lot of flaws, including only including published studies.\nSoon a pre-registered replication of one ego depletion experiment involving 23 labs and over 2,000 subjects will be published in Psychological Science. The result? If there is any effect of ego depletion - at least as captured in that experiment - it is close to zero.\nSo what is going on here? Why all these failures? One, there is likely publication bias. Only those studies with positive results make it into print. Small sample sizes in many studies make it likely that any positive results are false positives.\nThen there is p-hacking. People play around with their hypotheses and the data until they get the result they want.\nThen there is the garden of forking paths, which is the more subtle process whereby people choose their method of analysis or what data to exclude by what often seems to be good reasons after the fact. All of these lead to a higher probability of positive results and these positive results end up being the ones that we read.\nNow that these bodies of research are crumbling, some of the obfuscation going on is deplorable. John Bargh’s concerning the Florida effect is one of the more extreme examples. Many of the original study proponents erect the defences and claim poor replication technique or that they haven’t captured all the subtleties of the situation. Personally, I’d like to see a lot more admissions of “well, that didn’t turn out”.\nBut what is also surprising was the level of confidence some people had in these findings. Here’s a passage from Kahneman’s Thinking, Fast and Slow - straight out of the chapter on priming:\n\nWhen I describe priming studies to audiences, the reaction is often disbelief. This is not a surprise: System 2 believes that it is in charge and that it knows the reasons for its choices. Questions are probably cropping up in your mind as well: How is it possible for such trivial manipulations of the context to have such large effects? …\nThe idea you should focus on, however, is that disbelief is not an option. The results are not made up, nor are they statistical flukes. You have no choice but to accept that the major conclusions of these studies are true.\n\nErr, no.\nSo, don’t believe every study you read. Maintain some of that scepticism even for large bodies of published research. Look for pre-registered replications where people clearly stated what they were going to do before they did it.\nAnd I should say, this recommendation doesn’t just apply to academic studies. There are now plenty of governments and consultants running around advertising the results of their behavioural work with approaches also likely to be subject to similar problems.\nStory two - the jam study\nOn two Saturdays in a California supermarket, Mark Lepper and Sheena Iyengar (ungated pdf) set up tasting displays of either six or 24 jars of jam. Consumers could taste as many jams as they wished, and if they approached the tasting table they received a $1 discount coupon to buy the jam.\nFor attracting initial interest, the large display of 24 jams did a better job, with 60 per cent of people who passed the display stopping. Forty per cent of people stopped at the six jam display. But only three per cent of those who stopped at the 24 jam display purchased any of the jam, compared with almost 30 per cent who stopped at the six jam display.\nThis result has been one of the centrepieces of the argument that more choice is not necessarily good. The larger display seemed to reduce consumer motivation to buy the product. The theories around this concept and the associated idea that more choice does not make us happy are often labelled the choice overload hypothesis or the paradox of choice. Barry Schwartz wrote a whole book on this topic.\nFast-forward 10 years to another paper, this one by Benjamin Scheibehenne and friends (ungated pdf). They surveyed the literature on the choice overload hypothesis – there is plenty. And across the basket of studies - shown in this chart - evidence of choice overload does not emerge so clearly. In some cases, choice increases purchases. In others it reduces them. Scheibehenne and friends determined that the mean effect size of changing the number of choices across the studies was effectively zero.\n\n\nScheibehenne\n\n\n\nThese reviewed studies included a few attempts to replicate the jam study results. An experiment using jam in an upscale German supermarket found no effect. Other experiments found no effect of choice size using chocolates or jelly beans. There were small differences in study design between these and the original jam study (as original authors are often quick to point out when replications fail), but if studies are so sensitive to study design and hard to replicate, it seems foolhardy to extrapolate the results of the original study too far.\nThere is a great quote from one of my favourite books, Jim Manzi’s Uncontrolled, which captures this danger.\n\n[P]opularizers telescoped the conclusions derived from one coupon-plus-display promotion in one store on two Saturdays, up through assertions about the impact of product selection for jam for this store, to the impact of product selection for jam for all grocery stores in America, to claims about the impact of product selection for all retail products of any kind in every store, ultimately to fairly grandiose claims about the benefits of choice to society.\n\nWhile these study results often lead to grandiose extrapolations, the defences of these studies when there is a failure to replicate or ambiguous evidence often undermine the extent of these claims. Claiming that the replication didn’t perfectly copy the original study suggests the original effect applies to a small set of circumstances. This is no longer TED talk material that can be applied across our whole life.\nThat is not to say that there is not something interesting going on in these choice studies. Scheibehenne and friends suggest that there may be a set of restrictive conditions under which choice overload occurs. These conditions might involve the complexity (and not the size) of the choice, the lack of dominant alternatives, assortment of options, time pressure or the distribution of product quality (as also suggested by another meta-analysis). And since the jam study appears tough to replicate, these conditions might be narrow. They suggest more subtle solutions than simply reducing choice. Let’s not recommend supermarkets get rid of 75% of their product lines to boost their sales by 900%.\nSo, even if a study suggests something interesting is going on, don’t immediately swallow the TED talk and book on how this completely changes our understanding of the world. Even if the result is interesting, the story is likely more subtle than the way it is told.\nStory three - organ donation\nOrgan donation rates are an often used example of the power of defaults. I’m now going to take a moment to read a passage by Dan Ariely explaining how defaults affect organ donation rates. He refers to this chart from Johnson and Goldstein (2003) (ungated pdf):\n\nOne of my favorite graphs in all of social science is the following plot from an inspiring paper by Eric Johnson and Daniel Goldstein. This graph shows the percentage of people, across different European countries, who are willing to donate their organs after they pass away. When people see this plot and try to speculate about the cause for the differences between the countries that donate a lot (in blue) and the countries that donate little (in orange) they usually come up with “big” reasons such as religion, culture, etc.\n\nBut you will notice that pairs of similar countries have very different levels of organ donations. For example, take the following pairs of countries: Denmark and Sweden; the Netherlands and Belgium; Austria and Germany (and depending on your individual perspective France and the UK). These are countries that we usually think of as rather similar in terms of culture, religion, etc., yet their levels of organ donations are very different.\nSo, what could explain these differences? It turns out that it is the design of the form at the DMV. In countries where the form is set as “opt-in” (check this box if you want to participate in the organ donation program) people do not check the box and as a consequence they do not become a part of the program. In countries where the form is set as “opt-out” (check this box if you don’t want to participate in the organ donation program) people also do not check the box and are automatically enrolled in the program. In both cases large proportions of people simply adopt the default option.\n\n\nJohnson and Goldstein (2003)\n\n\n\nBut does this chart seem right given that story? Only 2 in every 10,000 people fail to opt-out in Austria? Only 3 in 10,000 in Hungary? It seems too few. And for Dan Ariely’s story, it is too few, because the process is not as described.\nThe hint is in the term “presumed consent” in the chart description. There is actually no time where Austrians or Hungarians are presented with a form where they can simply change from the default. Instead, they are presumed to consent to organ donation. To change that presumption, they have to take steps such as contacting government authorities to submit forms stating they don’t want their organs removed. Most people probably don’t even think about it. It’s like calling my Australian citizenship - resulting from my birth in Australia - a default and praising the Australian Government for its fine choice architecture.\nAnd what about the outcomes we care about - actual organ donation rates. Remember, the numbers on the Johnson and Goldstein chart aren’t the proportion of people with organs removed from their bodies. It turns out that the relationship is much weaker there.\nHere is a second chart with actual donation rates - the same countries in the same order. The relationship suddenly looks a lot less clear. Germany at 15.3 deceased donors per million people is not far from Austria’s 18.8 and above Sweden’s 15.1. For two countries not on this chart, Spain, which has an opt-out arrangement, is far ahead of most countries at 33.8 deceased donors per million, but the United States, an opt-in country, is also ahead of most opt-out countries with a donation rate of 26.0.\n\n\nDeceased donors per million people\n\n\n\n[To be clear, I am not suggesting that Johnson and Goldstein did not analyse the actual donation rates, nor that no difference exists - there is an estimate of the effect of presumed consent in their paper, and other papers also attempt to do this. Those papers generally find a positive effect. However, the story is almost always told using the first chart. A difference of 16.4 versus 14.1 donors per million (Johnson and Goldstein’s estimate) is not quite as striking as 99.98% for Austria versus 12% for Germany. Even my uncontrolled chart could be seen to be exaggerating the difference - the averages in my chart are 13.1 per million for opt out and 19.3 per million for presumed consent.]\nSo, if you can, read the original papers, not the popularised version - and I should say that although I’ve picked on Dan Ariely’s telling of the story here, he is hardly Robinson Crusoe in telling the organ donation story in that way. I’ve lost count of the number of times reading the original paper has completely derailed what I thought was the paper’s message.\nIn fact, sometimes you will discover there is no evidence for the story at all - Richard Titmuss’s suggestion that paying for blood donations might reduce supply by crowding out intrinsic motivations was a thought experiment, not an observed effect. Recent evidence suggests that - as per most economic goods - paying for blood could increase supply.\nAnd this organ donation story provides a second more subtle lesson - if you can, look at the outcomes we want to influence, not some proxy that might not lead where you hope.\nStory four - the hot hand\nThis last story is going to be somewhat technical. I actually chose it as a challenge to myself to see if I could communicate this idea to a group of intelligent non-technical people. It’s also a very cool story, based on work by Joshua Miller and Adam Sanjurjo. I don’t expect you to be able to immediately go and give these explanations to someone else at the end of this talk, but I hope you can see something interesting is going on.\nSo, when people watch sports such as basketball, they often see a hot hand. They will describe players as “hot” and “in form”. Our belief is that the person who has just hit a shot or a series of shots is more likely to hit their next one.\nBut is this belief in the ‘hot hand’ a rational belief? Or is it the case that people are seeing something that doesn’t exist? Is the ‘hot hand’ an illusion?\nTo answer this question, Thomas Gilovich, Robert Vallone and Amos Tversky took masses of shot data from a variety of sources, including the Philadelphia 76ers and Boston Celtics, and examined it for evidence of a hot hand. This included shots in games, free throws and a controlled shooting experiment.\nWhat did they find? The hot hand was an illusion.\nSo, let’s talk a bit about how we might show this. This table shows a set of four shots by a player in each of 10 games. In the first column is the results of their shots. An X is a hit, an O is a miss. This particular player took 40 shots and hit 20 – so they are a 50% shooter.\nSo what would count as evidence of a hot hand? What we can do is compare 1) the proportion of shots they hit if the shot immediately before was a hit with 2) their normal shooting percentage. If their hit rate after a hit is higher than their normal shot probability, then we might say they get hot.\nThe second column of the table shows proportion of shots hit by the player if the shot before was a hit. Looking at the first sequence, the first shot was a hit, and it is followed by a hit. The second shot, a hit, is followed by a miss. So, for that first sequence, the proportion of hits if the shot before was a hit is 50%. The last shot, the third hit, is not followed by any other shots, so does not affect our calculation. The rest of that column shows the proportion of hits followed by hits for the other sequences. Where there is no hit in the first three shots, those sequences don’t enter our calculations.\nBasketball player shot sequences (X=hit, O=miss)\n\n\n\nShots\np(Xt|Xt+1)\n\n\n\n\nXXOX\n50%\n\n\nOXOX\n0%\n\n\nOOXX\n100%\n\n\nOXOX\n0%\n\n\nXXXX\n100%\n\n\nXOOX\n0%\n\n\nXXOO\n50%\n\n\nOOOO\n-\n\n\nOOOX\n-\n\n\nOOXX\n100%\n\n\nAVERAGE\n50%\n\n\n\nAcross these sequences, the average proportion of hits following a hit is 50%. (That average is also the expected value we would get if we randomly picked one of these sequences.) Since the proportion of hits after a hit is the same as their shooting percentage, we could argue that they don’t have a hot hand.\nNow, I am going to take you on a detour, and then we’ll come back to this example. And that detour involves the coin flipping that I got everyone to do before we commenced.\n34 people flipped a coin four times, and I asked you to try to flip a heads on each flip. [The numbers obtained for the coin flipping were, obviously, added after the fact. The raw data is here. And as it turned out they did not quite tell the story I expected, so there are some slight amendments below to the original script.] Here are the results of our experiment. In the second column is the proportion of heads that you threw. Across all of you, you flipped heads 49% of the time, pretty close to 50%. Obviously you have no control over your flips. But what is more interesting is the second column. On average, the proportion of heads flipped after an earlier flip of heads looks to be closer to 48%.\nMeetup experiment results - flipping a coin four times\n\n\n\nNumber of players\np(H)\np(H|H)\n\n\n\n\n34\n49%\n48%\n\n\n\nNow, intuition tells us the probability of a heads after flipping an earlier heads will be 50% (unless you suffer from the gambler’s fallacy). So this seems to be the right result.\nBut let’s have a closer look at this. This next table shows the 16 possible combinations of heads and tails you could have flipped. Each of these 16 combinations has an equal probability of occurring. What is the average proportion of heads following a previous flip of a heads? It turns out it is 40.5%. That doesn’t seem right. But let’s delve deeper into this. In the third column is how many heads follow a heads, and the fourth how many tails follow a heads. If we count across all the sequences, we see that we have 12 heads and 12 tails after the 24 earlier flips of heads - spot on the 50% you expect.\n16 possible combinations of heads and tails across four flips\n\n\n\n\n\n\n\n\n\nFlips\np(Ht+1|Ht)\nn(Ht+1|Ht)\nn(Tt+1|Ht)\n\n\n\n\nHHHH\n100%\n3\n0\n\n\nHHHT\n67%\n2\n1\n\n\nHHTH\n50%\n1\n1\n\n\nHHTT\n50%\n1\n1\n\n\nHTHH\n50%\n1\n1\n\n\nHTHT\n0%\n0\n2\n\n\nHTTH\n0%\n0\n1\n\n\nHTTT\n0%\n0\n1\n\n\nTHHH\n100%\n2\n0\n\n\nTHHT\n50%\n1\n1\n\n\nTHTH\n0%\n0\n1\n\n\nTHTT\n0%\n0\n1\n\n\nTTHH\n100%\n1\n0\n\n\nTTHT\n0%\n0\n1\n\n\nTTTH\n-\n-\n-\n\n\nTTTT\n-\n-\n-\n\n\nAVERAGE\n40.5%\n12\n12\n\n\n\nSo what is going on in that second column. By looking at these short sequences, we are introducing a bias. Most of the cases of heads following heads are clustered together - such as the first sequence which has three cases of a heads following a heads. Yet it has the same weight in our average as the sequence TTHT - with only one shot occurring after a heads. The reason a tails appears more likely to follow a heads is because of this bias. The actual probability of a heads following a heads is 50%.\nAnd if we do the same exercise for your flips, the result looks now look a bit different - you flipped 28 heads and 22 tails for the 50 flips directly after a head. 56% heads, 44% tails. It seems you have a hot hand, although our original analysis clouded that result (Obviously, they didn’t really have a hot hand - it is a chance result. There was a 24% probability of getting 28 or more heads. Ideally I should have got a larger sample size.)\nMeetup experiment results - flipping a coin four times\n\n\n\n\n\n\n\n\n\n\nNumber of players\np(H)\np(H|H)\nn(H|H)\nn(T|H)\n\n\n\n\n34\n49%\n48%\n28\n22\n\n\n\nTurning back to the basketball example I showed you at the beginning, there I suggested there was a 50% chance of a hit after a hit for a 50% shooter - the first two columns of the table below. But let’s count the shots that occur after a hit. There are 12 shots that occur after a hit, and it turns out that 7 of these shots are a hit. Our shooter hits 58% of shots immediately following a hit. They miss on only 42% of those shots. They have a hot hand (noting the small sample size here……but you get the picture).\nBasketball player shot sequences (X=hit, O=miss)\n\n\n\n\n\n\n\n\n\nShots\np(Xt+1|Xt)\nn(Xt+1|Xt)\nn(Ot+1|Xt)\n\n\n\n\nXXOX\n50%\n1\n1\n\n\nOXOX\n0%\n0\n1\n\n\nOOXX\n100%\n1\n0\n\n\nOXOX\n0%\n0\n1\n\n\nXXXX\n100%\n3\n0\n\n\nXOOX\n0%\n0\n1\n\n\nXXOO\n50%\n1\n1\n\n\nOOOO\n-\n-\n-\n\n\nOOOX\n-\n-\n-\n\n\nOOXX\n100%\n1\n0\n\n\nAVERAGE\n50%\n7\n5\n\n\n\nSo, why have I bothered with this stats lesson? By taking short sequences of shots and measuring the proportion of hits following a hit, I have introduced a bias in the measurement. The reason this is important is because the papers that supposedly show that there is no hot hand used a methodology that suffered from this same bias. When you correct for the bias, there is a hot hand.\nTaking the famous paper by Tom Gilovich and friends that I mentioned at the beginning, they did not average across sequences as I have shown here. But by looking at short sequences of shots, selecting each hit (or sequence of hits) and seeing the result of the following shot, they introduced the same bias. The bias acts in the opposite direction to the hot hand, effectively cancelling it out and leading to a conclusion that each shot is independent of the last.\nMiller and Sanjurjo crunched the numbers for one of the studies in the Gilovich and friends paper, and found that the probability of hitting a three pointer following a sequence of three previous hits is 13 percentage points higher than after a sequence of three misses. There truly is a hot hand. To give you a sense of the scale of that difference, Miller and Sanjurjo note that the difference between the median and best three point shooter in the NBA is only 10 percentage points.\nApart from the fact that this statistical bias slipped past everyone’s attention for close to thirty years, I find this result extraordinarily interesting for another reason. We have a body of research that suggests that even slight cues in the environment can change our actions. Words associated with old people can slow us down. Images of money can make us selfish. And so on. Yet why haven’t these same researchers been asking why a basketball player would not be influenced by their earlier shots - surely a more salient part of the environment than the word “Florida”? The desire to show one bias allowed them to overlook another.\nSo, remember that behavioural scientists are as biased as anyone.\nIf you are interested in learning more….\nBefore I close, I’ll leave with a few places you can go if you found tonight’s presentation interesting.\nFirst is Andrew Gelman’s truly wonderful blog Statistical Modeling, Causal Inference, and Social Science. Please don’t be put off by the name - you will learn something from Gelman even if you know little about statistics. Personally, I’ve learnt more about statistics from this blog than I did through the half a dozen statistics and econometrics units I completed through university. This is the number one place to see crap papers skewered and for discussion about why we see so much poor research. Google Andrew Gelman and his blog will be at the top of the list.\nSecond, read Jim Manzi’s Uncontrolled. It will give you a new lens with which to think about causal associations in our world. Manzi’s plea for humility about what we believe we know is important.\nThird, read some Gerd Gigerenzer. I only touched on a couple of the critiques of behavioural science tonight. There are many others, such as the question of how irrational we really are. On this angle, Gigerenzer’s work is among the most interesting. I suggest starting with Simple Heuristics That Make Us Smart by Gigerenzer, Peter Todd and the ABC Research Group, and go from there.\nFinally, check out my blog at jasoncollins.blog. I’m grumpy about more than the material that I covered tonight. I will point you to one piece – Please Not Another Bias: An Evolutionary Take on Behavioural Economics - where I complain about how behavioural economics needs to be more than a collection of biases, but hopefully you will find more there that’s of interest to you.\nAnd that’s it for tonight."
  },
  {
    "objectID": "posts/bad-statistics-cancer-edition.html",
    "href": "posts/bad-statistics-cancer-edition.html",
    "title": "Bad statistics - cancer edition",
    "section": "",
    "text": "Are two-thirds of cancer due to bad luck as many recent headlines have stated? Well, we don’t really know.\nThe paper that triggered these headlines found that two-thirds of the variation in log of cancer risk can be explained by the number of cell divisions. More cell divisions - more opportunity for “bad luck”.\nBut, as pointed out by Bob O’Hara and GrrlScientist, an explanation for variation in incidence is not an explanation for the absolute numbers. As they state, although all variation in the depth of the Marianas trench might be due to tides, tides are not the explanation for the depth of the trench itself. There might be some underlying factor X affecting all cancers.\nMy reason for drawing attention to this misinterpretation is that a similar confusion occurs in discussions of heritability. Heritability is the proportion of variation in phenotype - an organism’s observable traits - due to genes. If heritability of height is 0.8, 80 per cent of variation in height is due to genetic variation. But your height is not “80 per cent due to genes”.\nTo make this distinction clear, consider the number of fingers on your hand. Heritability of the number of fingers on your hand is close to zero. This is because most variation is due to accidents where people lose a finger or two. But does this mean that the number of fingers on your hand is almost entirely due to environmental factors? No, it’s almost entirely genetic - those five fingers are an expression of your genes. There is an underlying factor X - our genes - that are responsible for the major pattern.\nTurning back to the cancer paper, as PZ Myer points out, there may be no underlying factor X affecting cancer and the two-thirds figure could be correct. Extrapolating one chart hints that might be the case. But that’s not what the paper states.\nAs an endnote, a recent study pointed out that most errors in scientific reporting start in the research centre press release. This case looks like no exception. From the initial John Hopkins press release (underlining mine):\n\nScientists from the Johns Hopkins Kimmel Cancer Center have created a statistical model that measures the proportion of cancer incidence, across many tissue types, caused mainly by random mutations that occur when stem cells divide. By their measure, two-thirds of adult cancer incidence across tissues can be explained primarily by “bad luck,” when these random mutations occur in genes that can drive cancer growth, while the remaining third are due to environmental factors and inherited genes.\n\nAnd from their updated press release:\n\nScientists from the Johns Hopkins Kimmel Cancer Center have created a statistical model that measures the proportion of cancer risk, across many tissue types, caused mainly by random mutations that occur when stem cells divide. By their measure, two-thirds of the variation in adult cancer risk across tissues can be explained primarily by “bad luck,” when these random mutations occur in genes that can drive cancer growth, while the remaining third are due to environmental factors and inherited genes.\n\nGood on them for updating, but it would have been nice if they had clarified why their first release was problematic."
  },
  {
    "objectID": "posts/banking-as-an-ecosystem.html",
    "href": "posts/banking-as-an-ecosystem.html",
    "title": "Banking as an ecosystem",
    "section": "",
    "text": "Most of my interest in the use of biology in economics concerns humans being subject to the forces of selection like any other biological organism. With this starting point, it is natural to use many of the tools, models and methods of analysis that evolutionary biologists use.\nBut sometimes those models and tools are of value without the biological underpinnings. Evolutionary economics is one area where this is done, with the concepts of selection applied at the level of firms (as discussed in my last post).\nAnother instance of this crossover was in an article published by Andrew Haldane and Robert May, who have proposed that analysis of complexity and stability in ecosystems (dating from the 1970s) is useful in examining financial systems.\nHaldane and May’s starting point was the recognition that complex ecosystems are not necessarily stable, with instability increasing with the number and strength of interactions. As an example, they noted recent work by Caccioli and colleagues which suggested that very strong fluctuations in the volume of trading in derivative markets could occur in a complex but complete market. As long as there is a positive premium to trading, banks will supply new financial instruments despite the lack of demand from non-banks. This expansion in derivatives comes at the cost of stability. There is no benefit to this expansion as market completeness has already been achieved.\nHaldane and May developed a model which examined how banks may fail in response to a shock. In their model, each bank is linked to the same number of other banks and each has the same size of loans, capital reserves and ratio of loans to total assets. The more banks each bank is linked to reduces the number of failures following from the first bank failure, as the losses are spread more broadly. However, when later failures do occur, they will involve more banks. The model also showed the potential of small liquidity shocks to amplify through the system. Liquidity “hoarding” can have significant effects, as we saw in the recent crisis.\nThey also noted that their model reflected earlier work that had shown that as banks become increasingly homogeneous in their holdings (as they seek to cut their risks through diversification), the probability of the entire system collapsing increases. Once they are the same, the probability of one bank failing is the probability of all banks failing.\nHaldane and May list a number of policy implications of their model. The first is that there is a broader role to minimum capital requirements for banks than simply reducing risk to each bank. Capital requirements could increase the entire system’s stability. Regulators should set capital limits with the broader systemic implications in mind.\nA second implication concerned the goal of regulatory intervention. Typically, regulation might seek to reduce the probability of failure of all institutions to below a certain threshold. Haldane and May suggest that particular institutions that pose broad systemic risk should face higher regulatory requirements.\nThe most interesting suggestion concerned the desire to shape the topology of the financial system. As banks diversify, they became homogeneous. Accordingly, Haldane and May noted that a diversity objective of regulators may have merit. Trying to introduce “modularity” to prevent cascades through the entire system may also be desirable.\nNature published two responses to Haldane and May’s article: one in support of the use of such analysis by Thomas Lux, while Neil Johnson suggests that a model as simple as that used by Haldane and May will produce unreliable predictions that are only as robust as the assumptions used to prepare the model.\nI do not have much sympathy for Johnson’s argument. While it is appropriate for models to contain health warnings about how broadly applicable the model is, models should by their nature have some simplicity. The question is whether any concepts are usefully illustrated. Haldane and May’s paper has several. Without a doubt, further work on these models by adding more elements and testing the robustness of the assumptions could be useful. That is often the way that science progresses. But to suggest that we cannot scale up a paper plane to a full-scale 747 does not mean that a paper plane can teach nothing about flight.\nWe may see more of these types of studies, or at least in Nature, as an editorial in the same issue suggested that where economic research has significant implications for fields such as behaviour, conservation biology, systems biology or physics, they would be happy to publish it. The editors suggested that this could benefit both economic science and natural science. My instinct is that economics has the most to gain."
  },
  {
    "objectID": "posts/baumeister-and-tierneys-willpower-rediscovering-the-greatest-human-strength.html",
    "href": "posts/baumeister-and-tierneys-willpower-rediscovering-the-greatest-human-strength.html",
    "title": "Baumeister and Tierney’s Willpower: Rediscovering the Greatest Human Strength",
    "section": "",
    "text": "After the recent hullabaloo about whether ego depletion was a real phenomenon, I decided to finally read Roy Baumeister and John Tierney’s Willpower cover to cover (I’ve only flicked through it before).\nMy hope was that I’d find some interesting additions to my understanding of the debate, but the book tended into the pop science/self-help genre and there was rarely enough depth to add anything to the current debates (see Scott Alexander on that point). That said, it was an easy read and pointed me to a few studies that seem worth checking out.\nOne area that I have been interested in is the failure of the mathematics around glucose consumption to add up. Baumeister’s argument is that glucose is the scarce resource in the ego depletion equation. Exercising self control depletes our glucose, making us more likely to succumb to later temptations. Replenishing glucose restores our ego.\nAs plenty of people have pointed out - Robert Kurzban is the critic I am most familiar with - the maths on glucose simply does not add up. The brain does not burn more calories when making a quick decision. Even if it did (say, doubling while making a decision), the short time in which the decision is made means the additional energy expenditure would be miniscule.\nBaumeister and Tierney indirectly dealt with the criticism, writing:\n\nDespite all these findings, the growing community of brain researchers still had some reservations about the glucose connection. Some skeptics pointed out that the brain’s overall use of energy remains about the same regardless of what one is doing, which doesn’t square easily with the notion of depleted energy. Among the skeptics was Todd Heatherton….\nHeatherton decided on an ambitious test of the theory. He and his colleagues recruited dieters and measured their reactions to pictures of food. Then ego depletion was induced by asking everyone to refrain from laughing while watching a comedy video. After that, the researchers again tested how their brains reacted to pictures of food (as compared with nonfood pictures). Earlier work by Heatherton and Kate Demos had shown that these pictures produce various reactions in key brain sites, such as the nucleus accumbens and a corresponding decrease in the amygdala. The crucial change in the experiment involved a manipulation of glucose. Some people drank lemonade sweetened with sugar, which sent glucose flooding through the bloodstream and presumably into the brain.\nDramatically, Heatherton announced his results during his speech accepting leadership of the Society for Personality and Social Psychology … Heatherton reported that the glucose reversed the brain changes wrought by depletion, a finding he said, that thoroughly surprised him. … Heatherton’s results did much more than provide additional confirmation that glucose is a vital part of willpower. They helped resolve the puzzle over how glucose could work without global changes in the brain’s total energy use. Apparently ego depletion shifts activity from one part of the brain to another. Your brain does not stop working when glucose is low. It stops doing some things and starts doing others.\n\nIn an hour of searching, I couldn’t find a publication arising from this particular study - happy for any pointers. (Interestingly, Demos is author of a paper on a failed replication of an ego depletion experiment.) I’m guessing that the initial findings didn’t hold up.\nGiven the challenges to ego depletion theory, it seems Baumeister is considering tweaking the theory (I found an ungated copy here). If you want a more recent, although not necessarily balanced view on where the theory is at, skip Willpower and start there.\nFor another perspective on Willpower, see also Steven Pinker’s review."
  },
  {
    "objectID": "posts/becker-on-evolution-and-economics.html",
    "href": "posts/becker-on-evolution-and-economics.html",
    "title": "Becker on evolution and economics",
    "section": "",
    "text": "Gary Becker was one of the first economists to seriously contemplate the role that evolutionary biology could play in economics. In 1976, he wrote:\n\nI have argued that both economics and sociobiology would gain from combining the analytical techniques of economists with the techniques in population genetics, entomology, and other biological foundations of sociobiology. The preferences taken as given by economists and vaguely attributed to “human nature” or something similar – the emphasis on self-interest, altruism toward kin, social distinction, and other enduring aspects of preferences – may be largely explained by the selection over time of traits having greater genetic fitness and survival value.\n\nOver the last two days, a couple of people have classed him as the greatest social scientist of the last 50 years. I am happy to grant the title of greatest economist over that period, but I’ll reserve the social scientist title. Becker broke down a lot of barriers in other fields, but I am not sure that many of his applications will outlast approaches grounded in biology. In the long-run, E.O. Wilson and his groundbreaking work starting with Sociobiology might be the more important piece.\nIn the meantime, below are links to some posts over the last few years I have written about Becker’s work:\n\nRotten kids and altruism.\nAltruists and the knowledge problem.\nConsumption and fitness.\nDeriving the demand for children.\nThe evolution of happiness."
  },
  {
    "objectID": "posts/behavioural-economics-underrated-or-overrated.html",
    "href": "posts/behavioural-economics-underrated-or-overrated.html",
    "title": "Behavioural economics: underrated or overrated?",
    "section": "",
    "text": "Tyler Cowen’s Conversations with Tyler feature a section in which Cowen throws a series of ideas at the guest, and the guest responds with whether each idea is overrated or underrated. In a few of the conversations, Cowen asks about behavioural economics. Here are three responses:\nAtul Gawande\nCOWEN: The idea of nudge.\nGAWANDE: I think overrated.\nCOWEN: Why?\nGAWANDE: I think that there are important insights in nudge units and in that research capacity, but when you step back and say, “What are the biggest problems in clinical behavior and delivery of healthcare?” the nudges are focused on small solutions that have not demonstrated capacity for major scale.\nThe kind of nudge capability is something we’ve built into the stuff we’ve done, whether it’s checklists or coaching, but it’s been only one. We’ve had to add other tools. You could not get to massive reductions in deaths in surgery or childbirth or massive improvements in end-of-life outcomes based on just those behavioral science insights alone. We’ve had to move to organizational insights and to piece together multiple kinds of layers of understanding in order to drive high-volume change in healthcare delivery.\n\nSteven Pinker\nCOWEN: Behavioral economics. Economists playing at psychology. Obviously you have a stronger background in psychology than the economists. What do you think of behavioral econ?\nPINKER: I’m for it.\nCOWEN: What’s it missing?\nPINKER: I’m completely out of my depth here, but I do think it is too quick to dismiss classical economics. Is this maybe another false dichotomy?\nThe idea that the rational actor and models derived from it are obsolete because humans make certain irrational choices, have certain rules of thumb that can’t be normatively defended — those aren’t necessarily incompatible, because even though every individual human brain might have its quirks and be irrational, it is possible for a collective enterprise that works by certain rules to have a kind of rationality that none of the individual minds has.\nAlso it’s possible because we’re corrigible, because the mind is many parts. We can override some of our biases and instincts either though confrontations with reality, through education, through debate.\nWe do know even that people who are experienced in market transactions, for example, don’t fall for the kinds of fallacies that behavioral economists are so fond of pointing out. You really can’t turn a person into a money pump, even though in the lab I can set up a demo that shows people can be intransitive in their preferences.\nYou actually put a person in a situation where there’s real money at stake, and all of a sudden they’re not so irrational.\nCOWEN: They walk away.\n\nJonathan Haidt\nCOWEN: You’re a trained psychologist, in addition to your most famous work, you have a lot of other papers which are very well cited, but less famous for other public intellectuals doing what you’d call traditional psychological research. Here we have these economists, they do what they call behavioral economics, and they tread into the field of psychology, do they know what they’re doing? Behavioral economics, underrated or overrated?\nHAIDT: Properly rated right now, with one caveat. We psychologists have long felt, “Oh those economists they’re the only ones that are ever consulted in Congress, and they have all these high‑prestige jobs, they have a Nobel Prize, nobody listens to us.”\nSome economists beginning with Robert Frank, and Dan Kahneman, Dick Thaler, the fact that economists have been listening to psychologists, and making our work more well‑known, of course Kahneman did a lot of that work, and he is a psychologist.\nThat’s all good, I’m thrilled with the way that’s going. The only caveat that I would put which I would say if they don’t do this soon, then they would be overrated, is the behavioral economics work is an example of this wonderful dictum from Robert Zion, the famous social psychologist, which is that cognitive psychology is social psychology with all the interesting variables set to zero.\nTo the extent that behavioral economists are saying, “Look at a person shopping, what influences their decision? If the apple is at eye‑level — .” They’re looking at lone consumers who are trying to make choices to optimize their outcomes. That’s great work, but that’s setting all the interesting variables to zero. The interesting stuff is all social. It’s what does this say about me? Will I be ostracized from my group?\nIf behavioral economics becomes more social, which I think will be the next phase, then I would say it would deserve ever‑rising market value.\nCOWEN: Thorstein Veblen, that was his initial vision for it actually, was that it be quite social and that the idea of a social reference class was central to people’s behavioral biases.\nHAIDT: Interesting. Again, this is a critique from outside, but what a lot of people say which sounds right to me is that the early economists were great social theorists. My God, you read Adam Smith, what a brilliant world philosopher, historian, they thought so broadly and you tell me, but it seems there was a weird turn in the mid‑20th century towards mathematics.\nCOWEN: Yes.\nHAIDT: I think it made economists set all the interesting variables to zero.\n\nThese three conversations are worth reading or listening to in full. The episodes with Malcolm Gladwell and Joe Henrich are also excellent."
  },
  {
    "objectID": "posts/beinhockers-the-origin-of-wealth.html",
    "href": "posts/beinhockers-the-origin-of-wealth.html",
    "title": "Beinhocker’s The Origin of Wealth",
    "section": "",
    "text": "In The Origin of Wealth: Evolution, Complexity, and the Radical Remaking of Economics, Eric Beinhocker argues that the economy should be studied as a complex adaptive system made up of adaptive agents. The economy emerges from the interactions of those agents. It is an excellent book and possibly the best discussion of why the economy should be studied as a complex adaptive system. But as for other explorations of this area, Beinhocker does not successfully bring complexity economics to life as an applied science.\nModelling in “traditional economics” uses closed, static systems that are in equilibrium. Agents are modelled collectively with no mechanisms for endogenous novelty. In contrast, complex adaptive systems are open, dynamic and modelled individually. Macroeconomic outcomes reflect microeconomic behaviour. Beinhocker’s units of selection - the equivalent of genes in biology - are the modules of business plans, with these undergoing an evolutionary process of differentiation, selection and amplification.\nBeinhocker’s critique of neo-classical economics and its foundations does not completely avoid caricature, but his argument that an economy is a complex adaptive system is strong. This naturally leads to observations about the importance of path dependence in outcomes, the possibility of markets failing, the existence of bubbles and so on. For someone who has read much on complexity theory, the usual pieces of work and suspects are wheeled out, from Brian Arthur’s El Farol Bar problem to Doyne Farmer’s trading markets. It is interesting, but it is also a sign of a field struggling to gain traction when the same examples are wheeled out repeatedly.\nBeinhocker generally stays at Stage 2 of the stages of evolutionary economics, whereby evolutionary biology is not directly incorporated into the analysis. At times, this results in some laboured explanations. For example, in attempting to find the appropriate unit of selection, Beinhocker argues that the definition of a gene is fuzzy and changes depending on whether it is undergoing selection. By attempting to cast uncertainty over the biology, the difficulty in defining the units of selection in the economy might seem less so.\nSimilarly, gaps can be seen when Beinhocker suggests that the economy has shifted from being a “Big Man” economy, in which someone at the top of a hierarchy directs economic activity and obtains the economic surplus, to a market economy. This results in a shift from survival selection to what Beinhocker calls social selection. Technologies in Big Man economies spread with the survival of their carriers, but now that link is divorced. However, this has not changed to the extent that Beinhocker suggests. Human genes are still under selection, regardless of the form of economic system, although the favoured traits may vary. Survival and reproduction still matter, and the transmission of technologies and ideas remain linked to this. An approach that ignores biological motivations also provides limited insight into the formation of the Big Man or market economies. They too are endogenous to the biological actors.\nAt times Beinhocker heads towards a stronger evolutionary basis, such as in his suggestion that evolutionary psychology should be used to understand human preferences. However, this ultimately short-changes what evolution can offer. Beinhocker notes its central role when he writes:\n\nEconomic wealth and biological wealth are thermodynamically the same sort of phenomena, and not just metaphorically. Both are systems of locally low entropy, patterns of order that evolved over time under the constraint of fitness functions. Both are forms of fit order. And the fitness function of the economy - our tastes and preferences - is fundamentally linked to the fitness function of the biological world - the replication of genes. The economy is ultimately a genetic replication strategy.\n\nThe book closes with a discussion of the lessons from a complexity framework. Beinhocker warns at the beginning of the book that he does not give concrete answers, but he does offer suggestions.\nFor businesses, Beinhocker’s analysis is similar to (but predates) the argument by Tim Harford in Adapt. Beinhocker encourages experimentation within companies, with greater tolerance for failure and appropriate feedback mechanisms to tell the business when it is time to drop a particular strategy. Unfortunately, Beinhocker then turns to a discussion of whether companies should pursue narrow shareholder value or long-term growth, at which point the argument becomes weak.\nOn policy, Beinhocker claims to overcome the left-right continuum through his complexity approach, which acknowledges the emergent and useful behaviour of markets but the possibility of market failure. However, after making this claim, he then suggests a group of policy prescriptions that place him on the left-right continuum and that are only weakly derived the complexity approach that forms the bulk of the book. For example, he adopts arguments concerning the lack of intergenerational mobility in the United States and the importance of parental influences on children, despite the dearth of any evidence for parental influence, yet he fails to mention the argument about path dependence provided by complexity theory (nor the implications of heritable traits). Instead, he falls back on Rawlsian arguments for justice.\nOne interesting observation by Beinhocker is his description of the role of government as a fitness function shaper. By introducing market based regulations (such as an emissions trading scheme), government shapes the landscape of what strategies will have highest fitness, without prescribing which particular strategies should be used. In such a case, it is not clear that the change in landscape will be efficiency destroying, although Beinhocker does note the potential for what Hayek would term the fatal conceit.\nOverall, Beinhocker’s book is a great synopsis of the area, but it confirms that since the formation of the Santa Fe Institute 30 years ago, the field of complexity economics has moved slowly. Beinhocker suggests it takes some time for changes in frameworks to be absorbed. There is no sign of change coming for complexity economics yet."
  },
  {
    "objectID": "posts/best-books-i-read-in-2011.html",
    "href": "posts/best-books-i-read-in-2011.html",
    "title": "Best books I read in 2011",
    "section": "",
    "text": "As for last year, this year’s top book list comprises the best books I have read this year. I haven’t read enough books published in 2011 to be able to apply a decent filter, plus there are many books out there that we should not forget. In no particular order:\nFlatland: a romance of many dimensions by Edwin Abbott - Clever, fun satire\nSex, Genes & Rock ‘N’ Roll: How Evolution has Shaped the Modern World by Rob Brooks - While the book is generally a fun read, it makes this list for two specific parts: the discussions of sexual conflict in the context of population and on obesity (my discussion of the obesity chapter).\nFrankenstein by Mary Wollstonecraft Shelley - One of the classics that lives up to its reputation.\nSpent: Sex, Evolution and Behavior by Geoffrey Miller - Miller’s analysis of consumer culture under the lens of evolution is the sharpest I have read - and the most fun.\nThe Darwin Economy: Liberty, Competition, and the Common Good by Robert Frank - While the main point in this book could have been presented as an essay and I disagree with many of the applications, the central concept that competition is not always for the common good, as we can see in evolution, is important.\nThe Origins of Political Order: From Prehuman Times to the French Revolution by Francis Fukuyama - Fukuyama’s use of kin selection as the foundation to his analysis gives this book a solid foundation lacking from most grand histories (some earlier comments).\nTheory of the Leisure Class by Thorstein Veblen - This book was hard to read and at times inconsistent, but it is clear why it is one of those important books for economists to read.\nThe Lucifer Effect: Understanding How Good People Turn Evil by Philip Zimbardo - Although these books are in no particular order, I am going to suggest that this is probably the best. It had me thinking like few others (my review and a later musing).\nThere are a few books not on this list that were released this year and I have have high hopes for - Pinker’s The Better Angels of Our Nature: Why Violence Has Declined, Kahneman’s Thinking, Fast and Slow and Triver’s The Folly of Fools: The Logic of Deceit and Self-Deception in Human Life."
  },
  {
    "objectID": "posts/best-books-i-read-in-2014.html",
    "href": "posts/best-books-i-read-in-2014.html",
    "title": "Best books I read in 2014",
    "section": "",
    "text": "Continuing my tradition of giving the best books I read in the year - generally released in other years - the best books I read in 2014 are below (albeit from a smaller pool than usual).\n\nJohn Coates’s (2012)  The Hour Between Dog and Wolf: Risk Taking, Gut Feelings and the Biology of Boom and Bust: The best book I read this year. An innovative consideration of how human biology affects financial decision making.\nGregory Clark’s (2013) The Son Also Rises: Surnames and the History of Social Mobility: Not the most exciting book to read, but an important examination of social mobility.\nDavid Colander and Roland Kupers’s (2014) Complexity and the Art of Public Policy: An important way to think about policy, even though I’m not convinced by many of their proposed applications.\nGerd Gigerenzer’s (2010) Rationality for Mortals: How People Cope with Uncertainty: Essays capturing a lot of Gigerenzer’s important work. I reread it following his disappointing Risk Savvy. I didn’t write a review, but posted about parts of the book here, here and here.\nSendhil Mullainathan and Eldar Shafir’s (2013) Scarcity: Why Having Too Little Means So Much: A novel way of looking at scarcity that extends beyond the typical analysis in economics, but I’m not convinced it presents a coherent new perspective on how the world works.\nP.G. Wodehouse’s (1936) Young Men in Spats: Magic."
  },
  {
    "objectID": "posts/best-books-i-read-in-2016.html",
    "href": "posts/best-books-i-read-in-2016.html",
    "title": "Best books I read in 2016",
    "section": "",
    "text": "The best books I read in 2016 - generally released in other years - are below (in no particular order). For the non-fiction books, the links lead to my reviews.\n\nJoe Henrich’s The Secret of Our Success: How Culture Is Driving Human Evolution, Domesticating Our Species, and Making Us Smarter (2015): A lot of interesting ideas, but left me with a lot of questions.\nGarett Jones’s Hive Mind: How Your Nation’s IQ Matters So Much More Than Your Own (2015): A fantastic exposition of some important but neglected features of the world.\nPhil Rosenzweig’s Left Brain, Right Stuff: How Leaders Make Winning Decisions (2014): An entertaining examination of how behavioural economics findings hold up for real world decision making.\nPhilip Tetlock’s Expert Political Judgment: How Good Is It? How Can We Know? (2006): I re-read this before reading Tetlock’s also good Superforecasting, but Expert Political Judgement is the superior of the two books.\nJonathan Last’s What to Expect When No One’s Expecting: America’s Coming Demographic Disaster (2014): Much to disagree or argue with, but entertaining and a lot to like.\nJohn Kay’s Other People’s Money (2015): A fantastic critique of the modern financial system and regulation.\nFyodor Dostoevsky’s Crime and Punishment: The classic I most enjoyed."
  },
  {
    "objectID": "posts/best-books-i-read-in-2019.html",
    "href": "posts/best-books-i-read-in-2019.html",
    "title": "Best books I read in 2019",
    "section": "",
    "text": "Better late than never….\nThe best books I read in 2019 - generally released in other years - are below. Where I have reviewed, the link leads to that review (not many reviews this year).\n\nNick Chater, The Mind is Flat: A great book in which Chater argues that there are no ‘hidden depths’ to our minds.\nStephan Guyenet, The Hungry Brain: Outsmarting the Instincts that Make us Overeat: Excellent summary of modern nutrition research and how the body “regulates” its weight.\nJonathan Morduch and Rachel Schneider, The Financial Diaries: How American Families Cope in a World of Uncertainty: I find a lot of value reading about the world outside of my bubble. I learnt a lot from this book.\nPaul Seabright, The Company of Strangers: An excellent exploration of the evolutionary foundations of cooperation. A staple of my evolutionary biology and economics reading list.\nLenore Skenazy, Free-Range Kids: How to Raise Safe, Self-Reliant Children (Without Going Nuts with Worry): A fun read of some wise advice.\nM Mitchell Waldrop, The Dream Machine: J.C.R. Licklider and the Revolution That Made Computing Personal: A bit too much detail, but a worthwhile story about the origins of personal computing. Many of the concepts about human-machine interaction remain relevant today.\n\nBelow is the full list of books that I read in 2019 (with links where reviewed and starred if a re-read). The volume of my reading declined year-on-year again, with 61 books total (40 non-fiction, 21 fiction). Most of that decline came in the back half of the year when I spent a lot of time reading and researching around some narrow academic topics. 45 of the below were read before June. I could add a lot of children’s books to the list (especially Enid Blyton), but I’ll leave those aside.\nNon-Fiction\n\nDan Ariely and Jeff Kreisler, Dollars and Sense: Money Mishaps and How to Avoid Them\nChristopher Chabris and Daniel Simons, The Invisible Gorilla\nNick Chater, The Mind is Flat\nMihaly Csikszentmihalyi, Flow\nNir Eyal, Hooked\nNir Eyal, Indistractable\nTim Ferris, Four Hour Work Week\nTim Ferris, Tribe of Mentors\nVictor Frankl, Man’s Search for Meaning\nJason Fried and David Heinemeier Hansson, It Doesn’t have to be Crazy at Work\nJason Fried and David Heinemeier Hansson, Rework\nJason Fried and David Heinemeier Hansson, Remote\nAtul Gawande, Better\nStephan Guyenet, The Hungry Brain: Outsmarting the Instincts that Make us Overeat\nJonathan Haidt, The Righteous Mind*\nAdam Kay, This is Going to Hurt: Secret Diaries of a Junior Doctor\nPeter D. Kaufman (ed), Poor Charlie’s Almanac: The Wit and Wisdom of Charles T. Munger, Expanded Third Edition\nThomas Kuhn, The Structure of Scientific Revolutions\nDavid Leiser and Yhonatan Shemesh, How We Misunderstand Economics and Why it Matters: The Psychology of Bias, Distortion and Conspiracy\nGerry Lopez, Surf is Where You Find It\nJonathan Morduch and Rachel Schneider, The Financial Diaries: How American Families Cope in a World of Uncertainty\nCal Newport, Digital Minimalism\nKarl Popper, The Logic of SCientific Discovery\nJames Reason, Human Error\nBen Reiter, Astroball\nMatthew Salganik, Bit by bit: Social Research in the Digital Age\nBarry Schwartz, The Paradox of Choice\nPaul Seabright, The Company of Strangers*\nByron Sharp, How Brands Grow\nPater Singer, A Darwinian Left\nLenore Skenazy, Free-Range Kids: How to Raise Safe, Self-Reliant Children (Without Going Nuts with Worry)\nEugene Soltes, Why They Do It: Inside the Mind of the White-Collar Criminal\nDilip Soman, The Last Mile: Creating Social and Economic Value from Behavioral Insights\nMatthew Syed, Black Box Thinking: Marginal Gains and the Secrets of High Performance\nEd Thorpe, Beat the Dealer\nM Mitchell Waldrop, The Dream Machine: J.C.R. Licklider and the Revolution That Made Computing Personal\nMike Walsh, The Algorithmic Leader\nCaroline Webb, How to Have a Nice Day: A Revolutionary Handbook for Work -and Life\nRobert Wright, The Moral Animal\nScott Young, Ultralearning: Accelerate Your Career, Master Hard Skills and Outsmart the Competition\n\nFiction\n\nFyodor Dostoevsky, The Brothers Karamazov\nF Scott Fitzgerald, The Beautiful and The Dammed\nF Scott Fitzgerald, This Side of Paradise\nGraham Greene, My Man in Havana*\nRobert Heilein, Starship Troopers\nMichael Houellebecq, Submission\nJack London, Call of the Wild\nJohn Le Carre, Call for the Dead\nJohn Le Carre, A Murder of Quality\nJohn Le Carre, The Looking Glass War\nJohn Le Carre, A Small Town in Germany\nChuck Palahniuk, Fight Club*\nEdgar Allan Poe, The Tell-Tale Heart and Other Stories\nJ.K. Rowling, Harry Potter and the Philosopher’s Stone\nJ.K. Rowling, Harry Potter and the Chamber of Secrets\nJ.K. Rowling, Harry Potter and the Prisoner of Azkaban\nJ.K. Rowling, Harry Potter and the Goblet of Fire\nJ.K. Rowling, Harry Potter and the Order of the Phoenix\nJ.K. Rowling, Harry Potter and the Half-Blood Prince\nJ.K. Rowling, Harry Potter and the Deathly Hallows\nTim Winton, Breath\n\nPrevious lists: 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018"
  },
  {
    "objectID": "posts/best-books-i-read-in-2021.html",
    "href": "posts/best-books-i-read-in-2021.html",
    "title": "Best books I read in 2021",
    "section": "",
    "text": "The best books I read in 2021 - generally released in other years - were:\n\nDavid Badre, On Task: How Our Brain Gets Things Done: Effectively spans from the latest neuroscience to practical applications. The presentation of some computational models of our brains was great. I think about them a lot.\nHugo Mercier and Dan Sperber, The Enigma of Reason: A New Theory of Human Understanding: I’m not sold on the overarching thesis, but I love the way they pull apart many different theories and approaches in psychology. I have a bunch of draft posts in the pipeline about some of the topics covered in the book.\nNorton Juster, The Phantom Tollbooth: The kids loved it (ages 7, 7 and 5) and so did I.\n\nIt’s a short list. There just wasn’t much I was inspired by this year. Maybe I’m getting old and grumpy. Perhaps my book selection was not great. What I pick up next is too often dictated by mood rather than a plan.\nBelow is the full list of books that I read in 2021 (starred if I have read before). The volume of my reading of books cover-to-cover continues to fall year-on-year, with 33 total (23 non-fiction, 10 fiction). The pandemic and continued lack of commute is one driver beyond the decline. You can see from the list of the books I have read to the kids (48 of them!) where some of that extra time at home is going.\nI’ve also included a list of books commenced but abandoned. There are 9 on that list. I want to be more ruthless in stopping reading books that aren’t worth the effort. There are more than enough on the reading pile to justify moving on if the gain isn’t there. As you can see from that short, albeit incomplete list, abandoning books is a skill I need to build.\nNon-Fiction\n\nAjay Agrawal, Joshua Gans and Avi Goldfarb, Prediction Machines: The Simple Economics of Artificial Intelligence\nDavid Badre, On Task: How Our Brain Gets Things Done\nKaushik Basu, The Republic of Beliefs\nClayton Christensen, The Innovator’s Dilemma\nRyan Holiday, The Obstacle Is The Way\nMaria Konnikova, The Biggest Bluff: How I Learned to Pay Attention, Take Control and Master the Odds\nCharles C Mann, 1491: The Americas Before Columbus\nAntonio Garcia Martinez, Chaos Monkeys: Inside the Silicon Valley Money Machine\nGary Marcus and Jeremy Freeman (eds), The Future of the Brain\nHugo Mercier and Dan Sperber, The Enigma of Reason: A New Theory of Human Understanding\nSiddhartha Mukherjee, The Gene: An Intimate History (Stuart Ritchie’s review captures some of my headline thoughts)\nCal Newport, A World Without Email\nC. Northcote Parkinson, Parkinson’s Law or The Pursuit of Progress\nThomas Piketty, Capital and Ideology\nEric Schlosser, Command and Control: Nuclear Weapons, the Damascus Accident, and the Illusion of Safety\nJames C Scott, Seeing Like a State\nJanelle Shane, You Look Like a Thing and I Love You\nEdward Snowden, Permanent Record\nRory Sutherland, Alchemy: The Surprising Power of Ideas That Don’t Make Sense\nNassim Nicholas Taleb, Fooled by Randomness*\nMax Tegmark, Life 3.0: Being Human in the Age of Artificial Intelligence\nGillian Tett, Anthro-Vision: How Anthropology Can Explain Business and Life\nFred Waitzkin, Searching for Bobby Fisher\n\nFiction\n\nChristopher Buckley, Thank You for Smoking*\nFyodor Dostoyevsky, Notes from the Underground\nAlexandre Dumas, The Three Musketeers\nElizabeth Gaskell, North and South\nNathaniel Hawthorne, The House of the Seven Gables\nCixin Liu, The Three-Body Problem\nJack London, White Fang\nCormac McCarthy, The Road*\nHerman Melville, The Piazza Tales\nH.G. Wells, The Invisible Man*\n\nCommenced but abandoned (incomplete list)\n\nKaren Armstrong, A History of God\nIsaac Asimov, I Asimov: A Memoir\nJeff Bezos, Invent & Wander: The Collected Writings of Jeff Bezos\nGyörgy Buzsáki, The Brain from Inside Out\nAndy Clark, Surfing Uncertainty: Prediction, Action and the Embodied Mind (I tried this twice, once in 2020 and again in 2021.)\nGeorge RR Martin, A Game of Thrones\nNathaniel Hawthorne, The Scarlet Letter\nWill Page, Tarzan Economics\nJimmy Soni and Rob Goodman, A Mind at Play: How Claude Shannon Invented the Information Age\n\nRead to the kids\n\nCarlo Collodi, Pinocchio\nFrances Hodgson Burnett, The Secret Garden*\nRoald Dahl, Charlie and the Chocolate Factory\nRoald Dahl, Fantastic Mr Fox\nRoald Dahl, Matilda\nKenneth Grahame, Wind in the Willows*\nNorton Juster, The Phantom Tollbooth\nRupyard Kipling, The Jungle Book\nRoger Lancelyn Green, The Adventures of Robin Hood*\nAndy Griffiths and Terry Denton, The 13-Storey Treehouse*\nAndy Griffiths and Terry Denton, The 26-Storey Treehouse*\nAndy Griffiths and Terry Denton, The 39-Storey Treehouse*\nAndy Griffiths and Terry Denton, The 52-Storey Treehouse*\nAndy Griffiths and Terry Denton, The 65-Storey Treehouse*\nAndy Griffiths and Terry Denton, The 78-Storey Treehouse*\nAndy Griffiths and Terry Denton, The 91-Storey Treehouse*\nAndy Griffiths and Terry Denton, The 104-Storey Treehouse*\nAndy Griffiths and Terry Denton, The 117-Storey Treehouse*\nAndy Griffiths and Terry Denton, The 130-Storey Treehouse*\nAndy Griffiths and Terry Denton, The 143-Storey Treehouse\nEmily Rodda, The Maze of the Beast\nEmily Rodda, The Valley of the Lost\nEmily Rodda, Return to Del\nEmily Rodda, Cavern of the Fear\nEmily Rodda, The Isle of Illusion\nEmily Rodda, The Shadowlands\nEmily Rodda, Dragon’s Nest\nEmily Rodda, Shadowgate\nEmily Rodda, Isle of the Dead\nEmily Rodda, The Sister of the South\nJK Rowling, Harry Potter and the Philospher’s Stone*\nJK Rowling, Harry Potter and the Chamber of Secrets*\nJK Rowling, Harry Potter and the Prisoner of Azkaban*\nJK Rowling, Harry Potter and the Goblet of Fire*\nTui T. Sutherland, The Dragonet Prophecy\nTui T. Sutherland, The Lost Heir\nTui T. Sutherland, The Hidden Kingdom\nTui T. Sutherland, The Dark Secret\nTui T. Sutherland, The Brightest Night\nTui T. Sutherland, Moon Rising\nTui T. Sutherland, Winter Turning\nTui T. Sutherland, Escaping Peril\nTui T. Sutherland, Talons of Power\nTui T. Sutherland, Darkness of Dragons\nTui T. Sutherland, Darkstalker\nTui T. Sutherland, Winglets Quartet\nP.L. Travers, Mary Poppins\nJRR Tolkien, The Hobbit*\n\nPrevious best book lists: 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020"
  },
  {
    "objectID": "posts/bias-in-the-world-bank.html",
    "href": "posts/bias-in-the-world-bank.html",
    "title": "Bias in the World Bank",
    "section": "",
    "text": "Last year’s World Development Report 2015: Mind, Society and Behaviour from the World Bank documents many of what seem to be successful behavioural interventions. Many of the interventions are quite interesting and build a case that a behavioural approach can add something to development economics.\nThe report also rightly received some praise for including a chapter which explored the biases of development professionals. World Bank staff were shown to subjectively interpret data differently depending on the frame, to suffer from the sunk cost bias and to have little idea about the opinions of the poor people they might help. Interestingly, in the brief discussion about what can be done to counteract these biases, there is little discussion about whether it might be better to simply not conduct certain projects.\nOn a more critical front, Andreas Ortmann sent me a copy of his review of the report that was published in the Journal of Economic Psychology. Ortmann has already put a lot of my reaction into words, so here is an excerpt (a longer excerpt is here):\n\nWhat the Report does not do, unfortunately, is the kind of red teaming that it advocates as “one way to overcome the natural limitations on judgement among development professionals … In red teaming, an outside group has the role of challenging the plans, procedures, capabilities, and assumptions of an operational design, with the goal of taking the perspective of potential partners or adversaries. …” …\nOverall, and notwithstanding the occasional claim of systematic reviewing (p. 155 fn 6), the sampling of the evidence seems often haphazard and partisan. Take as another example, in chapter 7, the discussion of reference points and daily income targeting that was started by Camerer, Babcock, Loewnstein, and Thaler (1997) and brought about studies such as Fehr and Goette (2007). These studies suggested that taxi drivers and bike messengers in high-income settings have target earnings or target hours and do not intertemporally maximize allocation of labor and leisure. The problem with the argument is that several follow-up studies (prominently, the studies by Farber (2005, 2008) questioned the earlier results. Here no mention is made of these critical studies. Instead the authors argue that the failure to maximize intertemporally can also be found in low-income settings. They cite an unpublished working paper investigating bicycle taxi drivers in Kenya and another unpublished working paper citing fishermen in India. Tellingly, the authors (and the scores of commentators they gave them feedback) did not come across a paper, now forthcoming in Journal of Labor Economics, that has been circulating for a couple of years (see Stafford, in press) and that shows, and shows with an unusually rich data set for Florida lobster fishermen, that both participation decisions and hours spent on sea are consistent with a neoclassical model of labor supply. …\nThere are dozens of other examples of review of the literature that I find troublingly deficient on the basis of articles I know. … But, as mentioned and as I have illustrated with examples above, there is little red teaming on display here. Not that that is a particularly new development. Behavioural Economics, not just in my view, has since the beginning been oversold and much of that over-selling was done by ignoring the considerable controversies that have swirled around it for decades (Gigerenzer, 1996; Kahneman & Tversky, 1996 anyone? …).\nThe troubling omission of contrarian evidence and critical voices on display in the Report is deplorable because there are important insights that have come out of these debates and the emerging policy implications would be based on less shifty ground if these insights would be taken into account in systematic ways. If you make the case for costly and policy interventions that might affect literally billions of people, you ought to make sure that the evidence on which you base your policy implications is robust.\nIn sum, it seems to me that the resources that went into the Report would have been better spent had there been adversarial collaborations (Mellers, Hertwig, & Kahneman, 2001) and/or had reviews gone through a standard review process which hopefully would have forced some clear-cut and documented review criteria. A long list of people that gave feedback is not a good substitute for institutional quality control."
  },
  {
    "objectID": "posts/books-i-read-in-2022.html",
    "href": "posts/books-i-read-in-2022.html",
    "title": "Books I read in 2022",
    "section": "",
    "text": "The best books I read in 2022 - generally released in other years - were:\n\nBryan Caplan, The Case Against Education: I find the argument compelling and somewhat disheartening.\nStanlislas Dehaene, How We Learn: The optimistic case for doing education right. Despite Caplan’s argument, there is a niche where this is important.\nDavid Epstein, Range: Why Generalists Triumph in a Specialized World: Generally great book, although the end about diversity fell flat.\n\nBelow is the full list of books that I read in 2022 (starred if I have read before). The volume of my reading of books cover-to-cover was on par with last year, with 34 total (18 non-fiction, 16 fiction). I read to the kids much less than the previous year as they are taking on more reading themselves. Somehow that didn’t translate to more reading by myself.\nWhat’s interesting about this list is how little behavioural science is on it. I find it hard to stomach - or even start - most of the popular books on the subject. The replication crisis is changing the way science is done (at a glacial pace), but the popular book production line looks pretty much the same.\nI’ve also included an incomplete list of books commenced but abandoned. There are 12 on that list. I’m still not ruthless enough in moving on.\nNon-Fiction\n\nAnanyo Bhattacharya, The Man from the Future: The Visionary Life of John von Neumann\nBryan Caplan, The Case Against Education\nBrian Christian, The Alignment Problem\nStanlislas Dehaene, How We Learn\nDavid Epstein, Range: Why Generalists Triumph in a Specialized World\nOded Galor, The Journey of Humanity\nDavid Graeber, Bullshit Jobs\nAdam Kay, This Is Going to Hurt*\nMichael Lewis, Moneyball*\nGary Marcus and Ernest Davis, Rebooting AI\nTom Mead, Killers of Eden*\nHaruki Murakami, What I Talk About When I Talk About Running*\nCarl Rhodes, Woke Capitalism: How Corporate Morality is Sabotaging Capitalism\nStuart Russell, Human Compatible\nDavid Sinclair, Lifespan: Why We Age–And Why We Don’t Have to\nRichard Thaler, The Winner’s Curse\nPeter Wohlleben, The Hidden Life of Trees\nGregory Zuckerman, The Man Who Solved the Market: How Jim Simons Launched the Quant Revolution\n\nFiction\n\nArthur Conan Doyle, The Adventures of Sherlock Holmes*\nArthur Conan Doyle, The Sign of Four*\nArthur Conan Doyle, A Study in Scarlet*\nArthur Conan Doyle, The Memoirs of Sherlock Holmes*\nArthur Conan Doyle, The Hound of the Baskervilles*\nArthur Conan Doyle, The Return of Sherlock Holmes*\nArthur Conan Doyle, The Valley of Fear*\nArthur Conan Doyle, His Last Bow*\nArthur Conan Doyle, The Case-Book of Sherlock Holmes (Conan Doyle had really given up by these final stories.)\nMichael Houellebecq, Submission*\nVictor Hugo, The Hunchback of Notre Dame*\nGeorge RR Martin, A Game of Thrones\nGeorge RR Martin, A Clash of Kings\nGeorge RR Martin, A Storm of Swords\nHaruki Muakami, 1Q84\nNeal Stephenson, Termination Shock\n\nCommenced but abandoned\n\nOliver Burkeman, Four Thousand Weeks\nScott Cunningham, Causal Inference: The Mixtape\nSuelette Dreyfus, Underground\nTiago Forte, Building a Second Brain\nPaul Gilding, The Great Disruption\nNick Hornby, About a Boy\nNick Hornby, High Fidelity\nMichael Houellebecq, Serotonin\nEric Kaufmann, White Shift (will probably come back to this)\nHaruki Murakami, Hear the Wind Sing\nMarcel Proust, Swann’s Way\nM Mitchell Waldrop, Complexity\n\nRead to the kids (a lot of other kids books were commenced but abandoned)\n\nJeffrey Brown, Jedi Academy\nLewis Carroll, Alice’s Adventures in Wonderland*\nLewis Carroll, Through the Looking-glass*\nGillian Cross, The Iliad (three times)\nGillian Cross, The Odyssey (twice)\nJ.K. Rowling, Harry Potter and the Philosopher’s Stone* (twice)\nAnna Sewell, Black Beauty\nLemony Snickett, The Bad Beginning\nJohanna Spyri, Heidi\nTui T. Sutherland, Dragonslayer\nTui T. Sutherland, The Lost Continent\nTui T. Sutherland, The Hive Queen\nTui T. Sutherland, The Poison Jungle\nTui T. Sutherland, The Dangerous Gift\nTui T. Sutherland, Flames of Hope\nGeronimo Stilton, Multiple\nJ.R.R. Tolkein, The Hobbit*\nJ.R.R.Tolkein, The Fellowship of the Ring*\n\nPrevious best book lists: 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021"
  },
  {
    "objectID": "posts/books-i-read-in-2024.html",
    "href": "posts/books-i-read-in-2024.html",
    "title": "Books I read in 2024",
    "section": "",
    "text": "The books I enjoyed the most in 2024, although all were published in different years:\n\nAshlee Vance, When the Heavens Went on Sale: For all the grumbling about how progress in software isn’t matched in the physical world, Vance tells some amazing stories. The tale about how Planet Labs got going using smartphone technology to make shoebox size satellites was fantastic.\nJulia Galef, The Scout Mindset: Why Some People See Things Clearly and Others Don’t: Wonderfully practical. I enter almost every psychology or behavioural science related book dreading the shaky scientific studies I will be dragged through. But Galef did a great job of knowing when to pull out a study (not often) and when to build a more practical case.\nWilliam Poundstone, Fortune’s Formula: The Untold Story of the Scientific Betting System That Beat the Casinos and Wall Street: A good balance of technicality and colour.\nKurt Vonnnegut, Player Piano: Simply a great story.\n\nBelow is the list of books I read in 2024 (with a star if I have read them before). My volume of my reading of books cover-to-cover has increased from 2023, with 49 total (26 non-fiction, 23 fiction).\nNon-Fiction\n\nKathryn Astbury and Robert Plomin, G is for Genes: The Impact of Genetics on Education and Achievement\nEric Berger, Liftoff: Elon Musk and the Desperate Early Days that Launched SpaceX\nEric Berger, Reentry: SpaceX, Elon Musk, and the Reusable Rockets that Launched a Second Space Age\nDalton Conley and Jason Fletcher, The Genome Factor: What the Social Genomics Revolution Reveals about Ourselves, Our History, and the Future\nTyler Cowen and Daniel Gross, Talent: How to Identify Energizers, Creatives and Winners Around the World\nKate Crawford, Atlas of AI: Power, Politics, and the Planetary Costs of Artificial Intelligence\nNir Eyal, Indistractable: How to Control Your Attention and Choose Your Life\nBent Flyvbjerg and Dan Gardner, How Big Things Get Done\nMartin Ford, Architects of Intelligence\nJulia Galef, The Scout Mindset: Why Some People See Things Clearly and Others Don’t\nMichael Greger, How Not to Age\nSaul Griffith, Electrify: An Optimist’s Playbook for Our Clean Energy Future\nKathryn Paige Harden, The Genetic Lottery: Why DNA Matters for Social Equality *\nMichel Lewis, Going Infinite: The Rise and Fall of a New Tycoon\nEthan Mollick, Co-Intelligence: Living and Working with AI\nCharles Murray, Human Diversity: The Biology of Gender, Race and Class\nCal Newport, Slow Productivity\nToby Ord, “The Precipice: Existential Risk and the Future of Humanity”\nRobert Plomin, Blueprint: How DNA makes us who we are\nWilliam Poundstone, Fortune’s Formula: The Untold Story of the Scientific Betting System That Beat the Casinos and Wall Street\nNichola Raihani, The Social Instinct\nRebecca Reider, Dreaming the Biosphere\nEdward O Thorpe, A Man for All Markets: Beating the Odds from Las Vegas to Wall Street\nAshlee Vance, When the Heavens Went on Sale\nWalter Willett, Eat, Drink and Be Healthy\nScott H Young, Get Better at Anything\n\nFiction\n\nEdward Abbey, The Brave Cowboy\nIsaac Asimov, I, Robot\nIsaac Asimov, The Caves of Steel\nIsaac Asimov, The Naked Sun\nIsaac Asimov, The Robots of Dawn\nIsaac Asimov, The Complete Robot\nIsaac Asimov, Robots and Empire\nMargaret Atwood, The Handmaid’s Tale *\nOrson Scott Card, Ender’s Game\nOrson Scott Card, Speaker for the Dead\nErnest Cline, Ready Player One\nPhilip K Dick, Do Androids Dream of Electric Sheep\nRobert Heinlein, Starship Troopers\nDaniel Keyes, Flowers for Algernon\nMadeleine L’Engle, A Wrinkle in Time\nMadeleine L’Engle, A Wind in the Door\nMadeleine L’Engle, A Swiftly Tilting Planet\nJohn le Carrè, A Perfect Spy *\nCormac McCarthy, Stella Maris\nCormac McCarthy, The Passenger\nRichard Matheson, I Am Legend\nWalter Tevis, The Queen’s Gambit *\nKurt Vonnnegut, Player Piano\n\nPrevious annual book lists: 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023"
  },
  {
    "objectID": "posts/bowles-and-gintiss-a-cooperative-species.html",
    "href": "posts/bowles-and-gintiss-a-cooperative-species.html",
    "title": "Bowles and Gintis’s A Cooperative Species",
    "section": "",
    "text": "Sam Bowles and Herb Gintis have an interesting reputation within the fields of economics and evolutionary biology. The recent paper of Nowak and colleagues has given Bowles and Gintis some competition as the most prominent advocates of a group selectionist approach (or multi-level selection as Nowak and colleagues would term it), but I still have not come across anyone in evolutionary biology who will argue for the importance group selection to the extent they will (as this video of Herb Gintis demonstrates).\nA Cooperative Species: Human Reciprocity and Its Evolution continues Bowles and Gintis’s group selection approach to the evolution of cooperation, with many of their earlier papers forming the basis for chapters or models.\nThe opening chapters are excellent as they tour the findings of experiments into cooperation (particularly dictator games and the prisoner’s dilemma). They argue that these present strong evidence that people undertake actions that are costly to themselves, and they address most of the typical criticisms of the experiments and the interpretations. While I did not always agree, their discussion is thorough and builds a strong argument. In particular, their discussion of whether people could distinguish between experiments and real world situations is well covered, with Bowles and Gintis arguing that people are aware that most experiments are single shot games.\nOnce we move from the experimental to the theoretical, the remainder of the book is more an exploration of evolutionary game theory models involving cooperation than an attempt to describe the evolution of cooperation. In many ways, I found this approach frustrating. For example, when examining the feasibility of the evolution of cooperation through inclusive fitness, they developed a model in which slight error rates in recognising whether someone was cooperating or not prevented cooperative traits from evolving. They then continually referenced this model as showing that cooperation could not evolve without group selection. That a single theoretical model can form the basis of such a claim, without seriously considering the broad literature and range of models in which it is argued that cooperation can evolve, leaves the argument short. Similarly, they constructed some very complicated game theory models and suggested they were too complicated for mere mortals as there were infinite equilibria.\nAs a test for these arguments, you might ask whether a generally cooperative approach leads to higher benefits or costs in modern society. It would seem the benefits are huge, with more cooperative people (also having higher IQ and patience) obtaining significant dividends. Even if you make the odd mistake, cooperation appears optimal for purely selfish reasons. If my error rate is so costly and my ability to coordinate with others is so difficult, as Bowles and Gintis imply, why do I find cooperation so rewarding?\nAt times their points were important. For example, Bowles and Gintis argued that while signalling theory tells you that agents will engage in costly activities to signal, their group selection argument provides a basis for why the signals are “positive” in the sense that people engage in costly cooperation, rather than simply beating the weaker person up. But, by this stage of the book, their dismissal of reciprocal altruism and inclusive fitness has left the obvious answer unavailable.\nTheir models of group selection are interesting, although hard work. They use an agent based approach, which is useful, but such models are hard to assess without playing with the models and the parameters yourself. Each chapter feels more like an academic paper than book chapter, which had me skimming much of the maths with occasional stops at interesting points.\nOne of the pillars to their claim that group selection was vital to the evolution of cooperation is that intergroup conflict was frequent and that groups regularly wiped each other out. They survey the evidence for group conflict and suggest that the frequency and severity of war created the intergroup selection pressures necessary for group selection to operate. I tend to agree with the evidence they present concerning war, but the evidence presented by Napoleon Chagnon, among others, suggests that war also has significant individual benefits.\nUltimately, the book’s weakness is the lack of time that is spent addressing models of the evolution of cooperation that do not rely on group selection. If the focus given to defending the experimental evidence was also given to to alternative models of the evolution of cooperation, the book may have delivered some serious arguments. In their absence, I am not sure this book will convince anyone to change their mind."
  },
  {
    "objectID": "posts/boyd-and-richersons-the-origin-and-evolution-of-cultures.html",
    "href": "posts/boyd-and-richersons-the-origin-and-evolution-of-cultures.html",
    "title": "Boyd and Richerson’s The Origin and Evolution of Cultures",
    "section": "",
    "text": "When I asked for suggestions for my evolutionary biology and economics reading list earlier this year, Boyd and Richerson’s The Origin and Evolution of Cultures was one of the most recommended. Their exploration of cultural evolution has many elements that are relevant to economics, including the development of institutional frameworks, the evolution of cooperation and the transmission of technology.\nThe book comprises 20 papers (published between 1987 and 2003) that are grouped into five thematic groups: the evolution of social learning; ethnic groups and markers; human cooperation, reciprocity and group selection; archaeology and culture history; and links to other disciplines. Each chapter was a stand-alone paper, so rather than going into any of them in further detail, I will save that for some later posts and give some more general observations here.\nFirst, Boyd and Richerson are clear in arguing that “culture” is a distinct feature from “environment”, and that it should be examined through an evolutionary lens:\n\n[C]ultural variation is transmitted from individual to individual, it is subject to population dynamic processes analogous to those that effect genetic variation and quite unlike the processes that govern other environmental effects. Combining cultural and environmental effects into a single category conceals these important differences.\n\nHaving been sceptical before reading the book, this is one issue on which I am a convert. I am still not convinced that it is always (or often) possible to identify practically which cultural trait is subject to selection or to differentiate it from the environment, but drawing this distinction led to some interesting and parsimonious models. Further, an evolving cultural trait may be the environment for another cultural trait.\nTheir exploration of cultural evolution often contains a genetic element, usually in the context of “gene-culture coevolution”. For example, they describe a process whereby cultural institutions might result in people with certain genetic predispositions beings weeded out.\n\nMechanisms by which cultural institutions might exert forces tugging in this direction are not far to seek. People are likely to discriminate against genotypes that are incapable of conforming to cultural norms (Richerson and Boyd, 1989; Laland, Kumm, and Feldman, 1995). People who cannot control their self-serving aggression ended up exiled or executed in small-scale societies and imprisoned in contemporary ones. People whose social skills embarrass their families will have a hard time attracting mates. Of course, selfish and nepotistic impulses were never entirely suppressed; our genetically transmitted evolved psychology shapes human cultures, and, as a result, cultural adaptations often still serve the ancient imperatives of inclusive genetic fitness. However, cultural evolution also creates new selective environments that build cultural imperatives into our genes.\n\nHowever, Boyd and Richerson’s exploration of gene-culture coevolution does not usually extend to developing models with where genes and culture simultaneously evolve. At times this is problematic, particularly where they incorporate cultural group selection into the picture, as it can be difficult to understand how the process would actually work from the often loose verbal descriptions. Conversely, a model incorporating these multiple evolving elements would lose the clarity and simplicity that allows most of the models in the book to be useful.\nThe indeterminate nature of the culture-environment distinction I alluded to above is also highlighted by this gene-culture evolution quote. Cultural evolution creates new selective environments. While a cultural trait is evolving, it is effectively creating an environment in which other cultural traits or genes evolve. This is similar to the idea that genes effectively create the environment in which other genes evolve, whether those other genes be in the same individual or in other individuals and species.\nBoyd and Richerson’s work shares some similarity with that of Sam Bowles and Herb Gintis, particularly in their approach to model development. Simulations are used as illustrations, the focus is more on demonstrating ideas than providing hard proofs, and agent based models are a common tool.  However, Boyd and Richerson have a stronger sense than Bowles and Gintis of the limitations of their models, and generally recognise their illustrative and not determinative nature. Bowles and Gintis have a habit of making a model and arguing that, since a certain feature didn’t work in their model (such as the evolution of cooperation by reciprocal altruism), their model is evidence that it can’t work at all. The problem with this approach is that the model only examines such a small subspace of the possibilities. Boyd and Richerson tend to be more constrained in their conclusions, although not always so.\nOne of the groups of papers focuses on group selection. I am more open to analysing the transmission of cultural traits through the lens of group selection (or multilevel selection) than I am for the transmission of genes, largely because cultural group selection is not necessarily undermined by migration between groups in the same way as genetic group selection. Boyd and Richerson note this when they state:\n\n[S]ocial extinction does not mean physical elimination of the entire group. Quite the contrary, most people survive defeat but flee as refugees to other groups, into which they are incorporated. This sort of extinction cannot support genetic group selection because so many of the defeated survive and because they would tend to carry their unsuccessful genes into successful groups, rapidly running down variation between groups. However, the effects of conformist cultural transmission combined with moralistic punishment makes between-group cultural variation much less subject to erosion by migration and within-group success of uncooperative strategies than is true in the case of acultural organisms.\n\nHowever, I am still not convinced that the cultural group selection approach provides the clearest method of analysis. I’ll save my specific issues with their approach in a separate post.\nMy favourite chapter of the book was the least theoretical. Boyd and Richerson (with Joseph Soltis) asked whether observed rates of group extinction could be sufficient for group selection to drive rapid cultural evolution. Based on an examination of hunter-gather tribe extinction rates, they concluded that group selection could not be responsible. It was refreshing to see some empirical analysis applied to this issue. For all the noise around group selection (both genetic and cultural), it is rare that the debates are accompanied by increasingly available data.\n*My later post with my thoughts on their approach to group selection can be found here."
  },
  {
    "objectID": "posts/brooks-on-hunter-gatherers-and-egalitarianism.html",
    "href": "posts/brooks-on-hunter-gatherers-and-egalitarianism.html",
    "title": "Brooks on hunter-gatherers and egalitarianism",
    "section": "",
    "text": "Fitting nicely with my recent post on human nature and libertarianism, Rob Brooks has the following to say on the mega-rich and people’s sense of fairness:\n\nHunter-gatherers keep their neighbours and tribe-mates in line. When everybody depends on everyone else, then reputation rules. You simply can’t afford to be selfish, whether by failing to share or by freeloading, in a small community. Your allies will desert you. …\nWhere our early ancestors kept one another honest, elites in hierarchical societies tend to socialise with other elites who are equally self-interested in maintaining their own power. The mega-rich don’t hang out with wage-earners, preferring to mix with politicians, media moguls and millionaire televangelists. …\nI would never suggest that flat egalitarianism is desirable, but societies in which wealth is spread more equitably experience less violence, better health, lower stress and greater happiness than highly inequitable societies. Which is why everybody has a duty to criticise opulence and ridicule greed rather than fantasising about making the BRW Rich Lists.\n\nI’ll post some detailed thoughts on hunter-gatherer society and egalitarianism in the future (Andrew’s recent post at Evolvify also requires a post or two). In the meantime, I have one question - why should I care about these elites?\nI care that the mega-rich influence government - Rob’s example of the mix of politicians and media moguls is particularly relevant. But from a hunter-gather perspective, the mega-rich are more like a distant tribe than part of any group of which I am a member. They don’t want to hang out with me, and they don’t form part of my circle of kin, friends or neighbours. It is only through modern media that I know who these people are. So should I want to cut them down to size just because they are rich? Or is it their ability to project that wealth that should be the subject of concern?"
  },
  {
    "objectID": "posts/business-adaptation.html",
    "href": "posts/business-adaptation.html",
    "title": "Business adaptation",
    "section": "",
    "text": "Rafe Sagarin blogs at the Harvard Business Review:\n\nAll of Earth’s successful organisms have thrived without analyzing past crises or trying to predict the next one. They haven’t held “planning exercises” or created “predictive frameworks.” Instead, they’ve adapted. Adaptability is the power to detect and respond to change in the world, no matter how surprising or inconvenient it may be.\nWhile there’s much chatter in the management world about the need to be adaptable, only a few creative companies and innovative managers have probed the natural world for its adaptability secrets. But when they have, they’ve been remarkably successful. A study of nature offers straightforward guidance through four key practices of adaptable systems.\n\nSagarin’s four practices are decentralisation, redundancy, symbiotic relationships and recursive processes.\nI’m sympathetic to the idea that businesses might benefit by experimenting more (small failures) and taking advantage of the collective wisdom of their employees. But there is one fundamental difference between a business wanting to be adaptable and the operation of natural selection in nature. Evolution does not work through each organism surviving no matter what the circumstances. Rather, a few selected organisms that are more adapted to the current environment will have higher fitness and form the population of the future. Each individual organism does not actually adapt. This concept was the part of Tim Harford’s Adapt that I thought was done particularly well.\nAs a result, carrying a costly trait on the possibility that it might be required in the future is perilous in evolutionary terms. Even though we see some organisms carrying redundant features, evolution tends to eliminate costly traits if they are not required, or more particularly, those individuals still possessing the costly traits.\nThus, when Sagarin suggests that if a business want to be “adaptable” it needs to “make multiple copies of everything and modify the copies to hedge against uncertainty”, we need to consider the costs. What happens when the business with built-in redundancy runs into the lean business that ignores all of Sagarin’s advice but hits on the right answer?\nConsider Silicon Valley, with thousands of start-ups experimenting with various ideas. Some succeed, most fail. This is natural selection in action. For a large tech player, there is no way to replicate the massive experimentation that these startups are conducting. If they do experiment and cover all possible bases, they are carrying much larger costs than the single company that lucks onto the right idea. By carrying those costs, will the experimenting company be able to beat the experimentation of the broader market in the long-run?\nAs a result, Sagarin’s advice to build in redundancy is no guarantee of success. Businesses could learn from nature, and becoming more adaptable may yield benefits. But nature also suggests that, ultimately, a business will fail (as have over 99 per cent of species that have been on this earth) as it simply cannot efficiently replicate the experimentation of the broader market place."
  },
  {
    "objectID": "posts/carol-dwecks-mindset-changing-the-way-you-think-to-fulfil-your-potential.html",
    "href": "posts/carol-dwecks-mindset-changing-the-way-you-think-to-fulfil-your-potential.html",
    "title": "Carol Dweck’s Mindset: Changing the Way You Think to Fulfil Your Potential",
    "section": "",
    "text": "I did not find Carol Dweck’s Mindset: Changing the Way You Think to Fulfil Your Potential to be a compelling translation of academic work into a popular book. To all the interesting debates concerning growth mindset - such as Scott Alexander’s series of growth mindset posts (1, 2, 3 and 4), the recent meta-analysis (with Carol Dweck response), and replication of the effect - the book adds little material that might influence your views. If you want to better understand the case for (or against) growth mindset and its link with ability or performance, skip the book, follow the above links and go to the academic literature.\nAs a result, I will limit my comments on the book to a few narrow points, and add a dash of personal reflection.\nIn the second in his series, Alexander describes two positions on growth mindset. The first is the “bloody obvious position”:\n\n[I]nnate ability might matter, but that even the most innate abilityed person needs effort to fulfill her potential. If someone were to believe that success were 100% due to fixed innate ability and had nothing to do with practice, then they wouldn’t bother practicing, and they would fall behind. Even if their innate ability kept them from falling behind morons, at the very least they would fall behind their equally innate abilityed peers who did practice.\n\nDweck and Alexander (and I) believe this position.\nThen there is the controversial position:\n\nThe more important you believe innate ability to be compared to effort, the more likely you are to stop trying, to avoid challenges, to lie and cheat, to hate learning, and to be obsessed with how you appear before others. …\n\nTo distinguish the two, Alexander writes:\n\nIn the Bloody Obvious Position, someone can believe success is 90% innate ability and 10% effort. They might also be an Olympian who realizes that at her level, pretty much everyone is at a innate ability ceiling, and a 10% difference is the difference between a gold medal and a last-place finish. So she practices very hard and does just as well as anyone else.\nAccording to the Controversial Position, this athlete will still do worse than someone who believes success is 80% ability and 20% effort, who will in turn do worse than someone who believes success is 70% ability and 30% effort, all the way down to the person who believes success is 0% ability and 100% effort, who will do best of all and take the gold medal.\n\nThe bloody obvious and controversial positions are often conflated in popular articles, and in Dweck’s book the lack of differentiation is shifted up another gear. The book is interspersed with stories about people expending some effort to improve or win, with almost no information as to what they believe about growth and ability. The fact that they are expending effort is almost taken to be evidence of the growth mindset. At best the stories are evidence toward the bloody obvious position.\nBut Dwecks’s strong statements about growth mindset through the book make it clear that she holds the controversial position. Here are some snippets from the introduction:\n\nBelieving that your qualities are carved in stone—the fixed mindset—creates an urgency to prove yourself over and over.\n[I]t’s startling to see the degree to which people with the fixed mindset do not believe in effort.\nIt’s not just that some people happen to recognize the value of challenging themselves and the importance of effort. Our research has shown that this comes directly from the growth mindset.\n\nAlthough Dweck marshals her stories from business and sports to support these “controversial position” claims, it does’t work absent the evidence of beliefs. Add in the survivorship bias in the examples at hand, plus the halo effect in assessing whether the successful people have a “growth mindset”, and there is little compelling evidence that these people held a growth or fixed mindset (as per the “controversial position) and that the mindset in turn caused the outcomes.\nTo find evidence in support of Dweck’s statements you need to turn to the academic work, but Dweck covers her research in little detail. From the limited descriptions in the book, it was often hard to know what the experiment involved and how much weight to give it. The book pointed me to interesting papers, but absent that visit to the academic literature I felt lost.\nOne point that becomes clear through the book is that Dweck sees people with growth mindsets having a host of other positive traits. At times this feels like growth mindset is being expanded to encompass all positive behaviours. These included:\n\nEmbracing challenges and persisting after setbacks\nSeeking and learning from criticism\nUnderstanding the need to invest effort to develop expertise\nSeeking forgiveness rather than revenge against those who have done them wrong, such as when they are bullied\nBeing happy whatever their outcomes (be it their own, their team’s or their child’s)\nCompassion and consideration when coaching, rather than through fear and intimidation\nMore accurate estimation of performance and ability\nAccurately weighting positive and negative information (compared to the extreme reactions of the fixed mindset people)\n\nI need to better understand the literature on how growth mindset correlates with (or causes) these kinds of behaviours, but a lot is being put into the growth mindset basket.\nIn some of Dweck’s examples of people without a growth mindset, there is a certain boldness. John McEnroe is a recurring example, despite his seven singles and ten doubles grand slam titles. On McEnroe’s note that part of 1982 did not go as well as expected when little things kept him off his game (he still ended the year number one), Dweck asks “Always a victim of outside forces. Why didn’t he take charge and learn how to perform well in spite of them?” McEnroe later recorded the best single season record in the open era (82-3 in 1984), ending the year at number one for the fourth straight time. McEnroe feels he did not fulfil his potential as he often folded when the going got tough, but would have he had really been more successful with a “growth mindset”?\nSimilarly, Mike Tyson is labelled as someone who “reached the top, but … didn’t stay there”, despite having the third longest unified championship reign in heavyweight history with eight consecutive defences. Tyson obviously had some behavioural issues, but would he have been the same fighter if he didn’t believe in his ability?\n\nOn a personal angle. Dweck’s picture of someone with “fixed mindset” is a good description of me. Through primary and high-school I was always the “smartest” kid in (my small rural then regional) school, despite investing close to zero effort outside of the classroom. I spent the evenings before my university entrance exams shooting a basketball.\nMy results gave me the pick of Australian universities and scholarships, but I then dropped out of my first two attempts at university, and followed that by dropping out of Duntroon (Australia’s army officer training establishment, our equivalent to West Point). For the universities, lecture attendance alone was not enough. I was simply too lazy and immature to make it through Duntroon. (Maybe I lacked “grit”.)\nAfter working in a chicken factory to fund a return to university (not recommended), I finally obtained a law degree, although I did so with a basic philosophy of doing just enough to pass.\nThrough this stretch, I displayed a lot of Dweck’s archetypical “fixed mindset” behaviours. I loved demonstrating how smart I was in domains where I was sure I would do well, and hated the risk of being shown up as otherwise in any domain where I wasn’t. (My choice of law was somewhat strange in this regard, as my strength is my quantitative ability. I chose law largely because this is what “smart” kids do.) I dealt with failure poorly.\nIt took five years after graduation before I finally realised that I needed to invest some effort to get anywhere - which happened to be a different direction to where I had previously been heading. I have spent most of my time since then investing in my intellectual capital. I am more than willing to try and fail. I am always looking for new learning opportunities. I am happy asking “dumb” questions. I want to prove myself wrong.\nDo I now have a “growth mindset”? I don’t believe that anyone can achieve anything. IQ is malleable but only at the margins, and we have a very poor understanding of how to do this. But I have a strong belief that effort pays off, and that absent effort natural abilities can be wasted. I hold the bloody obvious position but not the controversial position. If I was able to blot from my mind the evidence for, say, the genetic contribution to intelligence, could I do even better?\nDespite finding limited value in the book from an intellectual standpoint, I can see its appeal. It was a reminder of the bloody obvious position. It highlighted that many of the so-called growth mindset traits or behaviours can be valuable (whether or not they are accompanied by a growth mindset). There was something in there that suggested I should try a bit harder. Maybe that makes it a useful book after all."
  },
  {
    "objectID": "posts/charity-as-conspicuous-consumption.html",
    "href": "posts/charity-as-conspicuous-consumption.html",
    "title": "Charity as conspicuous consumption",
    "section": "",
    "text": "At the end of Moav and Neeman’s paper on conspicuous consumption and poverty traps, about which I posted yesterday, the authors suggest an experiment:\n\nIt is well known that the rich spend a lot more on charitable contributions than the poor. There are at least three different explanations for this behaviour: charitable contribution is a luxury good, there is stronger social pressure on the rich to contribute more to society and our suggestion that charitable contributions provide a signal about unobserved income (‘success’). Consider the effect of the revelation of information about individuals’ incomes. Such a revelation should have no effect on charitable contributions if they are mostly a luxury good. It should lead to a rise in charitable contributions if they are mostly due to social pressure on the rich to contribute to society and it should lead to a decrease in charitable contributions if they are mostly about signalling. It ought to be possible to examine empiricaly the effect of such a revelation of information on charitable giving.\n\nI like the idea, but how could this experiment practically be implemented? Their suggestion reminded me of the closing paragraph of an article by Griskevicius and colleagues, in which they reported the results of experiments that tested the desire to engage in benevolent activity in response to priming with mating motives. They write:\n\n[M]edia mogul Ted Turner once bemoaned the influence of the Forbes 400 list of richest Americans, pointing out that it discouraged the wealthy from giving away their money for fear of slipping down in the rankings (Plotz, 2006). He suggested instead that a public ranking of top philanthropists could inspire the wealthy to compete in a more beneficial way—in essence, by shifting the signaling arena from conspicuous consumption to blatant benevolence. Perhaps it was not a coincidence that just such a list—the Slate 60—was established the same year that Ted Turner pledged $1 billion to humanitarian relief. When asked about the reaction of his then-wife Jane Fonda to this donation, Turner fondly reminisced, “It brought tears to her eyes . . . . She said, ‘I’m so proud to be married to you’”\n\nThe need to engage is conspicuous consumption drops away if your wealth is published each year."
  },
  {
    "objectID": "posts/chris-vosss-never-split-the-difference-negotiating-as-if-your-life-depended-on-it.html",
    "href": "posts/chris-vosss-never-split-the-difference-negotiating-as-if-your-life-depended-on-it.html",
    "title": "Chris Voss’s Never Split the Difference: Negotiating as if your life depended on it",
    "section": "",
    "text": "Summary: Interesting ideas on how to approach negotiation, but I don’t know how much weight to give them. How much expertise could be developed in hostage negotiations? Can that expertise be distilled into principles, or is much of it tacit knowledge?\n\nChris Voss’s Never Split the Difference: Negotiating as if your life depended on it (written with Tahl Raz) is a distillation of Voss’s approach to negotiation, developed through 15 years negotiating hostage situations for the FBI. Voss was the FBI’s lead international kidnapping negotiator, and for the last decade he has run a consulting firm that guides organisations through negotiations.\nI am not sure how I should rate the book. There are elements I like, elements that seem logical, and yet a sense that much is just storytelling. I don’t know enough of the negotiation literature to understand what other support there might be for Voss’s approach - and Voss generally doesn’t draw on the literature - so it is not clear what weight I should give to his arguments.\nVoss’s central thread is that we should not approach negotiation as though it is a purely rational exercise. No matter how you frame the negotiation in advance, there is no escaping the humans that will be engaging in that negotiation.\nThis argument seems obvious, as in many negotiations you will be dealing with emotional people. Yet a flip through some of the classic negotiating texts, such as Getting to Yes, shows that the consideration of emotion is often shallow. Emotion is largely discussed as something to be overcome so that a mutually beneficial deal can be reached.\nA deeper level to understanding the role emotion is to see how integral it is to the negotiating process. Emotion and decision-making cannot be disentangled.\nIn the opening chapter, Voss links this need to consider emotions to the work of Daniel Kahneman and Amos Tversky (unfortunately described as University of Chicago professors who discovered more than 150 cognitive biases). Voss draws on Kahneman’s distinction between the two modes of thought described in Thinking, Fast and Slow: the fast, instinctive and emotional System 1, and the slow, deliberative and logical System 2. If you go into a negotiation with all the tools to deal with System 2 without the tools to read, understand and manipulate System 1, you were trying to make an omelette without cracking an egg.\nDespite being prominent in the opening, Kahneman and Tversky’s work is only briefly considered in other parts of the book, mainly in one chapter that includes examination of anchoring and loss aversion. By manipulating someone’s reference point and capitalising on their fear of loss, you can shift the terms of what they will agree to.\nFor instance, Voss suggests that you might initially anchor the other side’s expectations through an “accusation audit”, whereby you list every terrible thing the other side could say about you in advance. You then create a frame so that the agreement is to avoid loss. Putting those together, you might start out by saying that you have a horrible deal for them, but still want to bring it to them before you give it to somebody else. By taking the sting out of the low offer and framing acceptance of that offer as an opportunity to avoid loss, you might induce acceptance.\nVoss also discusses the idea of setting a very high or low anchor early in negotiations, although he notes that this comes at a cost. It might be effective against the inexperienced, but you lose the opportunity of learning from the other side when they go first. If prepared, you can resist their anchor, and if you are in a low information environment, you might be pleasantly surprised.\nVoss recognises the human desire for fairness in another important factor. While Voss draws on the academic literature to demonstrate that desire, his proposed approaches to fairness in negotiation are not put in the context of that literature. As a result, I don’t have much of a grip on whether his ideas - such as avoiding accusations of unfairness, and giving the other side permission to stop you at any time of they feel you are being unfair - are effective. It’s polite, sounds reasonable, but does it work?\nThe concept that gets the most attention in the book is tactical empathy. This involves active listening, with tools such as mirroring (repeating the last few words someone said to induce them to keep explaining), labelling (giving a name to their feelings) and summarising their position back to them. I am partial to these ideas. By listening, you can learn a lot. I have always found that simple repetition of concepts, whether through mirroring, labelling or summarising, are powerful tools to get people to open up and to understand their position.\nAnother thread to the book is the idea of saying no without saying no, generally through the use of calibrated questions. Calibrated questions are questions with no fixed answer, and that can’t be answered with a yes or no. They typically start with “how” or “what”, rather than “is” or “does”. They can be used to give the other side the illusion of control while at the same time pushing them to think about solving your problem. If the price is higher than you want to pay, you might say “how am I supposed to pay that?” Calibrated questions also have broader use through the negotiation to learn more from your counterpart.\nIdeas such as this seem attractive, but I don’t know how much weight I should put on Voss’s arguments. This is largely because I don’t how much expertise you could develop in hostage negotiation, and the degree to which that expertise is tacit knowledge. Voss notes that his expertise is built from experience, not from textbooks, and that his approach is designed for the real world. Can a human build skills for this real world? Is there rapid feedback on decisions, with an opportunity to learn?\nIn one sense there is feedback, with the hostages released or not, and the terms of that release known. But each negotiation would involve a multitude of decisions and factors. Conversations might extend for days or weeks. How effectively can you isolate the cause of the outcome? How stable is that cause-effect relationship across different negotiations?\nIn a podcast episode with Sam Harris, Voss mentioned that he had been involved around 150 hostage negotiations around the world. That would seem a fair number to start to be able to identify patterns, particularly if you consider that through a negotiation there might be many smaller opportunities of feedback, such as extracting information. But as Voss’s stories through the book show, these negotiations span across many different countries and contexts. How many of those elements are common and stable enough for true expertise to develop? Most of his experience involved international kidnapping - a commodity business involving financial transactions. Can the lessons from these be applied elsewhere?\nVoss (and the FBI more generally) would have had a broader range of examples to draw on, and Voss’s more recent experience in consulting on negotiation could provide further opportunities to develop expertise. But it’s not obvious how that experience is incorporated into expertise that in turn can be effectively distilled into a book."
  },
  {
    "objectID": "posts/clark-on-the-remnants-of-rural-idiocy.html",
    "href": "posts/clark-on-the-remnants-of-rural-idiocy.html",
    "title": "Clark on the remnants of rural idiocy",
    "section": "",
    "text": "Another piece from the vault, this time A Farewell to Alms author Gregory Clark in an interview with Phillip Adams:\n\nJared Diamond in his famous text actually has has a throw away reference to this, where he speculates that New Guinea tribesmen are actually much smarter than Europeans because New Guinea tribesmen live by their wits, whereas Europeans it was their gut bacteria that determined whether they survived or not. And I actually knew that cities in Europe had very poor survival rates, that that’s where the educated people were, and I expected going into this to show that we were all, the current Europeans, were the remnants of rural idiocy.\n\nClark’s subsequent findings of the higher fertility of the rich and his association of this with the Industrial Revolution make this a relatively rare example in economics where someone can say that the evidence changed their mind.\nClark also makes some interesting comments on how he approached the genetic question in the book:\n\nI must admit that when I wrote the book I was a little hesitant and so I talked about cultural or genetic link. That led to a firestorm of criticism and it led me to actually examine that proposition more carefully and I’ve actually come to realise that I was being too hesitant. There’s absolutely a genetic link. We absolutely have changed genetically over this pre-industrial period.\n\nFinally, Clark on human ecology and the policy implications (which has some relevance to my earlier post):\n\nThere may be some groups that do face a disadvantage in operating in a capitalist economy, that their ecology is just not that of modern capitalism. But what I would argue is that might help us have some more understanding of why, these groups are not just being resistant or indolent. That really we have to understand that people have their own ecology and that these things are very hard to change suddenly."
  },
  {
    "objectID": "posts/cliodynamics-and-complexity.html",
    "href": "posts/cliodynamics-and-complexity.html",
    "title": "Cliodynamics and complexity",
    "section": "",
    "text": "At the Consilience Conference earlier this year, I saw Peter Turchin’s presentation on cliodynamics - the mathematical modelling of historical dynamics. I was relatively sceptical of what I saw, and a new Nature news piece by Laura Spinney on Turchin’s work captures some of this scepticism. Spinney describes cliodynamics as follows:\n\nIn their analysis of long-term social trends, advocates of cliodynamics focus on four main variables: population numbers, social structure, state strength and political instability. Each variable is measured in several ways. Social structure, for example, relies on factors such as health inequality — measured using proxies including quantitative data on life expectancies — and wealth inequality, measured by the ratio of the largest fortune to the median wage. Choosing appropriate proxies can be a challenge, because relevant data are often hard to find. No proxy is perfect, the researchers concede. But they try to minimize the problem by choosing at least two proxies for each variable.\nThen, drawing on all the sources they can find — historical databases, newspaper archives, ethnographic studies — Turchin and his colleagues plot these proxies over time and look for trends, hoping to identify historical patterns and markers of future events. For example, it seems that indicators of corruption increase and political cooperation unravels when a period of instability or violence is imminent. Such analysis also allows the researchers to track the order in which the changes occur, so that they can tease out useful correlations that might lead to cause–effect explanations.\n\nSome of the ideas behind cliodynamics are interesting, particularly the data collection, use of agent based models and nonlinear mathematics, but I am sceptical of the “chartist” approach of Turchin and others who extend their search of patterns and causation to identifying cycles. In particular, Turchin and colleagues have identified two historical cycles:\n\nThe first, which they call the secular cycle, extends over two to three centuries. It starts with a relatively egalitarian society, in which supply and demand for labour roughly balance out. In time, the population grows, labour supply outstrips demand, elites form and the living standards of the poorest fall. At a certain point, the society becomes top-heavy with elites, who start fighting for power. Political instability ensues and leads to collapse, and the cycle begins again.\nSuperimposed on that secular trend, the researchers observe a shorter cycle that spans 50 years — roughly two generations. Turchin calls this the fathers-and-sons cycle: the father responds violently to a perceived social injustice; the son lives with the miserable legacy of the resulting conflict and abstains; the third generation begins again. Turchin likens this cycle to a forest fire that ignites and burns out, until a sufficient amount of underbrush accumulates and the cycle recommences.\n\nThis conclusion is difficult to reconcile with a view of history as the outcome of a complex system, where what seem to be repeated cycles may be transitory and long periods of apparent calm may end with sudden shifts. Black swan events (even relatively small) can have large effects. Turchin appears to be aware of complexity theory from what I have read of his writings and he adopts many of the tools that has emerged from it, so I find his focus on cycles surprising.\nThe focus on cycles also appears to give a low weight to the strong trends underlying them, such as the general decline in violence or massive gains in wealth since the Industrial Revolution. That is where cliodynamics could add some real value. What caused these changes to what extent are the changes permanent and stable?\nThis is not to say, however, that cliodynamics can’t produce something interesting - the modelling and mass collection of data could be highly valuable. Herb Gintis captures some of my optimism:\n\nHerbert Gintis, a retired economist who is still actively researching the evolution of social complexity at the University of Massachusetts Amherst, also doubts that cliodynamics can predict specific historical events. But he thinks that the patterns and causal connections that it reveals can teach policy-makers valuable lessons about pitfalls to avoid, and actions that might forestall trouble. He offers the analogy of aviation: “You certainly can’t predict when a plane is going to crash, but engineers recover the black box. They study it carefully, they find out why the plane crashed, and that’s why so many fewer planes crash today than used to.”"
  },
  {
    "objectID": "posts/coal-and-the-industrial-revolution.html",
    "href": "posts/coal-and-the-industrial-revolution.html",
    "title": "Coal and the industrial revolution",
    "section": "",
    "text": "In a recent discussion as part of the Cato Unbound series, Matt Ridley suggested that we shouldn’t forget the Materialist explanation for the industrial revolution. The difference between the British and other bursts of economic activity, such as that of Ancient Greece, is that the British event did not peter out. The reason for that, says Ridley, is coal.\nOn one hand, this explanation feels like a Jared Diamond style geographic explanation, although in a more modern era. However, I am not convinced that the availability of coal is a unique enough circumstance to explain why the industrial revolution happened when and where it did. Why did the industrial revolution not occur during the Roman occupation of Britain,? Coal is also plentiful in China, with Marco Polo reporting to astonished European audiences on the flammable black rocks used by the Chinese. Why did coal not propel technologically advanced China into an industrial age? There are  also many readily accessible coal deposits in North America (although in the North American case, the Diamond geographic argument  and the need to reach a level of technology necessary to exploit the resource appears convincing).\nI agree with Ridley’s point that coal contributed to the wealth and pace of the industrial revolution. However, given that the United Kingdom is not unique in having accessible coal deposits, acknowledging the role of coal  does not answer the interesting question of why the industrial revolution occurred where and when it did. To tie coal into that question, why did the British people reach a degree of technological advancement and industry that they were able to use resources like coal to propel themselves and the rest of the world into an industrial age?"
  },
  {
    "objectID": "posts/complexity-versus-chaos.html",
    "href": "posts/complexity-versus-chaos.html",
    "title": "Complexity versus chaos",
    "section": "",
    "text": "Another clip from David Colander and Roland Kupers’s Complexity and the Art of Public Policy: Solving Society’s Problems from the Bottom Up - a nice description of how two often confused terms, complexity and chaos, differ and interrelate:\n\nChaos theory is a field of applied mathematics whose roots date back to the nineteenth century, to French mathematician Henri Poincaré. Poincaré was a prolific scientist and philosopher who contributed to an extraordinary range of disciplines; among his many accomplishments is Poincaré’s conjecture that deals with a famous problem in physics first formulated by Newton in the eighteenth century: the three body problem. The goal is to calculate the trajectories of three bodies, planets for example, which interact through gravity. Although the problem is seemingly simple, it turns out that the paths of the bodies are extraordinarily difficult to calculate and highly sensitive to the initial conditions.\nOne of the contributions of chaos theory is demonstrating that many dynamical systems are highly sensitive to initial conditions. The behavior is sometimes referred to as the butterfly effect. This refers to the idea that a butterfly flapping its wings in Brazil might precipitate a tornado in Texas. This evocative—if unrealistic—image conveys the notion that small differences in the initial conditions can lead to a wide range of outcomes.\nSensitivity to initial conditions has a number of implications for thinking about policy in such systems. For one, such an effect makes forecasting difficult, if not impossible, as you can’t link cause and effect. For another it means that it will be very hard to backward engineer the system—understanding it precisely from its attributes because only a set of precise attributes would actually lead to the result. How much time is spent on debating the cause of a social situation, when the answer might be that it simply is, for all practical purposes, unknowable? These systems are still deterministic in the sense that they can be in principle specified by a set of equations, but one cannot rely on solving those equations to understand what the system will do. This is known as deterministic chaos, but is mostly just called chaos.\nWhile chaos theory is not complexity theory, it is closely related. It was in chaos theory where some of the analytic tools used in complexity science were first explored. Chaos theory is concerned with the special case of complex systems, where the emergent state of the system has no order whatsoever—and is literally chaotic. Imagine birds on the power line being disrupted by a loud noise and fluttering off in all directions. You can think of a system as being in these three different kinds of states, linear, complex, or chaotic—sitting on the line, flying in formation, or scrambling in all directions.\n…\nLike chaos theory, complexity theory is about nonlinear dynamical systems, but instead of looking at nonlinear systems that become chaotic, it focuses on a subset of nonlinear systems that somehow transition spontaneously into an ordered state. So order comes out of what should be chaos. The complexity vision is that these systems represent many of the ordered states that we observe—they have no controller and are describable not by mechanical metaphors but rather by evolutionary metaphors. This vision is central to complexity science and complexity policy."
  },
  {
    "objectID": "posts/conflict-and-social-evolution.html",
    "href": "posts/conflict-and-social-evolution.html",
    "title": "Conflict and social evolution",
    "section": "",
    "text": "Last week’s edition of Science has an interesting article by Sam Bowles on the role of conflict on human social evolution. Bowles covers some familiar ground on the debate around the role of group selection in shaping human altruistic preferences:\n\n[F]or most animals, gene flow (due to migration) would minimize genetic differences between groups, and hence nullify the genetic effects of group competition. But recent evidence suggests that this may not be the case for a number of species, including recent human foragers, whose population structures may resemble those of our Late Pleistocene ancestors. Others may have doubted Darwin’s conflict-based account because they believe warfare to be a postagricultural revolution corruption of our naturally peaceful disposition. But hunter-gatherer burials with smashed skulls, broken or missing forearms (taken as trophies), and stone points embedded in bones tell a different story, as does ethnographic evidence that warfare was a leading cause of death among some recent hunters and gatherers.\nI have shown that we can plausibly infer from these data that the degree of mortal conflict and extent of genetic differences among ancestral forager groups were jointly sufficient to have allowed the evolution of a genetically transmitted predisposition to contribute to common projects (including defense and predation vis a vis other groups), even when one’s individual fitness would have been enhanced by free riding on those who would “aid and defend each other.”\n\nBowles cites his theoretical models that show this point, but is there any society whose warlike behaviour against other groups extended to elimination of the women of their vanquished rival? If any of the women of the defeated group are absorbed into the victor, there will be substantial gene flow. Add exogamy to the mix, and I struggle to see how a human group could maintain the required level of genetic differentiation.\nBowles has a number of other interesting threads in the article. The most novel of these is the manner in which he links conflict with the modern liberal state. Bowles writes:\n\n\n\n\nSeven centuries ago, in what is now Italy, there were more than 200 distinct independent governing entities. Europe was governed by about 500 sovereign bodies: “empires, city states, federations of cities, networks of landlords, religious orders, leagues of pirates, warrior bands”. By World War I, fewer than 30 remained. A single political form had survived: the national state, a centralized bureaucratic structure maintaining order over a defined territory, with the capacity to mobilize substantial resources by taxation and borrowing and to deploy permanent armed forces.\nWhat explains the competitive success of this novel form of rule? The simple answer is that national states won wars.\n\n\n\nThis battle then continued within the state:\n\nAmerican high school students are taught that their democratic constitution was the gift of the landed and commercial elites of the 13 former colonies. James Madison and the other authors of The Federalist Papers, the story goes, convinced the haves that the have nots would never be able to unite sufficiently to redistribute wealth. The elites could safely take a chance on democracy. But that is just one of America’s national myths. The United States would wait more than a century and a half to meet the elementary standard of democratic rule by extending suffrage to virtually all adults (with the Voting Rights Act of 1965), a process propelled by the victories of abolitionists, slaves and their descendants, workers, and women demanding the vote .\n\nA natural corollary to the argument that group conflict was important in shaping human traits and our institutions is the question about whether that must necessarily have negative implications for conflict today. Bowles is optimistic on this front:\n\nIt seems likely, too, that conflict will remain important for human progress. But does this require the violence, suffering, bigotry, and waste characteristic of the conflicts of the past along with the cultural inheritance of this dismal trajectory, an unpleasant nexus of predispositions that Choi and I call “parochial altruism,” marked by generosity toward those we call “us” and hostility and intolerance toward “them” ?\n\n\nI do not think so: Our legacy need not be our fate. We could not have become what Gintis and I call a cooperative species  were we not, par excellence, a cultural animal. Among the lessons of our past are not only the grisly truths on which I have dwelled but also the fact that our us’s and them’s are not primordial. On world historic time scales, we make and unmake these pronouns of exclusion at lightning speed. For ancestral humans, making peace was no less essential than surviving wars [as Boehm points out in his contribution to this issue].\n\nAs is often the case, the whole article is worth a read."
  },
  {
    "objectID": "posts/consilience-conference-afterthoughts.html",
    "href": "posts/consilience-conference-afterthoughts.html",
    "title": "Consilience conference afterthoughts",
    "section": "",
    "text": "The Consilience Conference on evolution in biology, the social sciences and the humanities wrapped up on Saturday, and it was generally a high quality conference. It’s strength was that most of the presenters were doing work across multiple fields, usually with an evolutionary twist. Conferences such as these often involve people trying to frame existing work around the topic, even if it is a weak fit, but here the presenters’ work generally fitted the subject nicely.\nSome stray thoughts from the conference and presentations are below.\nUntil this conference, I had not realized the scope of the field of literary Darwinism. While I am slightly skeptical as to how far the idea can go, it was good to see some high quality thought being put into it. This article from Science provides some interesting background on the field, including comments from the conference organiser Joseph Carroll and one of the more impressive conference presenters, Jonathan Gottschall.\nBarb Oakley presented on the idea of pathological altruism. The basic idea is that too much altruism is not necessarily a good thing. In some ways, it is an obvious point as altruism is costly to the altruist by definition and can be exploited by cheaters. However, that point is often forgotten in descriptions of the virtue of altruism, and when taken to extremes, can impose significant costs on both oneself and others. Some of the left-right divide might be explained by debate about what is considered to be the optimal level of altruism before the costs and potential for exploitation become too large.\nPeter Turchin explored the quantification of history. Turchin argued that there is a need to move on from the tendency to accumulate theories without ever rejecting them. It sounds familiar to an economist.\nHenry Harpending presented some ideas around measures of kinship. He was surprised that his research to date showed that, within most populations, there appeared to be little benefit in determining the degree of kinship of someone who may be a beneficiary of your actions. The variation in kinship is simply too small to matter. However, there is some value to considering kinship in mixed populations.\nMassimo Pigliucci’s summaries of each day (days one, two and three) are worth reading, although Massimo played the role of the token skeptic about the integration of evolution into social sciences and the humanities, and his comments reflect this."
  },
  {
    "objectID": "posts/conspicuous-consumption-and-economic-growth-2.html",
    "href": "posts/conspicuous-consumption-and-economic-growth-2.html",
    "title": "Conspicuous consumption and economic growth",
    "section": "",
    "text": "A paper of mine has just been published in the Journal of Bioeconomics - Sexual selection, conspicuous consumption and economic growth (pdf).\nI posted about this article when the working paper was first released, and that post still does a good job of explaining the motivation behind the paper. In that post I wrote:\n\nAround ten years ago, I was rummaging through books in a bargain bookshop under Sydney’s Central Station when I came across a $2 copy of Geoffrey Miller’s The Mating Mind. It turned out to be a good use of my $2, as The Mating Mind is one of the most important books in shaping my thinking, and it was one of the first books I put on my economics and evolutionary biology reading list.\nMiller’s basic argument was that sexual selection shaped the human mind. Whether through runaway selection or the brain acting as a fitness indicator, female choice led to increasing mental capacity and shaped our propensity to be humorous, create art or engage in other displays of mental fitness.\nAs I read the Mating Mind, it occurred to me that the growing mental capability and tendency to display it would have direct economic effects. It would be possible to argue that sexual selection shapes economic growth. Ten years after that idea, my latest working paper (co-authored with my supervisors Boris Baer and Juerg Weber) seeks to flesh out one element of it. The working paper provides a theoretical model for the hypothesis that sexual selection and the resulting propensity to engage in conspicuous consumption has economic effects, and in particular, the desire to engage in conspicuous consumption is one of the pillars underlying the emergence of modern economic growth.\nThe concept behind the hypothesis is relatively simple. Men who signal their quality through conspicuous consumption have higher reproductive success, as the conspicuous consumption provides a reliable signal of their quality to potential mates. To engage in conspicuous consumption takes effort by the men – whether in the form of art, humour or entering the labour force to acquire resources to consume conspicuously. As the prevalence of males who conspicuously consume increases, the total level of these activities also increases. The increased participation in productive activities results in a scale effect, whereby the greater number of people involved in creative and productive activities results in increased technological progress, which underlies economic growth.\nThe evolutionary part of the model is more interesting than the economic as there is minimal feedback from the economy back into the evolutionary dynamics. The lack of feedback also means that it is not very representative of modern society, as conspicuous consumption in modern societies is of limited threat to survival. Still, the model provides a starting point and I have a few ideas to take it further.\nI have been introducing my talks on the paper with an example from Robert Frank’s Luxury Fever, in which Frank held up Patek Philippe’s Calibre 89 watch as an example of conspicuous consumption. Only four were made, with the first selling for $2.5 million and the latest auction price being over $5 million. Frank mocks the watch for its need for a tourbillon, a mechanism to account for the earth’s rotation, when his cheap quartz watch does not require such a mechanism, as gravity does not affect the vibrations of the crystal.\nNow consider the innovation and thought that went into the Patek Philippe watch, including that tourbillon. This watch has 1728 components, gives you the date of Easter each year, and unlike most mechanical watches, will not record the years 2100, 2200 and 2300 as leap years while still recording 2400 as one (as per the order of Pope Gregory XIII in 1582). If you look at Patek Philippe’s list of patents, you get a feel for the innovation involved in making watches for what is largely conspicuous consumption.\nWhen you also consider the innovation undertaken by the potential buyers as they seek to amass the wealth necessary to obtain such a watch, the positive angle to conspicuous consumption grows. As a result, curbing conspicuous consumption may have costs (although, I still prefer taxing consumption to income). If nothing else, we should appreciate the historical role of conspicuous consumption – competition for sexual partners is a driving force for many productive activities, and one generation’s conspicuous consumption is another generation’s day-to-day tool.\n\nIn its life as a working paper over the last few years the paper has received a variety of comments, including from Matt Ridley in The Wall Street Journal, Rob Brooks in The Huffington Post and Chris Dillow at Stumbling and Mumbling. The major difference between the working paper and published version is that the simulations have been shunted into the electronic supplementary material for final publication."
  },
  {
    "objectID": "posts/conspicuous-consumption-as-a-handicap.html",
    "href": "posts/conspicuous-consumption-as-a-handicap.html",
    "title": "Conspicuous consumption as a handicap",
    "section": "",
    "text": "In a recent post, I discussed Gianni De Fraja’s paper in which he proposed that sexual selection shaped the nature of conspicuous consumption by men. In his model, conspicuous consumption by men serves as a fitness indicator to women. Low and high quality men signal their differing wealth “honestly” (under certain conditions) as the consumption level of the high quality men is too large a handicap for the low quality men to copy.\nOne of the unsatisfying elements of the paper (although through no fault of De Fraja’s) was that the conditions under which high and low quality men signal honestly were not readily interpretable. The mathematics were too ugly.\nDe Fraja based his model on two models developed by Grafen (here and here), which were in turn the first mathematical demonstration that Zahavi’s handicap principle was theoretically sound and could work as a stand-alone process. Grafen’s first model was a simple game theoretic model for which he found the equilibrium. The second model was a population genetic model that built on the first by being explicit on how women used the information they obtained about the males’ quality. It also, as the name suggests, incorporated a genetic basis. Despite the differences in the two models, Grafen considered that the results from the population genetic model supported the use of the simpler game theoretic model and that the extra complications in the population genetic model did not negate the results of the other.\nOne virtue of Grafen’s game theoretic model is that it is possible to interpret the conditions under which it works. So, instead of trying to pull the conditions from the complicated mathematics of De Fraja’s model (for the moment), the conditions of Grafen’s game theoretic model are worth a look.\nPutting Grafen’s model into human terms, it had three elements. First, men vary in quality (say, wealth), which women cannot observe. If they could see it, women would use it as a basis for their choice. Second, men vary in their level of conspicuous consumption, which is a function of their quality. Third, women infer the man’s quality from the level of conspicuous consumption and use it to decide their choice of mate. The fitness of a male depends on his true quality, his level of advertising (which is costly) and the woman’s perception of his quality. A woman’s fitness will depend simply on how accurate she is in inferring true quality.\nGrafen showed that in this model an equilibrium exists where higher quality men advertise more than low quality men and the women use this information to correctly infer their quality.  The condition for this equilibrium is that the marginal cost of advertising should be higher for worse males.\nThe question becomes whether this condition could exist in the conspicuous consumption example? On the one hand, a BMW costs the same to anyone who buys it. The marginal cost appears the same to both low and high quality men. But suppose that the rich man can buy the BMW with cash. The poor man needs to take out a loan, max out his credit card and hock his watch. He will be paying high interest on the loan and credit card and will need to pay extra to get his watch back. As a result, even though they are both buying the same advertising, the BMW, the marginal cost of that advertising, is higher for the poor man. This condition for the handicap could hold. The condition also holds where the poor man cannot afford the BMW no matter what he does. His marginal cost at that point is effectively infinite. In equilibrium, the rich guy will pick a level of advertising that will simply be too much for the poor chap to match.\nThis condition is important. Previous mathematical attempts to explain the handicap principle had generally not succeeded as they had not incorporated this higher cost of the handicap for lower quality males. In this interview, John Maynard Smith explains how his earlier work on the handicap principle had failed to support Zahavi’s claims for this reason.\nThere is actually an economics model which mirrors this situation (and pre-dates Grafen’s model by 17 years) - the job market signalling model of Michael Spence. The model works similarly. Suppose there are low and high quality employees and they need to signal their unobservable quality to an employer. Spence showed that there could be a separating equilibrium where each signals their quality accurately through their level of education. The condition for this is that education must be more costly for low quality people. This makes sense - it is easier to learn something and pass the tests if the person is higher quality."
  },
  {
    "objectID": "posts/cooperation-and-conflict-in-the-family-conference-wrap.html",
    "href": "posts/cooperation-and-conflict-in-the-family-conference-wrap.html",
    "title": "Cooperation and Conflict in the Family Conference wrap",
    "section": "",
    "text": "Over the past year I have posted several times about the Cooperation and Conflict in the Family Conference, which was held in Sydney this week. It turned out to be a great conference, and I am very pleased with how it panned out.\nThe conference has increased my optimism about the potential for more work to be carried out at the inter-disciplinary boundaries between economics, evolutionary biology, anthropology, psychology and so on. When I compare it to the Social Decision Making: Bridging Economics and Biology conference I attended almost three years ago (an excellent conference), we managed to drag in a broader range of economists and other social scientists to this event. I suspect this is evidence for increasing interest on the part of social scientists in how sciences such as biology can add to the social science toolkit.\nAs a result, I hope this conference is the first of a continuing series (although hopefully with a wider group of organisers). An interesting challenge for the next iteration will be to pick an appropriate theme. In this case, the Cooperation and Conflict in the Family theme was useful in pulling together people who may not have necessarily considered that there were useful insights in other disciplines. We would not have gotten such an interesting mix of people if we had pitched the topic specifically around the integration of disciplines.\nIt was interesting to see the different presentation styles across disciplines, and I have to say that the biologists (on average) have the edge in presenting their work in an easy to understand way - particularly in relation to the submitted presentations. Us economists are still too tied to our equations to dump them. This was best illustrated in the presentations of two plenary speakers - Michael Jennions and Hanna Kokko - who used simple cartoons and illustrations to describe their models. If you go to their papers (particularly the supplementary materials), there can be some relatively hefty math behind them. Yet they are able to present the ideas without relying on the equations. And maybe this should also be taken as an indication for how economists write their papers - more of the math in the supplementary appendix, more time in the (shorter) main paper on the important intuition. And then dump the math when we intend to communicate our ideas verbally.\nThe conference also reminded me of how hard it is to work across disciplinary boundaries without full immersion in both sides (or having someone from both sides engaged in the work). Again turning the Michael Jennions presentation, he talked about Bateman’s gradient and the operational sex ratio, and about what each of them actually show (the paper on this is here). I thought I knew what each were about, but am now revisiting my understanding."
  },
  {
    "objectID": "posts/cooperation-is-intuitive.html",
    "href": "posts/cooperation-is-intuitive.html",
    "title": "Cooperation is intuitive",
    "section": "",
    "text": "From a recent letter in Nature by Rand, Greene and Nowak:\n\nWe find that across a range of experimental designs, subjects who reach their decisions more quickly are more cooperative. Furthermore, forcing subjects to decide quickly increases contributions, whereas instructing them to reflect and forcing them to decide slowly decreases contributions. Finally, an induction that primes subjects to trust their intuitions increases contributions compared with an induction that promotes greater reflection.\n\nThe interesting part of the article is the mechanism that the authors propose as being behind the intuitive cooperative response:\n\n[P]eople develop their intuitions in the context of daily life, where cooperation is typically advantageous because many important interactions are repeated, reputation is often at stake, and sanctions for good or bad behaviour might exist. Thus, our subjects develop cooperative intuitions for social interactions and bring these cooperative intuitions with them into the laboratory.\n\nThey tested this mechanism by checking whether cooperation was favoured where people were from a cooperative environment:\n\nEven in the presence of repetition, reputation and sanctions, cooperation will only be favoured if enough other people are similarly cooperative. We tested this prediction on AMT with a replication of our baseline correlational study. As predicted, it is only among subjects that report having mainly cooperative daily-life interaction partners that faster decisions are associated with higher contributions.\nThus, there are some people for whom the intuitive response is more cooperative and the reflective response is less cooperative; and there are other people for whom both the intuitive and reflective responses lead to relatively little cooperation. But we find no cases in which the intuitive response is reliably less cooperative than the reflective response. As a result, on average, intuition promotes cooperation relative to reflection in our experiments.\n\nIn many of the debates about cooperation and why it occurs, we forget that there is often a direct benefit to being cooperative. In most of life today, cooperative behaviour is the path to success.\nThe effect of the external environments highlights an important point on trust, a core part of the cooperative behaviour in the experiments. Trustworthiness is important generating trust. As Garett Jones recently wrote:\n\nWhen people are trustworthy, when cultures and laws make honorable behavior common, when people so fully take it for granted that promises are kept that they use the passive voice–because it just doesn’t matter who made the promise–that’s when trust blossoms."
  },
  {
    "objectID": "posts/courseras-data-science-specialisation-a-review.html",
    "href": "posts/courseras-data-science-specialisation-a-review.html",
    "title": "Coursera’s Data Science Specialisation: A Review",
    "section": "",
    "text": "As I mentioned in my comments on Coursera’s Executive Data Science specialisation, I have looked at a lot of online data science and statistics courses to find useful training material, understand the skills of people who have done these online courses, plus learn a bit myself.\nOne of the best known sets of courses is Coursera’s Data Science Specialisation, created by John Hopkins University. It is a ten course program that covers the data science process from data collection to the production of data science products. It focuses on implementing the data science process in R.\nThis specialisation is a signal that someone is familiar with data analysis in R - and the units are not bad if learning R is your goal. But this specialisation (nor any other similar length course I have reviewed to date) doesn’t offer a shortcut to the statistical knowledge necessary for good data science. A few university length units seem to be the minimum, and even they need to be paired with experience and self-directed study (not to mention some skepticism of what we can determine).\nThe specialisation assessments are such that you can often pass the courses without understanding what you have been taught. Points for some courses are awarded for “effort” (see Statistical Inference below). While capped at three attempts per 8 hours, the multiple choice quizzes have effectively unlimited attempts. I don’t have a great deal of faith in university assessment processes either - particularly in Australia where no-one wants to disrupt the flood of fees from international students by failing someone - but the assessment in these specialisations require even less knowledge or effort. They’re not much of a signal of anything.\nIf you are wondering whether you should audit or pay for the specialisation, you can’t submit the assignments under the audit option. But the quizzes are basic and you can find plenty of assignment submissions on GitHub or RPubs against which you can check your work.\nHere are some notes on each course. I looked through each of these over a year or so, so there might be some updates to the earlier courses (although a quick revisit suggests my comments still apply).\n\nThe Data Scientist’s Toolbox: Little more than an exercise in installing R and git, together with an overview of the other courses in the specialisation. If you are familiar with R and git, skip.\nR Programming: In some ways the specialisation could have been called R Programming. This unit is one of the better of the ten, and gives a basic grounding in R.\nGetting and Cleaning Data: Not bad for getting a grasp of the various ways of extracting data into R, but watching video after video of imports of different formats makes for less-than exciting viewing. The principles on tidy data are important - the unit is worth doing for this alone.\nExploratory Data Analysis: Really a course in charting in R, but a decent one at that. There is some material on principal components analysis and clustering that will likely go over most people’s heads - too much material in too little time.\nReproducible Research: The subject of this unit – literate (statistical) programming – is one of the more important subjects covered in the specialisation. However, this unit seemed cobbled together – lectures repeated points and didn’t seem produced to a logical structure. The last lecture is a conference video (albeit one worth watching). If you compare this unit to the (outstanding) production effort that has gone into the Applied Data Science with Python specialisation, this unit compares poorly.\nStatistical Inference: Likely too basic for someone with a decent stats background, but confusing for someone without. This unit hits home how it isn’t possible to build a stats background in a couple of hours a week over four weeks. The peer assessment caters to this through criteria such as “Here’s your opportunity to give this project +1 for effort.”, with option “Yes, this was a nice attempt (regardless of correctness)”.\nRegression Models: As per statistical inference, but possibly even more confusing for those without a stats background.\nPractical Machine Learning: Not a bad course for getting across implementing a few machine learning models in R, but there are better background courses. Start with Andrew Ng’s Machine Learning, and then work through Stanford’s Statistical Learning (which also has great R materials). Then return to this unit for a slightly different perspective. As for many of the other specialisation units, it is at a level too high for someone with no background. For instance, there is no point where they actually describe what machine learning is.\nDeveloping Data Products: This course is quite good, covering some of the major publishing tools, such as Shiny, R Markdown and Plotly (although skip the videos on Swirl). The strength of this specialisation is training in R, and that is what this unit focuses on.\nData Science Capstone: This course can be best thought of as a commitment device that will force you to learn a certain amount about natural language processing in R (the topic of the project). You are given a task with a set of milestones, and you’re left to figure it out for yourself. Unless you already know something about natural language processing, you will have to review other courses and materials and spend a lot of time on the discussion boards to get yourself across the line. Skip it and do a natural language processing course such as Coursera’s Applied Text Mining in Python (although this assumes a fair bit of skill in Python). Besides, you can only access the capstone if you have paid for and completed the other nine units in the specialisation."
  },
  {
    "objectID": "posts/coyle-on-happiness.html",
    "href": "posts/coyle-on-happiness.html",
    "title": "Coyle on happiness",
    "section": "",
    "text": "Over the weekend I read Diane Coyle’s The Economics of Enough. I particularly enjoyed her dismantling of the concept that to increase happiness we should forget about growth. My reading list on this area has increased considerably - and it seems that I should place Amartya Sen high on that list. Coyle writes:\n\nThose researchers like Richard Layard and Robert Frank who believe the link between growth and happiness tails off to nothing above a certain income level argue for taxes to make people stop working so hard or spend less on various consumer goods. The government must prod us into being happy because we’re simply adapting to each new level of income. The rat race means that like caged guinea pigs scrabbling around their wheel, we keep running to earn and spend more without making any progress in terms of happiness.\nHowever, this kind of policy conclusion has been strongly challenged by other researchers. In his book The Idea of Justice Amartya Sen agrees that people’s happiness depends on their expectations, which are shaped by their own social situation. But he turns the argument about adaptation and the hedonic treadmill back on the happiness crowd: if we just aim for people to be happy with their lot, where is the social discontent that will create the momentum for a better life? Would women have ever gained the vote if many had not been unhappy? Would there have been a civil rights movement without discontent? Is poverty acceptable because poor people say they are pretty content? Obviously not; most people would agree the world with the discontent and change was better than the contented and static one.\n\nAs noted in my recent post on happiness adjusting, dissatisfaction might be a major driver of progress. However, there is a distinction between the policy concerns of Frank and Sen. Frank intends his taxes to cut competition in areas where the effort is wasteful and focused on relative status, such as competition for the largest house. Further, he intends progressive consumption taxes to allow reduction of taxes in other areas, such as those on income. That is a large step from taxing the poor because they will not be happier when richer."
  },
  {
    "objectID": "posts/crime-and-biology.html",
    "href": "posts/crime-and-biology.html",
    "title": "Crime and biology",
    "section": "",
    "text": "The July/August 2011 edition of the Atlantic has a great article by David Eagleman on the implications of advances in brain science on the way we approach crime (HT: Jeffrey Horn). Eagleman argues that these advances will require a reshaping of the criminal justice system to reflect the declining gap between whether actions can be attributed to biology and free will. Eagleman writes:\n\nTechnology will continue to improve, and as we grow better at measuring problems in the brain, the fault line will drift into the territory of people we currently hold fully accountable for their crimes. Problems that are now opaque will open up to examination by new techniques, and we may someday find that many types of bad behavior have a basic biological explanation—as has happened with schizophrenia, epilepsy, depression, and mania.\n….\nThe crux of the problem is that it no longer makes sense to ask, “To what extent was it his biology, and to what extent was it him?,” because we now understand that there is no meaningful distinction between a person’s biology and his decision-making. They are inseparable.\n\nEagleman’s first response to this problem is to move away from blameworthiness. If you cannot distinguish the extent of volition (if it can even be argued to exist), it is hard to blame. As a result, Eagleman suggests that justice will need to become more forward-looking:\n\nInstead of debating culpability, we should focus on what to do, moving forward, with an accused lawbreaker. I suggest that the legal system has to become forward-looking, primarily because it can no longer hope to do otherwise. As science complicates the question of culpability, our legal and social policy will need to shift toward a different set of questions: How is a person likely to behave in the future? Are criminal actions likely to be repeated? Can this person be helped toward pro-social behavior? How can incentives be realistically structured to deter crime?\n…\nDeeper biological insight into behavior will foster a better understanding of recidivism—and this offers a basis for empirically based sentencing. Some people will need to be taken off the streets for a longer time (even a lifetime), because their likelihood of reoffense is high; others, because of differences in neural constitution, are less likely to recidivate, and so can be released sooner.\n\nWhile I agree the criminal justice system should ask what the law-breaker is likely to do in the future, it cannot desert the look in the rearview mirror. If you wish the justice system to offer an incentive to not commit crimes, it needs to act retrospectively or the threat of punishment will not be credible. If people have a biological propensity to commit a crime, you may need to make these incentives even stronger (Steven Pinker discusses this argument in The Blank Slate.\nTo make his forward-looking approach work, Eagleman suggests that the courts use statistically based sentencing. Statistical analysis can be used to find out which factors have the highest link to re-offending - and the evidence suggests that this is more accurate than leaving it to judges. I suggested this recently in response to the finding that the timing of lunch breaks in Israeli courts. Human judgement is a primary weakness in the criminal justice system. However, there will need to be a component of the algorithm that provides a certain, strong punishment that potential criminals can take into account.\nWhile Eagleman’s article is thorough, there is one biological element missing from his analysis - the dynamic effects. Incarceration removes young men from the mating market during their mating prime. As the propensity to commit crime is heritable, the removal of criminals from the mating market will reduce the frequency of the genes associated with crime in the next generation. As Eagleman notes:\n\n[I]f you are a carrier of a particular set of genes, the probability that you will commit a violent crime is four times as high as it would be if you lacked those genes. You’re three times as likely to commit robbery, five times as likely to commit aggravated assault, eight times as likely to be arrested for murder, and 13 times as likely to be arrested for a sexual offense. The overwhelming majority of prisoners carry these genes; 98.1 percent of death-row inmates do.\n\nInstead of worrying about how to control the biologically impulsive, incarceration can simply cut their prevalence in the future."
  },
  {
    "objectID": "posts/crisis-in-human-genetics.html",
    "href": "posts/crisis-in-human-genetics.html",
    "title": "Crisis in human genetics?",
    "section": "",
    "text": "It is a bit over a year since Geoffrey Miller wrote this piece foreshadowing a crisis in conscience by human geneticists that would become public knowledge in 2010. The crisis had two parts: that new findings in genetics would reveal less than hoped about disease and that they would reveal more than feared about genetic differences between classes, ethnicities and race.\nNow that we are through 2010 with no crisis (that I was aware of - is this crisis still happening in private?), I thought I’d revisit Miller’s suggestion that geneticists would show more than feared about class, ethnic and race differences.\nAt the time I first read the article, I found it hard to characterise this information as something to fear. As Miller identifies, it would be a consequence of some interesting progress:\n\nOnce enough DNA is analysed around the world, science will have a panoramic view of human genetic variation across races, ethnicities and regions. We will start reconstructing a detailed family tree that links all living humans, discovering many surprises about mis-attributed paternity and covert mating between classes, castes, regions and ethnicities.\n\nThis sounds good to me. To understand the way genes spread as people migrated and mixed across the world will be to gain an important insight into human history.\nMiller then points out that some people may be troubled when researchers start to identify genes that create physical and mental differences between populations and identify when those genes arose. Millers states:\n\nIf the shift from GWAS [genome wide association studies] to sequencing studies finds evidence of such politically awkward and morally perplexing facts, we can expect the usual range of ideological reactions, including nationalistic retro-racism from conservatives and outraged denial from blank-slate liberals.\n\nBut it is not all bad. He closes with:\n\nThe few who really understand the genetics will gain a more enlightened, live-and-let-live recognition of the biodiversity within our extraordinary species—including a clearer view of likely comparative advantages between the world’s different economies.\n\nReading that last sentence, the title to the article and the first paragraph appear over-inflated. People will always misuse information and there will be another body of people who will make great use of it.\nLooking at Miller’s article from the vantage point of 2011, I am not sure much has changed. If anything, there has been a slow trickling of some of these ideas into spaces where they are starting to add value. GWAS studies are filling the journals and the store of population genetic data is increasing quickly. While most blank slaters continue to ignore it and the retro-racists use bits as they see fit, some of us are ploughing through it to learn something new.\nAlthough Miller barely touches on it, the economic idea in that last sentence is interesting. If GWAS and sequencing studies identify different skills and comparative advantages across the world’s populations and economies, research into economic development could be vastly changed. However, I am not convinced that we are particularly close to obtaining that sort of information. As I noted in my last post, it seems that we are some distance from taking the load of genetic information and the associated picture of human evolutionary history and being able to link it to characteristics that matter economically. For the moment, basic information of human traits and heritability are filling that role."
  },
  {
    "objectID": "posts/dan-arielys-payoff-the-hidden-logic-that-shapes-our-motivations.html",
    "href": "posts/dan-arielys-payoff-the-hidden-logic-that-shapes-our-motivations.html",
    "title": "Dan Ariely’s Payoff: The Hidden Logic That Shapes Our Motivations",
    "section": "",
    "text": "If you have read Dan Ariely’s The Upside of Irrationality, there will be few surprises for you in his TED book Payoff: The Hidden Logic That Shapes Our Motivations. TED books are designed to be slightly longer explorations of topics from TED talks, but short enough to be read in one sitting. That makes it an easy, enjoyable, but not particularly deep read, with most of the results covered in The Upside. (Ariely’s TED talk can be viewed at the bottom of this post.)\nThe focus of Payoff is how we are motivated in the workplace, how easy it is to kill that motivation, and why we value the things we have made ourselves. It also touches on (in a slightly out-of-place and underdeveloped final chapter) how our actions are affected by what people will think about us after death.\nLike The Upside of Irrationality, Ariely sways between interesting experimental results and not particularly convincing riffs on their application to the real world. Take the following example (the major experimental result that appears unique to Payoff). Workers in a semi-conductor plant in Israel were sent a message on day one of their four-day work stretch offering one of the following incentives if they met their target for the day:\n\nA $30 bonus\nA pizza voucher\nA thank you text message from the boss\nNo message (the control group)\n\nFor people who were offered one of the three incentives, there was a boost to productivity on that day relative to the control: 4.9% for the cash group, 6.7% for the pizza group, and 6.6% for the thank you group.\nThe more interesting result was over the next three days. On day two, the group that had been incentivised with cash on day one had their productivity drop to 13.2% less than the control group. Absent the cash reward, they took their foot off the gas. On day three productivity was 6.2% worse. And on day four it was 2.9% worse. Over the four days, the productivity of the cash incentive group was 6.5% below that of the control. In contrast, the thank you group had no crash in productivity, with the pizza group somewhere in between. It seems the cash reward on day one, but not the other days, had sent a signal that day one was the only day when production mattered. Or the cash reward displaced some other form of motivation. What exactly is unclear.\nAriely turns the result into an attack on the idea that people work for pay and that more compensation will result in greater output. This is where Ariely’s riff and my take on the experimental results part.\nI agree that there is more to work than merely the exchange of money for labour. Poorly designed incentives can backfire. You can crush motivation despite paying well. The way an incentive is designed can magnify or destroy its effect.\nBut Ariely sells the cash incentive short by making almost no comment on alternative designs. What if the bonus persisted, rather than being in place for only one day? How would a daily cash incentive perform against a canned thank you every day? What would productivity look like after a year?\nI suspect Ariely is over-interpreting a narrow finding. The experiment was designed to demonstrate the poor structure of the existing incentive (the $30 bonus on day one) and to elicit an interesting effect, not to determine the best incentive structure. You only need to look at the overly creative ways people use to meet incentivised sales targets in financial services (e.g. Wells Fargo) to get a sense of how strongly people can be motivated by monetary bonuses. (Whether that is a good thing for the business is another matter. And to be honest, I haven’t actually checked that the Wells Fargo staff weren’t creating these fake accounts to receive more thank yous.)\nSo yes, think of motivation as being about more than money. Test whatever incentive systems you put in place. Test them over the long-term. But don’t start paying your staff in thank yous just yet.\nOf those experiments reported in the Upside of Irrationality and repeated in Payoff, one of the more interesting is the destruction of motivation in a pointless task. People were paid to construct Lego Bionicles at a decreasing pay scale. After constructing one, they were then asked if they would like to construct another at a new lower rate. These people were grouped into two conditions. In one, their recently completed Bionicle would be placed to the side. In the other, the Bionicle would be destroyed in front of them and placed back into the box (the Sisyphic condition).\nThose who saw their creations destroyed constructed less. Most notably, the decline in productivity in the Sisyphic group was strongest among those who liked making Bionicles, reducing their productivity to the level of those who couldn’t care less.\nOther random thoughts on the book:\n\nAriely suggests that we value our food, gardens and houses less by getting others to take care of them for us, and suggests we should invest more ourselves (related to the IKEA effect). But what would the opportunity cost of this investment be?\nAriely takes a number of unfair pokes at Adam Smith and his story of the pin factory. Ariely suggests that specialisation and trade will destroy motivation as the person cannot see the whole (a la Marx), and that Smith’s idea is no longer relevant. I trust he makes his own pins.\nOne scenario where I felt the opposite inclination to Ariely was the following:\n\n\nImagine, for example, that you worked for me and I asked you to stay late three times over the next week to help complete a project ahead of deadline. At the end of the week, you will not have seen your family but will have come close to a caffeine overdose. As an expression of my gratitude I present you with one of two rewards. In option one, I tell you how much your extra hard work meant to me. I give you a warm and sincere hug and invite you and your family to dinner. In option two, I tell you that I have calculated your marginal contribution to the company’s bottom line, it totaled $27.800, and I tell you that I will give you a bonus of 5 percent of this amount ($1,390). Which scenario is more likely to maximise your goodwill toward the company and me, not just on that day, but moving forward? Which will inspire you to push extra hard to meet the next deadline?"
  },
  {
    "objectID": "posts/darwin-and-marx.html",
    "href": "posts/darwin-and-marx.html",
    "title": "Darwin and Marx",
    "section": "",
    "text": "Yesterday I visited Down House, Charles Darwin’s home from 1842 until his death in 1882. Darwin wrote most of his major works there. The house contained a lot of interesting artefacts and bits of information, but one of the more interesting was a copy of Das Kapital sent from Karl Marx to Darwin. Inscribed inside the book was:\n\nMr. Charles Darwin On the part of his sincere admirer Karl Marx London 16 June 1873 Modena Villas Maitland Park\n\nSeeing this, I did a quick search to find out the extent to which Marx was in contact with and influenced by Darwin (having not yet read Das Kapital) and it seems that there is a history of storytelling and exaggeration around it. One now debunked myth was that Marx proposed dedicating Das Kapital to Darwin but that Darwin declined. The story was later found to be based on mixed up letters. However, in the German version of Das Kapital was the dedication “In deep appreciation - for Charles Darwin”.\nMore broadly, Marx has indicated some interest in Darwin’s work. In one letter, Marx wrote that:\n\nDarwin’s work is most important and suits my purpose in that it provides a basis in natural science for the historical class struggle. One does, of course, have to put up with the clumsy English style of argument. Despite all shortcomings, it is here that, for the first time, ‘teleology’ in natural science is not only dealt a mortal blow but its rational meaning is empirically explained.\n\nThe flow of ideas, however, was slightly lop-sided. On receipt of the book from Marx, Darwin responded:\n\n\nDear Sir: I thank you for the honour which you have done me by sending me your great work on Capital; & I heartily wish that I was more worthy to receive it, by understanding more of the deep and important subject of political Economy. Though our studies have been so different, I believe that we both earnestly desire the extension of Knowledge, & that this is in the long run sure to add to the happiness of Mankind. I remain, Dear Sir Yours faithfully, Charles Darwin\n\n\nDarwin does not seem to have read most of Das Kapital, with only the first 100 pages opened (books at the time were often printed such that the pages were joined and the reader needed to slit them open)."
  },
  {
    "objectID": "posts/darwins-conjecture-generalising-darwinism.html",
    "href": "posts/darwins-conjecture-generalising-darwinism.html",
    "title": "Darwin’s Conjecture - Generalising Darwinism",
    "section": "",
    "text": "Over the last couple of months I have been a silent participant in Geoffrey Hodgson and Thorbjørn Knudsen’s reading group for their book Darwin’s Conjecture: The Search for General Principles of Social and Economic Evolution. After finishing the book and following the reading group discussions, I’m not sure I am in a position yet to offer a strong review or critique. But in the meantime, here are some notes about the book.\nHodgson and Knudsen advocate that Darwinism should become the unified evolutionary framework for the social and behavioural sciences. They consider that this has benefits that include establishing the role of variety in the evolution of complex population systems, which economists often aggregate or assume away. It also provides a way of bringing the observed suboptimality prevalent in the natural world into the social context.\nTo achieve this, they seek to formalise the application of the Darwinian principles. This is the most important contribution of the book. Much of the research on cultural evolution feels, for want of a better word, slippery. When I read works on cultural evolution I often find myself asking what is being replicated? How is it being transmitted? How is it being selected? Is it different from contagion? This is particularly the case where group selection enters the picture. Hodgson and Knudsen tackle these questions by seeking to define what exactly is being replicated and transmitted and accordingly, what is the replicator (the cultural equivalent of the gene) and what is the interactor (the organism or object that natural selection acts upon).\nFor some chapters, this approach is  useful. The chapter on whether social evolution is Lamarckian (modifications acquired during a lifetime are passed to offspring) is excellent. To assess this claim you need to understand the nature of transmission, which in turn requires definition of the replicator. When and how does the replicator change? Even though I didn’t agree fully agree with their conclusions, their approach allowed a clear assessment of what they were arguing.\nTying down these definitions is not a riskless enterprise, as social evolution does not have a relatively clear entity in “the gene” to select as a replicator. At an individual level, they argue that habits are the appropriate replicator. They prefer habits over memes as they consider it possible to give habits a substrate of biologically inherited instincts, whereas memes are based on habits and instincts and cannot be sustained without them. But why dismiss memes on this basis when they can be given the substrate of habits and instincts (or even a state of the brain) in the same way habits are grounded in biologically inherited instincts? Memes were also attacked on the basis of looseness of terminology, but given the book was designed around formalised Darwinism, why not tighten it? In some respects, I felt as though their discussion on memes was a battle in a long war that I do not know enough about.\nWhere the authors really started to lose me was when they moved into higher levels of evolution. As is typical when assessing multilevel selection, they noted the Price equation and how it can be used to partition selection at various levels of organisation. But when they laid out their proposed six levels of replicators (genetic, individual, organisational, symbolic, legal, and scientific and technological) across four levels of interactors (individual, organisational, national, scientific and technological organisations), a lot of the crispness of terminology seemed to disappear, along with tools such as the Price equation. The sharpness they brought to the initial chapters of the book faded.\nThere were a few nice lines in the book.  In dismissing arguments that human intentionality renders analysis of social evolution irrelevant, the authors note that intentionality itself has evolved from similar but less developed attributes among pre-human ancestors. Similarly with artificial selection, it is evolved humans doing the selection\nOn the flipside, there was also the occasional argument that grated me the wrong way, such as their suggestion that the impulse to produce and acquire in all human societies is a cultural propensity. This is pushing the cultural explanations too far.\nUltimately, the test of their work will be in the application. Although I enjoyed the attempt to tighten the use of some concepts that are often loosely used, it is only when we gain new insights from these tools that the effort will be proved worthwhile. As to whether that is likely to occur, I am not yet convinced (I still have some Steven Pinker like tendencies in this area). I am also reluctant to get sucked into some of the issues in the book as, to my untrained eye, they often appear semantic (as does much of the conversation in the reading group). That is another area where some real-world application will help, with some practical examples to render the material more real.\nAnd as a last word, if you are interested in finding a book as a starting point on cultural evolution, this probably isn’t it. If you have already read a few books in the area, it is worth the effort."
  },
  {
    "objectID": "posts/dealing-with-algorithm-aversion.html",
    "href": "posts/dealing-with-algorithm-aversion.html",
    "title": "Dealing with algorithm aversion",
    "section": "",
    "text": "Over at Behavioral Scientist is my latest contribution. From the intro:\n\nThe first American astronauts were recruited from the ranks of test pilots, largely due to convenience. As Tom Wolfe describes in his incredible book The Right Stuff, radar operators might have been better suited to the passive observation required in the largely automated Mercury space capsules. But the test pilots were readily available, had the required security clearances, and could be ordered to report to duty.\nTest pilot Al Shepherd, the first American in space, did little during his first, 15-minute flight beyond being observed by cameras and a rectal thermometer (more on the “little” he did do later). Pilots rejected by Project Mercury dubbed Shepherd “spam in a can.”\nOther pilots were quick to note that “a monkey’s gonna make the first flight.” Well, not quite a monkey. Before Shepherd, the first to fly in the Mercury space capsule was a chimpanzee named Ham, only 18 months removed from his West African home. Ham performed with aplomb.\nBut test pilots are not the type to like relinquishing control. The seven Mercury astronauts felt uncomfortable filling a role that could be performed by a chimp (or spam). Thus started the astronauts’ quest to gain more control over the flight and to make their function more akin to that of a pilot. A battle for decision-making authority—man versus automated decision aid—had begun.\n\nHead on over to Behavioral Scientist to read the rest.\nWhile the article draws quite heavily on Tom Wolfe’s The Right Stuff, the use of the story of the Mercury astronauts was somewhat inspired by Charles Perrow’s Normal Accidents. Perrow looks at the two sides of the problems that emerged during the Mercury missions - the operator error, which formed the opening of my article, and the designer error, which features in the close.\nOne issue that became apparent to me during drafting was the distinction between an algorithm determining a course of action, and the execution of that action through mechanical, electronic or other means. The example of the first space flights clearly has this issue. Many of the problems were not that the basic calculations (the algorithms) were faulty. Rather, the execution failed. In early drafts of the article I tried to draw this distinction out, but it made the article clunky. I ultimately reduced this point to a mention in the close. It’s something I might explore at a later time, because I suspect “algorithm aversion” when applied to self-driving cars relates to both decision making and execution.\nAnother issue that became stark was the limit of the superiority of algorithms. In the first draft, I did not return to the Mercury missions for the close. It was a easy to talk of bumbling humans in the first space flights and how to guide them toward better use of algorithms. But that story was too neat, particularly given the particular example I had chosen. During the early flights there were plenty of times where the astronauts had to step in and save themselves. Perhaps if I had used a medical diagnosis or more typical decision scenario in the opening I could have written a cleaner article.\nRegardless, the mix of operator and designer error (to use Perrow’s framing) has led me down a path of exploring how to use algorithms when the decision is idiosyncratic or is being made in a less developed system. The early space flights are one example, but strategic business decisions might be another. What is the right balance of algorithms and humans there? At this point, I’m planning for that to be the focus of my next Behavioral Scientist piece."
  },
  {
    "objectID": "posts/deep-rationality-the-evolutionary-economics-of-decision-making.html",
    "href": "posts/deep-rationality-the-evolutionary-economics-of-decision-making.html",
    "title": "Deep Rationality: The Evolutionary Economics of Decision Making",
    "section": "",
    "text": "Even though I consider that I am across the literature at the boundary of economics and evolutionary biology, now and then an article pops up that I somehow missed. The latest article of this type is a 2009 article by Douglas Kenrick and colleagues, titled (as is this post) Deep Rationality: The Evolutionary Economics of Decision Making. I found it through Dan Ariely’s reading list for his Coursera course A Beginner’s Guide to Irrational Behaviour. Kenrick has also posted on the article over at his blog\nI don’t feel overly guilty about not seeing this article earlier, as the authors have not referenced a lot of the literature in economics that I would consider relevant. Regardless, there is a lot to like about this article, particularly the way that it looks to incorporate an evolutionary approach into behavioural economics. I have often posted my criticism that much behavioural economics lacks a framework, without which it is just a list of biases and heuristics. It is good to see someone trying to offer that framework.\nThe authors’ basic argument is that people have evolved domain specific decision rules. Decisions depend on the current environment, plus the decision maker’s sex, mating strategy and stage in the life cycle. As a result, many decisions that are called inconsistent or irrational in behavioural economics are actually “deeply rational” to the domain in which the decision is being made.\nIn making their case, the authors start out with a brief kick at economics by noting that most economic theorists “have remained relatively agnostic about the roots of utility.” They do note the work of Gandolfi, Gandolfi and Barash, but otherwise do not mention the wealth of articles on the evolution of preferences by the likes of Arthur Robson, Larry Samuelson and others (my economics and evolutionary biology reading list gives a taste). Thus, when they suggest that we need to go deeper than Gandolfi, Gandolfi and Barash’s approach of equating utility to fitness, they miss some literature which does just that.\nRegardless, the need to go beyond “fitness equals utility” by considering factors such as life history or differences in mating strategy is important. The authors suggest that we should consider human decision making as being geared to solve recurrent adaptive problems in different domains, whereby successful solutions in each are associated with increased fitness. The body of their article focuses on some examples of this approach.\nIn one section, they address attitudes to risk. Humans are normally risk averse, which Kenrick and colleagues suggest is consistent with empirical observations of loss aversion. Although this shorthand equating of risk aversion and loss aversion works some of the time, it sells these concepts short, along with the way that they are incorporated into Kahneman and Tversky’s prospect theory. Under prospect theory, people evaluate choices from a reference point, they show loss aversion (losses hurt more than gains) and they are risk averse when faced with two potential gains. However, in the domain of losses, they are actually risk seeking. When you combine these features with the human tendency to overweigh small probabilities, you obtain the fourfold pattern of risk attitudes. When an agent faces a moderate probability of a gain or a small probability of a loss, they will be risk averse. However, when faced with a low probability of a gain or a moderate probability of a loss, they will be risk seeking.\nKenrick and colleagues do make the important point that the attitudes to risk as predicted by prospect theory will vary with evolutionarily relevant factors. Men with mating motives will be more likely to take financial risks.  Women would not respond in the same way as women know that men give a lower value to the resources possessed by a mate. In the social domain, such as networks of friends, there tends to be loss aversion in both sexes, although this may reverse for men with mating motives.\nThis is a point of the article where a hat tip to the existing literature might have been most useful, as some economists have considered the evolutionary foundations of attitudes to risk. For example, Rubin and Paul examined the effect of mating motives on risk preferences in 1979. They developed a model where male fitness depended on attracting a mate, which was in turn a function of their resources (income). Rubin and Paul suggested that young men who do not have a mate are likely to be risk seeking in obtaining income as they have no mate to lose. Older men who already have a mate will tend to be risk averse, particularly given the huge level of income required to attract a second mate.\nIn another section, Kenrick and colleagues look at the economic approach to choosing a basket of goods within a budget constraint. They argue that the weighting of each good will depend upon the domain in which an agent is making a mate choice. For example, promotion of a colleague at work may influence status motives and accordingly, the worker’s preferences between more time in the office and leisure will shift.\nThey also make the interesting distinction between traits in a potential mate being necessities or luxuries. Consider a female who needs a male to have a minimal level of resources to make sure her offspring survive. Due to diminishing marginal utility (another economic concept) as the male’s resources increase, she may start to look at other traits if there are plenty of males with enough resources. The pattern of consumption will be that resources are a necessity, while other traits are luxuries. A similar pattern might emerge for male preferences, initially prioritising fertility related traits, but then considering other traits if there are plentiful fertile females. Thus, when the necessity traits are scarce, we might expect large sex differences in mate preferences as each sex focuses on obtaining their different necessities. As these traits become more plentiful, traits that are luxuries are sought. If there is overlap between the luxuries of one sex with the necessities of the other sex, we would see smaller differences between the sexes in the traits sought in mates.\nOne issue Kenrick and colleagues do not spend much time on is why evolution has shaped domain-specific and not general decision rules. This is addressed in the evolutionary psychology literature, but to sell their argument to economists, they need to sell them the constraint inherent in the modular approach. Most evolutionary analysis of economic preferences struggles to incorporate “irrationality” through constraints, often due to a view that evolution is the ultimate rationality machine (and most economists fixation, conscious or not, with rationality). Selling to economists the picture of constrained, path-dependent evolution that leads to modular decision making and “deep rationality” could improve the economic endeavour considerably."
  },
  {
    "objectID": "posts/defending-stephen-jay-gould.html",
    "href": "posts/defending-stephen-jay-gould.html",
    "title": "Defending Stephen Jay Gould",
    "section": "",
    "text": "I’ve been waiting for someone to defend Stephen Jay Gould from the accusations contained in a recent paper by Lewis and Colleagues. In a nutshell, the authors found that in Gould’s analysis of skull measurements by Samuel Morton, “most of Gould’s criticisms are poorly supported or falsified.”\nI haven’t yet found that specific defence, but John Horgan in Scientific American has stepped in to defend Gould’s broader crusade against “biological determinism”. Horgan writes:\n\nMaybe Gould was wrong that Morton misrepresented his data, but he was absolutely right that biological determinism was and continues to be a dangerous pseudoscientific ideology. Biological determinism is thriving today: I see it in the assertion of researchers such as the anthropologist Richard Wrangham of Harvard University that the roots of human warfare reach back all the way to our common ancestry with chimpanzees. In the claim of scientists such as Rose McDermott of Brown University that certain people are especially susceptible to violent aggression because they carry a “warrior gene.” In the enthusiasm of some science journalists for the warrior gene and other flimsy linkages of genes to human traits. In the insistence of the evolutionary biologist Jerry Coyne and neuroscientist Sam Harris that free will is an illusion because our “choices” are actually all predetermined by neural processes taking place below the level of our awareness. In the contention of James Watson, co-discoverer of the double helix, that the problems of sub-Saharan Africa reflect blacks’ innate inferiority. In the excoriation of many modern researchers of courageous anti-determinists such as Gould and Margaret Mead.\n\nHorgan’s examples of “biological determinism are interesting. Coyne is one of the more vocal critics of the”just so” stories coming out of evolutionary psychology, and Coyne’s arguments against the existence of free will are based on the effects of both biology and environment. While the “warrior gene” findings may not stand the test of time, the evidence for the heritability of violent tendencies is strong (I recently posted on the “missing heritability” problem). What makes findings of the type that Horgan describes generally scientifically unfounded? Neither Horgan’s post, nor my perusal of his blog back catalogue, makes this clear.\nHorgan’s attack on “biological determinism” is an attack on a straw man - that biology determines all. Every person I have met who argues the case for biological influence acknowledges the role of environment. Genes express in an environment. The question is the degree of that role - and in that area, there is still plenty of room to debate (as Coyne’s debates with the evolutionary psychology proponents show). Conversely, Gould attempted to erase the role of evolution in shaping human behaviour. Horgan wants to contain it. But as Coyne states in his response to Horgan:\n\n[T]o dismiss any claims about the genetic basis of modern human behavior as “biological determinism, therefore pseudoscientific ideology” is simply silly: it’s the same kind of knee-jerk rejection of all research on the evolution of human behavior that Gould sometimes engaged in.  Horgan wants to dismiss these studies simply because he doesn’t like what he sees as their implications:  “the way things are is the way they must be” and that “we have less choice in how we live our lives than we think we do.”  Well, tough.  Biological determinism, of both the anti-free-will and genes-determining-human-behavior variety, may be more pervasive than many people think, and is certainly more pervasive than Horgan thinks."
  },
  {
    "objectID": "posts/deriving-the-demand-for-children.html",
    "href": "posts/deriving-the-demand-for-children.html",
    "title": "Deriving the demand for children",
    "section": "",
    "text": "I’ve been working through Gary Becker’s A Treatise on the Family: Enlarged Edition over the last couple of weeks. One interesting section included Becker’s thoughts on why people demand their own children, as opposed to being satisfied with the children of others.\n\n[T]he demand for own children, the distinguishing characteristic of families, need not be postulated but can be derived.\nWomen producing children can use their own milk as food and can more readily take care of young children while pregnant than while working in the marketplace. Moreover, most women have been reluctant to commit so much time, effort, emotion, and risk to producing children without considerable control over rearing. Presumably the genetic similarity between parents and children further increases the demand for own children.\nOwn children are preferred also because of the value of information about children when investing in them. Information is more readily available about the intrinsic characteristics of own than adopted children, because parents and own children have half their genes in common and the health and some other characteristics of own children at birth and during infancy are directly observed. … This may also explain why orphaned children of siblings and other close relatives are more frequently adopted than are orphaned children of strangers (Goody, 1976), and even why adopted children are less valued as marriage partners.\n\nBecker introduces biological considerations at several points of the book, but this explanation of the demand for children is one of the more awkward. It’s not hard to see what would happen to those who overcome this information asymmetry to allow them to efficiently raise children that are not their own."
  },
  {
    "objectID": "posts/diamond-on-biological-differences.html",
    "href": "posts/diamond-on-biological-differences.html",
    "title": "Diamond on biological differences",
    "section": "",
    "text": "On Friday afternoon, as has happened a few times, I was asked if I had read Jared Diamond’s Guns, Germs and Steel. How could an evolutionary analysis of development accommodate Diamond’s thesis?\nAs Diamond frames his book in the prologue, Guns, Germs and Steel provides an environmental explanation of human development. Diamond states that you could summarise his book with the following sentence:\n\nHistory followed different courses for different peoples because of differences among peoples’ environments, not because of biological differences among peoples themselves.\n\nThe interesting thing about this characterisation of his book is the discussion over the following pages where Diamond counters those who seek to develop genetic explanations of development. While taking aim at those who suggest there is an inherent superiority to people from industrialised nations (which is fair enough), Diamond utilises an evolutionary argument himself.\nDiamond suggests, based on his observations of people in New Guinea, that modern stone-age people are on average probably more intelligent than industrialised people. For example, he suggests they have much better skills such as forming a mental map of unfamiliar surroundings.\nDiamond suggests two reasons for his impression that New Guineans are smarter than Westerners. One is environmental, with Diamond believing that, compared to the passive television based environment of Westerners, New Guinean children are exposed to a far more stimulating environment. The second is that New Guineans are more likely to have been selected for intelligence.\nComparing the environments of Westerners and New Guineans, Diamond submits that while most Western children survive to adulthood and reproduce regardless of their genes (or intelligence), New Guineans live in societies where population is too small for the epidemic diseases to evolve. Mortality came from murder, tribal warfare, accidents or failure to meet subsistence needs. In New Guinea, intelligent people are more likely than less intelligent people to escape those causes of high mortality.\nThis suggests that Diamond is not averse to arguments about the selective pressures in different societies. In the same way, I consider an approach to development that considers how humans have evolved is not inconsistent with most of Diamond’s work. As global populations were exposed, as Diamond catalogues, to a range of different environments and opportunities, they followed different developmental paths, and those developmental paths in turn could have subjected populations to varying selective pressures.\nThis issue would then be what those selective pressures are, how populations may have changed and whether this affected economic development. Diamond only dealt with this issue in the prologue, so it is not clear whether Diamond considers changes in the frequency of genes and traits could have played a role in development, but it would seem not.\nAs for my perspective, I find Diamond’s hypothesis useful in an evolutionary analysis of development. The environmental differences identified by Diamond would place different selective pressures on human populations, and as those populations change, that could in turn feed back into their environment. Take agriculture. Those populations who had access to the right plants, animals and geographic features are those that developed agriculture - an environmental argument. But within those populations, people with certain traits would have been more likely to take up the agricultural lifestyle and of those who did, those with certain traits more successful at it. A feedback loop would occur, with the environment shaping the people and vice versa. This is not, as Diamond frames it, a question of nature or nature. It is about the relationship between the two."
  },
  {
    "objectID": "posts/diversity-and-consumerism.html",
    "href": "posts/diversity-and-consumerism.html",
    "title": "Diversity and consumerism",
    "section": "",
    "text": "In Geoffrey Miller’s Spent: Sex, Evolution, and Consumer Behavior (earlier posts on his book here, here and here - this is the last one for now), Miller discusses how there might be a move away from a consumerist culture. To do this, there is a need to develop and maintain social norms that could act as an alternative to the typical displays of wealth. For example, in a religious community, signalling could be through time and resources given to the church.\nHowever, Miller stated that there is a major legal obstacle to establishing these norms. The obstacle is that the laws about property ownership and rental do not allow discrimination. Despite the good intentions, Miller considers that “they have toxic side effects on the ability of voluntarily organized communities to create the physical, social, and moral environments that their members want.”\nThere is no shortage of literature on the effects of diversity on trust and cooperation within communities. Miller notes some of Robert Putnam’s work:\n\nFor example, the political scientist Robert Putnam has found that American communities with higher levels of ethnic diversity tend to have lower levels of “social capital” - trust, altruism, cohesion, and sense of community. He and his colleagues analyzed data from thirty thousand people across forty-one U.S. communities, and found that people who live in communities with higher ethnic diversity (meaning, in the United States, more equal mixtures of black, Hispanic, white, and Asian citizens) tend to have lower:\n\n\n\n\n\ntrust across ethnic groups\n\n\n\n\ntrust within their own ethnic group\n\n\n\n\ncommunity solidarity and cohesion\n\n\n\n\ncommunity cooperation\n\n\n\n\nsense of political empowerment\n\n\n\n\nconfidence in local government and leaders\n\n\n\n\nvoter registration rates\n\n\n\n\ncharity and volunteering\n\n\n\n\ninvestment in common goods\n\n\n\n\ninterest in maintaining community facilities\n\n\n\n\nrates of carpooling\n\n\n\n\nnumbers of friends\n\n\n\n\nperceived quality of life\n\n\n\n\ngeneral happiness\n\n\nThese effects remained substantial even after controlling for each individual’s age, sex, education, ethnicity, income, and language, and for each community’s poverty rate, income inequality, crime rate, population density, mobility, and average education.\n\nAs communities cannot group on norms, Miller states that communities group based on income.  Wealth becomes the measure of status and competition between individuals in the community is then dependent on displays of wealth. He states that “if the local majority cannot impose some distinctive social norms on our forms of trait signaling, conspicuous consumption will remain the only game in town.” You are limited to choosing your neighbours through the use of economic stratification:\n\nSadly, it has become almost impossible now for like-minded people to arrange to live together in a small community with cohesive social norms. Real norms can be sustained effectively only by selecting who moves in, by praising or punishing those who uphold or violate norms as residents, and by expelling those who repeatedly violate the norms. These are the requirements to sustain the type of cooperation called network reciprocity, in which cooperators form local “network clusters” (communities) in which they help one another.\n\nI am not confident that if communities could discriminate and assort as they see fit that wealth would not be a major cause of assortment. Still, assuming that these laws are as much of a barrier as Miller suggests (I am not particularly familiar with United States discrimination laws), there would almost certainly be some groups that wish to assort on certain criteria. And why not let them? If it builds community trust and cooperation, that is a good thing. I am sure that some communities would form on bases that are abhorrent to others, but you don’t have to live there."
  },
  {
    "objectID": "posts/do-nudges-diminish-autonomy.html",
    "href": "posts/do-nudges-diminish-autonomy.html",
    "title": "Do nudges diminish autonomy?",
    "section": "",
    "text": "Despite the fact that nudges, by definition, do not limit liberty, many people often have a feeling of discomfort about governments using nudges. I typically find it difficult to elicit from them what precisely is the problem, but often it comes down to the difference between freedom and autonomy.\nIn an essay Debate: To Nudge or Not to Nudge (pdf), Daniel Hausman and Bryan Welch do a good job of pulling this idea apart:\n\nIf one is concerned with autonomy as well as freedom, narrowly conceived, then there does seem to be something paternalistic, not merely beneficent, in designing policies so as to take advantage of people’s psychological foibles for their own benefit. There is an important difference between what an employer does when she sets up a voluntary retirement plan, in which employees can choose to participate, and what she does when, owing to her understanding of limits to her employees’ decision-making abilities, she devises a plan for increasing future employee contributions to retirement. Although setting up a voluntary retirement plan may be especially beneficial to employees because of psychological flaws that have prevented them from saving on their own, the employer is expanding their choice set, and the effect of the new plan on employee savings comes mainly as a result of the provision of this new alternative. The reason why nudges such as setting defaults seem, in contrast, to be paternalist, is that in addition to or apart from rational persuasion, they may “push” individuals to make one choice rather than another. Their freedom, in the sense of what alternatives can be chosen, is virtually unaffected, but when this “pushing” does not take the form of rational persuasion, their autonomy—the extent to which they have control over their own evaluations and deliberation—is diminished. Their actions reflect the tactics of the choice architect rather than exclusively their own evaluation of alternatives.\n\nAnd not only might nudges diminish autonomy, they might be simply disrespectful.\n\nOne reason to be troubled, which Thaler and Sunstein to some extent acknowledge (p. 246/249), is that such nudges on the part of the government may be inconsistent with the respect toward citizens that a representative government ought to show. If a government is supposed to treat its citizens as agents who, within the limits that derive from the rights and interests of others, determine the direction of their own lives, then it should be reluctant to use means to influence them other than rational persuasion. Even if, as seems to us obviously the case, the decision-making abilities of citizens are flawed and might not be significantly diminished by concerted efforts to exploit these flaws, an organized effort to shape choices still appears to be a form of disrespectful social control.\n\nBut what if you believe that paternalistic policies are in some cases defensible? Are nudges the milder version?\n\nIs paternalism that plays on flaws in human judgment and decision-making to shape people’s choices for their own benefit defensible? If one believes, as we do, that paternalistic policies (such as requiring the use of seat belts) that limit liberty are sometimes justified, then it might seem that milder nudges would a fortiori be unproblematic.\nBut there may be something more insidious about shaping choices than about open constraint. For example, suppose, for the purposes of argument, that subliminal messages were highly effective in influencing behavior. So the government might, for example, be able to increase the frequency with which people brush their teeth by requiring that the message, “Brush your teeth!” be flashed briefly during prime-time television programs. Influencing behavior in this way may be a greater threat to liberty, broadly conceived, than punishing drivers who do not wear seat belts, because it threatens people’s control over their own evaluations and deliberation and is so open to abuse. The unhappily coerced driver wearing her seat belt has chosen to do so, albeit from a limited choice set, unlike the hypothetical case of a person who brushes his teeth under the influence of a subliminal message. In contrast to Thaler and Sunstein [authors of Nudge], who maintain that “Libertarian paternalism is a relatively weak and nonintrusive type of paternalism,” to the extent that it lessens the control agents have over their own evaluations, shaping people’s choices for their own benefit seems to us to be alarmingly intrusive.\n\nHausman and Welch outline three distinctions that can help us think about whether nudges should be permissible (which I am somewhat sympathetic to).\n\nFirst, in many cases, regardless of whether there is a nudge or not, people’s choices will be shaped by factors such as framing, a status quo bias, myopia and so forth. Although shaping still raises a flag because of the possibility of one agent controlling another, it arguably renders the action no less the agent’s own, when the agent would have been subject to similar foibles in the absence of nudges. When choice shaping is not avoidable, then it must be permissible.\nSecond, although informed by an understanding of human decision-making foibles, some nudges such as “cooling off periods” (p. 250/253) and “mandated choice” (pp. 86–7/88) merely counteract foibles in decision-making without in any way pushing individuals to choose one alternative rather than another. In this way, shaping apparently enhances rather than threatens an individual’s ability to choose rationally. …\nThird, one should distinguish between cases in which shaping increases the extent to which a person’s decision-making is distorted by flaws in deliberation, and cases in which decision-making would be at least as distorted without any intentionally designed choice architecture. In some circumstances, such as (hypothetical) subliminal advertising, the foibles that make people care less about brushing their teeth are less of a threat to their ability to choose well for themselves than the nudging. In other cases, such as Carolyn’s, the choices of some of the students passing through the cafeteria line would have been affected by the location of different dishes, regardless of how the food is displayed.\nThere remains an important difference between choices that are intentionally shaped and choices that are not. Even when unshaped choices would have been just as strongly influenced by deliberative flaws, calculated shaping of choices still imposes the will of one agent on another.\n\nOne funny line about all this, however, is whether it is actually possible to choose “rationally”. Hausman and Welch see this point:\n\nWhen attempting to persuade people rationally, we may be kidding ourselves. Our efforts to persuade may succeed because of the softness of our smile or our aura of authority rather than the soundness of our argument, but a huge difference in aim and attitude remains. Even if purely rational persuasion were completely impossible—that is, if rational persuasion in fact always involved some shaping of choices as well—there would be an important difference between attempting to persuade by means of facts and valid arguments and attempting to take advantage of loss aversion or inattention to get someone to make a choice that they do not judge to be best. Like actions that get people to choose alternatives by means of force, threats, or false information, exploitation of imperfections in human judgment and decision-making aims to substitute the nudger’s judgment of what should be done for the nudgee’s own judgment."
  },
  {
    "objectID": "posts/does-epigenetics-matter.html",
    "href": "posts/does-epigenetics-matter.html",
    "title": "Does epigenetics matter?",
    "section": "",
    "text": "A new book has just popped out - The Epigenetics Revolution by Nessa Carey - and accompanying it is the usual epigenetics-related suggestions that Darwin was wrong. Take this from Peter Forbes in the Guardian:\n\n[E]pigenetics finally reaches that “everything you’ve been told is wrong” moment when it claims that some epigenetic changes are so long-lasting they cover several generations: they can be inherited. This flouts one of biology’s most cherished dogmas – taught to all students – namely that changes acquired during life cannot be passed on – the heresy of Lamarckism.\nBut the evidence that this can occur in some cases appears to be growing. There are lab experiments with mice and rats in which epigenetic effects on coat colour and obesity can be inherited. More suggestive evidence comes from a vast, unwitting and cruel experiment played out in the second world war. In 1944, during the last months of the war, a Nazi blockade followed by an exceedingly harsh winter led to mass starvation in Holland. This had a huge effect on babies born at the time, and the effects of poor nutrition on the foetus seem to have persisted through subsequent generations.\n\nThankfully a few people such as Jerry Coyne are placing this noise in context:\n\nI haven’t read the book, and although it might make Darwin swoon if the old git were to be resurrected, the discoveries of genetics and the mechanism of inheritance itself would make him swoon far more readily.  And I know scientific revolutions; scientific revolutions are friends of mine; and believe me, epigenetics is no scientific revolution.\n….\nSo, Mr. Forbes, our “cherished dogma” of non-Lamarckian inheritance still holds strong, and you’ve done your readers a disservice by implying otherwise.  Lamarckism is not a “heresy,” but simply a hypothesis that hasn’t held up, despite legions of evolution-revolutionaries who argue that it flushes neo-Darwinism down the toilet.  If “epigenetics” in the second sense is so important in evolution, let us have a list of, say, a hundred adaptations of organisms that evolved in this Larmackian way as opposed to the old, boring, neo-Darwinian way involving inherited changes in DNA sequence.\nForbes can’t produce such a list, because there’s not one.  In fact, I can’t think of a single entry for that list.\n\nDespite having quoted Coyne with approval, I am still going to read the book. Although epigenetics may leave neo-Darwinism unscathed, it matters for economics - and people’s welfare. A single generation matters. If epigenetics suggests that some effects are more persistent or have an effect in the next generation, it is an important consideration. For example, will the current famine in East Africa be felt for one generation or two?"
  },
  {
    "objectID": "posts/does-genetic-diversity-increase-conflict.html",
    "href": "posts/does-genetic-diversity-increase-conflict.html",
    "title": "Does genetic diversity increase conflict?",
    "section": "",
    "text": "Ashraf and Galor’s hypothesis linking genetic diversity to economic growth has two limbs. The first, which I posted about last week, is that genetic diversity pushes out the production possibility frontier through increasing the range of traits in the population for developing and implementing new technologies. The second, the subject of today’s post, is that genetic diversity decreases trust and cooperation between people, increasing social disorder and conflict.\nThe measure of genetic diversity used by Ashraf and Galor is expected heterozygosity, which is a measure of the probability that two people selected from the population will have the same allele (variety of a gene), averaged across all measured genes. Genetic diversity is often confused with genetic distance, a measure of the time since two populations had a common ancestor. Genetic distance can be calculated using data of the type used by Ashraf and Galor, with that measure being the probability that two alleles from a given genetic locus selected from two different populations will be different. Genetic distance has a resemblance to the genetic diversity measure, but is across populations, not within them. Spolaore and Wacziarg used genetic distance in their 2009 paper where they proposed that genetic distance (or other measures proxied by genetic distance) hindered the transfer of technology, leading to technological and income differences between countries.\nGenetic diversity should also be distinguished from relatedness. Relatedness is the genetic similarity of two individuals, relative to average similarity of all individuals in the population. Since relatedness is measured relative to average similarity, relatedness does not increase in a less genetically diverse population. However, the average similarity between population members is higher where diversity is low.\nThis distinction is important, as Ashraf and Galor’s argument for the causative pathway for genetic diversity and its effect on cooperation is via relatedness. They state:\n\n[T]o the extent that genetic diversity is associated with a lower average degree of relatedness among individuals in a population, kin selection theory, which emphasizes that cooperation among genetically related individuals can indeed be collectively beneficial as it ultimately facilitates the propagation of shared genes to the next generation, is suggestive of the hypothesized mechanism through which diversity confers costs on aggregate productivity.\n\nIt may be possible to craft an argument that relatedness is higher in a less genetically diverse population if you considered the relevant population for measuring relatedness to be the global population. Across the global population, two individuals from a less diverse sub-population would have a relatedness marginally above zero. However, this is a stretch. A more feasible argument would be to take Ashraf and Galor’s use of the term relatedness to refer to genetic similarity.\nAshraf and Galor dedicate little time to building the evolutionary basis to their argument in the main paper, but give it some focus in the web appendix. They note a study in which long tailed tits (a bird) provided breeding support to kin, and an analysis of 18 vertebrate species that found a strong correlation between brood rearing assistance and relatedness. They also describe a study in which juvenile spiders cooperate with kin while feeding to increase feeding efficiency.\nThe disconnect between these studies and Ashraf and Galor’s argument is the degree of relatedness involved. The studies referenced in the web appendix involve relatively close kin, with relatedness a relative measure within the population. For the genetic diversity hypothesis to hold, humans would need a very fine tuned sense of relatedness in the broad sense noted above. Are people more likely to cooperate with those who they are more genetically similar, despite no immediate reference group for comparing that similarity? The problem is that, beyond close kin, there is poor empirical support for kin recognition.\nOne possible angle in support of Ashraf and Galor’s hypothesis might be to use evidence of the detection of genetic similarity and heterozygosity in mating decisions (such as here, here and here). However, the mating preferences are usually for more dissimilar or heterozygous individuals, suggesting diversity of this nature has a positive effect.\nAt the close of the article, Ashraf and Galor seek to build their case by using their genetic diversity dataset to examine whether genetic diversity affects trust. Across 58 countries for which a measure of trust can be gleaned from the World Values Survey, they found a significant relationship in the required direction. Increasing genetic diversity by 1 percentage point is associated with a 2 percentage point decrease in the prevalence of trust. In obtaining this result they included controls for geography, OPEC, and sub-Saharan Africa, and used continent fixed effects, meaning that the effect is within continents.\nAs was the case for the regressions of genetic diversity on measures of scientific output, I find this result unconvincing. The range of controls used and the question of whether there are other relevant variables (such as IQ), combined with the lack of causative pathway, leaves me needing much more evidence.\nSo, where to from here? As I noted in the discussion of innovation, cross-species comparison is one potential avenue for further research, as is examination of isolated human populations. Inbred human populations might also be an interesting source of evidence, although it can be difficult to separate relatedness, diversity and inbreeding effects. One consideration is that genetic diversity in particularly low in humans relative to other species due to some bottlenecks in our past. Could that low diversity be linked to the generally high levels of cooperative behaviour in humans?\nAs was the case for my post on the effect of genetic diversity on innovation, I have deliberately avoided the question of whether Ashraf and Galor were directly relating genetic diversity to economic development, or whether genetic diversity is a proxy for phenotypic diversity unrelated to that genetic diversity (such as language). As you can see below, my thoughts on that point are forthcoming.\nMy posts on Ashraf and Galor’s paper on genetic diversity and economic growth are as follows:\n\nA summary of the paper methodology and findings\nDoes genetic diversity increase innovation?\nDoes genetic diversity increase conflict? (this post)\nIs genetic diversity a proxy for phenotypic diversity?\nIs population density a good measure of technological progress?\nWhat are the policy implications of the effects of genetic diversity on economic development?\nShould this paper have been published?\n\nEarlier debate on this paper can also be found here, here, here and here."
  },
  {
    "objectID": "posts/does-mathematical-training-increase-our-risk-tolerance.html",
    "href": "posts/does-mathematical-training-increase-our-risk-tolerance.html",
    "title": "Does mathematical training increase our risk tolerance?",
    "section": "",
    "text": "Humans are inherently risk averse. When offered a coin toss with a reward of $10,000 for heads but a loss of $10,000 for tails, most people would decline. They would likely agree to pay a significant sum to avoid the gamble, despite the expected value of the gamble being zero.\nWhen economists describe the preferences of a person, they often build in some form of risk aversion. A risk averse person will always prefer a sum with certainty than a gamble with that expected value. One way that economists do this is by describing the preferences of a person as logarithmic. This means that the level of utility that an individual gets from, say, a sum of money increases at a diminishing rate. They might value an increase in their wealth from $1 to $10 the same as an increase from $10 to $100. These preferences then shape the choices that the person makes. For example, they might value of 50-50 gamble between $1 and $100 at only $10, despite the expected value of the gamble being slightly above $50.\nBeyond the extensive research on preferences in the behavioural economics literature, the use of logarithmic preferences to approximate decision-making has some support in empirical work on how we view numbers in our minds. In 2008, Stanislas Dehaene and colleagues made a useful contribution to this area through their examination of whether our mental mapping of numbers was inherent or trained.\nFirst, some background. As noted by the authors, there have been a number of experiments that showed that children mapped numbers to space in a logarithmic fashion. They noted that:\n\nWhen asked to point toward the correct location for a spoken number word onto a line segment labeled with 0 at left and 100 at right, even kindergarteners understand the task and behave nonrandomly, systematically placing smaller numbers at left and larger numbers at right. They do not distribute the numbers evenly, however, and instead devote more space to small numbers, imposing a compressed logarithmic mapping. For instance, they might place number 10 near the middle of the 0-to-100 segment.\n\nThis logarithmic view of the world does not last. Between first and fourth grade children start to move from logarithmic to a more linear mapping of numbers. This transition is most pronounced across small numbers, and moves to higher numbers as they age. Some logarithmic mapping persists for very large numbers.\nDehaene and colleagues contributed to this picture through their examination of whether the move from logarithmic to linear mapping was a result of formal schooling or a natural process of brain maturation. To study this, they undertook some number-mapping exercises with the Mundurucu, an Amazonian group with little access to education or other instruments that may affect their perception of numbers (such as maps and rulers).\nTo test how they map numbers, the participants were given a line with one dot at one end and 10 or 100 dots at the other. They were then given a number of dots between either one and 10 or one and 100 and were asked to place them at the proper place on the line.\nThis exercise showed that logarithmic mapping persisted into adulthood for the Mundurucu. This was even the case for numbers between one and ten. These findings suggest that it is the experience of children who receive formal education in mathematics that shifts their mental mapping to a linear way. This could be through either through the formal education itself or some other cultural cause.\nThis study raises a number of implications for the way people’s risk preferences are formed. If people perceive quantities in a logarithmic fashion, they will tend to be risk averse. As they move to a more linear way of mapping numbers, this could coincide with a reduction in risk aversion. Does the education of children in mathematics tend to increase their risk tolerance through changing the way they see numbers?\nThis could be argued to have a number of follow-on effects. A reduction in risk aversion would naturally see an increase in risk taking activity. They will weight the possibility of great wealth higher and be willing to accept the risk of a larger loss to make it. An economy full of people with a greater risk tolerance could have more entrepreneurial activity, greater wealth (with some unlucky losers) and a larger tendency to chase wealth.\nFrom a historical perspective, the questions become more interesting. Could the increasing degree of education over the last few hundred years have been training children to be more risk seeking in their activities? If so, could we argue that an environmental cue is part of the reason modern economies look the way they do?\nAn additional implication from the study is the manner in which large numbers are dealt with. From a logarithmic scale, large numbers appear closer together. If functioning with large numbers in an accurate fashion is important (for example, if it matters whether I pay you $100 or $110), the shift to a linear way of thinking will reap some important dividends. This is not so much a question of risk aversion but the ability to differentiate when numbers get large."
  },
  {
    "objectID": "posts/doing-cultural-evolution-right.html",
    "href": "posts/doing-cultural-evolution-right.html",
    "title": "Doing cultural evolution right",
    "section": "",
    "text": "A sojourn into the literature on cultural evolution can be confusing. Authors use the same terms in different ways. Unique models are used to reach opposite conclusions. And each author seems to find their own way to intertwine genetic evolution into the analysis.\nIn that light, a new article in the Journal of Evolutionary Biology (ungated pdf and supporting information) by Claire El Mouden and friends seeks to nail down some of the concepts of cultural evolution and to set up a general framework (thank you!). The paper is at a more basic level than that of Geoffrey Hodgson and Thorbjørn Knudsen’s book Darwin’s Conjecture, which also sought to define and generalise concepts in this area.\nEl Mouden and her colleagues’ paper covers a lot of interesting terrain, so I will cover it in two posts. In the first, I’ll cover the basics of a cultural evolution framework. In the second, I will look at how cultural and genetic evolution interact in this framework.\nThe authors set up their framework using the Price equation from evolutionary biology. The Price equation divides evolutionary change of a trait into two components. The first is a natural selection component resulting from the covariance between a trait and relative fitness. Where there is large covariance, evolution will be fast. Second is a transmission component, which is the fitness-weighted change in trait value between generations (for example, increasing height with improved nutrition across the population would be considered transmission). The Price equation has the neat property that it can be decomposed into within-group and between-group components, allowing analysis in a multilevel selection framework (although not everyone is happy with this decomposition).\nBut to use this framework, it is important to clarify some terms (which is one of my bugbears about the cultural evolution literature). First, relatedness. As the units of inheritance are cultural traits, the measure of relatedness is similarity in cultural traits. In a simple model where we have one cultural trait, anyone with that same cultural trait has a relatedness of one. In effect, when passing on a cultural trait to another person, they become kin.\nThe use of the term relatedness is often confusing in the cultural evolution literature as the relatedness of interest is typically genetic relatedness. That is fine, but we need to distinguish the two types of relatedness. Cultural kin are not necessarily genetic kin.\nSecond, fitness. Cultural fitness reflects the number of people who learn from an individual, plus the degree of influence that they have on those people. Degree of influence is important because, unlike genetic evolution where you have a known and fixed number of ancestors (one parent in the case of asexually reproducing species, two parents for sexually reproducing species such as humans) who contribute a specific amount of genetic material, the number of cultural ancestors may vary by trait and between people. How many people have influenced your cooking? Who was more influential?\nFurther, for each cultural trait, people will have different fitness. The authors offer the example of Beethoven, whose influence in cookery did not match his influence in music. This necessitates different measurements of cultural fitness for different traits.\nThird, generation. The ancestor-descendent relationship is defined by influence, and can have weak relation to biological age. Plato is still spawning direct cultural descendants today, whereas ideas can also spread through a population in days. However, it is only possible to influence people in the next cultural generation, as that is how generation is defined. If I influence someone, they are the next cultural generation in respect of that cultural trait.\nHaving defined these concepts, they are relatively easy to slot into a cultural Price equation (the maths is in the supplementary information to the paper). While there is extra complexity from considering the degree of influence rather than just the number of descendants, the form of the Price equation is effectively the same for both the genetic and cultural forms. It is just that each deals distinctly with genetic or cultural fitness.\nIt is also possible to derive a Cultural Hamilton’s Rule. In biology, Hamilton’s rule states that a gene will spread if the cost of the act to the altruist is less than the benefit accrued by the beneficiaries adjusted by the degree of relatedness. A gene can spread if you help kin who also have that gene, even if it comes to a cost to yourself.\nSimilarly, the Cultural Hamilton’s Rule states that “a behaviour that reduces the actor’s lifetime cultural influence can only be culturally selected for if the cost to him is less than the product of the cultural benefit to his interaction partners and their cultural relatedness to him”. On this point, the authors give an example of two philosophers with the same cultural views. If one chooses to farm to feed the other, allowing the other to focus on spreading the philosophy, the cultural trait may spread despite one of the philosophers effectively sacrificing his own influence.\nUnder this definition, cultural kin selection becomes a relatively parsimonious explanation for the spread of many cultural traits, such as altruism (and as noted above, this could also be converted into a multilevel selection framework). If people believe in altruism and help others who also do (who are their kin), then helping each other could assist in the further spread of the cultural trait of altruism.\nHowever, this story of spreading cultural altruism falls somewhat short of covering the examples in much of the gene-culture evolution literature. The issue is that, while culture is a part of the model and analysis, people are typically interested in genetic altruism.\nThus, the question of interest is how cultural evolution affects the evolution of genetic altruism? That will be the subject of my next post."
  },
  {
    "objectID": "posts/doubling-down.html",
    "href": "posts/doubling-down.html",
    "title": "Doubling down",
    "section": "",
    "text": "First, from Andrew Leigh, discussing Gregory Clark’s work showing that low social mobility persists across countries and policy environments:\n\nHow do we break the pattern? Part of the answer must lie in a fair tax system, a targeted social welfare system, effective early childhood programs, and getting great teachers in front of disadvantaged classrooms. We need banks willing to take a chance on funding an outsider, and it doesn’t hurt to maintain a healthy Aussie scepticism about inherited privilege.\n\nAs an aside, it appears Leigh (with Mike Pottenger) is finding the same low mobility in Australia as Clark has found elsewhere.\nIn contrast, from Arnold Kling:\n\nFor libertarians, the implications of Clark’s finding of strong heritability of social status are ambiguous. On the one hand, his findings argue against extensive efforts at social engineering that try to achieve parity across groups. … Attempts to engineer different outcomes tend to produce perverse results. …\nOn the other hand, his findings argue against the need to create strong incentives to succeed. If some people are genetically oriented toward success, then they do not need lower tax rates to spur them on. Such people would be expected to succeed regardless. The ideal society implicit in Clark’s view is one in which the role of government is to ameliorate, rather than attempt to fix, the unequal distribution of incomes.\n\nKling’s approach to Clark’s argument seems preferable to doubling down on measures that don’t appear to increase social mobility. That is, of course, if increased social mobility is what we should be chasing."
  },
  {
    "objectID": "posts/durants-the-paleo-manifesto.html",
    "href": "posts/durants-the-paleo-manifesto.html",
    "title": "Durant’s The Paleo Manifesto",
    "section": "",
    "text": "As someone whose diet broadly (in an 80:20 way) reflects paleo principles, I consume the occasional book on the subject. The latest is John Durant’s The Paleo Manifesto: Ancient Wisdom for Lifelong Health, which (thankfully) didn’t just repeat the same information you’ll hear over and over again if you dip your toes into the paleo literature.\nI won’t offer a blow-by-blow of the account of the book, but it has some nice elements.\nMy favourite is a story about a Western lowland gorilla called Mokolo in the Cleveland zoo. Fed a diet of salad, fruit and fibre-fortified gorilla biscuits, Mokolo was overweight, had high blood pressure, showed indicators of heart disease and was on two blood pressure medications.\nThe zoo switched Mokolo’s diet to one more closely resembling a wild gorilla’s diet. Mokolo lost 70 pounds, which was about 15 per cent of his body weight. But to do this they did not fly in plants from Africa, but instead they bought vegetables from the local grocery store. The diet was not the same as Mokolo would have eaten in the wild, but it was a lot closer to the natural diet than gorilla biscuits.\nThis story is a nice illustration to give to those who state “You can’t eat what our ancestors ate. Most vegetables in the shops weren’t even available then.” A paleo diet is not an attempt to re-enact history. Rather, evolutionary theory provides a guide to what types of foods might be more conducive to health.\nThe other point to the story is that Mokolo consumed twice the calories on the new diet. Diets are not simply about calories in-calories out.\nThere are plenty of other interesting parts to the book. Durant gives a novel review of cultural practices as adaptations - particularly religious practices relating to health and disease. He also provides a case for experiencing extremes in temperature, although I am not sure I buy his argument to the extent he does.\nOne point this book made clear, however, is that I still haven’t read a decent critique of the paleo diet or lifestyle. If critics such as Marlene Zuk responded to this book instead of to random blog comments, we could have a more interesting debate. Instead Zuk continues to wheel out arguments that Durant and others have already dealt with. There’s actually some interesting points that could be debated here - I provided a list a couple of years ago - but the paleo-critics don’t seem to have invested enough time getting across the literature to make it interesting."
  },
  {
    "objectID": "posts/e-o-wilsons-the-social-conquest-of-earth.html",
    "href": "posts/e-o-wilsons-the-social-conquest-of-earth.html",
    "title": "E.O. Wilson’s The Social Conquest of Earth",
    "section": "",
    "text": "The re-eruption of the war of words between E.O. Wilson and Richard Dawkins has occurred just as I have come around to reading Wilson’s 2012 book The Social Conquest of Earth. In an interview on BBC2 (watch it at the bottom of this post), Wilson stated:\nIt is an interesting call to authority that Wilson himself challenged in his reply to Dawkins’s stinging review of the book in Prospect magazine (Wilson’s reply is at the bottom of the Dawkins piece) - or even Wilson’s statement at the end of the book that:\nRegardless, the debate between Wilson and Dawkins is a continuation of the group selection debate that has been running since the 1960s, with Wilson now on the group selection side, and Dawkins on that of the selfish gene. But despite this framing of the debate as a confrontation between two apparently diametrically opposed views, The Social Conquest of Earth suggests that Wilson’s view is somewhat more complicated, and possibly confused."
  },
  {
    "objectID": "posts/e-o-wilsons-the-social-conquest-of-earth.html#the-old-and-new-group-selection",
    "href": "posts/e-o-wilsons-the-social-conquest-of-earth.html#the-old-and-new-group-selection",
    "title": "E.O. Wilson’s The Social Conquest of Earth",
    "section": "The old and new group selection",
    "text": "The old and new group selection\nAs background, it is worth defining three concepts: group selection, a newer conception of group selection called multilevel selection, and inclusive fitness.\nThe older form of group selection is a process where the differential survival of groups leads to the evolution of traits that benefit the group. This type of group selection, pushed in the 1960s by V.C Wynne-Edwards in particular, might involve members of a group restraining reproduction during times of scarcity so that the group does not experience resource shortages.\nThis concept received many harsh critiques, most famously by George Williams and in popular form by Dawkins in The Selfish Gene. The basic problem is that if someone cheats and does not restrain reproduction when others do, they will have more offspring and come to dominate the group. The altruistic trait will only emerge if groups with more altruists have a large enough advantage over other groups to compensate for their disadvantage within their groups. These conditions are generally considered to be met in limited circumstances, and most evolutionary biologists would say that the evolution of group adaptations in this way is a theoretical possibility, occurs in some circumstances, but is a practical rarity.\nGroup selection was somewhat reinvigorated in the late 1970s by David Sloan Wilson and friends under a reworking that is commonly called multilevel selection. The first distinguishing feature of multilevel selection is that the definition of “group” can include transitory groupings that regularly remix. You could consider two individuals who briefly trade to be a group. The second feature of multilevel selection is that selection is decomposed across multiple levels. The analysis would look at the fitness of the two trading individuals with respect to each other, which is the individual level selection, and of the fitness of their group relative to other groups.\nMultilevel selection has received a largely muted response, with inclusive fitness the alternative framework preferred by Dawkins and friends - not to mention the dominant paradigm in evolutionary biology. Inclusive fitness combines the direct effects of a trait on an individual with the indirect effects of the trait on other individuals who possess that trait. Kin selection, a strategy of favouring relatives, maximises inclusive fitness.\nInclusive fitness is famously captured by Hamilton’s rule, which states that an altruistic trait will spread if rb&gt;c. c is the cost to the altruist of the trait, b the benefits to others, and r the relatedness between the altruist and beneficiaries. A trait to favour your brother will spread if the benefits to the brother, who is 0.5 related to you, are double the costs to you. Or as J.B.S. Haldane put it, he would give his life for two brothers or eight cousins.\nWhile apparently opposing perspectives, inclusive fitness and multilevel selection are two sides of the same coin. If you can describe an evolutionary dynamic in terms of multilevel selection, you can also give an inclusive fitness story (many suggest the two approaches are mathematically equivalent, although this is debated). They are simply different accounting methods, or languages. The intuitive explanation for the link is that higher levels of selection (the level of groups) can favour the spread of a trait because the members of that group have a degree of relatedness."
  },
  {
    "objectID": "posts/e-o-wilsons-the-social-conquest-of-earth.html#wilsons-critique-of-kin-selection",
    "href": "posts/e-o-wilsons-the-social-conquest-of-earth.html#wilsons-critique-of-kin-selection",
    "title": "E.O. Wilson’s The Social Conquest of Earth",
    "section": "Wilson’s critique of kin selection",
    "text": "Wilson’s critique of kin selection\nWilson’s core argument through The Social Conquest of Earth is that the concept of inclusive fitness has been discredited. This claim stems from the infamous 2010 Nature paper by Martin Nowak, Corina Tarnita and Wilson on eusociality.\nAn E.O. Wilson drawn ant on the title page to my book:\n\nEusociality involves a division of reproductive labour, such as that which occurs in the bees, ants and wasps. Eusociality and kin selection are closely linked as the higher relatedness between sisters in the bees, ants and wasps has been used to explain the willingness of most females to forgo their reproductive success for one of their sisters, the queen.\nNowak, Tarnita and Wilson’s argument was that the evolution of eusociality could be explained through simple individual selection and did not require the framework of inclusive fitness. They presented a model in which eusociality evolved without any reference to relatedness.\nThe model itself was interesting, but it was sandwiched between a not particularly well thought-out or supported claim that “the production of inclusive fitness theory must be considered meagre” and that it “does not provide additional insight or information” to standard natural selection theory. I will let the many responses to the paper speak for themselves, including the main response (with the 130 odd signatures - ungated version here), which contains a table indicating the contributions of inclusive fitness. But if I were to single one paper out, it is this one by Garnder, West and Wild, which addresses many of the mathematical arguments. Its main point, in short, is that Nowak, Tarnita and Wilson fail to distinguish between general kin selection theory and the kin selection methodology used to address specific problems. Their criticisms do not apply to the general theory.\nComing back to Wilson’s book, however, Wilson seems to take an even stronger stance than in the paper. For example, he states that:\n\nMartin Nowak, Corina Tarnita, and I demonstrated that inclusive-fitness theory, often called kin selection theory, is both mathematically and biologically incorrect.\n\nThrough the book, Wilson’s characterisation of the paper’s reception has to be described as either deceptive or oblivious. Gems such as “The beautiful theory [inclusive fitness] never worked anyway, and now it has collapsed” contrasts with what even a cursory glance at the responses suggest. Nowak, Tarnita and Wilson’s critique has not generally been accepted, although reading the book gives no impression of the slightest opposition to Wilson’s position. The interview that triggered this latest spat suggests Wilson is still singing a deceptive tune. He states:\n\nI have abandoned it [the notion of the selfish gene] and I think most serious scientists working on it have abandoned it. Some defenders may be out there, but they have been relatively or almost totally silenced since our major paper came out.\n\nGiven the paper, it is no surprise that Wilson argues throughout The Social Conquest of Earth that individual level and group selection is all that is required to explain the evolution of eusociality in insects. Wilson argues that, after the emergence of eusociality in a single colony through individual level selection, “between-colony selection” leads to the wider spread of the eusocial trait. Its selection at the individual and group levels without a multi-level selection framework. As Wilson states:\n\nBut multilevel selection, in which colonial evolution is regarded as the interests of the individual worker pitted against the interests of its colony, may no longer be a useful concept on which to build models of genetic evolution is social insects.\n\nI have no idea why the preferred model isn’t simply a multilevel selection framework with alternative assumptions, and the confusion only increases from here."
  },
  {
    "objectID": "posts/e-o-wilsons-the-social-conquest-of-earth.html#eusociality-in-humans",
    "href": "posts/e-o-wilsons-the-social-conquest-of-earth.html#eusociality-in-humans",
    "title": "E.O. Wilson’s The Social Conquest of Earth",
    "section": "Eusociality in humans",
    "text": "Eusociality in humans\nWhere things get truly confusing is Wilson’s consideration of humans. Try as I could, I could not conceive of a sympathetic reading that would allow Wilson’s position to be seen as coherent.\nFirst, his branding of humans as eusocial is a stretch under any definition, although he is not alone in attempting that.\nBut more confusingly, his argument that eusociality arose in humans due to multilevel selection is hard to understand because I have no clear idea of what he actually means. As a start, its not multilevel selection in the traditional sense, as Wilson has rejected the other side of the multilevel selection coin, inclusive fitness. Initially, I put it down to his error, but when I hit the last chapter, I realised he was using the term “multilevel selection” to mean something different. When Wilson speaks of multilevel selection, he is generally referring to individual level and group selection occurring in tandem, the “groups” being as we would traditionally define them. But then why isn’t his dynamic in eusocial insects multilevel selection under his definition?\nPart of my confusion (and initial assumption) also stemmed from the contrast between Wilson’s past statements and what he wrote in the book. Compare these two paragraphs – the first from a 2007 paper co-authored with David Sloan Wilson, and the second from the last chapter of the book.\n\nThe theories that were originally regarded as alternatives, such that one might be right and another wrong, are now seen as equivalent in the sense that they all correctly predict what evolves in the total population. They differ, however, in how they partition selection into component vectors along the way.\nTheorists of inclusive fitness themselves have argued that kin selection can be translated into group selection, even though that belief has now been disproven mathematically.\n\nBased on this, it seems that E.O. Wilson is no longer on the same page as the number one champion of multi-level selection, David Sloan Wilson. It is particularly strange in that the two Wilsons characterise what multilevel selection means for humans in almost the same way. As E.O. Wilson writes, and I expect David Sloan Wilson would agree:\n\nSelection at the individual level tends to create competitiveness and selfish behaviour among group members - in status, mating, and the securing of resources. In opposition, selection between groups tends to create selfless behavior, expressed in greater generosity and altruism, which in turn promote stronger cohesion and strength of the group as a whole.\n\nE.O. Wilson’s varying use of these terms points to one of the problems group selection has in popular discourse. The term group selection has been used so inconsistently and used to refer to so many different dynamics, it is often hard to know what someone means when they refer to it. This article (ungated pdf) points to four different uses of the term “group selection”, although I have seen some suggestions that there are six different uses in the literature. When people like Wilson present their arguments in such a confusing manner, it is no surprise that others with less expertise are similarly confused. Look at Jonathan Haidt’s confusion of old group and multilevel selection as a prime example."
  },
  {
    "objectID": "posts/e-o-wilsons-the-social-conquest-of-earth.html#the-other-bits",
    "href": "posts/e-o-wilsons-the-social-conquest-of-earth.html#the-other-bits",
    "title": "E.O. Wilson’s The Social Conquest of Earth",
    "section": "The other bits",
    "text": "The other bits\nBeyond Wilson’s take on group selection, there are some interesting parts to the book.\nOne is Wilson’s argument that many examples of kin selection can be explained as pure self interest. For example, he describes how some bird and mammal offspring remain at their parents’ nest. This has been interpreted as an example of kin selection – it helps the bird or mammal’s parents and siblings. However, Wilson suggests direct self interest is at play. In cases of resource or territory scarcity, they remain with the parents to inherit the parents’ nest when the parents fall off the perch. Wilson provides several examples of this type, suggesting that the focus on kin selection clouds the assessment of what is actually occurring.\nFunnily enough, these arguments mirror an argument I often make about apparently altruistic acts sought to be explained by multilevel or group selection. Many apparently altruistic acts are self interested, such as the trade that characterises our economies. If you classed two people trading with each other as a group, as you might in a multilevel selection framework, you could class the person who gained the least surplus from the trade as an “altruist”. But the simplest explanation is that they seek to gain from trade.\nThe final sections of the book seek to explain “who we are”. I can only say that there are better places to read about the evolutionary origins of religion, art or language. While the last chapter of Sociobiology was revolutionary in its application of evolutionary theory to humans, the short snapshots Wilson provides in The Social Conquest of Earth do not do justice to the work that has occurred in the last 30 years. But that large body of work is, of course, one of Wilson’s great legacies. As Dawkins noted, despite The Social Conquest of Earth, Wilson’s place in history is assured."
  },
  {
    "objectID": "posts/economic-cosmology-the-invisible-hand.html",
    "href": "posts/economic-cosmology-the-invisible-hand.html",
    "title": "Economic cosmology - The invisible hand",
    "section": "",
    "text": "Adam Smith’s concept of the invisible hand is one of the more abused ideas in economics. Mentioned only once in The Wealth of Nations, and only then in the context of preferring domestic to foreign industry, the invisible hand has come to represent the idea that self-interest can improve the common good. The following phrase from The Wealth of Nations nicely captures the idea of the invisible hand (although it is not located with Smith’s use of the term):\n\nIt is not from the benevolence of the butcher, the brewer, or the baker that we expect our dinner, but from their regard to their own interest.\n\nThe invisible hand has taken on a life of its own since Smith’s nuanced book, with caricatured versions espoused and attacked along the political spectrum. In that context, the invisible hand is the second Western cosmology tackled by Gowdy and colleagues in their article Economic cosmology and the evolutionary challenge from the Journal of Economic Behavior & Organization special issue, Evolution as a General Theoretical Framework for Economics and Public Policy (I covered the first cosmology - the rational man - in an earlier post).\nThe first point they make is that the invisible hand operates in a context of human sociality and morals. Smith would not have disagreed with that claim, with any self-interested actions constrained by the norms of the group and the morals of the individual. But as for the first Western cosmology they considered - the rational man - Gowdy and colleagues suggest that selection at a higher level is required to explain these constraints.\nIn arguing this point, Gowdy and colleagues draw a distinction between social and non-social behaviours. They suggest that nonsocial behaviours that do not harm others can evolve through individual level selection. However, given that adaptations at a given level require selection at that level, social behaviours that increase the fitness of someone (or their genes) should not be expected to increase the fitness of the group. For that to occur, group adaptations such as group norms are required.\nThere are a few of points of interest this argument, although it is not clear to me how much these are due to framing rather than substance. First, consider two people who decide to trade with each other. Each has an object the other wants, they engage in the trade and both are better off as a result. Is this a social behaviour that has increased the fitness of the group? Using a multilevel selection framing, the two traders are a group, and their group is clearly better off. One of the traders may have gained an advantage over the other due to a larger gain from the trade, so within that group, one of the traders could be considered “weakly altruistic”. So, do we consider the trade a social behaviour and the group the two traders? And if so, can I also frame this as a simple self-interested action of each party to trade for something that they want more? That is of course a feature of the multilevel selection framework - the ability to frame it in inclusive fitness terms.\nA second point is the distinction between fitness and economic outcomes. In an evolutionary sense, anything that increases your fitness decreases the fitness of others. Fitness is defined in relative terms. In an economic sense, actions can make everyone absolutely better off, as in the trading example above. So when Gowdy and colleagues state that social behaviours that increase the fitness of someone should not be expected to increase the fitness of the group, this does not necessarily rule out increasing the economic benefits obtained by the group. Of course, we could also measure economic benefits in relative terms, but that is not typically what modern advocates of the invisible hand concept mean when they talk of the group benefits of selfish actions.\nFrom their argument that group selection is involved, Gowdy and colleagues seek to resuscitate the concept of the invisible hand by suggesting that the invisible hand is selection at the level of the group. Thousands of generations of group selection (both genetic and cultural) have shaped our psychological dispositions so that now there is no need to have any conception of the group in mind when we pursue our self-interest. Our group selection shaped self-interest tends to lead to group benefits, with these self-interested actions a very narrow subset of all the varieties of self-interest, most of which do not benefit the common good.\nThis is an interesting argument, although I’m not sure that I buy it. I agree that the set of actions that we can undertake to advance our own self-interest are constrained by other people, social norms and institutional frameworks. For example, in many societies, uncooperative behaviour can have severe costs. Further, we exhibit many constrained behaviours even when we are not actually constrained. But as I asked in my last post, if multilevel selection and inclusive fitness are just different ways of framing the same question, what does it actually mean to say that group selection was required? If it is simply a statement that if we use a multilevel selection framework and classify the groups in a particular way, most of the action will be at the group level, then it is a relatively controversial statement. But as I read the paper, I feel that the authors mean more than this, and are actually pointing to group selection in the older sense (see my last post on the rational man for some more discussion of this distinction). In other words, for the authors, this is more than just a question of framing._ _\nThe distinction between genetic and cultural group selection is also important. Cultural group selection is less vulnerable to critiques about the mixing in human populations that genetic group selection is subject to (although it still has plenty of critics), and these need to be addressed if they are implying older concepts of group selection.\nGowdy and colleagues close the article with the suggestion that the mix of self and other-regarding attitudes of humans, as shaped by individual and group selection, allow the division of labour and exchange to occur that drives economic activity. This regulation of competition and self-interest are the invisible hand that leads to the common good.\nThis is the practical implication of their article - the invisible hand requires constraint. Previously provided by group selection, constraint now needs to be provided by regulation. Gowdy and colleagues do not offer further detail on this point, but this is the ultimate purpose of this evolutionary foray into economics and where some interesting debates are going to occur.\nMy series of posts on the Journal of Economic Behavior & Organization special issue, Evolution as a General Theoretical Framework for Economics and Public Policy, are as follows:\n\nSocial Darwinism is back - a post on one of the popular press articles that accompanied the special issue, a piece by David Sloan Wilson called A good social Darwinism.\nFour reasons why evolutionary theory might not add value to economics - a post on David Sloan Wilson and John Gowdy’s article Evolution as a general theoretical framework for economics and public policy\nEconomic cosmology - The rational egotistical individual - a post on John Gowdy and colleagues’ article Economic cosmology and the evolutionary challenge \nEconomic cosmology - The invisible hand (this post) - a second post on Economic cosmology and the evolutionary challenge \nEconomic cosmology - Equilibrium - a third post on Economic cosmology and the evolutionary challenge\nDesign principles for the efficacy of groups - a post on David Sloan Wilson, Elinor Ostrom and Michael E. Cox’s article Generalizing the core design principles for the efficacy of groups"
  },
  {
    "objectID": "posts/economic-growth-and-evolution-parental-preference-for-quality-and-quantity-of-offspring.html",
    "href": "posts/economic-growth-and-evolution-parental-preference-for-quality-and-quantity-of-offspring.html",
    "title": "Economic growth and evolution: Parental preference for quality and quantity of offspring",
    "section": "",
    "text": "My first publication, Economic Growth and Evolution: Parental Preference for Quality and Quantity of Offspring, has just been released electronically in Macroeconomic Dynamics (pdf). With my co-authors Boris Baer and Juerg Weber, we simulate and extend Oded Galor and Omer Moav’s seminal paper Natural Selection and the Origin of Economic Growth (ungated working paper version here), the first paper that models an evolutionary trigger to the Industrial Revolution.\nGalor and Moav’s model has two types of people in the population, each with a genetically inherited preference for quality or quantity of children. The quality-preferring genotype wants their children to have higher human capital, so they invest more in their education, while the quantity-preferring genotype is more interested in raw numbers.\nDuring the long Malthusian era in which both genotypes struggle to earn enough to subsist (i.e. during the thousands of years leading up the Industrial Revolution), the quality-preferring genotype has a fitness advantage. As a quality-preferring genotype is of higher quality, they earn more income. This higher income is more than enough to cover education expenses, so they are also able to have more children than the quantity-preferring genotypes.\nThis fitness advantage leads the quality-preferring genotype to increase in prevalence. As this occurs, the increasing average level of education in the population drives technological progress. This in turn increases the incentive to invest in education, creating a feedback loop between technology and education.\nEventually, the rate of technological progress gets high enough to induce the quantity-preferring genotypes to invest in education also. When this happens, the average level of education jumps, boosting technological progress and causing the Industrial Revolution.\nDuring this process, the population growth rate changes. Up to the time of the Industrial Revolution, population growth increases with technological progress, meaning that per capita income remains at the Malthusian level. However, when the level of technology leaps with the Industrial Revolution, the level of education becomes so high that population growth drops dramatically. A demographic transition occurs.\nAt the time of this transition, the relative fitness of the different types changes. After the Industrial Revolution, the quality-preferring genotypes invest so much into education that they have lower fertility than the quantity-preferring genotypes. The quality-preferring genotypes reduce in prevalence, their fitness advantage erased.\nGalor and Moav worked through the dynamics of the model using phase diagrams. It is not particularly easy or intuitive to see the processes working together in their paper, so the first step in our paper is to simulate the model. This demonstrates the model’s feasibility, as well as showing the dynamics in a form that is easier to comprehend visually. In the chart below, you can see the dramatic jump in technological progress around generation 45 of the simulation, with per capita income growth also jumping at that time. Meanwhile, population growth drops to zero.\n\n\nFigure 3\n\n\n\nThis second chart shows the change population composition. The quality-preferring genotype (genotype a) steadily increases in prevalence through to the Industrial Revolution, peaking at just under five per cent of the population. After the transition, it declines due to its lower fitness.\n\n\nFigure 7\n\n\n\nThis change in selection pressure has an interesting implication. While natural selection is the trigger of the Industrial Revolution, the population composition before and after the transition is the same. There is no difference in population composition between developed and undeveloped countries. The only time there is a difference in population composition is during the transition, when the quality-preferring genotypes peak in prevalence. In some ways, the natural selection occurring in Galor and Moav’s model is a sideshow to the main event, the quality-quantity trade-off. In a similar model by Galor and Weil, a scale effect triggers the Industrial Revolution - that is, the concept that more people leads to more ideas, so technological progress increases with population growth.\nThat highlights the point where I am not convinced that the model describes what actually occurred. As far as human evolution relates to economic growth, I expect that inherent quality is at least (if not more) important than the quality-quantity trade-off. The Industrial Revolution was possible because higher quality (in an economic sense) people were selected for in the lead-up (with that lead up encompassing thousands of years). Further, for a man of low resources, his larger problem is convincing a woman to mate with him, not deciding on the right quantity-quantity mix.\nThe other thing that I should note is that, like most economic models, Galor and Moav’s model includes consumption with no clear evolutionary rationale (an issue I have discussed in an earlier post). Why do people in the model consume more than subsistence? If some people chose to focus all excess consumption into raising children they would come to dominate the population. This might be justified as being something to which the population has not yet adapted, but that explanation does not satisfy me.\nHaving made these quibbles, the model is still an impressive feat. It would not have been an easy task to create a model with technological progress, population and per capita income all following a path that resembles the last few thousand years of economic growth.\nIn our paper, we extend Galor and Moav’s model by considering the entry of people into the population that have a low preference for child quality - i.e. they weight child quantity more highly. Entry could be through migration or mutation. We show that if people with a low enough preference for quality enter the population, their higher fitness in the modern growth state can drive the economy back into Malthusian conditions.\nWe simulated a version of the model which had present in the initial population a genotype with a very low preference for educating their children. This strongly quantity-preferring genotype has a similar fitness to other genotypes that do not educate in the Malthusian state, and declines in prevalence while the quality-preferring genotype increases.\nOnce the economy takes off into the modern growth state, the strongly quantity-preferring genotype has the highest fitness as it dedicates the lowest proportion of its resources to educating its children. The strongly quantity-preferring genotype increases in prevalence until, eventually, the average level of education in the population plummets, undermining technological progress. The world returns to a Malthusian state, with high population growth eroding the income benefits of all earlier technological progress.\nThe following chart shows the rate of growth of population, technological progress and income per person. The first 70 generations look like the base model simulation shown above. However, after that point, technological progress plummets to zero. For the next 150 or so generations, population growth is positive, which can occur as per person income is above subsistence. Eventually, population growth drives income down to subsistence levels and population growth ceases.\n\n\nFigure 8\n\n\n\nIn the next figure, you can see that the strongly quantity-preferring genotype, genotype c, grows from being a negligible part of the population to over 90 per cent prevalence. It is this change in population composition that drives the return to Malthusian conditions (you can also see the small peak in quality-preferring types around generation 45 that kicks off the Industrial Revolution). The strongly quantity-preferring genotypes educate their children far less than the other genotypes, depressing technological progress.\n\n\nFigure 12\n\n\n\nThere is no escape from the returned Malthusian conditions. The quality-preferring genotype will have a fitness advantage in this new Malthusian state and will increase in prevalence. But although that increase in prevalence caused a take-off in economic growth the first time, this time there is no take-off. The strongly quantity-preferring types, which now dominate the population, cannot be induced to educate their children. They simply breed faster to take advantage of any technological progress spurred by the small part of the population that is educating their children.\nThis regression to Malthusian conditions could also be achieved by introducing the strongly quantity-preferring genotype into the simulation at other points in time, which might be representative of global migration. If it occurs after the Industrial Revolution, the timing of the return to Malthusian conditions will occur later. Short of restricting the range of potential quality-quantity preferences, there is no way to avoid the return to Malthusian conditions in this version of model. The strongly quantity-preferring genotypes will always have a fitness advantage when income is above subsistence and their population growth will drive income back down to subsistence levels.\nOne possible way to prevent a return to the Malthusian state is if there is also a scale effect in the population, whereby more people results in more ideas. This would give a basis for continuing growth after the take-off in population in the lead up to the transition. However, this scale effect must not be dependent on the level of education in the population, as that will still decline to zero. Further, if there is a scale effect, it raises the question of why the evolutionary trigger is required at all.\nThere are, of course, a few possible interpretations of the result that the economy returns to Malthusian conditions. The model or assumptions may be wrong. Humans may only have quality-quantity preferences in the growth promoting range. Or if we take Galor and Moav’s model seriously, modern levels of economic growth may be transient.\n*I constructed this post out of two old posts I wrote when this work was first released as a working paper. The original posts with associated comments are here and here. The R code for simulating the model can be downloaded from here."
  },
  {
    "objectID": "posts/economics-and-biology-of-contests-conference-2016.html",
    "href": "posts/economics-and-biology-of-contests-conference-2016.html",
    "title": "Economics and Biology of Contests Conference 2016",
    "section": "",
    "text": "The Cooperation and Conflict in the Family conference of early last year has resulted in a follow-up event - the Economics and Biology of Contests Conference 2016:\n\nIn February 2016, Brisbane will play host to an exciting gathering of economic and evolutionary thinkers who will explore the potential for a closer synthesis between evolution and economics in order to understand both the economics and biology of behaviour in contest.\nThe conference aims to bring together leading economists, psychologists and evolutionary biologists to explore contest behaviour in economic, political and social environments. The conference will provide an opportunity for researchers to discuss the economic, psychological and evolutionary biology approaches to this topic, explore common ground and identify collaborative opportunities.\n\nThe keynote speakers are Roy Baumeister and Roman Sheremeta.\nThe call for papers is now open and closes on November 15, 2015."
  },
  {
    "objectID": "posts/economics-is-a-branch-of-ecology.html",
    "href": "posts/economics-is-a-branch-of-ecology.html",
    "title": "Economics is a branch of ecology",
    "section": "",
    "text": "In an interview published in 1996, Garret Hardin stated:\n\nThe view that I and a number of other ecologists share is that ecology is the overall science of which economics is a minor specialty.\n\n(HT to Rob Brooks for the quote - it is at the beginning of his book Sex, Genes & Rock ‘n’ Roll)\nI agree with Hardin’s sentiment. As I have advocated in this blog before, economics would be much richer, have more predictive power and offer a better description of the world if economists used the fact that humans are a product of billions of years of evolution and this process shaped our traits and preferences. Economics will always be within that framework.\nHowever, I have two disagreements with Hardin’s choice of wording.  First, I would leave out the word “minor”. While humans are one of millions of species, humans are important - with this belief held by most humans. When we seek to explain these rather important creatures, such as examining our day-to-day life, economics provides some of the most important tools. Despite being a part of ecology, economics is important for us.\nAlso, on ecological grounds, humans are rather dominant. Paul MacCready estimated that humans and their livestock have moved from less than 1 per cent of the world’s vertebrate biomass before the dawn of agriculture to around 98 per cent now (I picked this factoid up from a speech by Daniel Dennett). When we address the ecological issues humans have created, economics plays a central role.\nThe second point of disagreement is the implicit description of ecology as the “overall science”. It is, of course, possible to take this deconstruction further. Is biology just glorified chemistry? Or physics?\nDespite the potential for arguments that some branches of science are derivative of other, the division of the branches is useful. In economics, we don’t always need to ask what the evolutionary basis of a specific characteristic is. If we see it in experimental results, it might be useful to take it as given and to ask what this trait means economically. Similarly, if we see a certain behaviour in animals, we don’t always need to inquire into the brain chemistry. What level is appropriate will depend on the hypothesis being tested.\nHowever, we do need to know when to call on these other sciences. In the same way that chemists should know some physics and biologists should know some chemistry, economists should know some biology to place the discipline its context and to know when it is useful to treat economics as a part of a larger science."
  },
  {
    "objectID": "posts/economics_and_evolutionary_biology_reading_list.html",
    "href": "posts/economics_and_evolutionary_biology_reading_list.html",
    "title": "Economics and evolutionary biology reading list",
    "section": "",
    "text": "Below is a suggested reading list for someone interested in the intersection of economics and evolutionary biology. If you have any recommendations for additions, please let me know."
  },
  {
    "objectID": "posts/economics_and_evolutionary_biology_reading_list.html#books-at-the-intersection",
    "href": "posts/economics_and_evolutionary_biology_reading_list.html#books-at-the-intersection",
    "title": "Economics and evolutionary biology reading list",
    "section": "1. Books at the intersection",
    "text": "1. Books at the intersection\nGregory Clark’s (2008) A Farewell to Alms: A Brief Economic History of the World: The Industrial Revolution was triggered by the reproductive success of the rich, as their traits spread downward through society.\nGregory Clark’s (2013) The Son Also Rises: Surnames and the History of Social Mobility: Social mobility is low across countries and time because there is a genetic component to social status.\nJohn Coates’s (2012)  The Hour Between Dog and Wolf: Risk Taking, Gut Feelings and the Biology of Boom and Bust: How our hormones affect decision making in finance - the idea that traders are rational calculating machines driven by their brains is torn apart.\nArthur Gandolfi, Anna Sachko Gandolfi and David P. Barash’s (2002) Economics as an Evolutionary Science: From Utility to Fitness: The title captures the book’s core focus on translating the economic concept of utility into the biological concept of fitness.\nOded Galor’s (2011) Unified Growth Theory: A fine summary of the most serious attempt to accommodate human evolution into theories of economic growth. Highly technical and not an easy read.\nHaim Ofek’s (2001) Second Nature: Economic Origins of Human Evolution: Ofek’s core argument is that humans were selected for exchange. The book has many novel arguments on how economic factors affected human evolution - although I read it so long ago that I need to re-read it in the light of what I have learnt since.\nPaul Rubin’s (2002) Darwinian Politics: The Evolutionary Origin of Freedom: Humans naturally seek political freedom and Modern Western societies do the best job of meeting these needs.\nPaul Seabright’s (2010) The Company of Strangers: A Natural History of Economic Life: An evolutionary examination of the complex web of interactions with strangers that underlie today’s economic institutions."
  },
  {
    "objectID": "posts/economics_and_evolutionary_biology_reading_list.html#other-books-that-affected-my-thinking-on-evolution-and-economics",
    "href": "posts/economics_and_evolutionary_biology_reading_list.html#other-books-that-affected-my-thinking-on-evolution-and-economics",
    "title": "Economics and evolutionary biology reading list",
    "section": "2. Other books that affected my thinking on evolution and economics",
    "text": "2. Other books that affected my thinking on evolution and economics\nBryan Caplan’s (2011) Selfish Reasons to Have More Kids: Parents can relax as there is not much they can do to change their children. And since they’re easier than you think, why don’t you have more?\nGregory Cochran and Henry Harpending’s (2010) The 10,000 Year Explosion: How Civilization Accelerated Human Evolution: Human evolution is getting faster.\nJared Diamond’s (2005) Guns, Germs, and Steel: The Fates of Human Societies: Jared Diamond gets a lot of flack for his strong support of environmental causation, but I believe the thesis in Guns, Germs and Steel is basically right - you just need to add in some evolutionary feedback with the environment.\nRobert Franks’s (1988) Passions Within Reason: The Strategic Role of the Emotions: A fantastic game theoretic approach to the role of the emotions.\nGerd Gigerenzer, Peter M. Todd and the ABC Research Group’s (1999) Simple Heuristics That Make Us Smart: As the title suggests, simple heuristics allow us to make smart choices. I could have put any number of Gigerenzer books here, but this is possibly the best.\nJonathan Haidt’s The Righteous Mind: Why Good People Are Divided by Politics and Religion: A brilliant analysis of why people are divided by politics and religion. Just don’t buy his arguments on group selection.\nTim Harford’s (2011) Adapt: Why Success Always Starts with Failure: Harford applies evolutionary thinking to business, war, accidents and other human pursuits. Excellent.\nJudith Rich Harris’s (2009) The Nurture Assumption: Why Children Turn Out the Way They Do: Beyond the package of genes, parents have limited influence on their children.\nJoe Henrich’s (2015) The Secret of Our Success: An excellent examination of cultural evolution, although plenty to question.\nDaniel Kahneman’s (2011) Thinking, Fast and Slow: While the field of behavioural economics has not yet made the step into evolutionary biology, this overview is the best. My thoughts on a second reading are here.\nGeoffrey Miller’s (2000) The Mating Mind: How Sexual Choice Shaped the Evolution of Human Nature: This might be the best modern exposition of sexual selection there is, and the argument that sexual selection shaped the human mind is compelling.\nGeoffrey Miller’s (2009) Spent: Sex, Evolution, and Consumer Behavior: Evolution shaped our consumer preferences but they do not always work perfectly in a modern environment.\nMatt Ridley’s (2003) The Red Queen: Sex and the Evolution of Human Nature: The book that triggered my interest in evolutionary biology and convinced me that it was relevant to human affairs.\nGad Saad’s (2007) The Evolutionary Bases of Consumption: This book has more material taking on the Standard Social Science Model approach to consumption than is fun to wade through, but Saad’s book is still the go to source for material on the evolutionary psychology approach to consumption.\nAmotz Zahavi and Avishag Zahavi’s (1999) The Handicap Principle: A Missing Piece of Darwin’s Puzzle: It’s all about honest signalling. Alan Grafen’s twin papers providing the mathematical basis for this signalling is also important (they are here and here)."
  },
  {
    "objectID": "posts/economics_and_evolutionary_biology_reading_list.html#books-on-evolutionary-economics-and-complexity-theory",
    "href": "posts/economics_and_evolutionary_biology_reading_list.html#books-on-evolutionary-economics-and-complexity-theory",
    "title": "Economics and evolutionary biology reading list",
    "section": "3. Books on evolutionary economics and complexity theory",
    "text": "3. Books on evolutionary economics and complexity theory\nRichard Nelson and Sidney Winter’s (1985) An Evolutionary Theory of Economic Change: The seminal book that established evolutionary economics as a serious field.\nEric Beinhocker’s (2006) Origin of Wealth: Evolution, Complexity, and the Radical Remaking of Economics: The best readable discussion of the field of “evolutionary economics” and the application of complexity theory to economics."
  },
  {
    "objectID": "posts/economics_and_evolutionary_biology_reading_list.html#articles-at-the-intersection-generally-technical",
    "href": "posts/economics_and_evolutionary_biology_reading_list.html#articles-at-the-intersection-generally-technical",
    "title": "Economics and evolutionary biology reading list",
    "section": "4. Articles at the intersection (generally technical)",
    "text": "4. Articles at the intersection (generally technical)\n\n4A. The evolution of preferences\nBecker, Gary (1976) Altruism, Egoism and Genetic Fitness: Economics and Sociobiology, Journal of Economic Literature 14(3), pp.817-826: One of the earliest calls for biology to be used in economics.\nDe Fraja, Gianni (2009) The origin of utility: Sexual selection and conspicuous consumption, Journal of Economic Behavior & Organization 72(1), pp.52-69: The desire for conspicuous consumption was shaped by sexual selection and is consistent with a utility maximising economic framework.\nHansson, Ingemar and Charles Stuart (1990) Malthusian Selection of Preferences American Economic Review 80(3), pp.529-544: The earliest article I am aware of that explicitly examines the evolution of an economic preference.\nHirshleifer, Jack (1977) Economics from a Biological Viewpoint Journal of Law of Economics 20(1), pp.1-52: Another of the earliest calls for biology to be used in economics.\nRayo, Luis and Gary Becker (2007)Evolutionary Efficiency and Happiness Journal of Political Economy 155(2), pp.302-337: An argument that if happiness is a tool through which human choices are directed to evolutionary goals, constraints on our feelings may lead to relative preferences and habit formation.\nRogers, Alan (1994) Evolution of Time Preference by Natural Selection American Economic Review 83(3), pp.460-481: One of the first articles to propose an evolutionary basis to time preference.\nRobson, Arthur and Larry Samuelson (2009) The Evolution of Time Preference with Aggregate Uncertainty American Economic Review 99(5), pp.1925-1953: Risk affects the optimal rate of time preference. Arthur Robson is the king of the study of the evolution of preferences, as a scan of the papers on his home page shows.\nRobson, Arthur and Larry Samuelson (2009) The Evolutionary Foundations of Preferences in Benhabib, Jess, Alberto Bisin and Matthew O. Jackson, Handbook of Social Economics (ungated working version here): A thorough review of the literature on the evolution of preferences.\nRubin, Paul and Chris Paul II (1979) An Evolutionary Model of Taste for Risk Economic Inquiry 17(4), pp.585-596: A fantastic example of how an evolutionary framework can yield new results.\n\n\n4B. Genoeconomics\nBeauchamp, Jonathan P. (2011) Molecular Genetics and Economics, Journal of Economic Perspectives, 25(4), pp.57–82: A good discussion of the potential use of molecular genetics in economics and economic policy (my post here).\nBenjamin, Daniel J. et al. (2012) The Promises and Pitfalls of Genoeconomics, Annual Review of Economics, 4(1), pp.627–662 (ungated working paper here): A sound reminder to take a lot of genoeconomics research with a grain of salt.\n\n\n4C. Evolution and economic dynamics\nClark, Gregory (2008) In defense of the Malthusian interpretation of history, European Review of Economic History, 12(2), pp.175-199 (ungated working paper here): This article is a response to critiques of A Farewell to Alms (critiques here, here, here and here), but it expands on some important points in the book.\nGalor, Oded and Omer Moav (2002) Natural Selection and the Origin of Economic Growth, The Quarterly Journal of Economics, 117(4), pp.1133–1191: The seminal article proposing that evolution affected economic growth through a genetically based preference for quality or quantity of children. With Juerg Weber and Boris Baer, I have simulated and extended this paper.\nSpolaore, Enrico and Romain Wacziarg (2013) How Deep Are the Roots of Economic Development?, Journal of Economic Literature, 51(2), pp.325–369: An outstanding discussion of the deep roots of economic development and how persistent differences in development are.\n\n\n4D. Macro-genoeconomics\nAshraf, Quamrul and Oded Galor (2013) The Out of Africa Hypothesis, Human Genetic Diversity, and Comparative Economic Development , American Economic Review, 103(1), pp.1-49 (ungated working paper here): Genetic diversity has positive effects on innovation but hampers trade, meaning that moderate levels of genetic diversity gives the right mix. I have written a series of posts on this paper, which can be found through this post. Andrew Gelman discusses the paper here and here. Jade d’Alpoim Guedes and friends also respond.\nSpolaore, Enrico and Romain Wacziarg (2009) The Diffusion of Development Quarterly Journal of Economics 124(2) pp.469-529 (ungated working paper here): An argument that the genetic distance between populations acts as a barrier to the transfer of technology, leading to income differences between groups (my post here)."
  },
  {
    "objectID": "posts/economics_and_evolutionary_biology_reading_list.html#articles-on-evolutionary-economics-and-complexity-theory",
    "href": "posts/economics_and_evolutionary_biology_reading_list.html#articles-on-evolutionary-economics-and-complexity-theory",
    "title": "Economics and evolutionary biology reading list",
    "section": "5. Articles on evolutionary economics and complexity theory",
    "text": "5. Articles on evolutionary economics and complexity theory\nAlchian (1950) Uncertainty, Evolution and Economic Theory Journal of Political Economy 58(3), pp.211-221: The 1950 proposal that examining the actions of economic agents as trial and error, and adaptive and imitative behaviour may be a better basis for examining economic behaviour than perfectly rational profit maximisation."
  },
  {
    "objectID": "posts/economists-and-biology.html",
    "href": "posts/economists-and-biology.html",
    "title": "Economists and biology",
    "section": "",
    "text": "Mike the Mad Biologist has posted this piece on economists’ understanding of biology. He pulls apart some statements by Russ Roberts and suggests that if economists are going to use biology as a model for the economics discipline, they should try to understand it first.\nNaturally, I agree with this. Apart from preventing the mangling of biological concepts when using a biological analogy, there is a lot in biology that could benefit economics.\nHowever, Mike then moves to one of his favourite topics, which is the use of “stupid … natural history facts” in biology and their seeming absence in economics. As he states when comparing economics and biology, “the really key difference is that biology has accepted modes of confronting theories and, importantly, discarding them”.\nI agree that economics has more models floating around for which there does not seem to be  factual support. But I am not sure that there is a general lack of empirical work in economics. This hits on one of Russ Robert’s favourite issues, which is the use of complex statistical techniques to empirically validate theories. Statistics can be as misused as theoretical models. Take the back-and-forth on “more guns, less crime” or the impact of legalised abortion on crime. The debate is now predominantly about data and neither side has conceded. As Roberts usually asks, how many economists have changed their mind on the basis of an empirical study? I don’t know of many.\nOn the flip side, did Dawkins or Gould (or their respective supporters) ever concede to the other side that they were wrong and substantially change their world view?\nSo why does biology discard theories out of sync with the facts more readily than economics? I can only suggest that economics is more prone to personal bias. The issue government spending tends to elicit a stronger response than whether a particular gene is pleiotropic. Many evolutionary biologists have strong views on economics (as a read of Mike’s blog will show), while most economists probably aren’t overly concerned about evolutionary biology. Perhaps we should ask how many evolutionary biologists have fundamentally changed their economic or political views in the face of data?\nUpdate: Some follow up on whether biologists can admit they are wrong by Razib Khan.\nUpdate 2: And Mike the Mad Biologist with some further thoughts."
  },
  {
    "objectID": "posts/economists-genetic-blindspot-the-data-were-not-collecting.html",
    "href": "posts/economists-genetic-blindspot-the-data-were-not-collecting.html",
    "title": "Economists’ Genetic Blindspot: The Data We’re Not Collecting",
    "section": "",
    "text": "This article was intended for a different forum. When that didn’t work out, I decided to park it here.\n–\nProfessor of Sociology William H. Sewell was deeply interested in social mobility. Do career aspirations affect career achievement? Do individual and social traits underlie those aspirations? Despite some preliminary research in the 1950s, Sewell lacked the data to answer these questions.\nIn 1962, Sewell made a lucky discovery: sitting unused in a University of Wisconsin administration building were the survey schedules and punch cards from a 1957 survey on the educational plans of all Wisconsin high school seniors. Now he had what he needed. He randomly selected 10,317 of these seniors and, in 1964, sent postcard surveys to their parents, asking about the seniors’ education, career aspirations, and socioeconomic status. Eighty-seven per cent of the parents responded. A 1975 telephone survey of the graduates themselves had a response rate of 89 per cent.\nThese steps began the Wisconsin Longitudinal Study; longitudinal studies observe subjects over time. In 1977, the survey expanded to include randomly selected siblings of the graduates. By 2020, the survey had accumulated over 60 years of data. The high school graduates were 81 years old.\nIn 2006-07 was a significant milestone. Saliva sample kits for genetic analysis were posted to participants. Kits were mailed in 2011 to those who missed the first round. Overall, 64 per cent of graduates and 36 per cent of siblings provided samples, covering about half of the 18,000 participants.\nAs genetic data doesn’t change, these samples enriched 50 years of data, including that initial collection in 1957. Each new collection, such as those in 2011 and 2020, is also augmented by this genetic data.\nIn 2018, Daniel Belsky and colleagues published a paper in the Proceedings of the National Academy of Sciences using the Wisconsin Longitudinal Study’s genetic data. For each student, they calculated an education “polygenic score”, a measure of the genetic influence on educational attainment. Students with higher scores had higher career success and social rank than their parents did in 1957. They were upwardly mobile. This observation held within families: siblings with higher scores achieved more as well.\nAs genetics are the cause of many phenomena we study, genetic data can be of immense value. Studying the mechanisms of DNA transmission and recombination between generations can help policymakers investigating issues such as social mobility, poverty, and inequality. We could assess interventions, from education to tax reform to childcare. By learning which interventions work, we could allocate public resources more efficiently. Kathryn Paige Harden makes the case for the value of genetic analysis comprehensively in the excellent The Genetic Lottery: Why DNA Matters for Social Equality. In what follows, I will remake that case in only the most superficial way.\nHowever, there is a major barrier to using genetic data in this way: most of the datasets we use to investigate socio-economic phenomena do not include genetic data, preventing us from including genetics in our analyses.\nWe should address this problem by collecting genetic data from the participants in longitudinal research. As our DNA is fixed for life, we should supplement old longitudinal data with genetic data, enriching decades of past work. The addition of genetic data to the Wisconsin Longitudinal Survey provides a template.\nWe could also be bringing genetic data to bear in our experimental work. The addition of genetic data to experimental panels could provide rich insight into the heterogeneity of behaviour.\nImportantly, we can do this now. While many discussions on genetic data in social science focus on growing sample size, new tools and future possibilities, existing tools can give us insight today and bring future possibilities to life.\nIn what follows, I will first touch on two topics of interest to economists (my own profession): using genetic transmission to infer causation and some examples of genetic data applied to questions of social mobility and inequality. That will take me to my main point, that we should be supplementing our core research datasets with genetic data now.\n\nCausation\nThere is abundant data indicating the intergenerational persistence of educational outcomes and socioeconomic status. Here are two Australian examples (I’ll lean on material from my home country, which I know best): A Year 9 student (aged 14-15) whose parents have a bachelor’s degree or higher will, on average, have numeracy skills almost four years ahead of those of classmates whose parents do not have a bachelor’s degree. A child born in Australia to a family in the bottom 20 per cent of parental incomes has a 12 per cent chance of being in the top 20 per cent of incomes 30 years later.\nFor an economic policymaker, these statistics raise questions. Do the children of educated, wealthy parents have an unfair advantage? Would equalising wealth through taxes and transfers close the intergenerational gap? Designing robust solutions requires accurately assessing causality, but a correlation between child and parent outcomes does not prove that higher parental education or socioeconomic class causes outcomes in the next generation. We need to consider if a third factor might be “confounding” the result.\nA likely third factor is genetics. Children and parents share DNA. If parents genetically transmitted traits like intelligence and conscientiousness to their children, we could see a correlation between the child and parent even if parental education or income had no direct effect on the child. It should not be controversial to say that genetics could underlie this result: the first law of behaviour genetics is that all human behavioural traits are heritable.\nThe genetic confound raises the question of how to infer the cause. Economists are infatuated with causation; absent a randomised controlled trial, they scour the world for interesting data sets and quasi-experiments to tease out causality. This pursuit led to innovative approaches to infer causation. Economists celebrate the “credibility revolution”, demanding rigorous study design.\nAn example quasi-experiment concerns protests in Paris in May 1968, which led authorities to be lenient in university entrance exams. Eric Maurin and Sandra McNally studied students who barely passed despite the increased leniency, and likely would not have been accepted in other years. These students earned higher future wages, and, in turn, their children obtained more education. As the riot did not affect the genetics of the parents, we can take the change in schooling as the cause.\nEconomists’ focus on causation does have a benefit. The social science literature is littered with studies for which genetics is an obvious confound. There are fewer examples in the economics literature (that is my impression, at least). Although economists rarely discuss genetics, they prefer experimental designs that avoid confounds. That, however, places a constraint on the questions that we can answer. Thankfully, overcoming the genetic confound does not always require a city to descend into riots.\n\nHuman genomes consist of 3 billion base pairs, with over 99 per cent of base pairs the same from person to person. Most of the variation between people are single nucleotide polymorphisms (SNPs), changes in just one base pair that are present in at least 1 per cent of the population. Genetic databases, like the Wisconsin Longitudinal Study, typically contain samples of SNPs. If you’ve taken a DNA test from companies like 23andMe (R.I.P.) or MyHeritage, they analysed your SNPs too.\nResearch using SNP data suggests that most traits are polygenic: that is, many genes underlie the traits’ heritability. For example, one study identified 3,952 SNPs linked to educational attainment. This reflects a proposed fourth law of behaviour genetics: ‘A typical human behavioural trait is associated with very many genetic variants, each of which accounts for a very small percentage of the behavioural variability.’\nA long list of SNPs alone is not useful for examining social science outcomes. As a result, scientists developed polygenic scores, a weighted count of SNPs enhancing a trait. To give an example, one study found that a person in the 84th percentile of polygenic scores for educational attainment was 19 per cent more likely to complete a university degree than someone with an average score. It may not sound like much, but this is a similar effect size to that of many social and cultural factors, like family socioeconomic status.\nDespite the apparent link between polygenic scores and outcomes, this is again correlation and not proof of causation. Does a link between polygenic score and outcome mean genetics caused the difference? We cannot jump to an answer of “yes”. Population stratification may be at play, where genetic differences arise between groups due to historical, cultural and social factors. For example, intermarriage among highly educated groups can lead to genetic variant concentration over time, showing genetics to be a historic contingency and not truly causal. We can at least rule out reverse causation; education does not alter DNA.\nHowever, the nature of DNA transmission from parent to child provides a mechanism by which we can get closer to the cause. Your genetic material is organized into 23 pairs of chromosomes, one-half of each pair coming from each of your father and mother. Each parent’s chromosomes in turn came from your grandparents, but you didn’t receive exact copies of your grandparents’ chromosomes. During the creation of your parents’ eggs and sperm, your grandparents’ chromosomes were spliced together, resulting in each egg or sperm having different combinations of your grandparents’ chromosomes. Women create about 45 splices, and men create about 26. The result is that siblings receive a random draw from each parent’s chromosomes. This draw involves a small number of chunks (around 23+45=68 from the mother and 23+26=49 from the father), not each of the three billion base pairs independently. Because of this small number of chunks, the genetic relatedness between siblings can vary significantly from the average of 50 per cent, with most siblings sharing between 43 per cent and 57 per cent of their genetic material. My identical twin sons share only 41 per cent of their DNA with their younger brother.\nUnderstanding how DNA is transmitted provides a fantastic opportunity. Historically, behaviour genetics relied on comparing identical and fraternal twins or examining adoptees. Relatedness data now allows an extension of the twin study methodology to families without twins. The lower variation in relatedness between siblings (as compared to identical versus fraternal twins) reduces the ability to link between genes and outcomes, but this is offset by the larger samples available when you are no longer constrained to twin samples. Researchers have since expanded this methodology to include broader population samples, not just siblings, to estimate the heritability of traits like educational attainment.\nWithin-family variation in SNPs also allows us to calculate more robust polygenic scores. Since each sibling’s set of SNPs is the outcome of a lottery, causation is less susceptible to bias.\nAnother opportunity arises because some genetic variants are not passed from parent to child. If these non-transmitted variants affect child outcomes, we can rule out genetic transmission and focus on environmental channels such as socioeconomic status. Kathryn Paige Harden and Philipp Koellinger call this a ‘virtual parent’ design, which mimics adoption studies in that the children are raised by someone with different genes. Kong and colleagues used this premise to show that the effect of the non-transmitted variants on child education was 30 per cent as strong as the transmitted polygenic score.\nThe methodologies underlying this research are still in development and subject to some interesting debates. Are we effectively controlling for population stratification when we don’t have family-based samples? However, that is not a barrier to building data now, and further data collection will support resolving such debates.\n\n\nIllustrating the applications\nFor all of the evidence indicating their influence, genes are not destiny. An example is Arthur Goldberger’s thought experiment about eyeglasses. Poor eyesight due to genetics might be corrected by an environmental intervention.\nAlthough Goldberger made this point to argue against using heritability in policy development, studies of intergenerational status have shown genes and environment are interconnected. Methodologies such as those described above can help us understand the causes of transmission, explore what policy interventions are most prospective and examine the distributional effects of those policies.\nThis article is not a comprehensive review of policy relevant research, but below are some brief illustrations related to social mobility and intergenerational transmission of socioeconomic status to give a flavour of the questions that can be illuminated with genetic data.\n\nWhat is the optimal level of social mobility?\nI don’t know the answer to that question, but any answer requires us to consider genetics. With genetic endowments, random sorting will not emerge in a society with equal environments. Consider the following. A study of Finnish twins found a substantial genetic effect on lifetime earnings: around 40 per cent of the variance in women’s earnings and around half for men’s. Twenty-one other studies in Australia, Sweden and the United States produced similar estimates for genetic contribution, with the effect of shared environment, comprising common environmental factors such as parental socioeconomic status, being around 9 per cent. This is the second law of behaviour genetics in action: the effect of being raised in the same family is smaller than the effect of genes. However, is heritability capturing an inherent characteristic of the child, the parental response to the child’s genotype, or the environment created due to the parental genotype that is also shared with the child?\nAnalyses using genetic data are nascent, but they can shed light on this question. Polygenic scores for education are linked to higher socioeconomic status and better labour outcomes, even among siblings. However, those who grow up in high-status households tend to have higher college completion and better socioeconomic outcomes independent of their score. Further, there is a stronger relationship between polygenic scores and college completion in higher socioeconomic groups. While this pattern may partly reflect unobserved genetic variation - polygenic scores capture only some of the heritability of traits - this evidence suggests an opportunity to improve outcomes for talented students in low-status families.\nThere is growing research into the transmission of skills, one of the pathways by which socioeconomic status might persist. One study identified three genetic pathways: the direct genetic effect, whereby both parent and child have genes that increase their skills; parental investment in children with higher polygenic scores; and parents with higher genetic factors themselves investing more in their children. This study indicated that ignoring genetics overestimates the effect of parental investment on child skills, but that the environment created by the parent, influenced by their genetics, also matters, at least for the children aged seven years or less examined in this study. Examining these investments may provide intervention ideas.\nAnother study used genetic data to provide insight into the accumulation of wealth. People with a higher polygenic score for years of schooling had greater household wealth at retirement. Those with scores in the 84th percentile had over $150,000 extra wealth. A polygenic score premium persisted even after controlling for education and income, suggesting the score captures other skills. One policy insight comes from a gene-environment interaction. The relationship between wealth and polygenic score was four times as large for those without defined benefit pensions, which involve a guaranteed income and require few decisions about money allocation. Those with lower polygenic scores tended to struggle with managing their retirement investments, indicating that freedom can harm those who find complex financial decisions difficult. This finding could inform policy. In Australia, compulsory defined accumulation plans divert a portion of income into tax-advantaged retirement accounts that cannot be accessed until retirement. This is an ‘eyeglasses’ solution to the problem, albeit everyone gets eyeglasses regardless of need. A more targeted response might be Australia’s relatively generous means-tested pension system.\nGenetic factors may also illuminate the distributional effects of policy. Taxes on tobacco were intended to curb usage, but as Jason Fletcher argued in a speculative article, genetically disadvantaged populations might bear a higher burden. Variants of nicotine receptor genes may trigger different responses; those with higher reward responses in the brain didn’t reduce smoking despite the higher cost. This suggests that diversion through nicotine substitutes may be more effective than taxes for some populations.\nExamples of heterogeneous genetic responses to policy interventions are growing. Raising the minimum school leaving age reduced the body mass index of those genetically prone to obesity. Students with low polygenic scores attending advantaged schools were less likely to drop out of math classes. Similarly, the relationship between polygenic scores and high school dropout was weaker in higher socioeconomic families, suggesting an ability to buffer against the worst outcomes. A useful heuristic to consider whether responses might vary with genetics is to consider whether they vary with education, income, or other socioeconomic factors. Genetics likely underlies many of these non-genetic variables.\nThe small but growing body of research provides a consistent picture: genetics influence social mobility and interact with environmental factors in non-obvious ways. Absent genetics, we struggle to assess the causes and overlook insights that can help us evaluate interventions.\n\n\nThe practical steps\nDespite the above examples, there are many unexploited opportunities to use genetic data in economics and policy development. The literature is rich but small.\nHowever, to exploit these opportunities, we need to enhance our datasets with genetic data. We need to build the genetic infrastructure. When economists plan research using longitudinal datasets or experimental panels, they shouldn’t have to think about how to collect saliva samples or the cost of genetic analysis. The genetic data should be there by default.\n\nEnhancing longitudinal data\nLongitudinal data sets are valuable for their multidimensionality over time, allowing researchers to track changes and examine the participants’ life paths. Ask an applied economist about their most valuable research resource, and they will often point to longitudinal data.\nIn Australia, our best-known longitudinal dataset is the Household, Income and Labour Dynamics in Australia (HILDA) Survey. Begun in 2001 with a sample of over 7,000 households, 22 years of data are now available to researchers. Over 1,000 papers have been published using HILDA data on topics ranging from how losing a job affects who does chores at home to the effect of stock market performance on well-being to intergenerational social mobility.\nIn the United States, perhaps the best-known longitudinal surveys are the National Longitudinal Surveys (NLS) sponsored by the U.S. Bureau of Labour Statistics. These include the National Longitudinal Study of Youth 1979 (tracking subjects born between 1957 and 1964) and 1997 (tracking subjects born between 1980 and 1984). Named for their starting years, both studies are still running today.\nWhat makes these longitudinal resources valuable is the sheer depth of the surveys across topic areas and time. HILDA includes data on ancestry, children, education, finance, health, housing, labour force outcomes, skills and relationships. The challenge, however, is that even with the richness of the resource, it can take some ingenuity to infer what is causing the observed outcomes. Researchers use mechanisms such as the ordering of events, assuming causation must flow forward through time. But this typically has severe constraints.\nAgain, genetics provides a potential tool. If seeking to examine how parent behaviour affects child outcomes over time, genetic data could be used to disentangle environmental and genetic causes, and where genetic, examine the pathways by which the genetics operate. Even where genetic data does not identify causation, it might provide insight.\nUnfortunately, many of our most valuable longitudinal datasets do not have associated genetic data. There are a few exceptions, including the Wisconsin Longitudinal Study and others that underlie the examples in this article. The Twins Early Development Study, which has been tracking 15,000 twins born in the UK between 1994 and 1996, is now augmented with genetic data. The Dunedin study and the National Longitudinal Survey on Adolescent Health also contain genetic data. There are others, some of which underlie the examples in this article.\nFor every longitudinal dataset, we should augment data collection with genetic data. This does not apply only to new surveys. We could augment existing surveys with that data. Genetic data obtained from participants today could be used to analyse Round 1 HILDA or National Longitudinal Survey data. There are many studies where people have been tracked for decades where participants are still available. If genetic data collection is extended to families, many of the approaches to causation described in this article become available. The Wisconsin Longitudinal Study is an example of this occurring.\nThe nature of longitudinal surveys makes genetic sampling prospective. Longitudinal surveys have the benefit of a strong relationship with the participants. Participants already provide much sensitive information and may be willing to contribute genetic data. Most longitudinal surveys already have strong privacy protocols in place, controlling access to data based on sensitivity. Similar privacy measures can be applied to genetic data, addressing concerns such as the use of the genetic data to identify individuals. Instead of making the SNP data directly available, a set of polygenic scores and relatedness data within families could be released. If researchers need access to more detailed data, the existing access controls for the more sensitive longitudinal data provide prior art. The Wisconsin Longitudinal Study has such tiered access, with more stringent applications for SNP data than for aggregated polygenic scores.\nThe sheer volume of research from the major longitudinal surveys makes sampling highly cost-effective. With costs comfortably below $100 per person, the cost of genotyping for, say, the NLS Youth Surveys or HILDA is less than $1 million each. Sample collection can be done by post. The thousands of papers using this data, not to mention its use in government and by policymakers, make this a high-value step.\nThe cost-effectiveness will only increase. Genotyping and sequencing are becoming cheaper. Soon genetic samples will expand to the whole genome, capturing more rare variants and mitigating problems of population stratification. The polygenic scores that can be constructed with the genetic data will only increase in power.\n\n\nEnhancing experimental data\nWhile most of the above discussion has been focussed on research using longitudinal datasets, there is also an opportunity to use genetic data to increase insight from experimental work.\nHistorically, students have been the typical subjects in economics or psychology experiments. They’re cheap and widely available on university campuses. This reliance on a narrow population slice led to an inevitable (at least in hindsight) critique. Behaviour varies across populations. The standard experimental subject - someone Western, Educated, Industrialised, Rich and Democratic (WEIRD) - is not representative. The resulting data skew exists both across populations or societies and within them. College-educated students are not representative of those without a college education. The same holds for the rich and the poor.\nOne solution is to broaden subject pools. Today, experiments are less often conducted with students (although it’s unclear whether experimental participants sourced through Amazon’s crowdsourcing platform Mechanical Turk are more representative of humanity).\nAnother approach has been to examine how participants’ responses vary within experiments. Behavioural economists often point to heterogeneity as the future of applied behavioural science. People vary in capabilities, resources, goals and preferences. As a result, behavioural economists need to move beyond a one-size-fits-all philosophy and use personalised nudges. Practically, researchers address this by collecting data on gender, income, education and other demographic variables and studying how responses vary with demographic differences.\nGenetics may enhance our understanding of heterogeneity. Per the first law of behavioural genetics, genetics may drive variability in experimental behaviour. Further, many characteristics that we identify as a source of variation may not capture the underlying cause. Genetic data can help us determine whether wealthy people respond differently because they are richer, or because they have characteristics that tend to lead to wealth. Genetic data offers a way to check that randomised controlled trials are balanced - that is, to test the assumption that each group is the same. It also allows us to increase the power of the analysis - our ability to detect an effect - by accounting for some of the variation between the experimental participants.\nIntegrating genetic data into experiments can let us investigate associations discovered in observational data. For example, the link between polygenic scores for educational attainment and wealth at retirement may be due to differences in the ability to make complex decisions. We could investigate that hypothesis through experiments examining how complex decision-making skills vary with the polygenic score. Experiments could provide a test bed for interventions, or at least inform policy discussions.\nEnabling the use of genetics in experimental work requires building panels - collections of people who have registered to participate in experiments - with genetic data available for each participant. Each panel member could be genotyped, with the experimenter provided with polygenic scores for a range of outcomes of interest for each participant. The aggregated nature of these scores makes reidentification near impossible. As the effect sizes associated with polygenic scores are typically as strong or stronger than those for many social science interventions, genetic effects could be detected in experiments with as few participants as the typical lab experiment.\nA version of this has been done in the past with panels of twins, although in that case the genetic data is typically limited to whether the twins are mono- or dizygotic. For example, Twins Research Australia maintains a panel of 35,000 twin pairs, which researchers may apply to access existing data or run a new study. One team of researchers developed the Australian Twins Economic Preferences Survey using that data.\nIncorporating genetic data into experimental work may be a bigger challenge than for longitudinal data, as there are no ready-made data collection and access arrangements. These would require investment. However, genotyping or sequencing cost are unlikely to be prohibitive. As panel participants typically participate in many experiments, the cost of the genetic tests could be spread over them. Since genetic data doesn’t change - unlike other attributes measured in experimental studies - it only needs to be collected once. Further, genetic data is concrete and not self-reported, so is thereby more consistent and reliable.\nAlternatively, existing genetic research resources could be expanded in purpose. The UK Biobank database contains genetic, lifestyle and health data for half a million UK participants (albeit it could be stronger with more family-based participants). Estonia, Iceland and the Scandinavian countries have large genetic databases. These could provide experimental participants together with polygenic scores. Experimental data then forms part of the growing data resource. Commercial providers such as 23andMe with large customer databases could even seek alternative revenue sources by facilitating the provision of participants for experimental studies.\nCritical to the value of any future data sets is representative population sampling. However, polygenic scores are typically developed from homogeneous population groups, most commonly with European ancestry. As a result, polygenic scores cannot simply be plugged into analyses of diverse groups. Genetic data itself doesn’t solve the WEIRD problem if research panels remain WEIRD. These are the important but ultimately tractable issues we should be grappling with.\n\nMy PhD was on the link between human evolution and economic growth. When I presented a draft of my PhD research proposal, the first comment I received was that I should refuse any grants from men with funny little moustaches and straight-arm salutes. Although that commenter came around, this initial reaction is a typical response to a discussion of genetics in social science.\nThat ‘fear’ of the implications of genetics, however, is not the only obstacle to its use. Genetic data is simply not available for many studies. Its absence makes it easy to ignore. If the authors don’t mention genetics in their analysis, few peer reviewers will criticise them for overlooking an obvious confound. They can’t ask for further analysis when the data is not there.\nThe result is that, despite examples of the type I have discussed above, the potential for genetics to inform our thinking on important policy questions is untapped. I trawled policy-focused papers on social mobility in Australia, including Treasury policy briefs, reports by the Australian Institute for Health and Welfare and speeches by the head of the Australian Government’s major economic think tank. Genetics does not get a mention. It is possible to dig up the occasional left-of-centre politician who realises genetics can affect social mobility (albeit that politician was an economics professor who published on social mobility). However, even they are silent on genetics when they move into political mode.\nThat said, it is hard for policymakers to engage with genetic questions when the research comes from longitudinal datasets without genetic data. Insightful papers examining the genetics of social mobility or other economic questions are rare. When faced with a particular policy question, it is unlikely that genetic analysis is available, especially one that matches relevant populations or precise policy measures.\nThat is why enriching our economic datasets is so important. A robust genetic data foundation is crucial for advancing our understanding of policy questions such as social mobility, inequality, skill development, and the diversity of our responses to government interventions. By enabling more studies that incorporate genetics, we can expose the limitations of research that ignores this critical factor. Only then can we answer vital questions and develop more effective policies."
  },
  {
    "objectID": "posts/economy-iq-feedback.html",
    "href": "posts/economy-iq-feedback.html",
    "title": "Economy-IQ feedback",
    "section": "",
    "text": "Scientific American has an excellent podcast of an interview with James Flynn on his new book, Are We Getting Smarter? The podcast accompanies an article from the latest issue.\nFlynn makes some interesting points throughout the discussion, including the following thoughts on the interaction between the economy and IQ:\n\nOne of the big surprises is that Scandinavia, the IQ gains tailed off towards the end of the last century and many of us thought and I had an open mind that that would mean that they would tail off in the rest of the developed world. Well, three data sets are in now from America, Britain and Germany and they haven’t. They seem to be humming along on the Wechsler tests, you know the WISC and the WAIS, at just about three points per decade. We’re in the 21st century a decade now and there they still are.\nAnd this revises one’s calculations a bit. I’d thought the 21st century would see the developing world catching up. Well it is because their gains are going even faster. It will be tougher to catch up than I thought. I think it’s wrong to look at the developing world in isolation from what’s going on there economically because IQ rises with modernity. Lynn and many others make the mistake of sort of thinking that you have to leap from an IQ of 70 to 100 and then you modernize. Well, it’s like going up a ladder. You gain a bit in terms of the modern mindset, you modernize the economy a bit and then you go up again, you know, slowly.\n\nIn this scenario, there can be some strong benefits to IQ enhancing policy interventions as it can set the population into the virtuous circle of modernisation and IQ gain."
  },
  {
    "objectID": "posts/education-income-and-children.html",
    "href": "posts/education-income-and-children.html",
    "title": "Education, income and children",
    "section": "",
    "text": "In my recent post on whether children are normal goods (demand for children increasing with income), I dodged questions around the effect of education. Most recent studies into the effect of income on children control for the level of education, as did Bryan Caplan in his analysis that found a positive correlation between income and children in the United States.\nI am torn over whether controlling for education gives us meaningful information. Education clearly has a negative effect on fertility. Education takes time, which reduces the time available for children. However, education increases income. If education is simply an investment of time to earn income in the same way that labour is, then maybe we should not control for it in determining the effect of income on number of children. Rather, we should combine the education and labour time and examine the opportunity cost and effects of the entire investment.\nOf course, education serves purposes other than increasing income. It is a signal in itself. But this only raises the question of why someone would undertake this costly education to obtain income or send a signal, which is in turn used to attract a mate, when the ultimate effect is less children.\nIn part, we need to consider that mating markets are two-sided. While the man might increase his marketability through education, the market in which he then shops is full of educated women who are strongly affected by the time expended on education. There is also the issue of the timing of income, with many years of student life preceding the income boost. By the time the income arrives, the eligible partners are older. But why then would these potential partners not assess their mate’s expected income? Is there too much uncertainty?\nUltimately, an evolutionary perspective does not hand you the answer on a plate. Humans are now exposed to an environment so far from their environment of evolutionary adaptedness that many of our actions can only be considered sub-optimal from the crude perspective of biological fitness. The rationale for the investment in education will likely have evolutionary elements, such as acquisition of resources and signalling to mates and allies, but the motivations driving those actions do not deliver fitness benefits like they once did. The proximate objective of attracting a high quality mate does not guarantee the ultimate objective."
  },
  {
    "objectID": "posts/entanglement.html",
    "href": "posts/entanglement.html",
    "title": "Entanglement",
    "section": "",
    "text": "There is an interesting podcast on Science Talk titled The Coming Entanglement, with Fred Guterl interviewing Bill Joy, co-founder of Sun Microsystems, and Danny Hillis.\nThe basic argument by Joy and Hillis is that we are reaching a point where our systems are so entangled (most of the conversation is in terms of technical systems) that no one understands the whole thing. They suggest that we need to get over the fact that there is no expert who understands what is going on.\nOne example is the Windows Operating System, with no-one understanding the entire code. That contrasts to their earlier days when they could hold an entire operating system code in their heads. Another example they gave was that no one knows what would happen if GPS got turned off.\nThe consequence of this is that when something goes wrong, we no longer have the ability to step back and determine what went wrong. Each system so dependent on the other that you might not be able to restart one system without the other, so cannot restore two mutually dependent systems.\nUnlike Y2K, which they (not sure which of the two said this) were optimistic about, the entanglement has moved far beyond the level of complication that existed at that time.\nThis sounds somewhat like the concept of normal accidents, whereby complex systems will fail. The solution does not lie in preventing accidents, but in decoupling from that system so that the collateral damage is minimised when it surely fails."
  },
  {
    "objectID": "posts/eugenics-and-regression-to-the-mean.html",
    "href": "posts/eugenics-and-regression-to-the-mean.html",
    "title": "Eugenics and regression to the mean",
    "section": "",
    "text": "Richard Conniff’s Yale Alumni Magazine articleon Irving Fisher’s support of eugenics has some great stories (HT Arnold Kling), but one of the closing paragraphs caught my attention.\n\nWe know better now, of course. And yet eugenic ideas still linger just beneath the skin, in what seem to be more innocent forms. We tend to think, for instance, that if we went to Yale, or better yet, went to Yale and married another Yalie, our children will be smart enough to go to Yale, too. The concept of regression toward the mean—invented, ironically, by Francis Galton, the original eugenicist—says, basically: don’t count on it. But outsiders still sometimes share our eugenic delusions. Would-be parents routinely place ads in college newspapers and online offering to pay top dollar to gamete donors who are slender, attractive, of the desired ethnic group, with killer SAT scores—and an Ivy League education.\n\nRegression to the mean often gets trotted out in discussion of eugenics or human evolution, but it is a misunderstanding of the concept. If we eliminated the bottom half of the population by measured IQ, the average IQ of that population will change as the underlying genotype will have changed. The next generation will not have as high IQ as the survivors, as we would have eliminated some people with high-IQ genotypes who had low phenotypic IQ due to luck or environment, and vice versa. However, that population and their descendants will have a higher mean than the original population. Gregory Clark amusingly pointed out the evolutionary implications of the regression to the mean argument in response to critiques (pdf) of his book A Farewell to Alms.\n\nAdapting one of Bowles’s points McCloskey tries to land a knockout blow. Regression to the mean would in a few generations destroy any effects of ‘survival of the richest’ on behavior, by taking descendants back to the average characteristics of the population. Such selection could thus only influence behavior for any descendants of the economically successful for a few generations.\nThis is just a misunderstanding of the concept of regression to the mean. If McCloskey was right, farmyard animals would all be at their medieval sizes still, and instead of the wonderful modern extravagance of dog breeds, all dogs would have the characteristics of wolves and would make bad house pets. As a further reductio ad absurdum, humans would never have evolved from apes in the first place. Why haven’t creationists latched onto this wonderful insight, which, according to McCloskey, Galton, the great social Darwinist, fully appreciated (and yet clung to social Darwinism)?\n\nThe problem with eugenics is not that it could not change the average of traits in the population. The stronger arguments rest on human decency, freedom from interference, untended consequences and the tyranny of government.\nConniff’s example of seeking a high-IQ Yalie as a mate has another amusing problem. The process of selecting mates and the evolutionary result is known as sexual selection. As described in Geoffrey Miller’s The Mating Mind, sexual selection may have shaped our brains as those with higher intelligence were more reproductively successful.  Those who did not seek intelligent, attractive mates are no longer with us as their unintelligent, ugly children left no descendants. The process of seeking suitable mates described by Conniff , whatever the difficulty in observing the traits we care about, is not delusional. It is also a large reason why we are as we are."
  },
  {
    "objectID": "posts/europeans-and-economic-growth.html",
    "href": "posts/europeans-and-economic-growth.html",
    "title": "Europeans and economic growth",
    "section": "",
    "text": "A new NBER paper by Bill Easterly and Ross Levine proposes that a large proportion of global development is attributable to European settlement, even where Europeans formed a small minority of the population. The abstract:\n\nA large literature suggests that European settlement outside of Europe shaped institutional, educational, technological, cultural, and economic outcomes. This literature has had a serious gap: no direct measure of colonial European settlement. In this paper, we (1) construct a new database on the European share of the population during the early stages of colonization and (2) examine its impact on the level of economic development today. We find a remarkably strong impact of colonial European settlement on development. According to one illustrative exercise, 47 percent of average global development levels today are attributable to Europeans. One of our most surprising findings is the positive effect of even a small minority European population during the colonial period on per capita income today, contradicting traditional and recent views. There is some evidence for an institutional channel, but our findings are most consistent with human capital playing a central role in the way that colonial European settlement affects development today.\n\nThe authors are agnostic on the reason for the relationship:\n\n[W]e do not identify a single mechanism through which the European share of the population during colonization shaped long-run economic development. We show that European share is strongly associated with human capital and democratic political institutions today, but we do not trace the impact of Europeans on human capital and political institutions over time, nor do we exclude other potential mechanisms through which the European share of the population during colonization might influence economic development.\n\nHow does the proposal in this paper fit with Easterly’s usual skepticism about what the West can do for the rest of the world?\nI will write a longer post on the paper when I have given it some more attention.\nHT Nicholas Gruen"
  },
  {
    "objectID": "posts/evolution-and-irrationality.html",
    "href": "posts/evolution-and-irrationality.html",
    "title": "Evolution and irrationality",
    "section": "",
    "text": "In a classic behavioural economics story, research participants are offered the choice between one bottle of wine a month from now and two bottles of wine one month and one day from now (alternatively, substitute cake, money or some other pay-off for wine). Most people will choose the two bottles of wine. However, when offered one bottle of wine straight away, more people will take that bottle and not wait until the next day to take up the alternative of two bottles. This suggests that people discount the value of goods received after short delays at a higher rate than they do for longer delays.\nThis set of decisions could be argued to be irrational. To understand why, suppose you face the first set of choices for one or two bottles of wine in 30 or 31 days. You choose the two bottles. Then, on the 30th day, you are allowed to reconsider your decision, which is effectively making the choice in the second scenario above. Some people will change their mind and take the single bottle of wine. Why would they make one decision at one point of time and then change their mind later? This preference reversal is a result of what is called time inconsistency, which some consider to be evidence of irrationality.\nWhile the evidence of time inconsistent behaviour has grown, evolutionary explanations of how rates of time preference could have evolved generally do not generate these preference reversals. Time preference is consistent as any genes that increase an individual’s predisposition to have irrational decision rules should be progressively eliminated from the population. In most papers on time preference, such as those by Hansson and Stuart, Rogers and Robson and Samuelson, decisions are time consistent.\nOne useful paper in this area is by Peter Sozou, who seeks to offer a basis for this behaviour, which could be applied in an evolutionary context. The model in the paper matched the intuition I have in my head, so it is nice to come across a paper that formalises the concept.\nSozou’s idea is that uncertainty as to the nature of any underlying hazards can explain time inconsistent preferences. Suppose  there is a hazard that may prevent the pay-off from being realised. This would provide a basis (beyond impatience) for discounting a pay-off in the future. But suppose further that you do not know what the specific probability of that hazard being realised is (although you know the probability distribution). What is the proper discount rate?\nSozou shows that as time passes, one can update their estimate of the probability of the underlying hazard. If after a week the hazard has not occurred, this would suggest that the probability of the hazard is not very high, which would allow the person to reduce the rate at which they discount the pay-off. When offered with a choice of one or two bottles of wine 30 or 31 days into the future, the person applies a lower discount rate in their mind than for the short period because they know that as each day passes in which there has been no hazard preventing the pay-off, their estimate of the hazard’s probability will drop.\nThis example provides a nice evolutionary explanation of the shape of time preferences. In a world of uncertain hazards, it would be appropriate to apply a heavier discount rate for a short-term pay-off. It is rational and people who applied that rule would not have lower fitness than those who apply a constant discount rate.\nWhile this is a neat scenario, it does leave some questions open. The most obvious is that in many of the experiments that have demonstrated time-inconsistent preferences, there is clearly no hazard. The pay-off is near certain. We could question whether time-inconsistent behaviour under certainty is simply an evolutionary hang-up from more hazardous and uncertain times - although those types of explanations seem to be a “just-so” story.\nIf Sozou’s explanation represents an underlying predisposition, it also seems that some people are better at overcoming it than others. As I have blogged about before, people vary widely in their ability to delay gratification (with strong links to life outcomes), and variation can be seen across countries. If this trait is sitting in our sub-conscious, it seems that some people are far better at putting aside that urge to discount in a time-inconsistent manner in situations where the pay-off is certain to occur.\nThere are also some questions about what form the probability distribution of the underlying hazard needs to take to generate the form of time-inconsistency shown in experiments. In Sozou’s paper, he used an exponential probability distribution, and sensitivity analysis showed that this could be relaxed somewhat. However, the question becomes what types of hazards humans faced during their evolution and what the probability distributions of these hazards are. To look at this question, Sozou suggests some cross-species analysis to examine discount rates and the particular ecological hazards faced by those species.\nOne other outstanding issue is that this explanation offered in the paper does not explain the irrationality in the example I used above. If someone did originally accept the two bottles of wine at 31 days, under Sozou’s model they would not change their mind at day 30 if given the chance. They now have 30 days of observation of the underlying hazard rate and would not want to discount the remaining day of waiting at a high rate. Irrationality of this form is still not explained."
  },
  {
    "objectID": "posts/evolution-and-the-invisible-hand.html",
    "href": "posts/evolution-and-the-invisible-hand.html",
    "title": "Evolution and the invisible hand",
    "section": "",
    "text": "In David Sloan Wilson’s blog series Economics and Evolution as Different Paradigms (which I have recently posted about here and here), Wilson discusses the invisible hand metaphor that is used in economics.\nThe invisible hand metaphor comes from Adam Smith’s Wealth of Nations, and although the particular use of the invisible hand metaphor by Smith relates to preferring domestic to foreign industry, the metaphor has come to encapsulate this broader idea of Smith:\n\nIt is not from the benevolence of the butcher, the brewer or the baker, that we expect our dinner, but from their regard to their own self interest. We address ourselves, not to their humanity but to their self-love, and never talk to them of our own necessities but of their advantages.\n\nWilson contrasts this view of self-interest enhancing the common good with an evolutionary perspective in which a slacker or exploiter within a group will have an evolutionary advantage, suggesting that natural selection favours traits that undermine the common good. To reconcile evolution and the invisible hand, Wilson states that group-level selection must operate as a counterbalance to within-group selection. He suggests that the invisible hand can operate because:\n\n[M]any of our psychological traits evolved by virtue of causing groups to survive and reproduce better than other groups, not by causing individuals to best members of their own groups.\n\nI am not sure that a group selection argument is required to show why the invisible hand metaphor might have some value.  (In fact, I am not sure the group selection arguments put forward by Wilson add much to evolutionary theory - when I was looking around to understand what precisely is Wilson’s position, I found this great presentation by Stuart West which seems to nail all the main arguments in less than 20 minutes.) To explain why the invisible hand metaphor has such power, I’d prefer a simpler explanation around the need to produce something of value in a market economy.\nWhere I do (obliquely) agree with Wilson is that evolutionary theory can show that when everyone acts in their own self-interest, there can be negative aggregate outcomes. Economics has no shortage of those - the tragedy of the commons and failure to internalise externalities such as pollution being the classics. Over-investment in signalling would be another. However, while recognising that there can be (and are) problems, I’m generally fairly optimistic. But this is not through any principle of evolution, or group-selection based argument. Wilson’s following illustration provides an indication why.\nOver his next two posts (here and here), Wilson looked at the work of Elinor Ostrom, winner of the Nobel Memorial Prize in Economic Sciences. He considered that Ostrom’s work illustrated the invisible hand operating through group selection. Ostrom’s career focused on the use of common pool resources and how institutional arrangements could emerge to manage those resources and often avoid the “tragedy of the commons” despite there being no private ownership or government regulation.\nWilson sees Ostrom’s work as showing that life is full of situations where the invisible hand does not operate. Behaviours for the good of the group require constraint and coordination.\nI take a different perspective on Ostrom’s work. Ostrom’s work expands the range of situations where it can be argued that positive outcomes can emerge without top down interference. The rules and norms that develop to manage the resource could be argued to be as driven by the invisible hand as the distribution and the use of the resource itself. This reflected my familiarity with Ostrom’s work via libertarian and Austrian (school) economists who used her work to show that even the tragedy of the commons does not always require a solution to be imposed from above.\nOstrom identified a number of conditions for these emergent solutions, which Wilson sees as being a mix of conservative and liberal principles. While there is local autonomy, Wilson argues that the groups, not individuals, must have autonomy. He also suggests that there is a need for a framework to govern relationships between groups. This leads to a middle-ground on which political discourse can be held.\nI am not sure how Wilson saw Ostrom’s work as promoting a middle-ground of discourse, nor of it being a mix of liberal and conservative principles. Where the conditions identified by Ostrom for these emergent local solutions are not met, we are back to the usual dichotomy between state-ownership and top down control versus private ownership. If they are met, the libertarians have their way.\n*My four posts on David Sloan Wilson’s Economics and Evolution as Different Paradigms can be found here, here, here and here."
  },
  {
    "objectID": "posts/evolution-the-human-sciences-and-liberty-meeting.html",
    "href": "posts/evolution-the-human-sciences-and-liberty-meeting.html",
    "title": "Evolution, the Human Sciences and Liberty meeting",
    "section": "",
    "text": "I had the following Mont Pelerin Society Special Meeting pointed out to me. It has a great bunch of speakers - Robert Boyd, Robin Dunbar, Leda Cosmides, Matt Ridley, Richard Wrangham, Pascal Boyer and Gary Becker among them. Not a bad location either, if you ignore the expense. Unfortunately, its for MPS members and their guests only.\n\nEvolution, the Human Sciences and Liberty\nWhat?\nThis Mont Pelerin Society Special Meeting has the objective to link the concept of evolution to freedom, reinforce the debate that opposes classical liberal society and statism using biology and anthropology as theoretical foundations, and to understand cultural evolution of open societies as a mean to escape from the tribal order.\nThe Universidad San Francisco de Quito (USFQ), from Ecuador, will host this world summit on its Galapagos campus (GAIAS) located on the island of San Cristóbal.\nWhy?\nFriedrich Hayek asserted that: “cultural evolution is not the result of human reason consciously building institutions, but of a process in which culture and reason developed concurrently…”. The co-evolution of human preferences and institutions poses serious problems to anyone who promotes policies that supposedly will alter only one of the two. It is the old problem of culture versus institutions. Freedom, property rights, rule of law, how is it that all these elements evolved to promote peace and prosperity? Why some are more prominent only in some societies while in others they are almost inexistent? During this world summit, scholars with training in the natural and social sciences will gather to discuss the evolution of and the current challenges to freedom. Galapagos provides a unique environment for this; it inspired Charles Darwin, more than one hundred fifty years ago, to make his groundbreaking contributions to the biological sciences."
  },
  {
    "objectID": "posts/evolutionary-economics-and-group-selection.html",
    "href": "posts/evolutionary-economics-and-group-selection.html",
    "title": "Evolutionary economics and group selection",
    "section": "",
    "text": "As my research intersects economics and evolution, I have found it inconvenient that the term “evolutionary economics” is already taken. Evolutionary economics is an area of economics inspired by biological processes, with interactions between firms, industries and institutions examined using evolutionary methods. The economics is evolutionary by analogy.\nI find the ideas in evolutionary economics attractive, which is natural given my interest in complex systems, out-of-equilibrium processes and the dynamic, emergent properties of economies. However, each time I read an evolutionary economics paper or book, I wonder if they are looking at the right level of selection. Should the agents in the models be firms or should they be the employees or managers of those firms (or their genes)? Putting it more bluntly, is evolutionary economics based on an inappropriate use of group selection?\nThe actions of firms in the lead up to the financial crisis provides an illustration. Could the web of financial firms and their interactions be usefully modelled without consideration of the range of incentives faced by employees and managers? Think Dick Fuld, his brinkmanship around saving Lehman brothers and the half a billion he was left with after it all went bad. An evolutionary economic model of this sector at the firm level might miss the major incentives (this could lead us to my previous question of what the objectives of these agents are).\nSo why we don’t start from a biological basis to begin with and then work up? Evolutionary economics would then become evolutionary in the truest sense. The flip side is that it is already difficult to model the interactions between firms. Adding more layers of employees, managers, creditors and shareholders may make the model more opaque, need a more complex set of assumptions and be more difficult to interpret. After all, the purpose of a model is to offer a set-up simple enough for analysis.\nTo assess what is the right balance, a fair starting point for analysis of an evolutionary economics paper is to ask whether the model would give the same predictions if an alternative, lower level of selection was examined? If not, it may be time for that evolutionary economic model to be evolutionary in fact and not by analogy."
  },
  {
    "objectID": "posts/evolutionary-policy-making.html",
    "href": "posts/evolutionary-policy-making.html",
    "title": "Evolutionary policy making",
    "section": "",
    "text": "Project Syndicate has published one of the last pieces by Elinor Ostrom, in which she gives her views on the upcoming Rio+20 summit. The article reflects Ostrom’s wariness of blanket, top-down solutions:\n\nInaction in Rio would be disastrous, but a single international agreement would be a grave mistake. We cannot rely on singular global policies to solve the problem of managing our common resources: the oceans, atmosphere, forests, waterways, and rich diversity of life that combine to create the right conditions for life, including seven billion humans, to thrive.\n\nOstrom prefers a more evolutionary form of policy making, and notes that this is occurring in many places.\n\nThe good news is that evolutionary policymaking is already happening organically. In the absence of effective national and international legislation to curb greenhouse gases, a growing number of city leaders are acting to protect their citizens and economies.\nThis is hardly surprising – indeed, it should be encouraged. …\nWhen it comes to tackling climate change, the United States has produced no federal mandate explicitly requiring or even promoting emissions-reductions targets. But, by May last year, some 30 US states had developed their own climate action plans, and more than 900 US cities have signed up to the US climate-protection agreement.\n\nTim Harford noted in Adapt that for successful evolutionary decision-making, there are two steps beyond experimentation of the type noted by Ostrom. There must be a willingness to decide when policies are failing, and failure must be survivable. Policy experimentation must include a willingness to measure and change.\nHarford’s additional steps are particularly important when we note the potential for evolutionary processes to go astray. As I quoted in my last post, Ostrom was aware that evolutionary paths do not always lead to the optimal solution.\nAnother element of evolutionary policy making worth noting is that for those subject to a new city scale policy, it will not feel as though it is an experiment, and the citizens bear the costs it imposes. Take Mayor Bloomberg’s proposal to ban large soda drinks in New York. It is not a blanket national response and allows for a smaller scale experiment than a national ban would entail. But what might be described as policy experimentation at a national level is top down decree for the residents of the city."
  },
  {
    "objectID": "posts/evolutionary-psychology-fertility-and-economic-ambition.html",
    "href": "posts/evolutionary-psychology-fertility-and-economic-ambition.html",
    "title": "Evolutionary psychology, fertility and economic ambition",
    "section": "",
    "text": "Since the time of Darwin, the same evolutionary psychology debates have played out over and over. Here is Ronald A. Fisher in The Genetical Theory of Natural Selection (1930), addressing the type of argument that you can still hear today about the evolution of the human mind:\n\n[I]t is often felt to be derogatory to human nature, and especially to such attributes as man most highly values — as if I had said that the human brain was not more important than the trunk of an elephant, or as if I had said that it ought not to be more important to us, if only we were as rational as we should be. These statements would be unnecessarily provocative: in addition they are scientifically void. And lest there should be any doubt upon a matter, which does not in the least concern science, I may add that, being a man myself, I have never had the least doubt as to the importance of the human race, of their mental and moral characteristics, and in particular of human intellect, honour, love, generosity and saintliness, wherever these precious qualities may be recognized. … [N]atural causation … introduces the strongest motive for striving to know, as accurately and distinctly as possible, in what ways natural causes have acted in their evolutionary upbuilding, and do now act in making them more or less abundant.\n\nLess played out today are Fisher’s concerns about the direction of the selective forces acting upon our mental traits. Consider the interaction between the desire for economic acquisition and fertility, for which Fisher appears equivocal about which way the selective pressures might be acting. First he notes that those with economics ambition tend to curtail their fertility:\n\nParents in whom economic ambition is strong, will, in like circumstances, be more inclined to limit their families than those in whom it is weak. Consequently a progressive weakening of the economic ambition, or at least in the average intensity with which this motive is felt among the great body of citizens, is to be expected as a concomitant to the strengthening of the moral aversion towards family limitation.\n\nBut where there is potential for famine, economic ambition is required to survive.\n\nThe attitude of men and women towards their economic welfare cannot, however, be ordinarily reduced by this cause to indifference, for in countries in which the poorest class are frequently decimated by famine, it is apparent that a stage will be reached at which what is gained in the birth-rate is lost in the death-rate; and even where the extremes of distress are ordinarily avoided, some loss of civil liberty, and of the opportunities for reproduction, has been the common effect of indigence.\n\nAnd even in developed countries, acquisitive impulses may be required.\n\nMoreover the rational pursuit of economic advantage must, even in the most civilized countries, frequently place the individual in a position favourable to normal reproduction. It would, apparently, in most societies, be as disastrous to the biological prospects of the individual to lack entirely the acquisitive instincts as to lack the primary impulses of sex, notwithstanding that the abuse of either passion must meet with counter-selection. The moral attitude of civilized peoples towards money, as towards sex, must be therefore the product of much more complicated evolutionary forces than is his attitude towards infanticide or feticide, as might perhaps be inferred from the hypocrisy and fanaticism, the passions and the passionate inhibitions, found among long-civilized peoples on both subjects."
  },
  {
    "objectID": "posts/evolutionary-strategies.html",
    "href": "posts/evolutionary-strategies.html",
    "title": "Evolutionary strategies",
    "section": "",
    "text": "In Tim Harford’s discussion in Adapt of the benefits to experimentation , Harford notes that experimentation by individuals is often at great potential cost. As when a species evolves, what appears to be beneficial experimentation at a societal level involves frequent failure to survive by individuals. While Harford suggests that people should consider experimenting in a manner that avoids failure that threatens survival, the reality is that many people take risks with a large downside.\nHarford’s discussion reminded me of a section in Daniel Kahneman’s Thinking, Fast and Slow, where Kahneman notes the economic benefits of optimism. Entrepreneurs often engage in “optimistic” behaviour, in that they vastly overestimate the probability of business success. Kahneman writes:\n\nThe chances that a small business will survive for five years in the United States are about 35%. But the individuals who open such businesses do not believe that the statistics apply to them. A survey found that American entrepreneurs tend to believe they are in a promising line of business: their average estimate of the chances of success for “any business like yours” was 60% — almost double the true value. The bias was more glaring when people assessed the odds of their own venture. Fully 81% of the entrepreneurs put their personal odds of success at 7 out of 10 or higher, and 33% said their chance of failing was zero. …\nThe optimistic risk taking of entrepreneurs surely contributes to the economic dynamism of a capitalistic society, even if most risk takers end up disappointed.\n\nThey do not consider their conduct to be risk seeking, as the odds they have calculated suggest they believe they are taking a reasonable bet. Kahneman notes that we need optimistic people to run these experiments, as they are responsible for the economic dynamism in a capitalist society.\nThe penchant for entrepreneurial activity in people suggests that such optimistic and apparently risk-seeking behaviour may have evolutionarily foundations. Take the historical case where members of a clan left for new territories with significant potential for death. Those who departed were usually on the fringe of the group or faced severe resource constraints. As a result, even if a move to a new frontier was likely to end badly, the low benefits to staying and the potential upside of departing make the departure worthwhile. Or consider the evolutionary bonanza for the early settlers to eastern Canada and the north-eastern United States, who often experienced order of magnitude population increases within five generations of arrival. Many moved from being fringe members of societies they left to comprising significant portions of the new populations.\nThis brings me to a post by Bryan Caplan on the benefits of meekness. Caplan suggests those at the bottom of the social ladder are “dysfunctionally assertive” and would benefit from being meeker. Caplan states that advice to be assertive often comes from those with power who can afford to be assertive. When someone with low-status stands-up, it may be disastrous.\nIf we ignore whether there is evidence of the lower classes being more assertive, it raises the question of whether assertive behaviour of low-status people is, on average, costly. There may be a low expected monetary pay-off to an assertive act as they lose jobs or other privileges, but what are the costs and benefits in dimensions that matter? Is the person balancing a meek and certain passage into genetic oblivion, versus a risky shot at reproductive success? We should not derive the optimal strategy by a simple cost-benefit analysis in monetary terms. When facing the end of the genetic line, strategies with an expected negative monetary and social pay-off, but high variance, may be the preferred path.\nThis issue is similar to a paper I posted about concerning risk-seeking behaviour by those without a mate. A utility function measured in terms of mates leads to significantly different behavioural predictions than one in financial terms."
  },
  {
    "objectID": "posts/excess-males.html",
    "href": "posts/excess-males.html",
    "title": "Excess males",
    "section": "",
    "text": "Robin Hansen writes on the sex selective abortion of females:\n\nA simple supply and demand analysis says that selective abortion both expresses a preference for boys and causes a reduction in that preference as wives become scarce. In South Korea this process is mostly complete, with excess girls down from 15% in the 1990s to 7% today (with ~5% as the biologically natural excess). …\n\nOver an evolutionary time scale, the pay-offs to male and female children is, on average, equal, which would tend to balance numbers. Over shorter time scales, parents adjust socially, whether that be due to a demand response or changing preferences with income. However, females born today will not affect the ratio of males to females in their reproductive prime for another 15 to 20 years.  The ratio of males to females in China in the 0-4 years age brackets was 110 in 1990, 120 in 1999 and 123 in 2005. We have 20 to 30 years of excess males in the pipeline before any demand response can be realised. The long-term self-correction does not change the short-term issue. (It should also be noted that these ratios are higher than the sex ratio at birth, suggesting there is post-birth mistreatment and infanticide - the preference is not only “benignly” expressed by selective abortion.)\nA more interesting point is what we mean when we say there are excess males. Mating is not simply a case of equal numbers matching equal numbers. Females have a stronger preference for quality. Men seek quantity, but quality is not irrelevant.\nTake the following chart of Australian marriage rates (data from Heard (2011)):\n\nThere is no shortage of unmarried males in low-income groups even where there is no (or minimal) sex selective abortion. Women care about quality. It could be argued that in Australia, without sex selective abortion, there is a shortage of high-quality males.\nSo, what is the status of the excess males in China? In the article that triggered Hansen’s post, Nicholas Eberstadt writes:\n\nFor one thing, abnormal sex ratios appear to be almost entirely a Han phenomenon within China — and China’s Han are, generally speaking, better educated and more affluent than the country’s non-Han minorities.\n\nFrom a female perspective, this may be good news as more of the product they desire is being produced - although quality is relative. For low-status males, the equation is worse."
  },
  {
    "objectID": "posts/exploring-genes.html",
    "href": "posts/exploring-genes.html",
    "title": "Exploring genes",
    "section": "",
    "text": "David Dobbs has written a great National Geographic piece on the human compulsion to explore. At the centre of the article is the question of genetic influence.\nFirst, on whether migration has a genetic basis:\n\n[T]here is a mutation that pops up frequently in such discussions: a variant of a gene called DRD4, which helps control dopamine, a chemical brain messenger important in learning and reward. Researchers have repeatedly tied the variant, known as DRD4-7R and carried by roughly 20 percent of all humans, to curiosity and restlessness. …\nMost provocatively, several studies tie 7R to human migration. The first large genetic study to do so, led by Chuansheng Chen of the University of California, Irvine in 1999, found 7R more common in present-day migratory cultures than in settled ones. …\nAnother recent study backs this up. Among Ariaal tribesmen in Africa, those who carry 7R tend to be stronger and better fed than their non-7R peers if they live in nomadic tribes, possibly reflecting better fitness for a nomadic life and perhaps higher status as well. However, 7R carriers tend to be less well nourished if they live as settled villagers. The variant’s value, then, like that of many genes and traits, may depend on the surroundings. A restless person may thrive in a changeable environment but wither in a stable one; likewise with any genes that help produce the restlessness. …\n\nI’m not sold on the DRD4-7R variant story yet, but I am open to the idea that there is a basket of traits that differ between migrants and those who stay. If that is the case, the interesting question is what effect those traits have in other spheres. For example, Galor and Michalopoulos note the DRD4-7R variant in their argument that selection for entrepreneurial traits plays a role in the economic development process. Are migrants naturally more entrepreneurial?\nOf course, those who migrate may be different from those they leave simply through chance:\n\n[A] migratory wave can concentrate not just particular types of people on its frothy front edge; it can also concentrate and aid the expansion of any genes that may encourage those people to migrate.\nSometimes a gene rides such a wave passively, more or less by accident—the gene just happens to be common in the leading migrators, so it becomes common in the communities they establish. …\nBut a migratory wave can also allow genes friendly to migration to drive their own selection. …\nLaurent Excoffier, a population geneticist at the University of Bern, thinks something similar occurred with the Quebec loggers. In a 2011 paper Excoffier and some colleagues analyzed centuries of Quebec parish birth, marriage, settlement, and death records and found that the pioneer families behaved and bred in a way that spread both their genes and the traits that drove them to the front. These wave-front couples married and mated sooner than did couples back home, perhaps because they were more impatient folks to begin with and because the frontier gave them access to land and a social atmosphere favorable to starting sooner. This alone produced more children than the “core” families who stayed behind did (9.1 per family versus 7.9, or 15 percent more). … In this case it rapidly raised the share of these families’ genes and cultures within their own population—and thus within North America’s larger population.\nExcoffier believes that if this “gene surfing,” as some call it, happened often as humans scattered around the globe, it would have selected for multiple genes that favor curiosity, restlessness, innovation, and risk taking. This could, he says, “help explain some of our exploratory behavior.”\n\nMigration provides an opportunity for strong selection through the new environment that the migrants are exposed to, possibly favouring traits that were neutral or deleterious in the earlier environment. Even if migrants are no different from those they leave, it may not stay the case for long.\nAs an end note, Razib rightfully asks whether heritability is a better place to focus in exploring this genetic link than looking for genes such as DRD4-7R. I think this is the case in a lot of fields in the short term, including genoeconomics."
  },
  {
    "objectID": "posts/failure-to-respond-as-a-measure-of-conscientiousness-and-iq.html",
    "href": "posts/failure-to-respond-as-a-measure-of-conscientiousness-and-iq.html",
    "title": "Failure to respond as a measure of conscientiousness and IQ",
    "section": "",
    "text": "A few weeks ago, Bryan Caplan pointed out this great working paper by David Hedengren and Thomas Stratmann:\n\nThe Dog that Didn’t Bark: What Item Non-Response Shows about Cognitive and Non-Cognitive Ability \nWhat survey respondents choose not to answer (item non-response) provides a useful task based measure of cognitive ability (e.g., IQ) and non-cognitive ability (e.g., Conscientiousness). Using the German Socio-Economic Panel (SOEP) and the National Longitudinal Survey of Youth 1997 (NLSY97), we find consistent correlation between item non-response and traditional measures of IQ and Conscientiousness. We also find that item non-response is more strongly correlated with earnings in the SOEP than traditional measures of either IQ or Conscientiousness. We also use the Survey of Income and Program Participation (SIPP) Gold Standard, which has no explicit measure of either cognitive or non-cognitive ability, to show that item non-response predicts earnings from self-reported and administrative sources. Consistent with previous work showing that Conscientiousness and IQ are positively associated with longevity, we document that item non-response is associated with decreased mortality risk. Our findings suggest that item non-response provides an important measure of cognitive and non-cognitive ability that is contained on every survey.\n\nThe triumph of this paper is the mass of survey data to which this technique can be applied. As the authors state in the opening paragraph:\n\nStudying the importance of non-cognitive skills, such as conscientiousness, perseverance, and motivation, has been hamstrung by the fact that many popular data sets in economics do not contain information on an individual’s personality traits. However, surveys contain a valuable but neglected piece of data: what respondents do not say. Respondents skip, refuse to answer, or claim ignorance on at least a few questions in virtually all surveys. When a respondent forgets to fill in answers to some questions on the survey form, or refuses to provide an answer to the interviewer, we gain important information about the respondent."
  },
  {
    "objectID": "posts/ferguson-on-malthus-again.html",
    "href": "posts/ferguson-on-malthus-again.html",
    "title": "Ferguson on Malthus again",
    "section": "",
    "text": "Niall Ferguson has a slight Malthusian thread running through his book, Civilization: The West and the Rest. At one point, Ferguson touches on the mass emigration from England to the Americas. Ferguson writes:\n\n[A]s England’s population accelerated in the late seventeenth century, overseas expansion played a vital role in propelling the country out of the Malthusian trap. Transatlantic trade brought an influx of new nutrients like potatoes and sugar – an acre of sugar cane yielded the same amount of energy as 12 acres of wheat – as well as plentiful cod and herring. Colonization allowed the emigration of surplus population. Over time, the effect was to raise productivity, incomes, nutrition and even height.\n…\nThe Chinese and Japanese route – turning away from foreign trade and intensifying rice cultivation – meant that with population growth, incomes fell, and so did nutrition, height and productivity. When crops failed or their cultivation was disrupted, the results were catastrophic.\n\nIf people are the ultimate resource, emigration of surplus population would have negative consequences. A lower population will generate fewer ideas. However, the surplus population that emigrated to the Americas was not completely lost to Europe. Those people continued to innovate, and ideas could flow between Europe and its colonies.\nFerguson suggests that emigration is positive as it loosens the Malthusian bindings on the population. It may have been the case for a time. Yet, the English population more than tripled between 1740 and 1860 as the flow of English emigrating declined and fertility boomed. It was during this population boom that per person income finally reached the levels seen in the low population period after the black death. England exited the Malthusian state at a time that the Malthusian model would suggest it was least likely to occur."
  },
  {
    "objectID": "posts/fergusons-civilization-the-west-and-the-rest.html",
    "href": "posts/fergusons-civilization-the-west-and-the-rest.html",
    "title": "Ferguson’s Civilization: The West and the Rest",
    "section": "",
    "text": "With the cover of Niall Ferguson’s Civilization: The West and the Rest stating that it is “Now a Major Channel Four Series”, I should have foreseen the pace and structure of the book would be designed for entertainment and not presenting a painstakingly worked-through framework. Ferguson attributes the West’s ascension to six “killer apps”: competition, science, property rights, medicine, the consumer society and work ethic. As the West possessed all six of these apps, it was able to dominate the world for 500 years, with no other region possessing the full combination.\nFor the first three, Ferguson undertakes a pairwise comparison with other regions - the competition in Europe compared to China, the use of science in the West compared its suppression in the Muslim world and the introduction and broad spread of property rights in North America compared to the centralised and highly concentrated property ownership in South America. While each of these comparisons provides a strong argument about why these features are important, it sometimes leaves open the question of how this comparison would apply to other countries.\nTake the explanation for property rights. Ferguson attributes the difference in property rights in North and South America to differences in the way the colonial powers allocated land. In the North, almost any new immigrant could eventually be a property owner and the institutions introduced supported the productive use of this land. In South America, a few people monopolised the land available, with most of the population effectively serfs. This comparison works well, but why did many other British colonies, such as those in Asia and Africa not have the same benefit. Ferguson may have a simple explanation, but I would like to hear it (My guess would relate to prevalence of indigenous people, one of the factors in the North-South America divide. For the United States in 1825, less than 4 per cent of the population was indigenous, allowing land to be freely allocated to new immigrants).\nFerguson does come to Africa in his discussion of medicine. However, this is where the television series drivers seem to have taken over. While Ferguson suggests the West introduced new medicines to Africa, there is little discussion in how this aided the West’s advantage. Instead, much of the discussion focuses on eugenics and the use of African troops in War. Interesting topics, yes, but the discussion I was expecting was missing. If anything, medicine’s introduction appears to be a benefit, unless you consider that there may be Malthusian consequences to higher population.\nMy usual critique of these kinds of books is that they spend too much time considering the incentives and actions of States and not enough of the people within them. On this, Ferguson comes closer to examining the incentives and actions of people than many other grand histories (such as Morris’s Why the West Rules … For Now), but he does not weave this fully into his narrative. For example, in his discussion of competition, he notes the competition between states in Europe that is absent in China. He then notes the competition within European states, such as the city versus the crown and the professions against each other. There is competition at all levels. However, he does not match this with discussion of what competition occurred between people in Chinese cities. Similarly, the Ottoman Empire may have suppressed science at the State level, but why were Muslim entrepreneurs not engaging in innovation to compete in commerce?\nI found the most interesting topic to be the last - Ferguson’s discussion of religion. Ferguson argues that the work ethic in the West largely reflected the spread of Protestantism. This topic allows Ferguson to discuss whether the West will decline. As Ferguson notes, religion is declining in Europe, as “4 per cent of Norwegians and Swedes and 8 per cent of French and Germans attend a church service at least once a week, compared with 36 per cent of Americans, 44 per cent of Indians, 48 per cent of Brazilians and 78 per cent of sub-Saharan Africans.” While the American-European comparison is Ferguson’s main point, the sub-Saharan African figure stands out. Ferguson also talks of the rise of Christianity in China. However, having noted these trends, he also points out that the focus of religion in the United States has changed from one of saving and service to one of consumption. His discussion of the topic left me confused about how he saw these trends playing out. Are these changes in religious trends driving growth? It also highlights the usual correlation-causation question. Does someone work harder because they are a protestant or are they a protestant as it appeals to hard-working people? Or is there another relevant factor?\nFerguson does not convert these observations into bold predictions. He spends some time noting the complexity of global political systems, with small changes possibly leading to sudden, discontinuous changes. While the decline of empires may seem slow and obvious in retrospect, the fall is often sudden and unforeseen. As Ferguson writes:\n\nIt is historians who retrospectively portray the process of dissolution as slow-acting, with multiple over-determining causes. Rather, civilizations behave like all complex adaptive systems. They function in apparent equilibrium for some unknowable period. And then, quite abruptly, they collapse. To return to the terminology of Thomas Cole, the painter of The Course of Empire, the shift from consummation to destruction and then to desolation is not cyclical. It is sudden. A more appropriate visual representation of the way complex systems collapse may be the old poster, once so popular in thousands of college dorm rooms, of a runaway steam train that has crashed through the wall of a Victorian railway terminus and hit the street below nose first. A defective brake or a sleeping driver can be all it takes to go over the edge of chaos.\n\nThe sum of these parts makes for an interesting book. However, it is one to be read for entertainment and some interesting ideas. Those looking for a grand history of everything will be disappointed."
  },
  {
    "objectID": "posts/fifty-years-of-twin-studies.html",
    "href": "posts/fifty-years-of-twin-studies.html",
    "title": "Fifty years of twin studies",
    "section": "",
    "text": "If you’re familiar with the literature, this is unsurprising. A meta-analysis in Nature Genetics of 2,748 twin study publications points to the strong role of genetics and the weak role of family influence (a major component of “shared environment”) in shaping human traits. The abstract:\n\nDespite a century of research on complex traits in humans, the relative importance and specific nature of the influences of genes and environment on human traits remain controversial. We report a meta-analysis of twin correlations and reported variance components for 17,804 traits from 2,748 publications including 14,558,903 partly dependent twin pairs, virtually all published twin studies of complex traits. Estimates of heritability cluster strongly within functional domains, and across all traits the reported heritability is 49%. For a majority (69%) of traits, the observed twin correlations are consistent with a simple and parsimonious model where twin resemblance is solely due to additive genetic variation. The data are inconsistent with substantial influences from shared environment or non-additive genetic variation. This study provides the most comprehensive analysis of the causes of individual differences in human traits thus far and will guide future gene-mapping efforts. All the results can be visualized using the MaTCH webtool.\n\nAlso of interest, most of the resemblance between twins is due to additive genetic variation. This is a positive sign for plans to identify the causal variants behind complex traits. It’s just a matter of getting those sample sizes up in genome-wide association studies.\nAmusingly, in one case this study has been flagged as a tie between nature and nurture. But the “nurture” we are talking about here isn’t reading to your kids."
  },
  {
    "objectID": "posts/fisher-on-the-evolution-of-time-preference.html",
    "href": "posts/fisher-on-the-evolution-of-time-preference.html",
    "title": "Fisher on the evolution of time preference",
    "section": "",
    "text": "I am re-reading Fisher’s The Genetical Theory of Natural Selection and was reminded of this passage that predates modern economic arguments about the evolution of the rate of time preference by over 50 years. For those who want to follow the maths, m is the Malthusian parameter (the relative rate of increase or decrease of a population), lx is the number living to age x, and bx is the rate of reproduction at age x:\n\nIn view of the close analogy between the growth of a population supposed to follow the law of geometric increase, and the growth of capital invested at compound interest, it is worth noting that if we regard the birth of a child as the loaning to him of a life, and the birth of his offspring as a subsequent repayment of the debt, the method by which m is calculated shows that it is equivalent to answering the question—At what rate of interest are the repayments the just equivalent of the loan? For the unit investment has an expectation of a return lxbxdx in the time interval dx, and the present value of this repayment, if m is the rate of interest, is e-mxlxbxdx; consequently the Malthusian parameter of population increase is the rate of interest at which the present value of the births of offspring to be expected is equal to unity at the date of birth of their parent. The actual values of the parameter of population increase, even in sparsely populated dominions, do not, however, seem to approach in magnitude the rates of interest earned by money, and negative rates of interest are, I suppose, unknown to commerce.\n\nFisher’s result that time preference should reflect the population growth rate matches that of Hansson and Stuart, about which I have posted before. Fisher also notes that this outcome is not what we see. This is where we need to call on other ideas, such as aggregate risk."
  },
  {
    "objectID": "posts/flynns-are-we-getting-smarter.html",
    "href": "posts/flynns-are-we-getting-smarter.html",
    "title": "Flynn’s Are We Getting Smarter?",
    "section": "",
    "text": "James Flynn of Flynn effect fame has a relatively new book out, Are We Getting Smarter? I have found Flynn’s earlier books to be easy but not great reads, and this book followed that pattern. However, reading them is worthwhile as they tend to provide a comprehensive update on the latest in IQ testing from around the globe. Flynn is also not afraid to throw in some interesting arguments.\nThe question in the title of the book has two elements. First, does the Flynn effect mean that we are getting smarter? Flynn prefers to say that we are not necessarily smarter, but that we are more modern. We are born with the same mental hardware, but in a more complex world humans are becoming more “scientific” in their thinking. We are better able to characterise and abstract.\nThe second is whether this trend is continuing, and generally it is. Except for the Scandinavian countries, IQ is still going up in the United States, England, Germany and other developed countries. IQ is also increasing at a slightly faster rate in developing countries, but not fast enough to close the gap with developed countries in the near future. There are also some notable exceptions, such as Sudan.\nOne of the more interesting chapters is Flynn’s discussion of IQ testing of death row inmates. If an inmate has a tested IQ of 65 or below, they are spared execution. But consider two inmates of the same intelligence who were both tested for intelligence in 1975, but one was tested using a 1972 normed test, while the other completed a test normed in 1947-48 (tests need to be normed regularly because of the Flynn effect). Given the different dates on which the tests were normed, the former can have their IQ measured at 65 and be spared, while the latter would be measured at 73 and face the death penalty. The view of courts on this point appears mixed. More broadly, however, it hints at an important discipline in considering IQ test results. If you are confronted with an IQ test result, you should ask when was the test normed and when was the test taken. It is only in that context that the result can be meaningful.\nFlynn spends some time revisiting old debates with Arthur Jensen about whether the Flynn effect is measuring anything meaningful. In particular, Flynn accuses Jensen of psychometric obsessions by only caring about whether the Flynn effect is relevant to g, and not about whether there are any real world implications.\n\nThe relative cognitive complexity of the tasks (or their relative g-loadings) is beside the point. If you do not care about anything but finding an absolute measure of our ability to deal with cognitive complexity, an absolute measure of intelligence if you will, you will not be interested. Since you cannot correlate IQ gains with g, you dismiss them as “hollow” (Jensen, 1998). But that is only because you have been blinded to social significance by psychometric obsessions.\n\nDespite their disagreements, however, Flynn credits Jensen with triggering his interest in the area, and dedicates the book to him. Flynn writes:\n\nPsychologists should thank Jensen for pursuing his life-long mission, against great odds, to clarify the concept of g. In addition to intellectual eminence, he had the courage to face down opposition often political rather than scientific. If I have made a significant contribution to the literature, virtually every endeavor was in response to a problem set by Arthur Jensen.\n\nThe book also contains come interesting ideas about of the growing gap in language between parents and children (the problem teenage years) and the rise of single motherhood among black women in the absence of eligible men. Flynn also explores what he calls the bright tax and bright bonus - the relatively steeper decline in analytical ability but slower decline in verbal ability as those of high intelligence age (which I have posted about before)."
  },
  {
    "objectID": "posts/foresight-by-h-g-wells.html",
    "href": "posts/foresight-by-h-g-wells.html",
    "title": "Foresight by H. G. Wells",
    "section": "",
    "text": "From When the Sleeper Wakes (1910):\n\n“What of the yellow peril?” he asked and Asano made him explain. The Chinese spectre had vanished. Chinaman and European were at peace. The twentieth century had discovered with reluctant certainty that the average Chinaman was as civilised, more moral, and far more intelligent than the average European serf, and had repeated on a gigantic scale the fraternisation of Scot and Englishman that happened in the seventeenth century. As Asano put it; “They thought it over. They found we were white men after all.”"
  },
  {
    "objectID": "posts/four-reasons-why-evolutionary-theory-might-not-add-value-to-economics.html",
    "href": "posts/four-reasons-why-evolutionary-theory-might-not-add-value-to-economics.html",
    "title": "Four reasons why evolutionary theory might not add value to economics",
    "section": "",
    "text": "In the lead article to a recent Journal of Economic Behavior & Organization special issue, Evolution as a General Theoretical Framework for Economics and Public Policy, David Sloan Wilson and John Gowdy examined four reasons why someone may not need to consult an evolutionary framework in examining economic and policy questions. The four arguments are riffs on the same theme, but the distinctions between them are worth making.\nThe first reason is that smart people will come to similar conclusions even if they use different approaches. Wilson and Gowdy note that this argument does not hold because people do not always come to the same conclusion. The neoclassical view of absolute utility maximisation differs from the evolutionary measure of relative fitness, and this relative measure should be reflected in economists’ utility curves. Smart economists have not come to the same conclusions as people using evolutionary theory.\nThis possibility of drawing different conclusions is why I am a fan of the evolutionary approach, although I would offer a different example, that of behavioural economics (or behavioural science). Many behavioural and neoclassical economists haven’t come to the same conclusions. And given the lack of theory in behavioural science (giving an observed bias a name is not theory), not only have people come to different conclusions, but they lack a theoretical basis to reconcile them.\nThe second reason provided by Wilson and Gowdy relates to design. If an object or process is well designed, it does not matter what process designed it - be that evolution or god. But as Wilson and Gowdy point out, knowing the design process will tell you if there actually is design. And if you don’t know if there is design, you may come to the wrong conclusions about causative processes. As an example, if you see that children resemble their parents, what is the causative process? By not knowing about the evolutionary design process, you may miss a major pathway by which this resemblance occurs.\nThe third reason has some truth. Why speculate with evolutionary theory when you can study the real thing? An example that I have posted about before is that estimates of time preference from evolutionary models tend not to reflect empirical measures. So why spend time finding the basis when you can measure the actual trait? Essentially, there are limits to what can be achieved without theory. Wilson and Gowdy ask us to imagine trying to examine adaptations in a bird by just looking at the brain. The behavioural science example also provides an illustration. Studying the real thing has gotten behavioural science to a certain point, but theory is likely needed to pull the list of heuristics and biases into something coherent. And even if theory has not generated rates of time preference that reflects empirical estimates, theory is likely required to deal with observations such as preference reversals.\nFinally, Wilson and Gowdy take on an argument that I regularly hear, being that we don’t need to consult every branch of the sciences all the time. Biologists don’t consult with quantum physicists on a regular basis. Wilson and Gowdy’s response to this question is nice, and reflects a theme through the article that for many economists it makes sense not to have an evolutionary framework as part of their analysis. But the problem arises if every economist does not have an evolutionary framework, or if those economists who do use an evolutionary framework never have their work incorporated across the broader field. In that case, there is potential for simply coming to the wrong conclusion (see point one) as the foundation block of the analysis is wrong. As an example, most people don’t consider the basis of their utility functions - they typically use standard forms. But what is the basis of that standard form?\nThis point also relates to some of the recent debates I have had about obesity. Feel free to come up with an explanation that has no reference to the physical processes of weight gain, or the evolutionary processes that shaped them. But if the explanation is inconsistent with these processes, something is likely wrong with your explanation.\nMy series of posts on the Journal of Economic Behavior & Organization special issue, Evolution as a General Theoretical Framework for Economics and Public Policy, are as follows:\n\nSocial Darwinism is back - a post on one of the popular press articles that accompanied the special issue, a piece by David Sloan Wilson called A good social Darwinism.\nFour reasons why evolutionary theory might not add value to economics (this post) - a post on David Sloan Wilson and John Gowdy’s article Evolution as a general theoretical framework for economics and public policy\nEconomic cosmology - The rational egotistical individual - a post on John Gowdy and colleagues’ article Economic cosmology and the evolutionary challenge \nEconomic cosmology - The invisible hand - a second post on Economic cosmology and the evolutionary challenge \nEconomic cosmology - Equilibrium - a third post on Economic cosmology and the evolutionary challenge\nDesign principles for the efficacy of groups - a post on David Sloan Wilson, Elinor Ostrom and Michael E. Cox’s article Generalizing the core design principles for the efficacy of groups"
  },
  {
    "objectID": "posts/franks-passions-within-reason.html",
    "href": "posts/franks-passions-within-reason.html",
    "title": "Robert Frank’s Passions Within Reason",
    "section": "",
    "text": "Since reading Robert Frank’s The Darwin Economy, I have been working through his back catalogue. The original and innovative Passions Within Reason: The Strategic Role of the Emotions is the best of Frank’s books I have read so far.\nFrank’s major proposal is that the emotions act as a commitment device. When a dispassionate calculation shows that it is better to cheat, the emotion of guilt can act as a constraint to the “rational” course of action. When a dispassionate calculation shows that it would be better not to incur the costs of punishing someone for cheating you, the emotion of anger acts as a constraint, and prompts you to punish them regardless of the personal cost to you. These emotional responses are particularly important where someone has a high discount rate and give a greater weight to present costs and benefits than those that come in the future.\nThese emotional traits are able to persist in an evolutionary setting as the “irrational” behaviour allows people to make credible commitments, which then allow them to gain the benefits of increased cooperation or protection from cheating. For example, if you can signal that you are trustworthy and your guilt will prevent you from cheating someone, you are more likely to be able to enter beneficial trades and cooperative arrangements. If someone knows you will get angry and attack them if they cheat you, even though it is not a rational thing to do in that particular instance, they are less likely to cheat you in the first place.\nFrank deals well with two of the obvious attacks on this argument: why did such a convoluted way of undertaking these actions evolve; and what might prevent someone faking the emotion to prevent being cheated or so that they can enter into a trade where they can cheat?\nOn the first, Frank recognises that evolution does not come up with the perfect solution, but evolves responses to the environment over time. There is path dependence. In this case, the mix of high discount rates and emotional constraints to those discount rates were the solution that was found, and not the ability to rationally assess that the short-term costs are worth it for the sake of reputation. If humans are cognitively limited, the other option may not be feasible.\nAs an example of this path dependence (although possibly not an accurate example), consider the time before the formation of complex societies where there are repeated opportunities for mutually beneficial trade. Then, a high discount rate might have been generally appropriate, with those who cheated being favoured as they incurred no long-term costs when they did. However, when greater complexity of society arose, those with a constraint to their high discount rates did better as they were able to refrain from short-term cheating behaviour and develop reputations for fairness. Rather then tempering those high discount rates, which might still be important in some circumstances, the evolved response was emotions to constrain them in other circumstances. This story might not be true, but it creates a picture whereby opposing tendencies cancel each other out rather than a person evolving the perfect foresight necessary to consider the repeated interactions and how to act strategically.\nFor the second argument, the prevention of cheating, Frank relies on signalling theory. Frank argues that emotions are not easy to fake, which makes them reliable signals of someone’s intentions. If they could be easily copied, they would cease to be used as signals and the benefits to copying the emotion would disappear. It is the fact that they are hard to fake that leads to them being used as signals in the first place.\nThere are parts of this argument that trouble me, as the costs to faking emotions seem low. The costs exist, as work by Centorrino and colleagues demonstrates, but there are successful fakers of emotion. Many psychopaths can appear trustworthy despite their lack of trustworthiness. Possibly cheaters do not proliferate because the benefits to faking emotion are low, but if that is the case, why do we put such faith in these emotions as signals? Possibly cheaters are simply found out too often, so despite one successful cheat, their reputation ruins future opportunity. In that case, the signalling role is not as important as establishing reputation.\nRegardless, Frank shows that it is possible for mixes of cheaters and honest people to emerge, with the frequency dependent selection ensuring that neither type disappears. If everyone is honest, there is no point signalling honesty or monitoring for cheating behaviour, so cheaters can thrive. If everyone cheats, the few honest people will spend great effort to seek each other out, and reap the benefits from specialisation and trade.\nThe book is not without its flaws, but given the strength of the fundamental ideas and the way Frank argues them, it feels like nitpicking to mention them. The only one I will note for the moment is that Frank does not give justice to rational behaviour, such as doing the right thing because people will find out. As experimental evidence shows, while humans can act in what seem to be non-self interested ways, people adapt to the probability of being caught cheating and the available rewards. Emotions are a commitment device, but in some cases, rational calculation can do the job. People with higher IQ are more trustworthy, yet I would argue they are not more emotional.\nHowever, that is being pedantic about an excellent book. It is entertaining and interesting throughout. Twenty four years after release, it is still a must read."
  },
  {
    "objectID": "posts/free-sterilisation.html",
    "href": "posts/free-sterilisation.html",
    "title": "Free sterilisation",
    "section": "",
    "text": "From Dan Ariely:\n\nLast year, the Danish government announced that sterilization, which had been free, would cost at least 7,000 kroner (~$1,300) for men and 13,000 kroner (~$2,500) for women as of January 1st, 2011. Following the announcement, doctors performing sterilizations found that their patient load suddenly surged. People were scrambling to get sterilized while it was still free.\nNow, it could be that the people who were already planning on getting sterilized at some point in the future just made their appointments a bit sooner, and conveniently saved some money.  But I can also imagine that (much like our research on free tattoos) there were many people who did not really think much about sterilization before the price change, but were so averse to giving up such a good deal that it pushed them to take the offer and undergo a fairly serious procedure.\n\nTraits that lead people to voluntarily cut their fertility should reduce in prevalence in the long-term, and this is an interesting (but small) acceleration.\nFor those that consider that people (and their capacity to create and invent) are the ultimate resource and that each person generates, on average, positive social welfare, this policy would have benefits in more than one dimension. An important question, however, is whether the average characteristics of those that undergo the now expensive sterilisation differ from those who underwent the procedure before. Is sterilisation in Denmark now the domain of the well-off?"
  },
  {
    "objectID": "posts/fukuyamas-the-origins-of-political-order.html",
    "href": "posts/fukuyamas-the-origins-of-political-order.html",
    "title": "Fukuyama’s The Origins of Political Order",
    "section": "",
    "text": "I have finally finished reading Francis Fukuyama’s The Origins of Political Order: From Prehuman Times to the French Revolution, several months after my initial comments.\nOf the grand history books I have read this year (the others being Ian Morris’s Why the West Rules for Now and Niall Ferguson’s Civilization: The West and the Rest), I found Fukuyama’s to be the most convincing. The focus on self-interest as a motivating factor of the individual actor, which is in turn underpinned by biological considerations, creates a more plausible story than one which talks of nations as actors. A nation acts subject to the motivations of its parts.\nMany of Fukuyama’s explanations might be considered to be just-so stories, and they might be wrong, but they feel as though they have a plausible foundation. In describing historical events with an effective sample size of one, it is hard to do better.\nIn the closing chapters, Fukuyama states his four biological factors that underlie the origins of political order. These are:\n\nHuman beings never existed in a precocial state.\nNatural human sociability is built around two principles, kin selection and reciprocal altruism.\nHuman beings have an innate propensity for creating and following norms and rules.\nHuman beings have a natural propensity for violence.\n\nI noted some of Fukuyama’s more interesting statements on the first two factors in a earlier post, but his observations on the last two are also interesting. On the third, Fukuyama states:\n\nRules can be rationally derived by individuals calculating how to maximize their own self-interest, which requires that they enter into social contracts with other individuals. Human beings are born with a suite of cognitive faculties that allow them to solve prisoner’s-dilemma-type problems of social cooperation. … The ability to make and obey rules is an economizing behavior in the sense that it greatly reduces the transaction costs of social interaction and permits effective collective action. …\nThe propensity of human beings to endow rules with intrinsic value helps to explain the enormous conservatism of societies. Rules may evolve as useful adaptations to a particular set of environmental conditions, but societies cling to them long after those conditions have changed and the rules have become irrelevant or even dysfunctional.\n\nOn the propensity for violence, he writes:\n\nIt is important to resist the temptation to reduce human motivation to an economic desire for resources. Violence in human history has often been perpetrated by people seeking not material wealth but recognition. Conflicts are carried on long beyond the point when they make economic sense."
  },
  {
    "objectID": "posts/galors-unified-growth-theory.html",
    "href": "posts/galors-unified-growth-theory.html",
    "title": "Galor’s Unified Growth Theory",
    "section": "",
    "text": "In 1798, Thomas Malthus described a world where technological progress did not increase per person income. Any additional income was consumed by population growth. It appeared a solid explanation of the world to that point, but Malthus had the misfortune of describing the “Malthusian world” just when some parts of that world were breaking their Malthusian shackles. This left Malthus with a somewhat tarnished reputation (I consider undeservedly), with the Malthusian model failing to offer an explanation for why it no longer seemed to apply.\nConversely, modern economic theories such as the neoclassical growth model and endogenous growth theory, while being useful in understanding some elements of economic growth, do a poor job of explaining the nature of the Malthusian state that existed for most of human history.\nUnified growth theory seeks to overcome the limitations of these approaches by presenting a coherent, single framework that captures the Malthusian era, the transition to higher growth and the modern growth state. The process of moving from one state to the next originates within unified growth models, with the seeds of the transition growing during the Malthusian era. As such, the Malthusian state is not an equilibrium, but a dynamic process leading to its own end. The benchmark for unified growth models is that they capture the patterns in income, technology and population through these various states and generate the transition between them.\nUnified growth theory was first proposed and has largely been developed in the work of Oded Galor, who with his co-authors has put together the building blocks of the theory. In his book Unified Growth Theory, Galor catalogues his work in the area and demonstrates the strength of the foundation he has built.\nThe book is an academic book that Galor has largely constructed from his published papers. Some chapters are heavy on the mathematics, although Galor is a clear writer and it is possible to get a sense of unified growth theory without working through the models. The first chapter, in which Galor works through some of the core features of economic history, would provide a useful grounding for any intelligent lay reader, as would his discussion of the causes of the demographic transition.\nI have posted about some of the papers that form book chapters before, so I won’t give a blow-by-blow account of the book. And if you are familiar with his papers, you won’t find many surprises. But what becomes clear from reading this work in a single collection is how coherently Galor’s work fits together (and that is from the perspective of someone who is skeptical of unified growth models as potentially trying to explain too much). While a couple of the chapters present full unified growth models, other chapters examine in detail particular elements of the theory. Galor examines the relationship between population, income and technology in the Malthusian state. He steps through the triggers of the demographic transition and examines which causes are plausible. He examines the accumulation of human capital across populations and how this coincides with changes in growth (the accumulation of human capital in response to technological progress is the core driver of the transition in Galor’s models). Put together, the case the Galor is building becomes clear.\nIn past posts I have focused on Galor’s work examining the interaction between evolutionary factors and economic growth. These evolutionary considerations are among the more peripheral parts of unified growth theory. Unified growth theory could survive without them. But what Galor emphasises is the range of theories that can be accommodated within a unified growth framework. For example, the effect of genetic diversity on innovation and cooperation could be taken to affect the population’s ability to accrue human capital. This then generates the divergence in economic outcomes within the unified growth framework, as one population accumulates enough human capital for a take-off in growth before the other. In this context, unified growth theory does not explain every facet of economic growth, but provides a framework under which much analysis can occur.\nThat said, it will be interesting to see which elements of Galor’s unified growth models stand the test of time, even if unified growth theory itself becomes a more broadly used approach. What is the nature of the trade-off between quantity and quality of children? What were the evolutionary changes during the Malthusian state? How do the models stand up when we examine specific questions under their lens, such as asking why England and not, say, China first experienced the take-off in economic growth? There is a lot of potential to put more flesh on the framework of unified growth theory and the models that Galor has developed within it.\nSo for those economists interested in the deep causes of economic growth, I would recommend Galor’s book, even if you are generally familiar with his work. Having that body of work systematically laid out in one piece gives it a strength not apparent when each part is taken alone.\nAnd for those who are interested on some of my earlier posts on Galor’s work (with the corresponding book chapter in brackets):\n\nDynamics and Stagnation in the Malthusian Epoch (chapter 3)\nThe Neolithic Revolution and Comparative Development (chapters 6.4.1)\nThe “Out of Africa” Hypothesis, Human Genetic Diversity, and Comparative Economic Development (chapter 6.4.2)\nNatural selection and the origin of economic growth (chapter 7)\nEvolution of Life Expectancy and Economic Growth (chapter 7.7.2)"
  },
  {
    "objectID": "posts/game-theory-and-the-peacocks-tail.html",
    "href": "posts/game-theory-and-the-peacocks-tail.html",
    "title": "Game theory and the peacock’s tail",
    "section": "",
    "text": "Over at Cheap Talk, Jeff Ely has posted on a presentation by Balazs Szentes at The Biological Basis of Preferences and Behavior conference. Ely writes:\n\nBalazs Szentes stole the show with a new theory of the peacock’s tail. …\nSuppose female peacocks choose which type of male peacock to mate with: small or large tails. Once the females sort themselves across these two separate markets, the peacocks are matched and they mate.\nThe female peacocks are differentiated by health, and within a peacock couple health partially compensates for the disadvantageous tail. In the model this means that healthy females who mate with big-tailed peacocks will produce almost as many surviving offspring as they would if they mated with peacocks without the disadvantage of the tail. …\nConsider what happens when a small-tailed peacock population is invaded by a mutation which gives some male peacocks large tails. Since female peacocks make up half the population of peacocks there is now an imbalance in the market for small-tailed peacocks. In particular the males are in excess demand and some females will have trouble finding a mate.\nOn the other hand the big-tailed male peacocks are there for the taking and its going to be the healthy female peacocks who will have the greatest incentive to switch to the market for big tail. The small cost they pay in terms of reduced quantity of offspring will be offset by their increased chance of mating. The big tails have successfully invaded.\n\nSzentes’s theory illustrates the mixed feelings I expressed in my recent post about some of the presentations at the conference. The model underlying the theory is clever and interesting, but Szentes’s focus is more on the game theory than the evolutionary problem that the model is proposed to address. The model offers limited insight into the evolution of the peacock’s tail as the assumptions underpinning the model do not hold.\nFirst, the model requires an assumption of monogamy, which peacocks are not. As for most males with ornaments, the peacock’s tail is used to attract multiple partners to make up for the handicap that the ornament imposes (as the more established theory suggests). The model also assumes that the males are indifferent as to who they mate with (despite being monogamous), with high quality females unable to attract male interest above that of females of low quality.\nWithout those assumptions, the findings derived from the model no longer hold. In some respects, the sexes in the model appear backward, as a lack of males willing to give sperm is not an issue in most species. The low investment by males makes females the scarce resource.\nI expect that these issues are of less concern to Szentes (or, based on his post, Ely) than they are for me, as his interest lies more in the model than its particular application. Szentes appeared to be aware of these critiques when they were raised at the conference.\nIf the model is not useful in explaining the peacock’s tail, what situation might the model describe? Instead of talking of disadvantaged males with tails, could we talk of low quality males and use the model to explain the persistence of low quality males in a population? This is an interesting model looking for a use.\n*The videos are many of the presentations are now up."
  },
  {
    "objectID": "posts/garons-beyond-our-means.html",
    "href": "posts/garons-beyond-our-means.html",
    "title": "Garon’s Beyond Our Means",
    "section": "",
    "text": "The core message of Sheldon Garon’s Beyond Our Means: Why America Spends While the World Saves is that people’s savings behaviour responds to incentives and in particular, to the institutional structure and norms created by government. These incentives range from the availability of convenient savings accounts to the establishment of social norms.\nThe incentives described by Garon are not those typically discussed by economists. Garon argues that economics has failed to explain cross-national differences in savings and that the life-cycle hypothesis of savings does poorly. Why are savings rates lower in the United States where there are few safety nets, and yet there are high savings rates in European countries with substantially more publicly funded social security?\nGaron makes a convincing case that the availability of savings accounts is important, as he traces hundreds of years of savings behaviour. Despite stereotypes of high-saving East Asians and spendthrift Americans, Garon demonstrates how the form savings in each country and within countries varies with the institutional structure at the time and the availability of simple options to save.\nThe book has plenty of interesting historical fodder. The Japanese, United Kingdom and United States were particularly effective in extracting savings from their citizens when it was required, such as during war. The history of credit and the restrictions on access faced in many European countries was eye-opening for someone from a country (Australia) with very easy access.\nI cannot offer a detailed analysis of whether Garon’s argument about savings accounts is countered by people saving in other ways, such as through the accumulation of financial assets or real-estate, and to what extent there are ethnic differences in savings rates (including within countries). Regardless, Garon’s picture of the economic approach to savings struck me as incomplete. Beyond the life-cycle approach, there are many other economic analyses of savings, particularly about the flip-side of savings - consumption. How is conspicuous consumption (saving’s wasteful corollary) or competition for positional goods relevant? Do they vary between countries? How might the forces described by Veblen, Frank and Miller be affecting savings rates?"
  },
  {
    "objectID": "posts/gary-kleins-sources-of-power-how-people-make-decisions.html",
    "href": "posts/gary-kleins-sources-of-power-how-people-make-decisions.html",
    "title": "Gary Klein’s Sources of Power: How People Make Decisions",
    "section": "",
    "text": "Summary: An important book describing how many experts make decisions, but with a lingering question mark about how good these decisions actually are.\n\nGary Klein’s Sources of Power: How People Make Decisions is somewhat of a classic, with the version I read being a 20th anniversary edition issued by MIT Press. Klein’s work on expert decision making has reached a broad audience through Malcolm Gladwell’s Blink, and Klein’s adversarial collaboration with Daniel Kahneman (pdf) has given his work additional academic credibility.\nHowever, throughout the growing application of behavioural science in public policy and the private sphere, I have rarely seen Klein’s work practically applied to improve decision making. The rare occasions where I see it referenced typically involve an argument that the conditions for the development of expertise do not exist in a particular domain.\nThis lack of application partly reflects the target of Klein’s research. Sources of Power is an exploration of what Klein calls naturalistic decision making. Rather than studying novices performing artificial tasks in the laboratory, naturalistic decision making involves the study of experienced decision makers performing realistic tasks. Klein’s aim is to document the strengths and capabilities of decision makers in natural environments with high stakes, such as lost lives or millions of dollars down the drain. It often involves uncertainty or missing information. The goals may be unclear. Klein’s focus is therefore in the field and the decisions of people such as firefighters, nurses, pilots and military personnel. They are typically people who have had many years of experience. They are “experts”.\nThe exploration of naturalistic decision making contrasts with the heuristics and biases program, which typically focuses on the limitations of decision makers and is the staple fodder of applied behavioural scientists. Using the findings of experimental outputs from the heuristics and biases program to tweak decision environments and measure the response across many decision makers (typically through a randomised controlled trial) is more tractable than exploring, modifying and measuring the effect of interventions to improve the rare, high-stakes decisions of experts in environments where the goal itself might not even be clear.\nIs Klein’s work “science”?\nThe evidence that shapes Sources of Power was typically obtained through case interviews with decision makers and by observing these decision makers in action. There are no experiments, with the data obtained through interviews. The interviews are coded for analysis to attempt to find patterns in the approaches of the decision makers.\nKlein is cognisant of the limitations of this approach. He notes that he gives detailed descriptions of each study so that we can judge the weaknesses of his approach ourselves. This brings his approach closer to what he considers to be a scientific piece of research. Klein writes:\n\nWhat are the criteria for doing a scientific piece of research? Simply, that the data are collected so that others can repeat the study and that the inquiry depends on evidence and data rather than argument. For work such as ours, replication means that others could collect data the way we have and could also analyze and code the results as we have done.\n\nThe primary “weakness” of his approach is the reliance on observational data, not experiments. As Klein suggests, there are plenty of other sciences that have this feature. His approach is closer to anthropology that psychology. But obviously, an approach constrained to the laboratory has its own limitations:\n\nBoth the laboratory methods and the field studies have to contend with shortcomings in their research programs. People who study naturalistic decision making must worry about their inability to control many of the conditions in their research. People who use well-controlled laboratory paradigms must worry about whether their findings generalize outside the laboratory.\n\nKlein has a faith in stories (the subject of one of the chapters) serving as natural experiments linking a network of causes to their effects. It is a fair point that stories can be used to communicate subtle points of expertise, but using them to reliably identify cause-effect relationships seems a step too far.\nRecognition-primed decision making\nKlein’s “sources of power” for decision-making by experts are intuition, mental simulation, metaphor and storytelling. This is in contrast to what might be considered a more typical decisions-making toolkit (the one you are more likely to be taught) of logical thinking, probabilistic analysis and statistics.\nKlein’s workhorse model integrating these sources of power is recognition-primed decision making. This is a two stage process, involving an intuitive recognition of what response is required, followed by mental simulation of the response to see if it will work. Metaphors and storytelling are mental simulation tools. The recognition-primed model involves a blend of intuition and analysis, so is not just sourced from gut feelings.\nFrom the perspective of the decision maker, someone using this model might not consider that they are making a decision. They are not generating options and then evaluating them to determine the best choice.\nInstead, they would see their situation as a prototype for which they know the typical course of action right away. As their experience allowed them to generate a reasonable response at the first instance, they do not need to think of others. They simply evaluate the first option, and if suitable, execute. A decision was made in that alternative courses of action were available and could have been chosen. But there was no explicit examination across options.\nKlein calls this process singular evaluation, as opposed to comparative evaluation. Singular evaluation may involve moving through multiple options, but each is considered on its own merits sequentially until a suitable option is found, with the search stopping at that point.\nThe result of this process is “satisficing”, a term coming from Herbert Simon. These experts do not optimise. They pick the first option that works.\nKlein’s examination of various experts found that the recognition-primed decision model was the dominant mode of decision making, despite his initial expectation of comparative evaluation. For instance, fireground commanders used recognition-primed decision making for around 80% of the decisions that Klein’s team examined. Klein also points to similar evidence of decision making by chess grandmasters, who spend little time comparing the strengths and weaknesses of one move to another. Most of their time involves simulating the consequences and rejecting moves.\nMental simulation\nMental simulation involves the expert imagining the situation and transforming the situation until can they picture it in a different way from the start. Mental simulations are typically not overly elaborate, and generally rely on just a few factors (rarely more than three). The expert runs the simulation and assesses: can it pass an internal evaluation? Sometimes mental simulation can be wrong, but Klein considers them to be fairly accurate.\nKlein’s examples of mental simulation were not always convincing. For example, he describes an economist who mentally simulated what the Polish economy would do following interventions to reduce inflation. It is hard to take seriously single examples of such mental simulation hitting the mark when I am aware of so many backfires in this space. And how would expertise in such economic simulations develop? (More on developing expertise below.)\nOne strength of simulations is that they can be used where traditional decision analytic strategies do not apply. You can use simulations (or stories) if you cannot otherwise remember every piece of information. Klein points to evidence that this is how juries absorb evidence.\nOne direct use of simulation is the premortem strategy. Imagine in the future plan has failed and you have to understand why. You can also do simulation through decision scenarios.\nNovices versus experts\nExpertise has many advantages. Klein notes experts can see the world differently, have more procedures to apply, notice problems more quickly, generate richer mental simulations and have more analogies to draw on. Experts can see things that novices can’t. They can see anomalies, violations of expectancies, the big picture, how things work, additional opportunities and improvisations, future events, small differences, and their own limitations.\nInterestingly, while experts tend not to carefully deliberate about the merits of different courses of action, novices need to compare different approaches. Novices are effectively thinking through the problem from scratch. The rational choice method helps us when we lack the expertise to assess a situation.\nAnother contrast is where effort is expended. Experts spend most of their effort on situation assessment - this gives the answers. Novices spend more time on determining the course of action.\nOne interesting thread concerned what happened when time pressure was put on chess players. Time constraints barely degraded the performance of masters, while it destroyed that of novices. The masters often came up with their best move first, so there is no need for the time to test a lot of options.\nDeveloping good decision making\nGiven the differences between novices and experts, how should novices develop good decision making? Klein suggests this should not be done through training in formal methods of analysis. In fact, this could get in the way of developing expertise. There is also no need to teach the recognition-primed model as it is descriptive: it shows what good decision makers already do. We shouldn’t teach people to think like experts.\nRather, we should teach people to learn like experts. They should engage in deliberate practice, obtain feedback that is accurate and timely, and enrich learning by reviewing prior experience and examining mistakes. The intuition that drives recognition grows out of experience.\nRecognition versus analytical methods\nKlein argues that recognition strategies are not a substitute for analytical methods, but an improvement. Analytical methods are the fallback for those without experience.\nKlein sees a range of environments where recognition strategies will be the superior options. These include the presence of time pressure, when the decision maker is experienced in the domain, when conditions are dynamic (meaning effort can be rendered useless if conditions shift), and when the goals ill-defend (making it hard to develop evaluation criteria). Comparative evaluation is more useful where people have to justify choice, where it is required for conflict resolution, where you are trying to optimise (as opposed to finding just workable option), and where the decision is computationally complex (e.g. investment portfolio).\nFrom this, it is hard to use a rigorous analytical approach in many natural settings. Rational, linear approaches run into problems when the goal is shifting or ill-defined.\nDiagnosing poor decisions\nI previously posted some of Klein’s views on the heuristics and biases approach to assessing decision quality. Needless to say, Klein is sceptical that poor decisions are largely due to faulty reasoning. More effort should be expended in finding the sources of poor decisions, rather than blaming the operator.\nKlein describes a review a sample of 25 decisions with poor outcomes (from 600 he had available) to assess what went wrong. Sixteen outcomes were due to lack of experience, such as someone not realising that construction of the building on fire was problematic. The second most common issue was lack of information. The third most common involved noticing but explaining away problems during mental simulation - possibly involving bias.\nConditions for expertise\nThe conditions for developing the expertise for effective recognition-primed decision making is delved into in depth in Klein’s article with Daniel Kahneman, Conditions for Intuitive Exertise: A Failure to Disagree (pdf). However, Klein does examine this area to some degree in Sources of Power.\nKlein notes that it is one thing to gain experience, and another to turn that into expertise.  It is often difficult to see cause and effect relationships. There is typically delay between the two. It is difficult to disentangle luck and skill. Drawing on work by Jim Shanteau, Klein also notes that expertise was hard to develop when the domain is dynamic, we need to predict human behaviour, there is less chance for feedback, there is not enough repetition to get sense of typicality or there are fewer trials. Funnily enough, this description seems to align somewhat with many of the naturalistic decision making environments.\nDespite these barriers, Klein believes that it is possible to get expertise in some areas, such as fighting fires, caring for hospitalised infants or flying planes. Less convincingly (given some research in the area), he also references the fine discrimination of wine tasters (e.g.).\nPossibly my biggest criticism of Klein’s book relates to this final point, as he provides little evidence for the conversion of experience into expertise beyond the observation that in many of these domains novices are completely lost. Is the best benchmark a comparison with a novice who has no idea, or is it better to look at, say, a simple algorithm, statistical rule, or someone with basic training?"
  },
  {
    "objectID": "posts/genes-economics-and-happiness.html",
    "href": "posts/genes-economics-and-happiness.html",
    "title": "Genes, economics and happiness",
    "section": "",
    "text": "From the Journal of Neuroscience, Psychology, and Economics (ungated prepublication version here):\n\nGenes, economics, and happiness De Neve, Jan-Emmanuel; Christakis, Nicholas A.; Fowler, James H.; Frey, Bruno S.\nWe explore the influence of genetic variation on subjective well-being by employing a twin design and genetic association study. In a nationally representative twin sample, we first show that ∼33% of the variation in life satisfaction is explained by genetic variation. Although previous studies have shown that baseline happiness is significantly heritable, little research has considered molecular genetic associations with subjective well-being. We study the relationship between a functional polymorphism on the serotonin transporter gene (5-HTTLPR) and life satisfaction. We initially find that individuals with the longer, transcriptionally more efficient variant of this genotype report greater life satisfaction (n = 2,545; p = .012). However, our replication attempts on independent samples produce mixed results, indicating that more work needs to be done to better understand the relationship between this genotype and subjective well-being. This work has implications for how economists think about the determinants of utility, and the extent to which exogenous shocks might affect individual well-being.\n\nWhile I’m glad this work is being done, there is a certain predictability to the result. Add this to the growing list of papers showing that a personality/psychological/economic trait has heritability of between 0.2 and 0.4. It’s also another example of twin studies being more useful than the molecular biology."
  },
  {
    "objectID": "posts/genetic-distance-and-income-differences-evidence-from-china.html",
    "href": "posts/genetic-distance-and-income-differences-evidence-from-china.html",
    "title": "Genetic distance and income differences - evidence from China",
    "section": "",
    "text": "In a paper in Economic Letters (ungated version here), Ying Bai and James Kung test Spolaore and Wacziarg’s hypothesis on genetic distant and economic development:\n\nSince 1949, trade and economic ties as well as the physical movement of people between Taiwan and the Chinese mainland had been banned. Given this disconnection, one would naturally expect the relative genetic distance – operating presumably via the diffusion of technology or institution – from Taiwan to have no effect on the income differences among provinces in the Chinese mainland. The ending of this cross-Strait “cold war” in 1987 has however drastically changed this situation. Indeed, by comparing the coefficients of relative genetic distance before and after 1987, we show that, with the removal of the restrictions previously placed upon personal exchange in 1987, the effect of relative genetic distance from Taiwan has increased, even though absolute genetic distance – which is highly correlated with the relative genetic distance from China’s technological frontier – has not changed significantly. This implies that relative genetic distance affects income difference through the channel of enhanced communication and economic exchanges.\n\nThe greater correlation of relative genetic distance than absolute genetic distance with income differences matches the theoretical and empirical findings of Spolaore and Wacziarg’s paper. However, I’m not convinced about what underlies this result, so am cautious about taking it as evidence that Spolaore and Wacziarg are on the right track.\nIn the light of the recent flurry of debate about Ashraf and Galor’s paper on genetic diversity (which should be distinguished from genetic distance) and economic development, it seems that Spolaore and Wacziarg had a rather smooth passage on the release of their paper. Spolaore and Wacziarg placed a much heavier emphasis on genetic factors being a proxy than did Ashraf and Galor, but I would be interested in seeing their work critiqued by those taking on Ashraf and Galor."
  },
  {
    "objectID": "posts/genetic-diversity-and-economic-development.html",
    "href": "posts/genetic-diversity-and-economic-development.html",
    "title": "Genetic diversity and economic development",
    "section": "",
    "text": "Quamrul Ashraf and Oded Galor’s paper linking genetic diversity and economic development has been available as a working paper for a few years, but it has now found a home in the American Economic Review (the latest available version of the working paper that I am aware of is available here).\nScience has picked up on the forthcoming publication is its editor’s choice section (unfortunately gated without subscription). Science summarises the results as follows:\n\nAshraf and Galor present the hypothesis that genetic diversity has exerted a long-lasting effect on economic development — which is quantified as population density in the precolonial era and as per-capita income for contemporary nations — beyond the influences of geography, institutions, and culture. They posit that intermediate levels of heterozygosity allow for a productive balance between the social costs of high diversity and the creative benefits of higher variance in cognitive skills. They show that the optimal level of diversity was approximately 0.68 in 1500 CE, and that this increased to 0.72 (which is pretty much where the United States sits) in the year 2000, with the most homogeneous country, Bolivia, placed at 0.63 and the most diverse country, Ethiopia, at 0.77.\n\nI recommend reading the comments on Marginal Revolution where Tyler Cowen has noted the Science piece.\nMy posts on Ashraf and Galor’s paper on genetic diversity and economic growth are as follows. I will link to each below as I post them:\n\nA summary of the paper methodology and findings\nDoes genetic diversity increase innovation?\nDoes genetic diversity increase conflict?\nIs genetic diversity a proxy for phenotypic diversity?\nIs population density a good measure of technological progress?\nWhat are the policy implications of the effects of genetic diversity on economic development?\nShould this paper have been published?\n\nOther debate on this paper can also be found here, here, here and here."
  },
  {
    "objectID": "posts/genetic-thresholds.html",
    "href": "posts/genetic-thresholds.html",
    "title": "Genetic thresholds",
    "section": "",
    "text": "In yesterday’s post on crime, I quoted David Eagleman’s statement that “we may someday find that many types of bad behaviour have a basic biological explanation—as has happened with schizophrenia, epilepsy, depression, and mania.” What we now consider culpable behaviour may fall into the class of mental illness, with the criminal justice system adjusting its threshold so.\nThis threshold issue extends beyond crime. Take IQ, which is highly heritable and correlates with income and most other life outcomes. Where one sits on the IQ bell curve is largely determined by genes. If one’s IQ falls below a certain level, they may receive special schooling, social security and other forms of special care. As is the case for criminal culpability, a threshold is set which considers biological factors.\nIn each case, is may be worse for someone to be just above the threshold than just below it. It is the person that has a strong genetic disposition to commit crimes, but not strong enough for the justice system to consider it a mental illness, that is the most likely to end up behind bars. Similarly, it is the person with the very low IQ, but not low enough IQ to be considered disabled, that may have the worst life outcomes. The pay-out from the genetic lottery is not monotonic."
  },
  {
    "objectID": "posts/genetics-and-education-policy.html",
    "href": "posts/genetics-and-education-policy.html",
    "title": "Genetics and education policy",
    "section": "",
    "text": "Philip Ball has an article in the December issue of Prospect (ungated on his blog) arguing that consideration of the genetic basis to social problems is a distraction from socioeconomic causes. The strawman punchline for the Prospect article is “It’s delusional to believe that everything can be explained by genetics”.\nThe article has drawn a response from one of the people named in the article, Dominic Cummings. Ball suggests that Cummings presents “genetics as a fait accompli – if you don’t have the right genes, nothing much will help”, although this statement suggests Ball had not invested much effort getting across Cummings’s actual position (as contained in this now infamous essay). Ball responded in turn, with Cummings firing back (in an update at the bottom of the page), and Ball responding again.\nBeyond the tit for tat - read their respective posts for that - there are some interesting points about whether genetics tells us anything about education policy.\nAs a start, Ball claims that “Social class remains the strongest predictor of educational achievement in the UK”, referencing this article. However, the authors of that article don’t consider the role of genetics or other potential predictors. The references that article gives for the claim are similarly devoid of relevant comparisons, which is unsurprising as they largely comprise policy positioning documents from various organisations. It’s hard to credibly claim something is a superior predictor when it is not assessed against the alternatives.\nSo, what is the evidence on this point? For one, we have twin and adoption studies. As a sample, Bruce Sacerdote studied Korean adoptees into the United States (admittedly, not the UK as per the quote) and found that shared environment (which would include socioeconomic status) explained 16 per cent of the variation in educational attainment. Genetic factors explained 44 per cent. This is a consistent finding in adoption studies, with children more closely resembling their biological parents than their adopted parents. For twin studies, an Australian analysis found a 57 per cent genetic and 24 per cent shared environment contribution to variation in education. A meta-analysis of heritability estimates of educational attainment found that, in the majority of samples, genetic variation explained more of the variation in educational attainment than shared environment.\nOf course, we don’t have the genetic data or understanding at hand just yet, but there are other factors such as IQ that are better predictors of education than social class. This territory is also complicated - there are genetic effects on both IQ and social class - but IQ tends to outperform. This meta-analysis shows that IQ is a better predictor of education, income and occupation than socioeconomic status - not overwhelmingly so, but superior nonetheless.\nThen there is the link between genetic factors and socioeconomic status, with a long line of studies finding a relationship. One of the more recent was by Daniel Benjamin and friends (ungated pdf). They found heritability of permanent income (20-year average) of 0.58 for men and 0.46 for women. Part of the predictive power of socioeconomic status comes from its genetic basis. Gregory Clark’s hypothesis of low social mobility being a result of genetic factors reflects this body of work.\nTurning next to Ball’s pessimism of the future of genetics, he states:\n\nIn September an international consortium led by Daniel Benjamin of Cornell University in New York reported on a search for genes linked to cognitive ability using a new statistical method that overcomes the weaknesses of traditional surveys. The method cross-checks such putative associations against a “proxy phenotype” – a trait that can ‘stand in’ for the one being probed. In this case the proxy for cognitive performance was the number of years that the tens of thousands of test subjects spent in education.\nFrom several intelligence-linked genes claimed in previous work, only three survived this scrutiny. More to the point, those three were able to account for only a tiny fraction of the inheritable differences in IQ. Someone blessed with two copies of all three of the “favourable” gene variants could expect a boost of just 1.8 IQ points relative to someone with none of these variants. As the authors themselves admitted, the three gene variants are “not useful for predicting any particular individual’s performance because the effect sizes are far too small”.\n\nThis, however, is only part of the picture. If we look at another study in which Benjamin was involved, three SNPs (single nucleotide polymorphisms - single base changes in the DNA code) were found to affect educational attainment. In total, they explained 0.02 per cent of the variation in educational attainment - practically nothing. But combine all the SNPs in the 100,000 person sample, and you edge up to 2.5 per cent. But even more interesting, they calculated that with a large enough sample they could explain over 20 per cent of the variation. Co-author Philipp Koellinger explains this in a video I recently linked. Although this study found variants with low explanatory power, it also points to the potential to explain much more with larger samples.\nFor more on the background to the feasibility of identifying the causal genetic variants for traits such as IQ, its worth looking at this paper by Steve Hsu. Possibly the most important point is that the causal variants for traits such as cognitive ability and height are additive in their effect. In his final response, Ball states that “And that might be because we are thinking the wrong way – too linearly – about how many if not most genes actually operate.” But the evidence shows that is how they largely work. Although a few years old now, this paper’s theoretical and empirical argument that genetic effects are largely additive has generally been affirmed in later research. This considerably simplifies the task of predicting outcomes based on someone’s genome. In fact, this is one reason selective breeding has been so successful and genetic data is already being used successfully in cattle breeding (There’s an example of the gap between entrepreneurship and policy development - while some of us are arguing whether this stuff is possible, someone else is already doing it).\nNow, supposing you have this genetic data, how might this change education? Returning to the article I linked above (ungated pdf), Benjamin and friends suggested this genetic information could be used to better target interventions. They propose early identification of dyslexia as an example.\nThey also suggest using genetic data as controls. This could provide more precision in studies of whether interventions to target socioeconomically disadvantaged children are effective. The genetic controls allow you to hone in on what you are interested in. In the question and answer session of a video of talk by Jason Fletcher I recently linked, Benjamin pointed to the famous Perry PreSchool Project and noted that additional precision through the use of genetic data would have been of great value.\nBall also indirectly alludes to another reason to learn about genetic factors. In his last response, he writes:\n\nPersonally, I find a little chilling the idea that we might try to predict children’s attainments by reading their genome, and gear their education accordingly – not least because we know (and Plomin would of course acknowledge this) that genes operate in conjunction with their environment, and so whatever genetic hand you have been dealt, its outcomes are contingent on experience.\n\nThis argument runs both ways. Supposing there are large gene-environment interactions, how can you understand the effects of changing the environment without looking at the way that environment affects people via their genome? As an example of this, Jason Fletcher examined how variation in a gene changed the response to tobacco taxation policy (he talks about this in a video I recently linked). Those with a certain allele responded to taxation and reduced smoking. Others didn’t. Too be honest, I’m not sold on the results of this particular study, but it illustrates that genetic factors that need to be considered if these gene-environment interactions are as large as people such as Ball believe.\n[I should admit at this point that G is for Genes: The Impact of Genetics on Education and Achievement is sitting unread in my reading pile….]\nPutting it together, Ball is off track in his suggestion that learning about and targeting genetic factors distracts from dealing with socioeconomic issues. Understanding of genetic and socioeconomic factors are complements, and by disentangling their effects, we could better tailor education to address each.\nThat is not to say that the genetic enterprise is guaranteed to be successful. But there is plenty of evidence that our genes are relevant and, on that basis, should be considered.\nFurther, there are changes we can make today. Ball asks what genetics can add beyond recognition that some children are more talented than others. The thing is, much schooling is still structured as though we are blank slates. Maybe it is an understanding of genetics that will finally get us to a point where education is better designed for people with different capacities, improving the experience across the full range of abilities and backgrounds."
  },
  {
    "objectID": "posts/genetics-without-genes.html",
    "href": "posts/genetics-without-genes.html",
    "title": "Genetics without genes",
    "section": "",
    "text": "A couple of weeks ago, Razib Kahn wrote a post in which he argued that “you don’t need to know the exact gene of major effect to conclude that a trait is genetic.” Where a lot of research is invested in finding the specific genes behind traits, and with a media hungry for these kinds of stories, many people have forgotten how much can be understood without knowledge of the specific genes. As Razib points out, much of genetics predates molecular biology and the discovery of the structure of DNA.\nFrom the perspective of integrating evolutionary biology and economics, a few of the papers I have posted about this year illustrate Razib’s point. Ashraf and Galor’s soon to be published paper on genetic diversity and economic development uses DNA-level data. While there is a statistically significant relationship between genetic diversity and economic development, few people seem convinced that there is a causative relationship between the two. How does the genetic diversity manifest itself into the economic outcomes? In developing the evidence for that causative relationship, it is not clear where you would start. Cross-species analysis might yield some insight on the cooperative effects of diversity, but how would you show the positive effects on innovation of higher levels of diversity? Linking the molecular biology to economic outcomes is difficult.\nWe’re also seeing an increasing number of genoeconomics papers on the genetic basis for economic traits, such as time and risk preference, that use molecular data. Unfortunately, many studies find spurious relationships and the low size of the effect of most alleles has resulted in this work being of limited use in economic analysis.\nHowever, this is not the point to give up on molecular biology as a tool for analysing economic traits and outcomes. The genoeconomics enterprise is in its early days and may bear fruit. And in the interim, we already have a lot of information that can already be used. Take the estimates of heritability of economic preferences we have from twin studies. We cannot pinpoint specific genes to account for even a small fraction of the heritability, but those estimates can still be useful for increasing our understanding of the role of genetics in economic outcomes, and may even be useful in policy development.\nMy last post on Gregory Clark and Neil Cummins’s use of surnames to track social mobility across the generations is another case in point. Combining  data about life outcomes across the generations could yield insight into the genetic factors underlying the transmission of socioeconomic status, adding to that already obtained through twin studies and shorter term analysis of intergenerational transmission. The data lend itself to quantitative genetic analysis.  And in this analysis, there is not a gene in sight."
  },
  {
    "objectID": "posts/genoeconomics-and-the-encode-project.html",
    "href": "posts/genoeconomics-and-the-encode-project.html",
    "title": "Genoeconomics and the ENCODE project",
    "section": "",
    "text": "The ENCODE (Encyclopedia of DNA Elements) project is an international collaboration that intends “to build a comprehensive parts list of functional elements in the human genome, including elements that act at the protein and RNA levels, and regulatory elements that control cells and circumstances in which a gene is active.”\nThe project has made a splash in the last couple of days with thepublication of thirty open access papers across Nature, Genome Research and Genome Biology describing some of the results. Much of the blogosphere has been hosing down the declarations of the accompanying press releases, so don’t expect any revolutions to come out of this work just yet. Similarly, the ENCODE project is not about to spur the genoeconomics revolution (the use of molecular genetics in economics). However, the project is a reminder that there is some very cool work going on (at least for those of us not already in the loop).\nOne important consideration for genoeconomics is how the ENCODE project might affect genome wide association studies (GWAS). ENCODE outputs were compared with previous results of GWAS for disease, and support was found for previous results. As described on the Nature News site:\n\nSince 2005, genome-wide association studies (GWAS) have spat out thousands of points on the genome in which a single-letter difference, or variant, seems to be associated with disease risk. But almost 90% of these variants fall outside protein-coding genes, so researchers have little clue as to how they might cause or influence disease.\nThe map created by ENCODE reveals that many of the disease-linked regions include enhancers or other functional sequences. And cell type is important. Kellis’s group looked at some of the variants that are strongly associated with systemic lupus erythematosus, a disease in which the immune system attacks the body’s own tissues. The team noticed that the variants identified in GWAS tended to be in regulatory regions of the genome that were active in an immune-cell line, but not necessarily in other types of cell and Kellis’s postdoc Lucas Ward has created a web portal called HaploReg, which allows researchers to screen variants identified in GWAS against ENCODE data in a systematic way. “We are now, thanks to ENCODE, able to attack much more complex diseases,” Kellis says.\n\nThe problem for the genoeconomics enterprise is that the existing GWAS on economic traits are often of questionable value. Any results that are not spurious are of such small effect that biochemical analysis is not much use. Further, converting genetic activity to outcomes such as time or risk preference is a much more difficult proposition than examining disease pathways.\nSo, for the moment, the genoeconomics enterprise is probably best left examining twin studies, GREML analysis or other techniques that don’t need a particular gene and trait to be nailed down. That said, despite being a long way from being able to control for genetic effects by examining someone’s genome, we are not short of information that we can use.\nThe more interesting part of the events of the last couple of days, as has been noted in many blogs, is the publication model adopted for this release of the ENCODE results. While not without problems (Daniel MacArthur’s mixed reaction is one example worth reading), the information available and the way it is presented is quite cool and hopefully another step towards more open access to data in the field. You can download an Ipad app which has the thirty open access papers, plus an interesting feature called “threads” which allows exploration of issues across the papers. Much of it is heavy going for someone not in the field, and it is useful to use the blogosphere to interpret the information, but there are worse ways to get up to speed with what is happening."
  },
  {
    "objectID": "posts/genoeconomics-molecular-genetics-and-economics.html",
    "href": "posts/genoeconomics-molecular-genetics-and-economics.html",
    "title": "Genoeconomics: molecular genetics and economics",
    "section": "",
    "text": "The Journal of Economic Perspectives has an excellent article by Beauchamp and colleagues titled Molecular Genetics and Economics (ungated pdf here). It is a nice contrast to another article in the same issue, Charles Manski’s bashing of the heritability straw man.\nThe authors argue that “genoeconomics”, the use of molecular genetics in economics, has the potential to supplement traditional behavioural genetic studies and build an understanding of the biology underlying economically relevant traits. They note that behavioural genetics, particularly research into heritability, has produced compelling evidence of the link between economically important characteristics and DNA. Molecular genetics is an “exciting tool” that can now be turned to this area.\nHowever, potential pitfalls mar the way forward. These pitfalls are beautifully illustrated by a study that the authors undertook in which they sampled over half a million single-nucleotide polymorphisms (SNPs) from each of 7,500 people. An SNP is a DNA sequence variation where a single nucleotide differs between people. They then searched for SNPs associated with educational attainment. They found a large number of associations, many passing significance tests of 10-6. Passing this test suggests that there is a one in a million chance that the association is by chance (of course, there were 500,000 chances). If they took this result to the right journal, they might have had their study published and got some headlines about “the education gene”.\nHowever, the authors took the 20 most significant associations from the first sample and checked them against the SNPs from another sample of 9,500 people. In the second sample, none of these 20 SNPs significantly affected educational attainment, even using a weak five per cent significance test. This showed that the results from the first sample were spurious.\nThe authors noted some important lessons from this. The first is that given the low sample size of many studies, the probability of a true association being discovered among the noise is minute. The studies are underpowered – power being the probability that an association between an SNP and the trait of interest will be found when there is a relationship. The fact that almost all SNPs reported in the literature can explain very little of the variation in most traits exacerbates this problem as the studies are trying to detect small effects. For example, no marker has been found to predict more than one per cent of the variation in height between people. As a result, very large samples are required to find true associations and sort them from the noise.\nFor example, with a five per cent threshold test for significance and an SNP that explains 0.1 per cent of variation in a trait, you need a sample of 4,000 subjects before the association has a 50 per cent chance of being found. Yet, in a 500,000 SNP panel there are likely to be thousands of false positives that meet the five per cent significance level.\nIf the significance test is increased to by a factor of one million to 10-8, which is appropriate given the huge number of potential associations being tested for in a 500,000 SNP panel, the need for a large sample size increases. For an SNP that explains 0.1 per cent of variation in a trait, the study will need a sample of around 25,000 to have a 50 per cent chance of detecting the relationship. If the SNP explains 0.01 per cent of the variation, a sample size of 200,000 results in only a 20 per cent chance of finding the relationship. However, the more stringent significance test reduces the number of false positives – it is just that the reduced number of false positives comes at the cost of power, which must be compensated for by increased sample size. At this time, there is little useful genetic data available in samples of this size.\nBeyond the power issue, the authors identified publication bias as a problem. Papers which find interesting relationships are more likely to be published, which creates incentives for data mining and the write-up of results that are interesting but not robust. It is not easy to find a publisher for a paper that shows no relationship. This paper by Beauchamp and colleagues is the exception that proves the rule. To get their negative finding published they turned it into an analysis of the broader use of genetics in economics.\nThey do note, however, that data mining in genoeconomics is not in itself bad. It is when it is not accompanied by robust methodologies and stringent review processes that the problems arise.\nBeauchamp and colleagues close their paper by noting some benefits of the genoeconomics enterprise. They endorse the use of genetic information in policy, even where the causal mechanisms are not known. They give the example of targeting children with markers for dyslexia with alternative teaching methods. This is a good long-term goal, but we will want to have SNPs explaining more variation in traits before this will be useful. For now, family history or information about siblings and twins is more useful information. How much of that  information is being used now?\nMore interestingly, they suggest that this genetic data could be used as a control variable in other economics studies. If it is known that, say, income varies with certain SNPs, those SNPs might be used as a control in a study of how certain environmental factors affect income.\nTheir last suggestion is that the information obtained from genoeconomics could be used to understand variation in policy response across people. Compared to the standard economic assumption that everyone is the same, this might be the most radical effect of the genoeconomics enterprise."
  },
  {
    "objectID": "posts/gerd-gigerenzers-gut-feelings-short-cuts-to-better-decision-making.html",
    "href": "posts/gerd-gigerenzers-gut-feelings-short-cuts-to-better-decision-making.html",
    "title": "Gerd Gigerenzer’s Gut Feelings: Short Cuts to Better Decision Making",
    "section": "",
    "text": "For many years I have been influenced by Gerd Gigerenzer’s arguments about the power of simple heuristics and the underlying rationality to many human decisions. But I have contrasting reactions to different parts of Gerd Gigerenzer’s body of work.\nHis published collections of essays - Simple Heuristics That Make Us Smart (with Peter Todd and the ABC research group), Adaptive Thinking and Rationality for Mortals - are fantastic, although some people might find them a touch academic.\nGigerenzer’s popular books are more accessible, but the loss of some of the nuance, plus his greater stridency of argument, push them to a point where I find a fair bit to disagree with.\nIn his most recent book, Risk Savvy, I struggled with how far Gigerenzer extended his arguments about the power of human decision-making. I agree that the heuristics and biases approach can lead us to be overeager in labelling decisions as “irrational” or sub-optimal. “Biased” heuristics can find a better point on the bias-variance trade-off. They are designed to operate in an uncertain world, not in a lab. But there is little doubt that humans err in some cases - particularly in environments with no resemblance to those in which we evolved. Gigerenzer can be somewhat quick to disparage use of data and praise gut instinct in environments where there is little evidence that these instincts work.\nGigerenzer’s earlier Gut Feelings: Short Cuts to Better Decision Making strikes perhaps the best balance between nuance and accessibility. While it still leaves an impression about the accuracy of our instincts that I’m not completely in agreement with, it provides a good overview of how our gut feelings can lead to good decisions.\nGigerenzer defines a gut feeling - which you might also call an intuition or hunch - as a feeling that appears quickly in consciousness, with us unaware of the underlying reasons, but strong enough for us to act on. Gut feelings work through simple rules of thumb that take advantage of the evolved capacities of the brain. The skill of the unconscious is knowing what rule to apply at what time.\nLet’s break this down.\nThe power of simple rules\nGut feelings can be powerful tools despite (and because of) their reliance rules of thumb. Often in decision-making, “less is more”, in that there is a beneficial degree of ignorance, or benefits to excluding information from consideration. The recognition heuristic is an example of this: if you recognise one option but not the other, infer that the recognised option has the higher value. The recognition heuristic only works if recognise one but not the other option.\nIn contrast, complex strategies can explain too much in hindsight. In an uncertain world where only part of the information is useful for the future, a simple rule that focuses on only the best or a limited subset of information has a good chance of hitting that useful information. Gigerenzer provides plenty of examples of the superiority or utility of simple rules of thumb, a point that many advocates of complex statistical methods and machine learning should hear.\nBut sometimes Gigerenzer’s examples drift toward becoming straw man competitions. For instance, he describes a competition between two models - multinomial regression and a heuristic called “Take the best” - in predicting school drop-out rates. Take the best operates by looking only at the cue which has the strongest relationship with drop-out rates (such as the attendance rate), and if one is higher than the other, you make a decision at that point. If the cues have the same value, move to the next cue and repeat.\nThe two models were trained on half the data, and tested against the other half of the data. Take the best achieved 65% accuracy in the training data, and 60% on the test data. In contrast, multinomial regression achieved 72% on training data, but this plunged to 54% on test data. (Gigerenzer only shows a chart in the book - I got the numbers from the related chapter of Simple Heuristics That Make Us Smart.) Multinomial regression overfit the training data.\nThis victory for Take the best sounds impressive, but there were observations for only 57 schools, with half the data used in training. Of course basing a prediction on a regression with 18 variables and twenty-odd observations is rubbish. I wouldn’t expect anything else. Gigerenzer often frames the victory of simple rules such as Take the Best as surprising to others (and originally to him), which it might be at a general level. But when you look at many of the specific examples and the numbers involved, the surprise doesn’t last long.\nThere is some more subtlety in the reporting of these results in Simple Heuristics That Make Us Smart, where the prediction of drop out rates was one of 20 “competitions” between Take the Best and multiple regression. The overall gap between Take the Best and multiple regression on the test data was 71% versus 68%, an impressive but narrow victory for Take the Best despite its reliance on far fewer cues.\nThat said, most of the competitions involved small samples - an area where the simple heuristics excel. Only three of the 20 had more than 30 examples available for training the model. The models also had access to dichotomised, not numerical, values, further decreasing the utility of regression. There is a tie at 76% apiece when numerical values were used. The tie is still an impressive result for the simple Take the Best heuristic, but this is now some way from the headline story we get in Gut Feelings. (Conversely, I should also note that the territory of these competitions was fairly stable, which might give more complex techniques an edge. Move to an uncertain dynamic environment, and the simple heuristics may gain an advantage even if the datasets are much larger.)\nHow humans use these heuristics\nAn important part of Gigerenzer’s argument is that these simple heuristics are used by humans. An example he provides is a picture of a boy’s head surrounded by four chocolate bars. Which bar does Charlie want? The one he is looking at. The simple heuristic is that “If a person looks at one alternative (longer than at others), it is likely the one the person desires.”\nThe gaze heuristic is another example. Someone seeking to catch a ball will run so as to maintain the angle of the ball in their gaze. The gaze heuristic will eventually lead them to where the ball will land. They don’t simply compute where the ball will land and then run there.\nThe question of whether humans use these heuristics has been tested in the lab. People have been demonstrated to rely heavily on the recognition heuristic when picking winners of tennis matches and football games, particularly where they are unfamiliar with the teams, or in determining which of two cities is larger. Less is more, as if you know all the teams or cities, you can’t use the recognition heuristic. This gives the people using these heuristics surprising predictive power, close (or superior) to more knowledgeable experts.\nAn interesting question about these heuristics is how someone knows when they should apply a particular heuristic. Gigerenzer notes that the skill of the unconscious is knowing, without thinking, what rule to apply at what time. This is the least satisfactory piece of the book, with little discussion as to how this process might work or be effective. It is fair to say the selection is unconscious - people are particularly poor at explaining what rule they applied - but are they skilful at this selection?\nThe other question mark relates to the inconsistency of our decisions. As Daniel Kahneman and friends have written about recently, human decisions are often noisy, with decisions varying across occasions. If we are applying heuristics, why do our decisions appear so haphazard in some environments? Does our selection of heuristics only work where we have had the right experience with feedback? More on that below.\nApplied gut feelings\nA point that Gigerenzer highlights - one of his important contributions to how we should think about the heuristics and biases approach - is that the structure of the environment is central to how well a rule of thumb works. A rule of thumb is not good or bad in itself, but depends on the environment in which it is used.\nThis point was earlier made by Herbert Simon, with his description of the capabilities of the decision maker, and the environment in which they are used, as blades on a pair of scissors. You cannot assess one without the other.\nWhere I find the discussion of rules of thumb becomes most interesting is in complex environments where we need to learn the rules of thumb to be applied. The heuristic of following someone else’s gaze to determine what they are talking about is something that one-year olds do. But consider a hospital, where a doctor is trying to determine whether someone is having a heart attack. Or a CEO deciding whether to support a merger.\nGigerenzer points out - as you can also see in work by others such as Gary Klein - that you need feedback to develop expertise. Absent feedback you are likely to fall back on rules that don’t work or that achieve other purposes. Gigerenzer gives the example of judges who are not given feedback on their parole decisions. They then fall back on the heuristic of protecting themselves from criticism by simply following the police and prosecution recommendation.\nGigerenzer offered a few examples where I was not clear on how that expertise could develop. One involves discussion of the benefits of strategies that involve incremental change toward a solution, rather than first computing the ideal solution and acting on it. The gaze heuristic is a good example of this, whereby someone seeking to catch a ball maintains the angle of the ball in their gaze, with this heuristic eventually leading them to where it will land. They don’t simply compute where the ball will land and then run there.\nGigerenzer extends this argument to the setting of company budgets:\n\nStrategies relying on incremental changes also characterize how organizations decide on their yearly budgets. At the Max Planck Institute where I work, my colleagues and I make slight adjustments to last year’s budget, rather than calculating a new budget from scratch. Neither athletes nor business administrators need to know how to calculate the trajectory of the ball or the business. An intuitive “shortcut” will typically get them where they would like to be, and with a smaller chance of making grave errors.\n\nThe idea of lower probability of “grave error” might be right. But how does someone learn this skill? And here is Dan Lovallo and Olivier Sibony writing on the same concept:\n\nIt has been another long, exhausting budget meeting. As the presenters showed you their plans, you challenged every number, explored every assumption. In the end you raised their targets a little, but, if you’re honest, you have to admit it: the budget this unit will have to deliver next year is not very different from the one they proposed at the beginning of the budget process, which in turn is not very different from the latest forecast for this year.\n\n\nWhat happened? The short answer is, you’ve been anchored. Anchoring is the psychological phenomenon that makes a number stick in your mind and influence you — even though you think you’re disregarding it.\n\nI have some sympathy to the Lovallo and Sibony assessment, having sat in numerous organisations where it was near unanimously agreed that the budget needed to be reallocated, but the status quo prevailed. But I’m not overly convinced it was due to anchoring, rather than trenchant self-interest of those who might be affected, and a timidity and desire to avoid conflict on the behalf of the decision makers. It would be interesting to see a study on this. (Maybe it’s out there - I briefly searched, but not particularly hard).\nAn interesting story in the chapter about medical environments concerned doctors who were required to judge whether someone was having a heart attack. The doctors were doing a generally poor job, defensively sending 90% of people with chest pain to the coronary care unit.\nSome researchers developed a process whereby doctors would use a complicated chart with 50-odd probabilities, a long formula and a pocket calculator to determine whether a patient should be admitted to the coronary care unit. The doctors didn’t like it and didn’t understand it, but its use improved their decision-making and reduced overcrowding in the coronary care unit.\nThe researchers then took the chart and calculator away from the doctors, with the expectation that the decision-making quality would decline back to what it was previously. But the decision quality did not drop. Exposure to the calculator had improved their intuition permanently. What the doctors needed was the cues that they could not learn from experience, but when provided with them, they applied them in a fast and frugal way that matched the accuracy of the more complicated procedure.\nAs an aside, the above is how the story is told in Gut Feelings, which might have been coloured by some discussion between Gigerenzer and the researchers. My reading of the related article (pdf minus charts) has a different chain of events. The researchers first developed the tool using patient data, and presented their results to the doctors. Seven months later, the tool was trialed. They found that admissions to the coronary care unit had declined following the presentation, but not on introduction of the tool, suggesting the doctors started using the cues after the presentation and could achieve equal superiority through their own decision processes. The paper notes that “Take the Best” and tallying - simply adding up the number of cues - would be good strategies. Gigerenzer takes the analysis further here.\nAs a second aside, this story is similar to one by Daniel Kahneman tells in Thinking Fast and Slow where military recruiters were asked to use a mechanical process to select candidates. After protesting that they were not robots, Kahneman suggested that after collecting the required data, the recruiters close their eyes, imagine the recruit as a soldier and assign a score of one to five. It turned out the “close your eyes” score was as accurate as the sum of the six factors that were collected, both being much better than the useless interviewing technique they had replaced. Intuition worked, but only after disciplined collection of data (cues).\nAnd as a third aside and contrast, here’s a story from another study (quoted text from here):\n\nDuring an 18 month period the authors used a computer-based diagnosis system which surpassed physicians in diagnostic accuracy. During the course of this research after each physician made a diagnosis, he or she was informed of the computer’s diagnosis. The diagnostic accuracy of the physicians gradually rose toward that of the computer during the 18 month period. The authors attributed this improvement in part to the “discipline” forced upon the physicians, the constraint of carefully collecting patient information, the “constant emphasis on reliability of clinical data collected, and the rapid ‘feedback’ from the computer,” which may have promoted learning. When the computer system was terminated, the physicians very quickly reverted to their previous lower level of diagnostic accuracy. Apparently discipline and reliability fell victim to creativity and inconsistency.\n\nThe rest of the book\nGigerenzer provides plenty of other thought-provoking material about the role of heuristics and gut feeling in various domains. Sometimes it feels a bit shallow Advertising is put down to the recognition heuristic. What about signalling, discussed shortly after in another context? The final couple of chapters relating to moral behaviour and social instincts seemed somewhat out-of-date when looked at next to the burgeoning literature on cultural transmission and learning. But there are enough interesting ideas in those chapters to make them worthwhile. And you can’t expect someone to pin every point down in-depth in a popular book.\nSo, if you want a dose of Gigerenzer, Gut Feelings is interesting and worth reading. But if you have the patience, I recommend starting with Simple Heuristics That Make Us Smart, Adaptive Thinking and Rationality for Mortals. Then if you want a slightly less “academic” Gigerenzer, move on to Gut Feelings."
  },
  {
    "objectID": "posts/getting-the-right-human-machine-mix.html",
    "href": "posts/getting-the-right-human-machine-mix.html",
    "title": "Getting the right human-machine mix",
    "section": "",
    "text": "Much of the storytelling about the future and humans and machines runs with a theme that machines will not replace us, but that we will work with machines to create a combination greater than either alone. If you have heard the freestyle chess example, which now seems to be everywhere, you will understand the idea. (See my article in Behavioral Scientist if you haven’t.)\nAn interesting angle to this relationship is just how unsuited some of our existing human-machine combinations are for the unique skills of a human brings. As Don Norman writes in his excellent The Design of Everyday Things:\n\nPeople are flexible, versatile, and creative. Machines are rigid, precise, and relatively fixed in their operations. There is a mismatch between the two, one that can lead to enhanced capability if used properly. Think of an electronic calculator. It doesn’t do mathematics like a person, but can solve problems people can’t. Moreover, calculators do not make errors. So the human plus calculator is a perfect collaboration: we humans figure out what the important problems are and how to state them. Then we use calculators to compute the solutions.\nDifficulties arise when we do not think of people and machines as collaborative systems, but assign whatever tasks can be automated to the machines and leave the rest to people. This ends up requiring people to behave in machine like fashion, in ways that differ from human capabilities. We expect people to monitor machines, which means keeping alert for long periods, something we are bad at. We require people to do repeated operations with the extreme precision and accuracy required by machines, again something we are not good at. When we divide up the machine and human components of a task in this way, we fail to take advantage of human strengths and capabilities but instead rely upon areas where we are genetically, biologically unsuited.\n\nThe result is that at the moments when we expect the humans to act, we have set them up for failure:\n\nWe design equipment that requires people to be fully alert and attentive for hours, or to remember archaic, confusing procedures even if they are only used infrequently, sometimes only once in a lifetime. We put people in boring environments with nothing to do for hours on end, until suddenly they must respond quickly and accurately. Or we subject them to complex, high-workload environments, where they are continually interrupted while having to do multiple tasks simultaneously. Then we wonder why there is failure.\n\nAnd:\n\nAutomation keeps getting more and more capable. Automatic systems can take over tasks that used to be done by people, whether it is maintaining the proper temperature, automatically keeping an automobile within its assigned lane at the correct distance from the car in front, enabling airplanes to fly by themselves from takeoff to landing, or allowing ships to navigate by themselves. When the automation works, the tasks are usually done as well as or better than by people. Moreover, it saves people from the dull, dreary routine tasks, allowing more useful, productive use of time, reducing fatigue and error. But when the task gets too complex, automation tends to give up. This, of course, is precisely when it is needed the most. The paradox is that automation can take over the dull, dreary tasks, but fail with the complex ones.\nWhen automation fails, it often does so without warning. … When the failure occurs, the human is “out of the loop.” This means that the person has not been paying much attention to the operation, and it takes time for the failure to be noticed and evaluated, and then to decide how to respond.\n\nThere is an increasing catalogue of these types of failures. Air France flight 447, which crashed into the Atlantic in 2009, is a classic case. The autopilot suddenly handed to the pilots an otherwise well-functioning plane due to an airspeed indicator problem, leading to disaster. But perhaps this new type of failure is an acceptable result of the overall improvement in system safety or performance.\nThis human-machine mismatch is also a theme in Charles Perrow’s Normal Accidents. Perrow notes that many systems are poorly suited to human psychology, with long periods of inactivity interspersed by bunched workload. The humans are often pulled into the loop just at the moments things are starting to go wrong. The question is not how much work humans can safely do, but how little."
  },
  {
    "objectID": "posts/gigerenzer-versus-kahneman-and-tversky-the-1996-face-off.html",
    "href": "posts/gigerenzer-versus-kahneman-and-tversky-the-1996-face-off.html",
    "title": "Gigerenzer versus Kahneman and Tversky: The 1996 face-off",
    "section": "",
    "text": "Through the late 1980s and early 1990s, Gerd Gigerenzer and friends wrote a series of articles critiquing Daniel Kahneman and Amos Tversky’s work on heuristic and biases. They hit hard. As Michael Lewis wrote in The Undoing Project:\n\nGigerenzer had taken the same angle of attack as most of their other critics. But in Danny and Amos’s view he’d ignored the usual rules of intellectual warfare, distorting their work to make them sound even more fatalistic about their fellow man than they were. He also downplayed or ignored most of their evidence, and all of their strongest evidence. He did what critics sometimes do: He described the object of his scorn as he wished it to be rather than as it was. Then he debunked his description. … “Amos says we absolutely must do something about Gigerenzer,” recalled Danny. … Amos didn’t merely want to counter Gigerenzer; he wanted to destroy him. (“Amos couldn’t mention Gigerenzer’s name without using the word ‘sleazeball,’” said UCLA professor Craig Fox, Amos’s former student.) Danny, being Danny, looked for the good in Gigerenzer’s writings. He found this harder than usual to do.\n\nKahneman and Tversky’s response to Gigerenzer’s work was published in 1996 in Psychological Review. It was one of the blunter responses you will read in academic debates, as the following passages indicate. From the first substantive section of the article:\n\nIt is not uncommon in academic debates that a critic’s description of the opponent’s ideas and findings involves some loss of fidelity. This is a fact of life that targets of criticism should learn to expect, even if they do not enjoy it. In some exceptional cases, however, the fidelity of the presentation is so low that readers may be misled about the real issues under discussion. In our view, Gigerenzer’s critique of the heuristics and biases program is one of these cases.\n\nAnd the close:\n\nAs this review has shown, Gigerenzer’s critique employs a highly unusual strategy. First, it attributes to us assumptions that we never made … Then it attempts to refute our alleged position by data that either replicate our prior work … or confirm our theoretical expectations … These findings are presented as devastating arguments against a position that, of course, we did not hold. Evidence that contradicts Gigerenzer’s conclusion … is not acknowledged and discussed, as is customary; it is simply ignored. Although some polemic license is expected, there is a striking mismatch between the rhetoric and the record in this case.\n\nBelow are my notes put together on a 16-hour flight on the claims and counterclaims across Gigerenzer’s articles, the Kahneman and Tversky response in Psychological Review, and Gigerenzer’s rejoinder in the same issue. This represents my attempt to get my head around this debate and to understand the degree to which the heat is justified, not to give final judgment (although I do show my leanings). I don’t go to work published after the 1996 articles, although that might be for another day.\nI will use Gigerenzer or Kahneman and Tversky’s words to make their arguments when I can. The core articles I refer to are:\n\nGigerenzer (1991) How to Make Cognitive Illusions Disappear: Beyond “Heuristics and Biases” (pdf)\nGigerenzer (1993) The bounded rationality of probabilistic mental models (pdf)\nKahneman and Tversky (1996) On the Reality of Cognitive Illusions (pdf)\nGigerenzer (1996) On Narrow Norms and Vague Heuristics: A Reply to Kahneman and Tversky (1996) (pdf)\nKahneman and Tversky (1996) Postscript (at the end of their 1996 paper)\nGigerenzer (1996) Postscript (at the end of his 1996 paper)\n\nI recommend reading those articles, along with Kahneman and Tversky’s classic Science article (pdf) as background. (And note that the below debate and Gigerenzer’s critique only relates to two of the 12 “biases” covered in that paper.)\nI touch on four of Gigerenzer’s arguments (using most of my word count on the first), although there are numerous other fronts:\n\nArgument 1: Does the use of frequentist rather than probabilistic representations make many of the so-called biases disappear? Despite appearances, Kahneman, Tversky and Gigerenzer largely agree on the answer to this question. However, it was largely Gigerenzer’s work that brought this to my attention, so there was clearly some value (for me) to Gigerenzer’s focus.\nArgument 2: Can you attribute probabilities to single events? Gigerenzer says no. Here there is a fundamental disagreement. I largely agree with Kahneman and Tversky as to whether this point is fatal to their work.\nArgument 3: Are Kahneman and Tversky’s norms content blind? For particular examples, yes. Generally? No.\nArgument 4: Should more effort be expended in understanding the underlying cognitive processes or mental models behind these various findings? This is where Gigerenzer’s argument is strongest, and I agree that many of Kahneman and Tversky’s proposed heuristics have weaknesses that need examination.\n\nPutting these four together, I have sympathy for Gigerenzer’s way of thinking and ultimate program of work, but I am much less sympathetic to his desire to pull down Kahneman and Tversky’s findings on the way.\nNow into the details.\nArgument 1: Does the use of frequentist rather than probabilistic representations make many of the so-called biases disappear?\nGigerenzer’s argues that many biases involving probabilistic decision-making can be “made to disappear” by framing the problems in terms of frequencies rather than probabilities. The back-and-forth on this point centres on three major biases: overconfidence, the conjunction fallacy and base-rate neglect. I’ll take each in turn.\nOverconfidence\nA typical question from the overconfidence literature reads as follows:\n\nWhich city has more inhabitants?\n\nHyderabad, (b) Islamabad\n\nHow confident are you that your answer is correct?\n50% 60% 70% 80% 90% 100%\n\nAfter answering many questions of this form, the usual finding is that where people are 100% confident they had the correct answer, they might be correct only 80% of the time. When 80% confident, they might get only 65% correct. This discrepancy is often called “overconfidence”. [I’ve written elsewhere about the need to disambiguate different forms of overconfidence.]\nThere are numerous explanations for this overconfidence, such as confirmation bias, although in Gigerenzer’s view this is “a robust fact waiting for a theory”.\nBut what if we take a different approach to this problem. Gigerenzer (1991) writes:\n\nAssume that the mind is a frequentist. Like a frequentist, the mind should be able to distinguish between single-event confidences and relative frequencies in the long run.\nThis view has testable consequences. Ask people for their estimated relative frequencies of correct answers and compare them with true relative frequencies of correct answers, instead of comparing the latter frequencies with confidences.\n\nHe tested this idea as follows:\n\nSubjects answered several hundred questions of the Islamabad-Hyderabad type … and in addition, estimated their relative frequencies of their correct answers. …\nAfter a set of 50 general knowledge questions, we asked the same subjects, “How many of these 50 questions do you think you got right?”. Comparing their estimated frequencies with actual frequencies of correct answers made “overconfidence” disappear. …\nThe general point is (i) a discrepancy between probabilities of single events (confidences) and long-run frequencies need not be framed as an “error” and called “overconfidence bias”, and (ii) judgments need not be “explained” by a flawed mental program at a deeper level, such as “confirmation bias”.\n\nKahneman and Tversky agree:\n\nMay (1987, 1988) was the first to report that whereas average confidence for single items generally exceeds the percentage of correct responses, people’s estimates of the percentage (or frequency) of items that they have answered correctly is generally lower than the actual number. … Subsequent studies … have reported a similar pattern although the degree of underconfidence varied substantially across domains.\nGigerenzer portrays the discrepancy between individual and aggregate assessments as incompatible with our theoretical position, but he is wrong. On the contrary, we drew a distinction between two modes of judgment under uncertainty, which we labeled the inside and the outside views … In the outside view (or frequentistic approach) the case at hand is treated as an instance of a broader class of similar cases, for which the frequencies of outcomes are known or can be estimated. In the inside view (or single-case approach) predictions are based on specific scenarios and impressions of the particular case. We proposed that people tend to favor the inside view and as a result underweight relevant statistical data. …\nThe preceding discussion should make it clear that, contrary to Gigerenzer’s repeated claims, we have neither ignored nor blurred the distinction between judgments of single and of repeated events. We proposed long ago that the two tasks induce different perspectives, which are likely to yield different estimates, and different levels of accuracy (Kahneman and Tversky, 1979). As far as we can see, Gigerenzer’s position on this issue is not different from ours, although his writings create the opposite impression.\n\nSo we leave this point with a degree of agreement.\nConjunction fallacy\nThe most famous illustration of the conjunction fallacy is the “Linda problem”. Subjects are shown the following vignette:\n\nLinda is 31 years old, single, outspoken and very bright. She majored in philosophy. As a student, she was deeply concerned with issues of discrimination and social justice, and also participated in antinuclear demonstrations.\n\nThey are then asked which of the following two alternatives was more probable (either as just those two options, as part of a longer list of options, or across different experimental subjects):\n\nLinda is a bank teller\nLinda is a bank teller and is active in the feminist movement\n\nIn the original Tversky and Kahneman experiment, when shown only those two options, 85% of subjects chose the second. Tversky and Kahneman argued this was an error as the probability of the conjunction of two events can never be greater than one of its constituents.\nOnce again Gigerenzer reframed for the frequentist mind (quoting from the 1996 article):\n\nThere are 100 persons who fit the description above (i.e. Linda’s). How many of them are:\n\nbank tellers\nbank tellers and active in the feminist movement.\n\n\nAs Gigerenzer states:\n\nIf the problem is phrased in this (or a similar) frequentist way, then the “conjunction fallacy” largely disappears.\n…\nThe postulated representativeness heuristic cannot account for this dramatic effect.\n\nGigerenzer’s 1993 article expands on this latter point:\n\nIf the mind solves the problem using a representative heuristic, changes in representation should not matter, because they do not change the degree of similarity. … Subjects therefore should still exhibit the conjunction fallacy.\n\nKahneman and Tversky’s response starts with the note that their first demonstration of the conjunction fallacy involved judgments of frequency. They asked subjects:\n\nto estimate the number of “seven-letter words of the form ‘—–n-’ in 4 pages of text.” Later in the same questionnaire, those subjects estimated the number of “seven-letter words of the form ‘—-ing’ in 4 pages of text.” Because it is easier to think of words ending with “ing” than to think of words with “n” in the next-to-last position, availability suggests that the former will bejudged more numerous than the latter, in violation of the conjunction rule. Indeed, the median estimate for words ending with “ing” was nearly three times higher than for words with “n” in the next-to-the-last position. This finding is a counter-example to Gigerenzer’s often repeated claim that conjunction errors disappear in judgments of frequency, but we have found no mention of it in his writings.\n\nHere Gigerenzer stretches his defence of human consistency a step too far:\n\n[T]he effect depends crucially on presenting the two alternatives to a participant at different times, that is, with a number (unspecified in their reports) of other tasks between the alternatives. This does not seem to be a violation of internal consistency, which I take to be the point of the conjunction fallacy.\n\nKahneman and Tversky also point out that they they had studied the effect of frequencies in other contexts:\n\nWe therefore turned to the study of cues that may encourage extensional reasoning and developed the hypothesis that the detection of inclusion could be facilitated by asking subjects to estimate frequencies. To test this hypothesis, we described a health survey of 100 adult men and asked subjects, “How many of the 100 participants have had one or more heart attacks?” and “How many of the 100 participants both are over 55 years old and have had one or more heart attacks?” The incidence of conjunction errors in this problem was only 25%, compared to 65% when the subjects were asked to estimate percentages rather than frequencies. Reversing the order of the questions further reduced the incidence to 11%.\n\nKahneman and Tversky go on to state:\n\nGigerenzer has essentially ignored our discovery of the effect of frequency and our analysis of extensional cues. As primary evidence for the “disappearance” of the conjunction fallacy in judgments of frequency, he prefers to cite a subsequent study by Fiedler (1988), who replicated both our procedure and our findings, using the bank-teller problem. … In view of our prior experimental results and theoretical discussion, we wonder who alleged that the conjunction fallacy is stable under this particular manipulation.\n\nGigerenzer concedes, but then turns to Kahneman and Tversky’s lack of focus on this result:\n\nIt is correct that they demonstrated the effect on conjunction violations first (but not for overconfidence bias and the base-rate fallacy). Their accusation, however, is out of place, as are most others in their reply. I referenced their demonstration in every one of the articles they cited … It might be added that Tversky and Kahneman (1983) themselves paid little attention to this result, which was not mentioned once in some four pages of discussion.\n\nA debate about who was first and how much focus each gave to the findings is not substantive, but Kahneman and Tversky (1996) do not leave this problem here. While the frequency representation can reduce error when there is the possibility of direct comparison (the same subject sees and provides frequencies for both alternatives), they have less effect in between-subject experiment designs; that is, where one set of subjects will see one of the options and another set of subject the other:\n\nLinda is in her early thirties. She is single, outspoken, and very bright. As a student she majored in philosophy and was deeply concerned with issues of discrimination and social justice.\nSuppose there are 1,000 women who fit this description. How many of them are\n\nhigh school teachers?\nbank tellers? or\nbank tellers and active feminists?”\n\nOne group of Stanford students (N = 36) answered the above three questions. A second group (N = 33) answered only questions (a) and (b), and a third group (N = 31) answered only questions (a) and (c). Subjects were provided with a response scale consisting of 11 categories in approximately logarithmic spacing. As expected, a majority (64%) of the subjects who had the opportunity to compare (b) and (c) satisfied the conjunction rule. In the between-subjects comparison, however, the estimates for feminist bank tellers (median category: “more than 50”) were significantly higher than the estimates for bank tellers … Contrary to Gigerenzer’s position, the results demonstrate a violation of the conjunction rule in a frequency formulation. These findings are consistent with the hypothesis that subjects use representativeness to estimate outcome frequencies and edit their responses to obey class inclusion in the presence of strong extensional cues.\n\nGigerenzer in part concedes, and in part battles on:\n\nHence, Kahneman and Tversky (1996) believe that the appropriate reply is to show that frequency judgments can also fail. There is no doubt about the latter …\n[T]he between subjects version of the Linda problem is not a violation of internal consistency, because the effect depends on not presenting the two alternatives to the same subject.\n\nIt’s right not to describe this as a violation of internal consistency, but for evidence of representativeness affecting judgement and doing so even with frequentist representations, it makes a good case. It is also difficult to argue that the subjects are making a good judgment. Kahneman and Tversky write:\n\nGigerenzer appears to deny the relevance of the between-subjects design on the ground that no individual subject can be said to have committed an error. In our view, this is hardly more reasonable than the claim that a randomized between-subject design cannot demonstrate that one drug is more effective than another because no individual subject has experienced the effects of both drugs.\n\nKahneman and Tversky write further in the postscript, possibly conceding on language but not on their substantive point:\n\nThis formula will not do. Whether or not violations of the conjunction rule in the between-subjects versions of the Linda and “ing” problems are considered errors, they require explanation. These violations were predicted from representativeness and availability, respectively, and were observed in both frequency and probability judgments. Gigerenzer ignores this evidence for our account and offers no alternative.\n\nI’m with Kahneman and Tversky here.\nBase-rate neglect\nBase-rate neglect (or the base-rate fallacy) describes situations where a known base rate of an event or characteristic in a reference population is under-weighted, with undue focus given to specific information on the case at hand. An example is as follows:\n\nIf a test to detect a disease whose prevalence is 1/1000 has a false positive rate of 5%, what is the chance that a person found to have a positive result actually has the disease, assuming you know nothing about the person’s symptoms or signs?\n\nThe typical result is that around half of the people asked will guess a probability of 95% (even among medical professionals), with less than a quarter giving the correct answer of 2%. The positive result, which has associated errors, is weighted too heavily relative to the base rate of one in a thousand.\nGigerenzer (1991) once again responds with the potential of a frequentist representation to eliminate the bias, drawing on work by Cosmides and Tooby (1990) [The 1990 paper was an unpublished conference paper, but this work was later published here (pdf)]:\n\nOne our of 1000 Americans has disease X. A test has been developed to detect when a person has disease X. Every time the test is given to a person who has he disease, the test comes out positive. But sometimes the test also comes out positive when it is given to a person who is completely healthy. Specifically, out of every 1000 people who are perfectly healthy, 50 of them test positive for the disease.\nImagine that we have assembled a random sample of 1000 Americans. They were selected by a lottery. Those who conducted the lottery had no information about the health status of any of these people. How many people who test positive for the disease will actually have the disease? — out of —.\n\nThe result:\n\nIf the question was rephrased in a frequentist way, as shown above, then the Bayesian answer of 0.02 - that is, the answer “one out of 50 (or 51); - was given by 76% of the subjects. The”base-rate fallacy” disappeared.\n\nKahneman and Tversky (1996) do not respond to this particular example, beyond a footnote:\n\nCosmides and Tooby (1996) have shown that a frequentistic formulation also helps subjects solve a base-rate problem that is quite difficult when framed in terms of percentages or probabilities. Their result is readily explained in terms of extensional cues to set inclusion. These authors, however, prefer the speculative interpretation that evolution has favored reasoning with frequencies but not with percentages.\n\nIt seems we have agreement on the effect, although a differing interpretation.\nKahneman and Tversky, however, more directly attack the idea that people are natural frequentists.\n\nHe [Gigerenzer] offers a hypothetical example in which a physician in a nonliterate society learns quickly and accurately the posterior probability of a disease given the presence or absence of a symptom. … However, Gigerenzer’s speculation about what a nonliterate physician might learn from experience is not supported by existing evidence. Subjects in an experiment reported by Gluck and Bower (1988) learned to diagnose whether a patient has a rare (25%) or a common (75%) disease. For 250 trials the subjects guessed the patient’s disease on the basis of a pattern of four binary symptoms, with immediate feedback. Following this learning phase, the subjects estimated the relative frequency of the rare disease, given each of the four symptoms separately.\nIf the mind is “a frequency monitoring device,” as argued by Gigerenzer …, we should expect subjects to be reasonably accurate in their assessments of the relative frequencies of the diseases, given each symptom. Contrary to this naive frequentist prediction, subjects’ judgments of the relative frequency of the two diseases were determined entirely by the diagnosticity of the symptom, with no regard for the base-rate frequencies of the diseases. … Contrary to Gigerenzer’s unqualified claim, the replacement of subjective probability judgments by estimates of relative frequency and the introduction of sequential random sampling do not provide a panacea against base-rate neglect.\n\nGigerenzer (1996) responds:\n\nConcerning base-rate neglect, Kahneman and Tversky … created the impression that there is little evidence that certain types of frequency formats improve Bayesian reasoning. They do not mention that there is considerable evidence (e.g., Gigerenzer & Hoffrage, 1995) and back their disclaimer principally with a disease-classification study by Gluck and Bower (1988), which they summarized thus: “subjects’ judgments of the relative frequency . . . were determined entirely by the diagnosticity of the symptom, with no regard for the base-rate frequencies of the diseases” … To set the record straight, Gluck and Bower said their results were consistent with the idea that “base-rate information is not ignored, only underused” (p. 235). Furthermore, their study was replicated and elaborated on by Shanks (1991), who concluded that “we have no conclusive evidence for the claim . . . that systematic base-rate neglect occurs in this type of situation” (p. 153). Adding up studies in which base-rate neglect appears or disappears will lead us nowhere.\n\nGigerenzer is right that Kahneman and Tversky were overly strong in their description of the findings of the Gluck and Bower study, but Gigerenzer’s conclusion seems close to that of Kahneman and Tversky. As Kahneman and Tversky wrote:\n\n[I]t is evident that subjects sometimes use explicitly mentioned base-rate information to a much greater extent than they did in our original engineer-lawyer study [another demonstration of base-rate neglect], though generally less than required by Bayes’ rule.\n\nArgument 2: Can you attribute probabilities to single events?\nWhile I leave the question of frequency representations with a degree of agreement, Gigerenzer has a deeper critique of Kahneman and Tversky’s findings. From his 1993 article:\n\nIs the conjunction fallacy a violation of probability theory? Has a person who chooses T&F violated probability theory? The answer is no, if the person is a frequentist such as Richard von Mises or Jerzy Neyman; yes, if he or she is a subjectivist such as Bruno de Finetti; and open otherwise.\nThe mathematician Richard von Mises, one of the founders of the frequency interpretation, used the following example to make his point:\n\nWe can say nothing about the probability of death of an individual even if we know his condition of life and health in detail. The phrase ‘probability of death’, when it refers to a single person, has no meaning at all for us. This is one of the most important consequences of our definition of probability.\n(von Mises, 1957/1928: 11)\n\nIn this frequentist view, one cannot speak of a probability unless a reference class has been defined. … Since a person is always a member of many reference classes, no unique relative frequency can be assigned to a single person. … Thus, for a strict frequentist, the laws of probability are about frequencies and not about single events such as whether Linda is a bank teller. There, in this view, no judgement about single events can violate probability theory.\n… Seen from the Bayesian point of view, the conjunction fallacy is an error.\nThus, choosing T&F in the Linda problem is not a reasoning error. What has been labelled the ‘conjunction fallacy’ here does not violate the laws of probability. It only looks so from one interpretation of probability.\n\nHe writes in his 1991 article somewhat more strongly (here talking in the context of overconfidence):\n\nFor a frequentist like the mathematician Richard von Mises, the term “probability”, when it refers to a single event, “has no meaning at all for us” … Probability is about frequencies, not single events. To compare the two means comparing applies with oranges.\nEven the major opponents of the frequentists - subjectivists such as Bruno de Finetti - would not generally think of a discrepancy between confidence and relative frequency as a “bias”, albeit for different reasons. For a subjectivist, probability is about single events, but rationality is identified with the internal consistency of subjective probabilities. As de Finetti emphasized, “however an individual evaluates the probability of a particular event, no experience can prove him right, or wrong; nor, in general, could any conceivable criterion give any objective sense to the distinction one would like to draw, here, between right and wrong” …\n\nKahneman and Tversky address this argument across a few of the biases under debate. First, on conjunction errors:\n\nWhether or not it is meaningful to assign a definite numerical value to the probability of survival of a specific individual, we submit (a) that this individual is less likely to die within a week than to die within a year and (b) that most people regard the preceding statement as true—not as meaningless—and treat its negation as an error or a fallacy.\n\nIn response, Gigerenzer makes an interesting point that someone asked that question might make a different inference:\n\nOne can easily create a context, such as a patient already on the verge of dying, that would cause a sensible person to answer that this patient is more likely to die within a week (inferring that the question is next week versus the rest of the year, because the question makes little sense otherwise). In the same fashion, the Linda problem creates a context (the description of Linda) that makes it perfectly valid not to conform to the conjunction rule.\n\nI think Gigerenzer is right that if you treat the problem as content-blind you might miss the inference the subjects are drawing from the question (more on content-blind norms below). But conversely, Kahneman and Tversky’s general point appears sound.\nKahneman and Tversky also address this frequentist argument in relation to over-confidence:\n\nProper use of the probability scale is important because this scale is commonly used for communication. A patient who is informed by his surgeon that she is 99% confident in his complete recovery may be justifiably upset to learn that when the surgeon expresses that level of confidence, she is actually correct only 75% of the time. Furthermore, we suggest that both surgeon and patient are likely to agree that such a calibration failure is undesirable, rather than dismiss the discrepancy between confidence and accuracy on the ground that “to compare the two means comparing apples and oranges”\n\nGigerenzer’s response here is amusing:\n\nKahneman and Tversky argued that the reluctance of statisticians to make probability theory of norm of all single events “is not generally shared by the public” (p. 585). If this was meant to shift the burden of justification for their norms from the normative theory of probability to the intuitions of ordinary people, it is exceedingly puzzling. How can people’s intuitions be called upon to substitute for the standards of statisticians, in order to prove that people’s intuitions systematically violate the normative theory of probability?\n\nKahneman and Tversky did not come back on this particular argument, but several points could be made in their favour. First, and as noted above, there can still be errors under frequentist representations. Even if we discard the results with judgments of probability for single events, there is still a strong case for the use of heuristics leading to the various biases.\nSecond, if a surgeon states they are confident that someone has a 99% probability of complete recovery when they are right only 75% of the time, they are making one of two errors. Either they are making a probability estimate of a single event, which has no meaning at all according to Gigerenzer and von Mises, or they are poorly calibrated according to Kahneman and Tversky.\nThird, whatever the philosophically or statistically correct position, we have a practical problem. We have judgements being made and communicated, with subsequent decisions based on those communications. To the extent there are weaknesses in that chain, we will have sub-optimal outcomes.\nPutting this together, I feel this argument leaves us at a philosophical impasse, but Kahneman and Tversky’s angle provides scope for practical application and better outcomes. (Look at the training for the Good Judgment Project and associated improvements in forecasting that resulted).\nArgument 3: Are Kahneman and Tversky’s norms content blind?\nAn interesting question about the norms against which Kahneman and Tversky assess the experimental subjects’ heuristics and biases is whether the norms are blind to the content of the problem. Gigerenzer (1996) writes:\n\n[O]n Kahneman and Tversky’s (1996) view of sound reasoning, the content of the Linda problem is irrelevant; one does not even need to read the description of Linda. All that counts are the terms probable and and, which the conjunction rule interprets in terms of mathematical probability and logical AND, respectively. In contrast, I believe that sound reasoning begins by investigating the content of a problem to infer what terms such as probable mean. The meaning of probable is not reducible to the conjunction rule … For instance, the Oxford English Dictionary … lists “plausible,” “having an appearance of truth,” and “that may in view of present evidence be reasonably expected to happen,” among others. … Similarly, the meaning of and in natural language rarely matches that of logical AND. The phrase T&F can be understood as the conditional “If Linda is a bank teller, then she is active in the feminist movement.” Note that this interpretation would not concern and therefore could not violate the conjunction rule.\n\nThis is a case where I believe Gigerenzer makes an interesting point on the specific case but is wrong on the broader point. As a start, in discussing their initial results for their 1983 paper, Kahneman and Tversky asked whether people were interpreting the language in different ways, such as asking whether people are taking “Linda is a bank teller” to mean “Linda is a bank teller and not active in the feminist movement.” They considered the content of their problem and ran different experimental specifications to attempt to understand what was occurring.\nBut as Kahneman and Tversky state in their postscript, critiquing the Linda problem on this point - and only the within subjects experimental design at that - is a narrow view of their work. The point of the Linda problem is to test whether the representativeness of the description changes the assessment. As they write in their 1996 paper:\n\nPerhaps the most serious misrepresentation of our position concerns the characterization of judgmental heuristics as “independent of context and content” … and insensitive to problem representation … Gigerenzer also charges that our research “has consistently neglected Feynman’s (1967) insight that mathematically equivalent information formats need not be psychologically equivalent” … Nothing could be further from the truth: The recognition that different framings of the same problem of decision or judgment can give rise to different mental processes has been a hallmark of our approach in both domains.\nThe peculiar notion of heuristics as insensitive to problem representation was presumably introduced by Gigerenzer because it could be discredited, for example, by demonstrations that some problems are difficult in one representation (probability), but easier in another (frequency). However, the assumption that heuristics are independent of content, task, and representation is alien to our position, as is the idea that different representations of a problem will be approached in the same way.\n\nThis is a point where you need to look across the full set of experimental findings, rather than critiquing them one-by-one. Other experiments have people violating the conjunction rule while betting on sequences generated by a dice, where there were no such confusions to be had about the content.\nMuch of the issue is also one of focus. Kahneman and Tversky have certainly investigated the question of how representation changes the approach to a problem. However, it is set out in a different way to that Gigerenzer might have liked.\nArgument 4: Should more effort be expended in understanding the underlying cognitive processes or mental models behind these various findings?\nWe now come to an important point: what is the cognitive process behind all of these results? Gigerenzer (1996) writes:\n\nKahneman and Tversky (1996) reported various results to play down what they believe is at stake, the effect of frequency. In no case was there an attempt to figure out the cognitive processes involved. …\nProgress can be made only when we can design precise models that predict when base rates are used, when not, and why\n\nI can see why Kahneman and Tversky focus on the claims regarding frequency representations when Gigerenzer makes such strong statements about making biases “disappear”. The statement that in no case have they attempted to figure out the cognitive processes involved is also overly strong, as a case could be made that the heuristics are those processes.\nHowever, Gigerenzer believes Kahneman and Tversky’s heuristics are too vague for this purpose. Gigerenzer (1996) wrote:\n\nThe heuristics in the heuristics-and-biases program are too vague to count as explanations. … The reluctance to specify precise and falsifiable process models, to clarify the antecedent conditions that elicit various heuristics, and to work out the relationship between heuristics have been repeatedly pointed out … The two major surrogates for modeling cognitive processes have been (a) one-word-labels such as representativeness that seem to be traded as explanations and (b) explanation by redescription. Redescription, for instance, is extensively used in Kahneman and Tversky’s (1996) reply. … Why does a frequency representation cause more correct answers? Because “the correct answer is made transparent” (p. 586). Why is that? Because of “a salient cue that makes the correct answer obvious” (p. 586). or because it “sometimes makes available strong extensional cues” (p. 589). Researchers are no closer to understanding which cues are more “salient” than others, much less the underlying process that makes them so.\n…\nThe reader may now understand why Kahneman and Tversky (1996) and I construe this debate at different levels. Kahneman and Tversky centered on norms and were anxious to prove that judgment often deviates from those norms. I am concerned with understanding the processes and do not believe that counting studies in which people do or do not conform to norms leads to much. If one knows the process, one can design any number of studies wherein people will or will not do well.\n\nThis passage by Gigerenzer captures the state of the debate well. However, Kahneman and Tversky are relaxed about the lack of full specification, and sceptical that process models are the approach to provide that detail. They write in the 1996 postscript:\n\nGigerenzer rejects our approach for not fully specifying the conditions under which different heuristics control judgment. Much good psychology would fail this criterion. The Gestalt rules of similarity and good continuation, for example, are valuable although they do not specify grouping for every display. We make a similar claim for judgmental heuristics.\nGigerenzer legislates process models as the primary way to advance psychology. Such legislation is unwise. It is useful to remember that the qualitative principles of Gestalt psychology long outlived premature attempts at modeling. It is also unwise to dismiss 25 years of empirical research, as Gigerenzer does in his conclusion. We believe that progress is more likely to come by building on the notions of representativeness, availability, and anchoring than by denying their reality.\n\nTo me, this is the most interesting point of the debate. I have personally struggled to grasp the precise operation of many of Kahneman and Tversky’s heuristics and how their operation would change across various domains. But are more precisely specified models the way forward? Which are best at explaining the available data? We have now had over 20 years of work since this debate to see if this is an unwise or fruitful pursuit. But that’s a question for another day."
  },
  {
    "objectID": "posts/gladwells-outliers.html",
    "href": "posts/gladwells-outliers.html",
    "title": "Gladwell’s Outliers",
    "section": "",
    "text": "After flipping through Malcolm Gladwell’s Outliers: The Story of Success late last year, I have finally read the book (nothing like over 30 hours of travel to get through a few).\nHaving heard a few podcasts involving Gladwell (such as this), I knew largely what to expect. Gladwell is strongly on the nurture side of the nature-nurture debate and is dismissive of explanations involving the individual or their inherent traits. While he does (at times) concede that nature might play a role, he suggests this is uninteresting and that we pull this explanation out too often. I think he is right that it may be pulled out too often in explaining the success of a particular person, but there is a large gap between giving nature the right level of focus and ignoring it altogether as Gladwell suggests.\nSo, rather than reviewing the book (as has been done plenty of times in the blogosphere already), there are a few specific parts of the book that I feel are worth mentioning.\nFirst, the main points of agreement. I don’t doubt that for the most extreme of outliers - take Bill Gates or the Beatles - that luck played a large part. If Bill Gates had been born in any other country or if computers were not available to him at school, he would not have founded Microsoft and become the richest man in the world. Similarly, I don’t doubt that an ice hockey player is more likely to become a star if they have the good fortune to be born at the right time of the year. Nassim Taleb makes many similar points in The Black Swan on the role of luck.\nThe other side of this point, however, is that there is still plenty of room for nature to play a part. Why did Bill Gates and Paul Allen, of all the students at their school, take advantage of this opportunity? The January born ice hockey stars are still a very small sample of those born in January, so what distinguishes those January born stars from the others born in January? And the December born players who make it despite their disadvantage?\nIn some ways, Gladwell’s focus on the most extreme of outliers for much of the book is what gives luck such an important role. Take the example he makes of the little benefit to having an IQ above 120. Even if that were true (I am not sure it is - in many sciences those extra IQ points are still worth a bit), 90 per cent of the population has an IQ below 120. IQ is a strong predictor of income, status, health and a raft of other factors. While someone with an IQ of 140 might be as likely as someone with an IQ of 180 to win a Nobel prize, Nobel prizes are not the measure of success for most of us. If Gladwell had been examining success in the way most of us think of (or experience) it, IQ and other inherent abilities cannot be ignored. Or to put it another way, the absence of difference in outcomes between someone at the 99.9th percentile and 99.99th percentile of IQ does not mean it is unimportant for everyone else.\nThis brings me to Gladwell’s strong focus on IQ, as opposed to other heritable characteristics. In Gladwell’s discussion on the link between hard work and maths results, he refers to the Trends in International Mathematics and Science Study (TIMSS) test. It was found that the ranking of countries in this test corresponded to the country ranking for the number of questions answered in the accompanying (and 120 question long) questionnaire. Children who did better at the maths test also filled out more information on their family, education and a raft of other background issues. Gladwell points to this as evidence of the link between hard work and mathematical achievement, as it takes patience to work hard and learn mathematics or to answer the questions. He suggests that any IQ (or inherent quality) based explanation is flawed.\nIgnoring that the ability to fill out a long questionnaire at a young age is probably influenced by IQ (answering questions on family education requires some cognitive skills), Gladwell lines up his attack on IQ but does not question whether a broader suite of heritable traits might be at play. If it is not IQ that is relevant, Gladwell suggests it must be environmental factors. Take time preference (patience), which has a heritable component and would undoubtedly influence competency at mathematics and willingness to complete the survey. Might that play a role? Gladwell draws a similar conclusion on the lack of success of Christopher Langan (with an IQ approaching 200) and suggests that compared to Robert Oppenheimer, he lacked the skills required to navigate the world. Once he claims to have eliminated IQ, Gladwell pins it to environmental causes, despite the possibility that these other skills have a genetic component. (Gladwell’s approach reminded me of a paper by Samuel Bowles and Herbert Gintis on the inheritance of inequality. The attempt to pin down the heritability of the level of income to IQ showed that other genetic factors would need to be considered. I’ll blog about this paper in the next couple of days.)\nTwo other anecdotes stuck out. First was Gladwell’s example of Jewish immigrants coming into New York with a wealth of tailoring skills at just the right time. He suggests that their children went on to become highly successful (usually as lawyers and doctors) after they saw the hard work of their parents in the home - hard work that those parents “lucked into” by having a skill that was suddenly in huge demand. It is a nice story, but it does not explain the success of Jewish immigrants in field after field where high cognitive ability is an advantage - be that banking, law, research (plenty of Nobel prizes there), medicine, and the list goes on. The success occurred on such a broad scale despite the varied (and often disadvantaged) family histories. Should Gladwell be looking further back in time for an explanation?\nHe does that in the other anecdote I found most interesting, which was Gladwell’s discussion of some intractable family disputes in some parts of the United States (think the Hatfields and McCoys). Gladwell suggests that their ancestors originally came from marginally fertile areas where herding dominated and there was a need to establish a strong reputation to protect their herds. This resulted in “cultures of honour” forming, in which misdeeds needed to be punished quickly and brutally to ward off future attacks. When they migrated to the United States, Gladwell suggested that these people brought their culture with them and it has persisted through generations, despite the shift in countries and for many people, significant changes in wealth and status. It is an interesting explanation, but it is one part of the book where innate traits are crying out to be examined. Gladwell did refer to some interesting studies on this issue, which is something that I will definitely be following up."
  },
  {
    "objectID": "posts/grade-inflation-and-the-dunning-kruger-effect.html",
    "href": "posts/grade-inflation-and-the-dunning-kruger-effect.html",
    "title": "Grade inflation and the Dunning-Kruger effect",
    "section": "",
    "text": "The famous Dunning-Kruger effect, in the words of Dunning and Kruger, is a bias where:\n\nPeople tend to hold overly favorable views of their abilities in many social and intellectual domains\n\nin part, because:\n\n[P]eople who are unskilled in these domains suffer a dual burden: Not only do these people reach erroneous conclusions and make unfortunate choices, but their incompetence robs them of the metacognitive ability to realize it.\n\nThere have been plenty of critiques and explanations over the years, including an article by Marian Krajc and Andreas Ortmann who argue the overestimation of ability is partly a signal extraction problem. In environments where people are not provided with feedback on their relative standing, they are will tend to make larger estimation errors.\nKrajc and Ortmann point out that the Dunning-Kruger study, as is typical, was done using psychology undergraduates at Cornell. This sample is already a self-selected pool that excludes those unable to gain admission. And once in University, the feedback they receive on their performance is not as useful as it could be. Krajc and Ortmann write [references largely excluded]:\n\nIn addition, it is well-known from studies of grade inflation that grades at the undergraduate level have – with the notable exception of the natural sciences – become less and less differentiating over the years: more and more students are awarded top grades. For example, between 1965 and 2000 the number of A’s awarded to Cornell students has more than doubled in percentage while the percentage of grades in the B, C, D and F ranges has consequently dropped (in 1965, 17.5% of grades were A’s, while in 2000, 40% were A’s). These data strongly suggest that Cornell University experiences the same phenomenon of (differential) grade inflation that Harvard experiences and the schools discussed in Sabot and Wakeman-Linn (1991). The dramatic grade inflation documented for the humanities and social-sciences devalues grades as meaningful signals specifically in cohorts of students that are newly constituted and typically draw on the top of high-school classes. Inflated grades complicate the inference problem of student subjects that, quite likely, were students in their first year or in their first semester.\n\nGrade inflation is robbing people of feedback they could use to understand their level of competence.\n*A post by Eaon Pritchard on the “Dunning-Kruger peak” reminded me that I was sitting on this passage."
  },
  {
    "objectID": "posts/group-selection-and-the-social-sciences.html",
    "href": "posts/group-selection-and-the-social-sciences.html",
    "title": "Group selection and the social sciences",
    "section": "",
    "text": "The first day of the Consilience Conference has strengthened my feeling that support for group selection is growing in the social sciences. While the slant of speakers such as Edward O. Wilson and Herb Gintis is no surprise, the degree of support among many conference participants that I have spoken to was. The general argument is that the evolution of “altruistic” and cooperative behaviour requires group selection.\nAgain, the question in my mind is at what point do the evolutionary biologist critics of the group selection approach enter into this evolution of cooperation debate that is happening outside of their field? Social scientists relying on group selection generally do not receive attention from evolutionary biologists as their respective silos do not meet.\nOn the speakers, Edward O. Wilson’s advocacy of group selection over inclusive fitness in his keynote address was not particularly convincing, and his broader argument did not require the kin selection critique that he offered. A core point to his support of group selection was that individual level selection is responsible for sin, while group selection is responsible for virtue.\nOf the arguments for group selection, the sin-virtue dichotomy is one of the weaker ones, and economics provides one of the best responses. Robert Frank responded to Wilson’s point at the end of his own presentation (which largely covered material from The Darwin Economy). He noted the benefits of exchange and the core insight (dating back to Adam Smith) that cooperative behaviour can emerge from self interested individuals. Despite Frank’s advocacy of Darwin as the father of economics, he has not forgotten the importance of Smith.\nFurther, I am uncomfortable with the sin and virtue dichotomy. The group selection argument relies on “virtuous” behaviour within the group, but for those outside the group, tough luck.\nMassimo Pigliucci has offered a thorough summary of the first day’s events (not that I agree with some of his analysis)."
  },
  {
    "objectID": "posts/haidts-group-selection.html",
    "href": "posts/haidts-group-selection.html",
    "title": "Haidt’s group selection",
    "section": "",
    "text": "Having given my thoughts on Haidt’s generally excellent The Righteous Mind in my last post, I want to turn to Haidt’s use of group selection in the last third of his book. The central themes of his book don’t rest on group selection (in my opinion), but Haidt is at the centre of the reemergence of group selection in the social sciences and his points are worth discussing.\nHaidt uses the more modern phrase “multilevel selection” in addition to “group selection” through the book. Multilevel selection refers to a method of accounting for selection at the different levels (e.g. the gene, individual, group etc.), while the older concept of group selection usually refers to natural selection between groups and the evolution of group-level adaptations. Multilevel selection also involves what are called “trait groups”, which may briefly form and break up, compared to the rigid reproducing groups of the old group selection.\nIt seems that Haidt has a reasonable grasp on these distinctions but his use of the term multilevel selection is often confusing. For example, he keeps using the phrase “product of multilevel selection”. But multilevel selection is, as the name suggests, selection at different levels. You can look at a scenario through a multilevel selection framework and conclude that all the selection occurs at the level of the gene or individual. Multilevel selection and inclusive fitness are just different accounting methods (or languages). It is not a case that one happens and the other doesn’t. What Haidt is implying, and the way he should state it, is that selection is occurring predominantly at the level of the group. Based on some passages of the book, it is clear that Haidt understands this, but at other times his language is loose.\nWhen Haidt argues for the importance of selection at the level of the group (I’ll refer to it as group selection for rest of this post), he offers four lines of evidence: the role of group selection in the major evolutionary transitions; the shared intentionality of humans; gene-culture evolution; and the potential for fast evolution.\nThe major evolutionary transitions, such as the emergence of eukaryotic cells from the combination of bacteria, are one of the few areas where many evolutionary biologists will agree that group selection occurred. Haidt characterises the major evolutionary transitions as times where methods to control freeriding evolved at one level, allowing superorganisms to arise at the next. Haidt then follows in the footsteps of biologists such as David Sloan Wilson and E. O. Wilson and argues that the evolution of “ultrasociality” in humans is a similar transition.\nI don’t want to rehash this argument in-depth, but it goes back to the classic group selection debate. In the evolutionary transitions of the past, a reproductive bottleneck was present. Once two bacteria are combined in a cell, the only way they can reproduce is if the “group” reproduces. But that bottleneck does not exist in human groups, so there is opportunity for freeriding. We then get to the old debate about the level of freeriding and whether the level of group extinction and the degree of gene flow between groups allows group selection to outweigh this freeriding.\nWhile Haidt follows in others’ footsteps in referring to the major evolutionary transitions, his other arguments are more his own. On shared intentionality, Haidt argues that the ability to share intentions between people allows collaboration, the division of labour and shared norms. While Haidt claims this is group selection, this is a case where the multilevel selection framework should be properly applied. How much benefit does one get as an individual from understanding what someone else is thinking, versus the benefit you get from pairing with someone who also has that ability and working together to succeed as a group? While having more people in your group who are able to share intentions will help you defeat other groups, shared intentionality is clearly beneficial to an individual. Being in a group of mind readers when you have no idea what is going on is suboptimal. Which level the selection predominantly operates at needs to be analysed (and will depend upon assumptions about what makes up a group). This is the type of scenario that I have argued before is simpler to analyse in an inclusive fitness framework.\nHaidt’s third line of evidence is a somewhat confusing take on gene-culture evolution. Haidt argues that cultural group selection supported “prototribalism”, which led to an environment that then supported genetic evolution. However, Haidt’s examples do not sound like group selection. For example, Haidt writes:\n\n[I]ndividuals who found it harder to play along, to restrain their antisocial impulses, and to conform to the most important collective norms would not have been anyone’s top choice when it came time to choose partners for hunting, foraging, or mating. In particular, people who were violent would have been shunned, punished, or in extreme cases killed.\n\nThis sounds like individual level selection against violent, non-conformist individuals. I am not sure why Haidt was so keen to covert Boyd and Richerson’s arguments on cultural group selection into genetic group selection, but try he did.\nHaidt’s biggest reach, however, comes with his argument that the potential for fast evolution supports group selection. Haidt notes that gene-culture evolution reached fever pitch in the last 12,000 years, and that is an assessment I would agree with. He refers to the group selection experiments conducted by William Muir, in which Muir rapidly improved egg laying by selecting groups of successful chickens (achieved, of course, through the effective creation of a reproductive bottleneck in the experimental design). Haidt then pushes the rapid evolution argument to the limits when he seeks to implicate group selection in the emergence of religion. As large-scale religion only emerged since the dawn of agriculture, Haidt suggests the rapid recent evolution identified by the likes of John Hawks, Greg Cochrane and Henry Harpending provides scope for recent group selection. He writes:\n\n[G]roup selection can work very quickly (as in the case of those group-selected hens that became more peaceful in just a few generations). Ten thousand years is plenty of time for gene-culture coevolution, including some genetic changes, to have occurred. And 50,000 years is more than plenty of time for genes, brains, groups, and religions to have coevolved into a very tight embrace.\n\nThe problem is that group extinction and reproduction generally occurs more slowly than individual level selection. At the individual level, we see large differences in fertility every generation. For many people, it is the end of the genetic line. To the extent heritable traits underlie this variation, we can see rapid changes in genotype. In contrast, studies of rates of group extinction suggest it is slow. Further, groups tend not to be simply wiped out, but the “loser” groups tend to merge into the victor, bringing their genes with them.\nHaving said all this, we might be able to build a multilevel selection model in which we allow temporary religious or other “trait groups” to form and break up in short periods and divide the degree of selection between the various levels. However, I still doubt we will see significant selection at the group level for most of these examples and I don’t feel that this was the sort of group selection Haidt was interested in. Further, I expect the inclusive fitness framework would give a clearer picture. If this trait group approach could have provided a stronger argument, Haidt might have used it."
  },
  {
    "objectID": "posts/hamermeshs-beauty-pays.html",
    "href": "posts/hamermeshs-beauty-pays.html",
    "title": "Hamermesh’s Beauty Pays",
    "section": "",
    "text": "It pays to be beautiful. Higher pay, increased chance of promotion, easier loans, more beautiful and intelligent partners, greater happiness - the benefits are significant. And Dan Hamermesh has done a nice job cataloguing these perks in his new book Beauty Pays: Why Attractive People Are More Successful.\nIn one United States study, an above-average looking man (rated 4 or 5 on a scale of 1 to 5) commanded a 4 per cent wage premium in the labour market. For an above-average looking woman, it was 8 per cent. Below-average women  (1 or 2 on the 5 point scale) experience a 4 per cent hit, while below-average looking men suffered a large 13 per cent penalty. In dollar terms, Hamermesh put the life-time earnings premium of being above-average as opposed to below-average at $230,000.\nHamermesh describes a raft of other benefits. Beautiful people are less likely to be booted off game shows. When applying for loans, beautiful people are more likely to succeed than other, less beautiful, but equally qualified applicants - and they then have higher default rates. Beautiful people are also more likely to be in jobs where presentation might matter - barristers are more attractive than back room tax lawyers - although it is difficult to confirm which way causation flows.\nHaving demonstrated the financial benefits, Hamermesh spends most of the book questioning whether beauty is productive. Are beautiful people paid more because they produce more for their employers, or is it employer bias? An if a beautiful person is worth more to an employer, is this socially productive or bias by customers?\nIn some cases, Hamermesh suggests that beauty is clearly productive, such as for prostitution or movie-acting. There is a clear consumer surplus from a more beautiful person. I would argue that the list is longer than Hamermesh suggests - someone might get as much consumer surplus from purchasing an item from an attractive sales attendant as from seeing an attractive actor. Or take his example of Maria Sharapova playing tennis - he suggests that sponsors benefit, but the quality of the tennis is no better. Tennis is a spectator sport, and despite the quality of tennis being no higher, there may be consumer surplus for the spectators if the participants are attractive.\nHamermesh’s discussion of whether beauty is productive (and how you might measure productivity) felt like a conversation between a group of academics at the lunch table. This makes for an enjoyable read where you can throw the theories back and forth and come up with your own rationalisations. But there were times where I wished that Hamermesh could have nailed an issue in more depth. One point where more detail would have benefited the book was in his discussion of whether beauty is correlated with other socially productive traits. Hamermesh referenced a couple of studies from which it might be claimed that there is no link between beauty and IQ, but they are unrepresentative of most investigations into the link between beauty and other traits, which tend to find a correlation. For example, Satoshi Kanazawa and Jody Kovar argued it was logical to expect a linkbetween beauty and IQ and they catalogued a raft of studies that support this claim (Hamermesh notes that this is also peoples’ general perception).\nBy assuming that beauty is not correlated with other traits, Hamermesh restricted the range of conclusions he could draw as to why beauty might pay more. Correlation with productive traits would provide a more parsimonious explanation for many of the observations than always searching for the bias. Many of Hamermesh’s musings on productivity are going to be tested over the next few years, whether by himself or others, and it will be interesting to see what comes out of it.\nConsideration of the evolutionary biology behind beauty might also have assisted the analysis (although Beauty Pays is not alone in that respect - I tend to believe that in relation to half the books I read). As noted in the paper by Kanazawa and Kovar, there is a good evolutionary reason for correlation in traits. If an above-average beauty attracts an above-average intelligence mate, their children will, on average, have higher than average intelligence and beauty. Beauty and intelligence will co-vary in the population. There are likely to be a range of other traits that also co-vary with beauty.\nFurther, when Hamermesh suggests there is no evolutionary basis for beauty (based on one study that found no link between beauty and medically assessed health, ignoring genetic quality or the ability to attract partners), he sidesteps the wealth of papers that consider how symmetry and perceptions of attractiveness are linked to perceptions of health. This was another area where a deeper review of the literature might have been interesting.\nHaving assessed that the beauty-disadvantaged suffer financial penalties for reasons not to do with their productivity, Hamermesh explores protection for the ugly. He suggests that the beauty-disadvantaged may become increasingly protected by regulation (it already is in some jurisdictions). Should there prohibitions against discriminating against the ugly? What of affirmative action? Hamermesh is not committal on this point, and provides an interesting discussion, but he makes a strong point that the justification for protections of race, gender and obesity often apply with similar strength to the issue of beauty-disadvantage.\nUnfortunately for the beauty-disadvantaged, I am not sure that protections can fix the hand they were dealt. Hamermesh’s focus on the financial elements underplays what many consider the important benefits of beauty. Being more attractive and attracting multiple partners or a beautiful wife has benefits beyond her 8 per cent wage premium. For below-average men, their 17 per cent financial hit relative to the attractive will further damage their chances in the competition for mates. The beauty-disadvantaged are always going to struggle in some important non-financial elements of their life. I can only suggest that they enlist Hamermesh to fight for their cause."
  },
  {
    "objectID": "posts/happiness-is-not-the-objective.html",
    "href": "posts/happiness-is-not-the-objective.html",
    "title": "Happiness is not the objective",
    "section": "",
    "text": "David Brooks has written an article on some of the poor trade-offs people make when they spend. In a nutshell, “as we spend more on something, what we gain in privacy and elegance we lose in spontaneous sociability.” Happiness research suggests that this trade-off does not maximise our happiness.\nJonah Lehrer picked up on this piece and raised some issues around what he describes as a paradox:\n\nOur poor intuitions about the pursuit of happiness are a genuine paradox. Daniel Kahneman summarizes decades of happiness research this way: “It is only a slight exaggeration to say that happiness is the experience of spending time with people you love and who love you.” The problem, of course, is that we don’t spend our money in accordance with this psychological principle. Instead, we squander cash on positional goods, saving up for Rolex watches, Louis Vuitton luggage and Prada T-shirts.\n…\nA few years ago, the Swiss economists Bruno Frey and Alois Stutzer outlined a human bias they called “the commuting paradox.” They found that, when people are choosing where to live, they consistently underestimated the pain of a long commute. As a result, they mistakenly believed that the McMansion in the suburbs, with its extra bedroom and sprawling lawn, will make them happier, even though it might force them to drive an additional forty-five minutes to work.\n…..\nI’m genuinely puzzled by our failure to spend money properly. In general, human intuition improves with experience – it gets better as we put in those 10,000 hours of practice, so to speak. And yet, this doesn’t appear to be true when it comes to our intuitions about the pursuit of happiness. … Either psychologists can’t measure happiness or human beings with disposable income are very confused.\n\nThe failure of our spending to lead to “happiness” is not a paradox. We did not evolve to be happy. Rather, feelings of happiness reinforce actions that increase our fitness. As a result, happiness adjusts to our current level of status, wealth or condition and relative status affects it. The McMansion in the suburbs may not maximise happiness, but it may be the best way to increase status and signal wealth (at least, that is what they believe). Perceptions of happiness might trigger the purchase, but once done, there is no benefit in further happiness unless it reinforces such actions in the future.\nFor example, happiness can disappear quickly where new options for increasing fitness arise. Lehrer refers to one example where people lost sight of life’s finer pleasures when shown cash:\n\nThe psychologists gathered 351 adult employees of the University, from custodial staff to senior administrators, for an online survey. The scientists primed the subjects by showing them a stack of Euro bills before asking them a bunch of questions which attempted to capture their “savoring ability.” Interestingly, the scientists found that people in the wealth condition – they’d been primed with all those Euros – had significantly lower savoring scores and were less likely to be delighted by the simple pleasures of ”sunny days, cold beers, and chocolate bars.” Furthermore, subjects who made more money in real life – the scientists asked all subjects for their monthly income – scored significantly lower on the savoring test.\n\nDespite there being many reasons why we do not seem to maximise happiness, there is one part of this paradox that I find more interesting, and that is the particular forms of consumption and spending that we undertake. While it does not surprise me that people buy McMansions to increase status despite the costs to happiness, I am not sure that the McMansion is the best status or wealth signal available. Or take this paragraph from Geoffrey Miller’s wonderful book Spent: Sex, Evolution, and Consumer Behavior:\n\nYou anticipate the minor mall adventure: the hunt for the right retail environment playing cohort-appropriate nostalgic pop, the perky submissiveness of sales staff, the quest for the virgin product, the self-restraint you show in resisting frivolous upgrades and accessories, the universe’s warm hug of validation when the debit card machine says “Approved,” and the masterly fulfillment of getting it home, turned on, and doing one’s bidding. The problem is, you’ve experienced all this hundreds of times before with other products, and millions of other people will experience it with the same product. The retail adventure seems unique in prospect but generic in retrospect. In a week, it won’t be worth talking about.\n\nThat conspicuous consumption makes us happy (or more accurately given the “paradox”, that we believe it will make us happy) seems logical - it is a signal of wealth. The good may also have some utility. But why this particular manifestation of conspicuous consumption occurs and why people don’t seek a more distinct form of consumption or status display is an interesting question. Miller captures the problem nicely:\n\nAs a self-display strategy, it is very inefficient to buy new, branded, mass-produced products from stores at the full manufacturer’s suggested retail price. The product comes into one’s life naked and mute, without any social context, memorable circumstances, or narrative value. Nothing about the purchase says anything about one’s traits, except one’s ability to afford the purchase. One can’t talk about the product as a distinctive object with a unique provenance. One can merely own it, use it, display it, and hope that someone appreciates its wealth display function. Almost every other way of acquiring and displaying human artifacts or experiences sends richer signals about one’s personal qualities - though it usually brings less revenue to the retailer and manufacturer.\n\nIt may be the case that wealth is the most important signal we can send, or that Miller’s assessment of inefficiency is wrong (always be careful of suggesting everyone is irrational), but my instinct is to agree with him. We did not evolve to be happy, but nor are we evolved to optimise our displays of status in a modern consumerist world.\n\nMy previous review of Spent can be found here."
  },
  {
    "objectID": "posts/harvard-academics-on-genetic-diversity-and-economic-development.html",
    "href": "posts/harvard-academics-on-genetic-diversity-and-economic-development.html",
    "title": "Harvard academics on genetic diversity and economic development",
    "section": "",
    "text": "A group of Harvard academics have penned a short response to Ashraf and Galor’s forthcoming American Economic Review paper, The Out of Africa Hypothesis, Human Genetic Diversity and Comparative Economic Development.\nAshraf and Galor argue that economic development is affected by genetic diversity, which increases innovation but also increases conflict and distrust. This leads to an optimum “goldilocks” level of diversity, with genetically diverse Africans and less genetically diverse native Americans falling on either side of that optimum.\nThe Harvard academics suggest that the findings of the paper are scientifically flawed and that Ashraf and Galor “misuse genetic, evolutionary, archaeological, historical and cultural data”. They question the causal mechanism proposed by Ashraf and Galor and their statistical treatment. I will give my views on the causative mechanisms when I write a full post on the paper (probably to coincide with its publication). However, the statistical issue is interesting. The academics write:\n\nThe argument is also statistically flawed by treating genetic data as each population having an entirely independent history both from a genetic and from a historical point of view, when in fact, they are highly correlated and inextricably entangled with genetic population structure and with contingent historical events. Such haphazard methods and erroneous assumptions of statistical independence could equally find a genetic cause for the use of chopsticks.\n\nThis argument is similar to the statement that the various independent origins of agriculture are not actually “independent”. Populations in each region may have developed the specific idea of agriculture themselves, but they had a shared cultural and evolutionary history and their state at the time of the development of agriculture reflected elements of that shared history. However, in the limit, there is little of interest in the social sciences that is truly independent, so the question is what level of independence is required and whether statistical techniques can draw out relationships that are not spurious. Still, the risks presented by population structure in research such as this is very real.\nIn the last part of the letter, the Harvard academics reflect on the implications of the research and raise the common argument that this area should not be investigated due to “the potential to be misused with frightening consequences to justify indefensible practices such as ethnic cleansing or genocide.” I do not find this argument compelling, and consider that there is a need for robust, scientific exploration of an area that will continue to be debated with or without that research.\n*Postscript: I somehow missed it, but Ashraf and Galor have written a response (Update: which now seems to have been taken down). Thanks Vincenzo for the pointer.\nMy posts on Ashraf and Galor’s paper on genetic diversity and economic growth are as follows:\n\nA summary of the paper methodology and findings\nDoes genetic diversity increase innovation?\nDoes genetic diversity increase conflict?\nIs genetic diversity a proxy for phenotypic diversity?\nIs population density a good measure of technological progress?\nWhat are the policy implications of the effects of genetic diversity on economic development?\nShould this paper have been published?\n\nOther debate on this paper can also be found here, here, here and here."
  },
  {
    "objectID": "posts/hayek-planning-and-eugenics.html",
    "href": "posts/hayek-planning-and-eugenics.html",
    "title": "Hayek, planning and eugenics",
    "section": "",
    "text": "In Friedrich Hayek’s magnificent essay The Use of Knowledge in Society, Hayek writes:\n\nIt is about this question that all the dispute about “economic planning” centers. This is not a dispute about whether planning is to be done or not. It is a dispute as to whether planning is to be done centrally, by one authority for the whole economic system, or is to be divided among many individuals.\n\nAfter writing my recent post on Richard Conniff’s article about Irving Fisher’s eugenic leanings, it struck me that a similar framing can be made with respect to eugenics.\nThe debate about eugenics, apart from being one-sided, is normally framed as to whether there should be centralised planning about who should be able to breed (or not). However, without eugenic decisions from the top, there is still planning about who breeds and who doesn’t. This is in the form of sexual selection, with males and females choosing their partners with the aim of having smarter, more attractive children. The result of this process is that some people do not get to reproduce, despite their wish to, due to the decisions of others.\nAccordingly, this is not a dispute about whether planning is to be done to not, It is a dispute about whether planning is to be done centrally, by one authority for the whole population, or is to be divided among many individuals."
  },
  {
    "objectID": "posts/height-through-the-millennia.html",
    "href": "posts/height-through-the-millennia.html",
    "title": "Height through the millennia",
    "section": "",
    "text": "For the last year or so, I have had sitting in my “to blog” pile a 2004 New Yorker article about the increasing height of Europeans relative to Americans. It has a lot of interesting content. It talks about how height peaked in Europe around 800 AD, before declining through to 1700 (largely associated with the rise of cities), and then commencing an upward climb. It notes how Mexican-American teenagers have now equalled the United States norm, while American Mayan teenagers have gained four inches on Guatemalan Mayan teenagers in around two decades. The overarching point of the article is also interesting, that being the failure of heights to increase in the United States (after screening for issues around immigration, race etc.) since the 1950s while European heights continue to rise.\nI’ve delayed putting a post up as I’m still getting my head around a lot of the research in the area, and I’m not sure if the main concern was still current following more recent studies. But some recent events have triggered me to put together this post despite still not being fully across the area. The triggers include some comments following my recent post on obesity (to which those observations on Latin American height directly relate), the death of Robert Fogel, a passage in Marlene Zuk’s Paleofantasy and some comments by James Flynn in Are We Getting Smarter?.  So, here are a few interesting snippets.\nRobert Fogel is interviewed in the New Yorker article, including about his work Time on the Cross: The Economics of American Slavery:\n\nHistorians had long insisted that slavery was not only inhuman; it was bad business—hungry, brutalized workers made the poorest of farmers. Fogel and Engerman found nearly the opposite to be true: Southern plantations were almost thirty-five per cent more efficient than Northern farms, their analysis showed. Slavery was a cruel and inhuman system, but more so psychologically than physically: to get the most work from their slaves, planters fed and housed them nearly as well as free Northern farmers could feed and house themselves. …\nSteckel decided to verify his mentor’s claims by looking at the slaves’ body measurements. He went through more than ten thousand slave manifests—shipboard records kept by traders in the colonies—until he had the heights of some fifty thousand slaves; then he averaged them out by age and sex. The results were startling: adult slaves, Steckel found, were nearly as tall as free whites, and three to five inches taller than the average Africans of the time.\nThe height study both redeemed and rebuked “Time on the Cross.” Although the adult slaves were clearly well fed, the children were extremely small and malnourished. (To eat, apparently, they had to be old enough to work.) But Fogel was more than willing to stand corrected.\n\nFrom Zuk’s Paleofantasy, a suggestion that the costs to stature of the shift to agriculture were only transitory while humans adapted to the new diet:\n\nThe skeletons of ancient farmers are filled with evidence of tooth decay, iron deficiency anemia, and other disorders. Diamond notes that the Greek and Turkish skeletons from preagricultural sites averaged 5 feet 9 inches in height for men and 5 feet 5 inches for women, but after farming became established, people were much shorter—just 5 feet 3 inches and 5 feet, respectively, by about 5,000 years ago, probably because they were suffering from malnutrition. The teeth from skeletons of Egyptians who died 12,000 years ago, about 1,000 years after their people had shifted from foraging to farming, were rife with signs of malnutrition in the enamel: a whopping 70 percent of them, up from 40 percent before agriculture became widespread.\nThen a funny thing happened on the way from the preagricultural Mediterranean to the giant farms of today: people, at least some of them, got healthier, presumably as we adapted to the new way of life and food became more evenly distributed. The collection of skeletons from Egypt also shows that by 4,000 years ago, height had returned to its preagricultural levels, and only 20 percent of the population had telltale signs of poor nutrition in their teeth. Those trying to make the point that agriculture is bad for our bodies generally use skeletal material from immediately after the shift to farming as evidence, but a more long-term view is starting to tell a different story. For example, Timothy Gage of the State University of New York at Albany examined long-term mortality records from around the world, along with the likeliest causes of death, and concluded that life span did not decrease, nor did many diseases increase, after agriculture. Some illnesses doubtless grew worse after humans settled down, but life has had its “nasty, brutish, and short” phases at many points throughout history.\n\nIn Are We Getting Smarter? Flynn offers some thoughts on whether height and IQ gains have a common cause in improved nutrition:\n\nThe connection between height gains and IQ gains over time is significant only because it may signal nutrition as a common cause. Coupled with the assumption that nutritional gains have affected the lower classes disproportionately, this brings us back to the IQ curve. Wherever height gains persist, presumably nutritional gains persist, and where nutritional gains persist, IQ gains should show the predicted pattern, that is, gains mainly in the lower half of the curve.\nThis is not always the case. Martonell (1998) evidences that height gains persisted in the Netherlands until children born about 1965. Yet, cohorts born between 1934 and 1964 show massive Raven’s-type gains throughout the whole range of IQs. The French gained in height until at least those born in 1965. Yet, cohorts born between 1931 and 1956 show massive Raven’s gains that were uniform up through the 90th percentile. …\nNorway … counts against the posited connection between height gains and IQ gains. The upper classes tend to be taller. Yet, height gains have been larger in the upper half of the height distribution than in the lower half (Sundet, Barlaug, & Torjussen, 2004). This combination, greater height gains in the upper half of the distribution, greater IQ gains in the lower, poses a serious problem. Are there two kinds of enhanced nutrition, one confined to the upper classes that raises height more than it does IQ, the other affecting the lower classes that raises IQ more than it does height?\n\nIf the above is of interest, also have a glance at an earlier post of mine on Fogel, which was triggered by a NYT profile in 2011."
  },
  {
    "objectID": "posts/henrichs-the-secret-of-our-success-how-culture-is-driving-human-evolution-domesticating-our-species-and-making-us-smarter.html",
    "href": "posts/henrichs-the-secret-of-our-success-how-culture-is-driving-human-evolution-domesticating-our-species-and-making-us-smarter.html",
    "title": "Henrich’s The Secret of Our Success: How Culture Is Driving Human Evolution, Domesticating Our Species, and Making Us Smarter",
    "section": "",
    "text": "When humans compete against chimps in tests of working memory, information processing or strategic play, chimps often come out on top. If you briefly flash 10 digits on a screen before covering them up, a trained chimp will often better identify the order in which the numbers appeared (see here). Have us play matching pennies, and the chimp can converge on the predicted (Nash equilibrium) result faster than the slow to adapt humans.\nSo given humans don’t appear to dominate chimps in raw brain power (I’ll leave contesting this particular fact until another day), what can explain the ecological dominance of humans?\nJoe Henrich’s answer to this question, laid out in The Secret of Our Success: How Culture Is Driving Human Evolution, Domesticating Our Species, and Making Us Smarter, is that humans are superior learning machines. Once there is an accumulated stock of products of cultural evolution – fire, cutting tools, clothing, hunting tools and so on – natural selection favoured those who were better cultural learners. Natural selection shaped us to be a cultural species, as Henrich explains:\n\nThe central argument in this book is that relatively early in our species’ evolutionary history, perhaps around the origin of our genus (Homo) about 2 million years ago … cultural evolution became the primary driver of our species genetic evolution. The interaction between cultural and genetic evolution generated a process that can be described as autocatalytic, meaning that it produces the fuel that propels it. Once cultural information began to accumulate and produce cultural adaptations, the main selection pressure on genes revolved around improving our psychological abilities to acquire, store, process, and organize the array of fitness-enhancing skills and practices that became increasingly available in the minds of the others in one’s group. As genetic evolution improved our brains and abilities for learning from others, cultural evolution spontaneously generated more and better cultural adaptations, which kept the pressure on for brains that were better at acquiring and storing this cultural information.\n\nThe products of cultural evolution make us (in a sense) smarter. We receive a huge cultural download when growing up, from a base 10 counting system, to a large vocabulary allowing us to communicate complex concepts, to the ability to read and write, not to mention the knowhow to survive. Henrich argues that we don’t have all these tools because we are smart – we are smart because we have these tools. These cultural downloads can’t be devised in a few years by a few smart people. They comprise packages of adaptations developed over generations.\nAs one illustration of this point, Henrich produces a model where people can be either geniuses who produce more ideas, or social with more friends. Parameterise the model right and social groups end up much “smarter” with a larger stock of ideas. It is better to be able to learn and have more friends to learn from (again, within certain parameters) than have a fewer number of smarter friends. The natural extension of this is that larger populations will have more complex technologies (as Michael Kremer and others have argued – although see my extension on the evolving capacity to generate ideas).\nOne interesting feature of these cultural adaptations is that the bearers don’t necessarily understand how they work. They simply know how to effectively use them. An example Henrich draws on are food processing techniques developed over generations to remove toxins from otherwise inedible plants. People need, to a degree, to learn on faith. An unwillingness to learn can kill.\nTake the consumption of unprocessed manioc (cassava), which can cause cyanide poisoning. South American groups that have consumed it for generations have developed multi-stage processes involving grating, scraping, separating, washing, boiling and waiting. Absent those, the poisoning emerges slowly after years of eating. Given the non-obvious nature of the negative outcomes and link between the practices and outcomes, the development of processing techniques is a long process.\nWhen manioc was transported from South America to West Africa by the Portuguese, minus the cultural protocols, the result has been hundreds of years of cyanide poisoning. The problem that remains today. Some African groups have evolved processing techniques to remove the cyanide, but these are only slowly spreading.\nBeyond the natural selection for learning ability, Henrich touches on a few other genetic and biological angles. One of the more interesting is the idea that gene-culture co-evolution can lead to non-genetic biological adaptations. The culture we are exposed to shapes our minds during development, leading to taxi drivers in London having a larger hippocampus, or people from different cultures having different perceptual ability when it comes to judging relative or absolute size. Growing up in different cultures also alters fairness motivations, patience, response to honour threats and so on.\nHenrich is right to point out that his argument does not imply that seeing differences between groups implies cultural differences. They could be genetic, and different cultures over time could have moulded group differences. That said, Henrich also suggests genes play a tiny role, although it’s not a position brimming with analysis. As an example, he points out the high levels of violence among Scottish immigrants in the US Deep South who transported and retained an honour culture, compared to the low levels of violence in Scotland itself (or New England where there were also Scottish immigrants), without investing much effort in exploring other possibilities.\nHenrich briefly addresses some of the competing hypotheses for why we evolved large brains and developed a the theory of mind (the ability to infer others’ goals). For example the Machiavellian hypothesis posits that our brains evolved to outthink each other in strategic competition. As Henrich notes, possessing a theory of mind can also lead to us more effectively copy and learn from them (the cultural intelligence hypothesis). Successful Machiavellian’s must be good cultural learners – you need to learn the rules before you can bend them.\nSince the release of Henrich’s book, I have seen little response from the Stephen Pinker’s and evolutionary psychologists of the world, and I am looking forward to some critiques of Henrich’s argument.\nSo let me pose a few questions. As a start, until the last few hundred years, most of the world’s population didn’t use a base 10 counting system, couldn’t write and so on. Small scale societies might have a vocabulary of 3,000 to 5,000 words, compared to the 40,000 to 60,000 words held in the mind of the typical American 17-year old. The cultural download has shifted from something that could be passed on in a few years to something that takes a couple of decades of solid learning. Why did humans have so much latent capacity to increase the size of the cultural download? Was that latent capacity possibly generated by other mechanisms? Or has there been strong selection to continue to increase the stock of cultural knowledge we can hold?\nSecond, is there any modern evidence for the success of those who have better cultural learning abilities? We have evidence the higher reproductive success of those who kill in battle (see Napoleon Chagnon’s work) or those with higher income. What would an equivalent study to show the higher reproductive success of better cultural learners look like (assuming selection for that trait is still ongoing)? Or is it superior learning ability that leads to people to have higher success in battle or greater income? And in that case, are we just talking IQ?\nHaving been reading about cultural evolution for a few years now, I still struggle to grasp the extent to which it is a useful framework.\nPartly, this question arises due to the lack of a well-defined cultural evolution framework. The definition of culture is often loose (see Arnold Kling on Henrich’s definition) and it typically varies between cultural evolution proponents. Even once it is defined, what is the evolutionary mechanism? If it is natural selection, what is the unit of selection? And so on.\nThen there is the question of whether evolution is the right framework for all the forms of cultural transmission? Are models for the spread of disease a better fit? You will find plenty of discussions of this type of question across the cultural evolution literature, but little convergence.\nContrast cultural evolution with genetic natural selection. In the latter, high fidelity information is transmitted from parent to offspring in particulate form. Cultural transmission (whatever the cultural unit is) is lower-fidelity and can be in multiple directions. For genetic natural selection, selection is at the level of the gene, but the future of a gene and its vessels are typically tightly coupled within a generation. Not so with culture. As a result we shouldn’t expect to see the types of results we see in population/quantitative genetics in the cultural sphere. But can cultural evolution get even close?\nYou get a flavour of this when you look through the bespoke models produced in Henrich’s past work or, say, the work by Boyd and Richerson. Lot’s of interesting thinking tools and models, but hardly a unified framework.\nA feature of the book that I appreciated was that Henrich avoided framing the group-based cultural evolutionary mechanisms he describes as “group selection”, preferring instead to call them “intergroup competition” (the term group selection only appears in the notes). In the cultural evolution space, group selection is a label that tends to be attached to all sorts of dynamics – whether they resemble genetic group selection processes or not – only leading to confusion. Henrich notes at one point that there are five forms of intergroup competition. Perhaps one of these might be described as approaching a group selection mechanism. (See West and friends on this point that in much of the cultural evolution literature, group selection is used to refer to many different things). By avoiding going down this path, Henrich has thankfully not added to the confusion.\nOne thread that I have rarely seen picked up in discussion of the book (excepting Arnold Kling) is the inherently conservative message that can be taken out of it. A common story through the book is that the bearers of cultural adaptations rarely understand how they work. In that world, one should be wary of replacing existing institutions or frameworks.\nWhen Henrich offers his closing eight “insights”, he also seems to be suggesting we use markets (despite the absence of that world). Don’t design what we believe will work and impose it on all:\n\nHumans are bad at intentionally designing effective institutions and organizations, although I’m hoping that as we get deeper insights into human nature and cultural evolution this can improve. Until then, we should take a page from cultural evolution’s playbook and design “variation and selection systems” that will allow alternative institutions or organizational forms to compete. We can dump the losers, keep the winners, and hopefully gain some general insights during the process."
  },
  {
    "objectID": "posts/heritability-political-views-and-personality.html",
    "href": "posts/heritability-political-views-and-personality.html",
    "title": "Heritability, political views and personality",
    "section": "",
    "text": "Chris Mooney of The Intersection has posted on another article (with follow-up by Razib at Gene Expression) supporting the well-established finding that political views are heritable. The research found evidence for linkage between political beliefs and genes.\nSome of the discussion is interesting. How do genetic influences flow through to political beliefs, particularly when what we consider to be conservative or liberal varies through time? As Razib writes:\n\nThe disposition toward conservatism and liberalism does not manifest in absolute tendencies, but attitudes and actions comprehensible only against a reference which allows for one’s own bias to come to the fore. This is why heritabilities of being conservative and liberal can remain the same over time and across cultures, even though conservative and liberal can mean very different things in different contexts.\n\nWe are observing the manifestation of personality in varied environments. It is not support for civil unions that will be found to have a consistent genetic basis, but rather the openness or agreeableness that might lead you to support or oppose it in certain environments.\nIt is for that reason that only yesterday I was writing how I should re-frame my discussion of the heritability of political attitudes in terms of intelligence and the big-five personality traits. Those personality traits line up fairly consistently with the conservative-liberal divide. However, across different times and places, the link between personality and specific issues is much looser. There will a pile of studies over the next few years finding heritability of a host of traits and beliefs, together with possible genetic associations. While each might seem unique, many of them will be manifestations of the same personality traits."
  },
  {
    "objectID": "posts/how-happy-is-a-paraplegic-a-year-after-losing-the-use-of-their-legs.html",
    "href": "posts/how-happy-is-a-paraplegic-a-year-after-losing-the-use-of-their-legs.html",
    "title": "How happy is a paraplegic a year after losing the use of their legs?",
    "section": "",
    "text": "From Dan Gilbert’s 2004 TED talk, now viewed over 16 million times:\n\nLet’s see how your experience simulators are working. Let’s just run a quick diagnostic before I proceed with the rest of the talk. Here’s two different futures that I invite you to contemplate. You can try to simulate them and tell me which one you think you might prefer. One of them is winning the lottery. This is about 314 million dollars. And the other is becoming paraplegic.\nJust give it a moment of thought. You probably don’t feel like you need a moment of thought.\nInterestingly, there are data on these two groups of people, data on how happy they are. And this is exactly what you expected, isn’t it? But these aren’t the data. I made these up!\nThese are the data. You failed the pop quiz, and you’re hardly five minutes into the lecture. Because the fact is that a year after losing the use of their legs, and a year after winning the lotto, lottery winners and paraplegics are equally happy with their lives.\n\nAnd here’s Dan Gilbert reflecting on this statement 10 years later:\n\nThe first mistake occurred when I misstated the facts about the 1978 study by Brickman, Coates and Janoff-Bulman on lottery winners and paraplegics.\nAt 2:54 I said, “… a year after losing the use of their legs, and a year after winning the lotto, lottery winners and paraplegics are equally happy with their lives.” In fact, the two groups were not equally happy: Although the lottery winners (M=4.00) were no happier than controls (M=3.82), both lottery winner and controls were slightly happier than paraplegics (M=2.96).\nSo why has this study become the poster child for the concept of hedonic adaptation? First, most of us would expect lottery winners to be much happier than controls, and they weren’t. Second, most of us would expect paraplegics to be wildly less happy than either controls or lottery winners, and in fact they were only slightly less happy (though it is admittedly difficult to interpret numerical differences on rating scales like the ones used in this study). As the authors of the paper noted, “In general, lottery winners rated winning the lottery as a highly positive event, and paraplegics rated their accident as a highly negative event, though neither outcome was rated as extremely as might have been expected.” Almost 40 years later, I suspect that most psychologists would agree that this study produced rather weak and inconclusive findings, but that the point it made about the unanticipated power of hedonic adaptation has now been confirmed by many more powerful and methodologically superior studies. You can read the original study here.\n\nIt’s great that he is able to step back and admit his mistakes. One thing that perplexes me, however, is that he purports to show the real data on a slide:\n \nAs you can see, this runs on a scale reaching up to 70, with both measured at 50. The actual measure was on a 5-point scale. Where did these numbers come from? Did Gilbert simply make these data up?\nIf this were just a case of misstating the point of the study, I would feel much sympathy. As he states:\n\nWhen I gave this talk in 2004, the idea that videos might someday be “posted on the internet” seemed rather remote. There was no Netflix or YouTube, and indeed, it would be two years before the first TED Talk was put online. So I thought I was speaking to a small group of people who’d come to a relatively unknown conference in Monterey, California, and had I realized that ten years later more than 8 million people would have heard what I said that day, I would have (a) rehearsed and (b) dressed better.\nThat’s a lie. I never dress better. But I would have rehearsed. Back then, TED talks were considerably less important events and therefore a lot more improvisational, so I just grabbed some PowerPoint slides from previous lectures, rearranged them on the airplane to California, and then took the stage and winged it. I had no idea that on that day I was delivering the most important lecture of my life.\n\nBut if that chart was made up, my sympathy somewhat fades away."
  },
  {
    "objectID": "posts/human-evolution-goes-on.html",
    "href": "posts/human-evolution-goes-on.html",
    "title": "Human evolution goes on",
    "section": "",
    "text": "I missed it when it first went up, but over at The Crux, Discover’s new group blog, Razib Khan has pointed to a couple of interesting papers on the heritability of fertility. As natural selection acts strongly on fertility and the traits that affect it, you might expect that the heritability of fertility would be low as variation is eliminated. But change the environment, and heritability can increase drastically. Razib writes:\n\nQuebec is also an ideal “natural experiment.” It began as a frontier society with a small founding population on the order of thousands, but now has a population of over 8 million. …\nOne study http://www.pnas.org/content/108/41/17040.abstract focused on a 140-year period on an island in the Gulf of St. Lawrence. Over five generations, the island’s population increased by a factor of 10 through natural increase, while the average age of first reproduction declined from 26 to 22. Just as menarche and menopause are heritable (variation in genes explain much of the variation in the outcome of the trait), so too age at first reproduction seems heritable. …\nAnother group focused on the differences between core and frontier populations in the Saguenay–Lac-Saint-Jean region of Quebec. It turns out that the majority of the modern population descends from those in the frontier zone, not the core. Women on the frontier were ~20% more fertile, and fertility as a trait was heritable on the frontier but not in the core. (On the frontier, there was a connection between relatives and how many offspring they would have, in direct proportion to the amount of genes they shared. E.g., two sisters tended to have similar numbers of offspring, while there was no correlation between strangers.) …\n\nChanges in the environment can dramatically change the power of selection on traits. While a trait may not be heritable in one environment, it may be in another. Considering the changes in environment to which humans have been exposed over the last couple of hundred years, there must be many traits now experiencing significant selective pressure. I recommend that you read the whole post by Razib."
  },
  {
    "objectID": "posts/human-nature-and-property-rights.html",
    "href": "posts/human-nature-and-property-rights.html",
    "title": "Human nature and property rights",
    "section": "",
    "text": "While the Cato Unbound discussion on Brain, Belief and Politics appears to have petered out (unfortunately, Shermer has not directly confronted most of the issues in the response essays), the site has linked to an interesting piece (pdf) by Will Wilkinson from the Cato Policy Report in 2005.\nIn the article, Wilkinson addresses the link between capitalism and human nature. On property rights, he states:\n\nProperty rights are prefigured in nature by the way animals mark out territories for their exclusive use in foraging, hunting, and mating. Recognition of such rudimentary claims to control and exclude minimizes costly conflict, which by itself provides a strong evolutionary reason to look for innate tendencies to recognize and respect norms of property.\nNew scientific research provides even stronger evidence for the existence of such property “instincts.” For example, recent experimental work by Oliver Goodenough, a legal theorist, and Christine Prehn, a neuroscientist, suggests that the human mind evolved specialized modules for making judgments about moral transgressions, and transgressions against property in particular. Evolutionary psychology can help us to understand that property rights are not created simply by strokes of the legislator’s pen.\n\nFrom the comments on my recent post on human nature and libertarianism, property rights and the ability to accumulate massive amounts of property underlie many concerns about a libertarian state. Wilkinson notes that the human mind is ill-equipped to deal with it:\n\nPerhaps the most depressing lesson of evolutionary psychology for politics is found in its account of the deep-seated human capacity for envy and of our related difficulty in understanding the idea of gains from trade and increases in productivity— the idea of an ever-expanding “pie” of wealth.\n… The EEA [Environment of Evolutionary Adaptedness] was for the most part a zero-sum world, where increases in total wealth through invention, investment, and extended economic exchange were totally unknown. More for you was less for me. Therefore, if anyone managed to acquire a great deal more than anyone else, that was pretty good evidence that his was a stash of ill-gotten gains, acquired by cheating, stealing, raw force, or, at best, sheer luck. Envy of the disproportionately wealthy may have helped to reinforce generally adaptive norms of sharing and to help those of lower status on the dominance hierarchy guard against further predation by those able to amass power.\nOur zero-sum mentality makes it hard for us to understand how trade, innovation, and investment can increase the amount of total wealth. We are thus ill-equipped to easily understand our own economic system.\n\nWhile we may historically have pulled down the rich, the immediate assumption underlying the desire to pull them down no longer holds."
  },
  {
    "objectID": "posts/humbling-wingnuts.html",
    "href": "posts/humbling-wingnuts.html",
    "title": "Humbling wingnuts",
    "section": "",
    "text": "I have just read Cass Sunstein’s short collection of essays How to Humble a Wingnut and Other Lessons from Behavioral Economics. It is a decent summary of the behavioural science literature on political bias, although there are few surprises and not a lot of fresh opinion.\nThe one piece new to me concerned the moderation of people’s views after they are forced to explain their understanding of an issue. The basic story is as follows:\n\n[C]onsider an intriguing study by Philip Fernbach, a University of Colorado business school professor, and his colleagues. …\nFirst, people were asked to state their positions on a series of political issues, including a cap-and-trade system for carbon emissions, a national flat tax, merit-based pay for teachers and unilateral sanctions on Iran for its nuclear program. They were asked to describe their position on a seven-point scale whose endpoints were “strongly in favor” and “strongly opposed.”\nSecond, people were asked to rate their degree of understanding of each issue on a seven-point scale. The third step was the crucial one; they were asked to “describe all the details you know about [for example, the impact of instituting a ‘cap and trade’ system for carbon emissions], going from the first step to the last, and providing the causal connection between the steps.” Fourth, people were asked to rerate their understanding on the seven-point scale and to restate their position on the relevant issue.\nThe results were stunning. On every issue, the result of requesting an explanation was to persuade people to give a lower rating of their own understanding—and to offer a more moderate view on each issue. In a follow-up experiment, Fernbach and his co-authors found that after being asked to explain their views, people were less likely to want to give a bonus payment to a relevant advocacy group.\nInterestingly, Fernbach and his co-authors found no increase in moderation when they asked people not to “describe all the details you know” about the likely effects of the various proposals, but simply to say why they believe what they do. If you ask people to give reasons for their beliefs, they tend to act as their own lawyers or public relations managers, and they don’t move toward greater moderation.\n\nI wonder how long the effect lasts."
  },
  {
    "objectID": "posts/hunter-gatherer-workouts.html",
    "href": "posts/hunter-gatherer-workouts.html",
    "title": "Hunter-gatherer workouts",
    "section": "",
    "text": "The idea that modern sedentary lifestyles are leading to obesity has come under attack in a New York Times article in which Herman Pontzer writes about a recent PLoS ONE paper that he co-authored.\nPontzer and his colleagues’ research showed that the number of calories burned in a typical day by a member of the Hadza, a hunter-gatherer tribe in Tanzania, was indistinguishable from that burned by a typical adult from Europe or the United States. This is despite the miles of hilly terrain covered by Hadza women while gathering and Hadza men while hunting. The reason the Hadza consume no additional calories in total is because they expend less energy in other areas, such as the background rate of metabolism.\nThe odd thing about Pontzer’s NYT article is the title “Debunking the Hunter-Gatherer Workout”. If Pontzer spent some time on the various hunter-gatherer and paleo websites, he would quickly discover that the typical paleo workout is not based upon burning more calories, but is framed around a fractal pattern of exercise - brief moments of great exertion, longer periods of moderate exercise such as walking, and lots of rest - roughly like the Hadza lifestyle.\nRazib at Gene Expression also makes an interesting point about the paper:\n\nI checked over their references, and the authors don’t note the rather numerous studies since the mid-2000s which indicate that metabolism has been one of the major targets for natural selection in the Holocene (last 10,000 years). For example, Adaptations to Climate-Mediated Selective Pressures in Humans, or, Adaptations to climate in candidate genes for common metabolic disorders. If I had to bet I think the authors of the PLoS ONE paper are on to something, but they need to be careful to generalize from the Hadza, Western populations. In fact, I would be very curious to see a similar survey of the Bushmen of South Africa, and the Pygmies of the Congo. Probably the results would be the same, but it would still be informative to check to see if in fact these deeply diverged human lineages tended toward the same metabolic housekeeping and accounting. If so, then that might be the ancestral state.\n\nAs I recently posted about, we may still reflect our ancestral state, but recent human evolution means that it should not be assumed."
  },
  {
    "objectID": "posts/hwang-and-horowitts-the-rainforest.html",
    "href": "posts/hwang-and-horowitts-the-rainforest.html",
    "title": "Hwang and Horowitt’s The Rainforest",
    "section": "",
    "text": "A couple of months ago I linked to a piece by Ronald Coase about the state of economics. Coase wrote:\n\nEconomics as currently presented in textbooks and taught in the classroom does not have much to do with business management, and still less with entrepreneurship. The degree to which economics is isolated from the ordinary business of life is extraordinary and unfortunate.\n\nAs readers of this blog would know, most of what I read and write is relatively isolated from ordinary business life. But reading a book such as Victor Hwang and Greg Horowitt’s The Rainforest: The Secret to Building the Next Silicon Valley shows that the topic of this blog can be relevant to the business world.\nThe world dealt with in The Rainforest is innovation ecosystems of the type that we see in Silicon Valley. Why is Silicon Valley such an innovative place, and why do most attempts to create new Silicon Valleys around the world usually end in failure?\nTo answer this, Hwang and Horowitt turn to a biological metaphor - the rainforest. In a “rainforest”, innovators are able to tinker and engage in trial and error to discover the most efficient ways of combining capital, talent and ideas. This provides for an evolutionary - not engineered - process, where new innovations can emerge.\nHwang and Horowitt argue that to understand a rainforest, we need to understand something of biology, psychology, neuroscience and sociology. Some of the background materials that I had read on the book, such as Victor Hwang’s blog at Forbes, pointed to E.O. Wilson’s recent forays into group selection. However, when you get into the book, it’s ideas from the Wilson of pre-2005 that dominate. And although not explicitly named, the fingerprints of the likes of Robert Trivers are also present.\nHwang and Horowitt described many elements of a successful rainforest, but the one that stood out for me was trust. When trust is high, transaction costs are low (Coase also plays a fairly prominent role) and people can easily enter into new engagements. Lawyers are not required to draft terms and the entrepreneur is willing to share their ideas without fear of them being stolen.\nSo why does this trust exist? In part, it is because there is a strong normative culture with punishment against defectors. Take a transaction between an entrepreneur and a venture capitalist. In a single transaction, there might be opportunity for the venture capitalist to take as much of a stake in the company as they can. However, with strong norms about what a fair agreement looks like and a willingness to punish those who push for unfair terms, that venture capitalist’s reputation will spread quickly and their one-off gain turns into a long-term loss. Trust is also supported by a strong culture of reciprocation (hence my reference to Trivers above).\nWhile Wilson’s group selection gets mentioned, the rationale given by Hwang and Horowitt for the forming of cooperative groups is generally rooted in the strong individual advantages. They relate the example of people moving to the Western frontier in the early days of settlement in the United States. Despite a reputation as a period of rugged individualism, almost no-one embarked on the journey alone. The personal benefits to cooperation were vital. In the same way, to succeed in an innovative ecosystem, a person needs to be be connected to a broad variety of people. One of the primary ways of measuring the health of the rainforest is to look at those connections and the flows that occur along them.\nThe question that these types of explanations naturally draw out is how this culture exists in the first place. As Hwang and Horowitt point out, while people are naturally groupish, we distrust people dissimilar from ourselves and will generally act in our self interest. Places like Silicon Valley are particularly diverse, so we might expect them to be relatively atomised.\nOne reason is what they call extra-rational motivations. People are not purely self-interested in a money sense, but also seek adventure, interest, membership of groups and the like. Embarking on a start-up venture has pay-offs beyond the financial. They pick on neoclassical economists for ignoring these motivations, but I think this is more a case of ignorance in the models than ignorance that they exist. These motivations mean that people are willing to cooperate and enter into new ventures for the non-monetary benefits that they receive.\nA second answer that they hint at is the self selection of the people in places such as Silicon Valley. I suspect this is where much of the answer lies, because apart from their wish to be entrepreneurs, one of the selected characteristics are high levels of intelligence, which is in turn correlated with trust (both trusting and trustworthiness). In a community where most people are trusting and trustworthy, and people are willing to punish the occasional defectors (even if that is by simply never dealing with them again), cooperation can be expected to be highly beneficial and will flourish. Hwang and Horowitt return to the self-selection issue at the end of the book when they suggest that the people now moving to Silicon Valley have different characteristics to those who create the innovative culture. They suggest that new arrivals who are more interested in employment than entrepreneurship may change the culture. I also wonder in what other characteristics they differ?\nHwang and Horowitt are regularly engaged to support the establishment of innovative ecosystems in various countries and parts of the world. An interesting experiment would be to run experimental games such as the prisoner’s dilemma and public goods game with the groups whom the authors work and see what the results are. Are the results of these games predictive of whether a vibrant innovative ecosystem will be established? Does the level of cooperation in these games increase in successful environments?\nThere are plenty of other interesting ideas in the book, although I’m still to be convinced about the degree of control we can have when trying to create these types of innovative ecosystems. I probably need to see it to believe it. Still, it is nice to see ideas that I often talk about abstractly, such as  reciprocation, altruism and trust, being used in some practical analysis."
  },
  {
    "objectID": "posts/ignorance-feels-so-much-like-expertise.html",
    "href": "posts/ignorance-feels-so-much-like-expertise.html",
    "title": "Ignorance feels so much like expertise",
    "section": "",
    "text": "In the Pacific Standard, David Dunning of the Dunning-Kruger effect writes:\n\nA whole battery of studies conducted by myself and others have confirmed that people who don’t know much about a given set of cognitive, technical, or social skills tend to grossly overestimate their prowess and performance, whether it’s grammar, emotional intelligence, logical reasoning, firearm care and safety, debating, or financial knowledge. College students who hand in exams that will earn them Ds and Fs tend to think their efforts will be worthy of far higher grades; low-performing chess players, bridge players, and medical students, and elderly people applying for a renewed driver’s license, similarly overestimate their competence by a long shot.\n\nBut education is not always the answer:\n\nWhile educating people about evolution can indeed lead them from being uninformed to being well informed, in some stubborn instances it also moves them into the confidently misinformed category. In 2014, Tony Yates and Edmund Marek published a study that tracked the effect of high school biology classes on 536 Oklahoma high school students’ understanding of evolutionary theory. The students were rigorously quizzed on their knowledge of evolution before taking introductory biology, and then again just afterward. Not surprisingly, the students’ confidence in their knowledge of evolutionary theory shot up after instruction, and they endorsed a greater number of accurate statements. So far, so good.\nThe trouble is that the number of misconceptions the group endorsed also shot up. For example, instruction caused the percentage of students strongly agreeing with the true statement “Evolution cannot cause an organism’s traits to change during its lifetime” to rise from 17 to 20 percent—but it also caused those strongly disagreeing to rise from 16 to 19 percent. In response to the likewise true statement “Variation among individuals is important for evolution to occur,” exposure to instruction produced an increase in strong agreement from 11 to 22 percent, but strong disagreement also rose from nine to 12 percent. Tellingly, the only response that uniformly went down after instruction was “I don’t know.”\n…\nThe way we traditionally conceive of ignorance—as an absence of knowledge—leads us to think of education as its natural antidote. But education, even when done skillfully, can produce illusory confidence. Here’s a particularly frightful example: Driver’s education courses, particularly those aimed at handling emergency maneuvers, tend to increase, rather than decrease, accident rates. They do so because training people to handle, say, snow and ice leaves them with the lasting impression that they’re permanent experts on the subject. In fact, their skills usually erode rapidly after they leave the course. And so, months or even decades later, they have confidence but little leftover competence when their wheels begin to spin.\nIn cases like this, the most enlightened approach, as proposed by Swedish researcher Nils Petter Gregersen, may be to avoid teaching such skills at all. Instead of training drivers how to negotiate icy conditions, Gregersen suggests, perhaps classes should just convey their inherent danger—they should scare inexperienced students away from driving in winter conditions in the first place, and leave it at that.\n\nThe full article is worth reading."
  },
  {
    "objectID": "posts/impatience-and-aggregate-risk.html",
    "href": "posts/impatience-and-aggregate-risk.html",
    "title": "Impatience and aggregate risk",
    "section": "",
    "text": "Imagine two populations of asexually reproducing people (asexual reproduction is where each child comes from a single parent, not a couple). In the first population, each person has a 50 per cent chance of having no children, and a 50 per cent chance of having two children. If there is no relationship between the outcomes for each person (i.e. they face idiosyncratic risk) and the population is large, we would expect the population to remain relatively constant over time.\nIn the second population, each person also has a 50 per cent chance of having no children, and a 50 per cent chance of having two children. However, in this case, the population faces an aggregate risk so every person in the population has the same outcome - either zero or two children. At any time in the future, the expected population (the mean of all possibilities) is constant, just like the first. However, most outcomes lead to the extinction of the population, and as time goes on, this will almost surely occur. The only reason the mean outcome is constant is the small probability of a very large population (doubling every generation).\nThis difference between aggregate and idiosyncratic risk was used by Arthur Robson and Larry Samuelson (ungated version here)  to partly explain the problem that the rate of time preference (also called the discount rate, or level of patience) estimated in many theoretical evolutionary papers is lower than we see - often around a couple of per cent per year compared to empirical findings that people discount the future at rates of 10 per cent per year or higher. People are far more impatient than many theoretical models predict.\nTheoretical evolutionary estimates of the rate of time preference are typically tied to the rate of population growth and the chance of death, as was the case in work by Ronald Fisher, Ingemar Hansson and Charles Stuart and Alan Rogers. If population is growing, investments in the future are worth less. More obviously, death before reproduction erases all future benefit, so a higher probability of death should lead you to discount the future more. But with long-term population growth through our evolutionary history being near flat and death rates each year a couple of per cent at most, theoretical calculations often come out at around two per cent per year.\nTo resolve this problem, Robson and Samuelson argue that idiosyncratic and aggregate risk affects the optimal discount rates. In a population with idiosyncratic risk, the rate of time preference should approximate the population growth rate and the death rate. But in a population with aggregate risk, the expected population growth rate is not what we should look at.\nTo see why, look at the example of aggregate risk at the beginning of this post. Although the expected population is constant, the reality is that the population will either double every year, or it will plunge to zero. If it plunges to zero and everyone is wiped out, the rate of time preference does not matter. But what if the aggregate risk did not strike. The population would double every year. As a result, people should discount for the one scenario that matters - the annual doubling - and have a higher discount rate than they would have in the idiosyncratic risk scenario.\nThis argument holds with less extreme examples. In another (excellent) paper (ungated version here) in which Robson and Samuelson survey the literature on the evolution of preferences, they work through an example similar to the one above, but where people have one or two children. Again, the population with the aggregate risk would be expected to have a higher rate of time preference.\nPutting this theory into context, human populations through our evolutionary history would have faced a lot of aggregate risk, such as change in climate, drought and crop failures. So although population growth is near zero, their rate of time preference would be higher.\nOne way to think about it is to picture population growth as occurring in a saw-tooth manner - increases and then sudden drops. The population growth rate during those periods of increase is more relevant for the rate of time preference than the fact that the sudden drops through droughts and disasters reduce long-term population growth to near zero. The rate of population growth during the good times is what matters.\nUltimately, this paper is one of my favourite in economics. It has a great idea that is not immediately obvious (nor intuitive), but once you wrap your mind around it, it makes a lot of sense."
  },
  {
    "objectID": "posts/in-contrast-to-less-is-more-claims-ignoring-information-is-rarely-if-ever-optimal.html",
    "href": "posts/in-contrast-to-less-is-more-claims-ignoring-information-is-rarely-if-ever-optimal.html",
    "title": "In contrast to less-is-more claims, ignoring information is rarely, if ever optimal",
    "section": "",
    "text": "From the abstract of an interesting paper Heuristics as Bayesian inference under extreme priors by Paula Parpart and colleagues:\n\nSimple heuristics are often regarded as tractable decision strategies because they ignore a great deal of information in the input data. One puzzle is why heuristics can outperform full-information models, such as linear regression, which make full use of the available information. These “less-is-more” effects, in which a relatively simpler model outperforms a more complex model, are prevalent throughout cognitive science, and are frequently argued to demonstrate an inherent advantage of simplifying computation or ignoring information. In contrast, we show at the computational level (where algorithmic restrictions are set aside) that it is never optimal to discard information. Through a formal Bayesian analysis, we prove that popular heuristics, such as tallying and take-the-best, are formally equivalent to Bayesian inference under the limit of infinitely strong priors. Varying the strength of the prior yields a continuum of Bayesian models with the heuristics at one end and ordinary regression at the other. Critically, intermediate models perform better across all our simulations, suggesting that down-weighting information with the appropriate prior is preferable to entirely ignoring it. Rather than because of their simplicity, our analyses suggest heuristics perform well because they implement strong priors that approximate the actual structure of the environment.\n\nThe following excerpts from the paper (minus references) help give more context to this argument. First, what is meant by a simple heuristic as opposed to a full-information model?\n\nMany real-world prediction problems involve binary classification based on available information, such as predicting whether Germany or England will win a soccer match based on the teams’ statistics. A relatively simple decision procedure would use a rule to combine available information (i.e., cues), such as the teams’ league position, the result of the last game between Germany and England, which team has scored more goals recently, and which team is home versus away. One such decision procedure, the tallying heuristic, simply checks which team is better on each cue and chooses the team that has more cues in its favor, ignoring any possible differences among cues in magnitude or predictive value. … Another algorithm, take-the-best (TTB), would base the decision on the best single cue that differentiates the two options. TTB works by ranking the cues according to their cue validity (i.e., predictive value), then sequentially proceeding from the most valid to least valid until a cue is found that favors one team over the other. Thus TTB terminates at the first discriminative cue, discarding all remaining cues.\nIn contrast to these heuristic algorithms, a full-information model such as linear regression would make use of all the cues, their magnitudes, their predictive values, and observed covariation among them. For example, league position and number of goals scored are highly correlated, and this correlation influences the weights obtained from a regression model.\n\nSo why might less be more?\n\nHeuristics have a long history of study in cognitive science, where they are often viewed as more psychologically plausible than full-information models, because ignoring data makes the calculation easier and thus may be more compatible with inherent cognitive limitations. This view suggests that heuristics should underperform full-information models, with the loss in performance compensated by reduced computational cost. This prediction is challenged by observations of less-is-more effects, wherein heuristics sometimes outperform full-information models, such as linear regression, in real-world prediction tasks. These findings have been used to argue that ignoring information can actually improve performance, even in the absence of processing limitations. … Gigerenzer and Brighton (2009) conclude, “A less-is-more effect … means that minds would not gain anything from relying on complex strategies, even if direct costs and opportunity costs were zero”.\nLess-is-more arguments also arise in other domains of cognitive science, such as in claims that learning is more successful when processing capacity is (at least initially) restricted.\n…\nThe current explanation for less-is-more effects in the heuristics literature is based on the bias-variance dilemma. … From a statistical perspective, every model, including heuristics, has an inductive bias, which makes it best-suited to certain learning problems. A model’s bias and the training data are responsible for what the model learns. In addition to differing in bias, models can also differ in how sensitive they are to sampling variability in the training data, which is reflected in the variance of the model’s parameters after training (i.e., across different training samples).\nA core tool in machine learning and psychology for evaluating the performance of learning models, cross-validation, assesses how well a model can apply what it has learned from past experiences (i.e., the training data) to novel test cases. From a psychological standpoint, a model’s cross-validation performance can be understood as its ability to generalize from past experience to guide future behavior. How well a model classifies test cases in cross-validation is jointly determined by its bias and variance. Higher flexibility can in fact hurt performance because it makes the model more sensitive to the idiosyncrasies of the training sample. This phenomenon, commonly referred to as overfitting, is characterized by high performance on experienced cases from the training sample but poor performance on novel test items. …\nBias and variance tend to trade off with one another such that models with low bias suffer from high variance and vice versa. With small training samples, more flexible (i.e., less biased) models will overfit and can be bested by simpler (i.e., more biased) models such as heuristics. As the size of the training sample increases, variance becomes less influential and the advantage shifts to the complex models.\n\nSo what is an alternative explanation to the performance of heuristics?\n\nThe Bayesian framework offers a different perspective on the bias-variance dilemma. Provided a Bayesian model is correctly specified, it always integrates new data optimally, striking the perfect balance between prior and data. Thus using more information can only improve performance. From the Bayesian standpoint, a less-is-more effect can arise only if a model uses the data incorrectly, for example by weighting it too heavily relative to prior knowledge (e.g., with ordinary linear regression, where there effectively is no prior). In that case, the data might indeed increase estimation variance to the point that ignoring some of the information could improve performance. However, that can never be the best solution. One can always obtain superior predictive performance by using all of the information but tempering it with the appropriate prior.\n…\nHeuristics may work well in practice because they correspond to infinitely strong priors that make them oblivious to aspects of the training data, but they will usually be outperformed by a prior of finite strength that leaves room for learning from experience. That is, the strong form of less-is-more, that one can do better with heuristics by throwing out information rather than using it, is false. The optimal solution always uses all relevant information, but it combines that information with the appropriate prior. In contrast, no amount of data can overcome the heuristics’ inductive biases.\n\nSo why have heuristics proven to be so useful? According this Bayesian argument, it is not because of a “computational advantage of simplicity per se, but rather to the fact that simpler models can approximate strong priors that are well-suited to the true structure of the environment.”\nAn interesting question from this work is whether our minds use heuristics as a good approximation of complex models, or whether heuristics are good approximations of more complex processes that the mind uses. The authors write:\n\nAlthough the current contribution is formal in nature, it nevertheless has implications for psychology. In the psychological literature, heuristics have been repeatedly pitted against full-information algorithms that differentially weight the available information or are sensitive to covariation among cues. The current work indicates that the best-performing model will usually lie between the extremes of ordinary linear regression and fast-and-frugal heuristics, i.e., at a prior of intermediate strength. Between these extremes lie a host of models with different sensitivity to cue-outcome correlations in the environment.\nOne question for future research is whether heuristics give an accurate characterization of psychological processing, or whether actual psychological processing is more akin to these more complex intermediate models. On the one hand, it could be that implementing the intermediate models is computationally intractable, and thus the brain uses heuristics because they efficiently approximate these more optimal models. This case would coincide with the view from the heuristics-and-biases tradition of heuristics as a tradeoff of accuracy for efficiency. On the other hand, it could be that the brain has tractable means for implementing the intermediate models (i.e., for using all available information but down-weighting it appropriately). This case would be congruent with the view from ecological rationality where the brain’s inferential mechanisms are adapted to the statistical structure of the environment. However, this possibility suggests a reinterpretation of the empirical evidence used to support heuristics: heuristics might fit behavioral data well only because they closely mimic a more sophisticated strategy used by the mind.\n…\nThere have been various recent approaches looking at the compatibility between psychologically plausible processes and probabilistic models of cognition. These investigations are interlinked with our own, and while most of that work has focused on finding algorithms that approximate Bayesian models, we have taken the opposite approach. This contribution reiterates the importance of applying fundamental machine learning concepts to psychological findings. In doing so, we provide a formal understanding of why heuristics can outperform full-information models by placing all models in a common probabilistic inference framework, where heuristics correspond to extreme priors that will usually be outperformed by intermediate models that use all available information.\n\nThe (open access) paper contains a lot more detail - and the maths - and I recommend reading it."
  },
  {
    "objectID": "posts/in-the-company-of-a-stranger.html",
    "href": "posts/in-the-company-of-a-stranger.html",
    "title": "In the company of a stranger",
    "section": "",
    "text": "I have just left the Social Decision Making: Bridging Economics and Biology conference, with one of the last speakers being Paul Seabright, author of The Company of Strangers. I will post some thoughts on Seabright’s presentation (and some of the other presentations at the conference) after Easter and once I have read the related papers.\nIn the meantime, the night before Seabright’s talk I flipped through the revised edition of his book (it is a few years since I read the first edition). In the introduction is the following:\n\nIf it were somehow possible to assemble together all your direct same-sex ancestors - your father and your father’s father and so on if you’re male, your mother and your mother’s mother and so on if you’re female; one for each generation right back to the dawn of agriculture - you and all of these individuals could fit comfortably in a medium-sized lecture hall. Only half of you would have known the wheel, and only 1 per cent of you the motor car. But you would be far more similar to each other - genetically, physically, and instinctually - than any group of modern men or women who might have assembled there by chance. … [E]xcept in some dimensions such as height and perhaps in skin color, the biological differences between you and your furthest ancestor would be hard to distinguish from random variation within the group. If you are reading this book in a train or an airplane, this means your most distant ancestor from Neolithic times was probably more like you, biologically, than the stranger sitting in the seat next to you now.\n\nAs occurred to me the time I first read it, I am not sure that the last sentence is true. I expect that this could be empirically tested on a genetic basis (so this post should be put in the “I might be wrong” category), but there are a couple of issues in my mind.\nFirst, it is likely that a Western European stranger would share many common ancestors with me within the last 10,000 years. If we go back 10,000 years, or around 400 generations, I have approximately 2^400 direct ancestors (which is approximately 10^120, or more than the number of atoms than there are estimated to be in the observable universe). Obviously, this is more ancestors than there were people at that time, which points us to the fact that many of my ancestors from this time are ancestors through multiple ancestral lines. Many ancestors are likely to be shared with anyone of a similar ethnic origin. On this basis, there is no ground to expect that the direct ancestor of 10,000 years ago would be more similar than the stranger.\nApplying Seabright’s particular scenario, I wrote this post while sitting on a train in Northern Italy, with an Italian the stranger sitting next to me. As far as I have traced, I have English, Welsh and Irish heritage. That reduces the chance that we share common ancestors within the last 10,000 years compared to a scenario where I sit next to someone from the United Kingdom or Ireland. Even so, there is still a significant probability of common ancestors, as would be suggested by my Y-chromosome haplogroup (R1b1a2a1a1a under the 2011 nomenclature), with earlier versions of this haplogroup present in both United Kingdom and Italian populations (and that, of course, is just the male-male lineage).\nA second issue is who this ancestor is. Depending on the genes of each ancestor and the traits expressed by those genes, selection will have resulted in certain ancestors contributing more to my genome than others. If we picked one of my ancestors who contributed through a single line 10,000 years ago, they are likely to be less similar to me than an ancestor who has contributed through ten orders of magnitude more lines (remembering that I have 10^120 ancestral lines from 400 generations ago). Whether the ancestor we use for the comparison is more similar to me than the modern stranger might depend on whether we picked an ancestor randomly, the ancestor who made the median or mean contribution to my genome or the ancestor who made the greatest."
  },
  {
    "objectID": "posts/inequality-and-declining-fertility.html",
    "href": "posts/inequality-and-declining-fertility.html",
    "title": "Inequality and declining fertility",
    "section": "",
    "text": "The Economist notes a new working paper (pdf) by Bloom and colleagues of the Harvard School of Public Health, which shows that a short-term implication of reduced fertility in poor countries may be increased inequality. As fertility declines initially among the richer residents of the country, they are the first to reap the demographic dividend. From The Economist:\n\nThe three countries in the Harvard study which saw the largest declines in child dependency were Côte d’Ivoire (with a GDP per head in 2011 of $1,800), Namibia ($6,800) and Peru ($10,300). The pattern of their demographic change shows a clear progression. Poor Côte d’Ivoire saw its child-dependency ratios fall most among the rich and least among the poor. In Namibia child dependency fell furthest in the middle of the income range; the decline in the second-poorest group was largest of all. In middle-income Peru, the pattern was different again. There, child dependency fell across the board by roughly equal amounts.\nWhat seems to happen is that falling fertility widens demographic differences in countries with a per-person GDP of around $2,000; that the forces of inequality and convergence are balanced in countries where GDP per person is $5,000; and that by the time this figure has reached $10,000 per person, the forces of convergence are dominant. To put it another way, the rich lead the decline in fertility, producing a short-term increase in income inequality as they capture the benefits of demographic change first. Then the middle catches up as they educate daughters and plan families, followed by the poor, so that eventually fertility is lower across the board and the economic benefits of the demographic dividend are spread more evenly.\n\nAs fertility is typically highest among the poor before the fertility transition, they would seem to have the most to gain, but are the last to transition. The paper would suggest that increased inequality should not, in itself, be seen as a negative thing in countries undergoing a demographic transition. However, there might be a large dividend if the exit of the poor from the low income-high fertility state could be accelerated.\nHT: Economics and Psychology Research blog"
  },
  {
    "objectID": "posts/institutions-are-endogenous.html",
    "href": "posts/institutions-are-endogenous.html",
    "title": "Institutions are endogenous",
    "section": "",
    "text": "From Jared Diamond’s review of Why Nations Fail:\n\nBut it’s obvious that good institutions, and the wealth and power that they spawned, did not crop up randomly. For instance, all Western European countries ended up richer and with better institutions than any tropical African country. Big underlying differences led to this divergence of outcomes. Europe has had a long history (of up to nine thousand years) of agriculture based on the world’s most productive crops and domestic animals, both of which were domesticated in and introduced to Europe from the Fertile Crescent, the crescent-shaped region running from the Persian Gulf through southeastern Turkey to Upper Egypt. Agriculture in tropical Africa is only between 1,800 and 5,000 years old and based on less productive domesticated crops and imported animals.\nAs a result, Europe has had up to four thousand years’ experience of government, complex institutions, and growing national identities, compared to a few centuries or less for all of sub-Saharan Africa. Europe has glaciated fertile soils, reliable summer rainfall, and few tropical diseases; tropical Africa has unglaciated and extensively infertile soils, less reliable rainfall, and many tropical diseases. Within Europe, Britain had the further advantages of being an island rarely at risk from foreign armies, and of fronting on the Atlantic Ocean, which became open after 1492 to overseas trade.\nIt should be no surprise that countries with those advantages ended up rich and with good institutions, while countries with those disadvantages didn’t. The chain of causation leading slowly from productive agriculture to government, state formation, complex institutions, and wealth involved agriculturally driven population explosions and accumulations of food surpluses, leading in turn to the need for centralized decision-making in societies much too populous for decision-making by face-to-face discussions involving all citizens, and the possibility of using the food surpluses to support kings and their bureaucrats.\n\nThe more interesting question is not how institutions support economic growth, but rather why growth promoting institutions arise in some places and not in others. As Diamond notes, there are some strong patterns in this."
  },
  {
    "objectID": "posts/intelligence-changes.html",
    "href": "posts/intelligence-changes.html",
    "title": "Intelligence changes",
    "section": "",
    "text": "Scott Haufman has written a post on the variation in IQ over a person’s life. He writes:\n\nIn 1932, the entire population of Scottish 11-year-olds (87, 498 children) took an IQ test. Over 60 years later, psychologists Ian Deary and Lawrence Whalley tracked down about 500 of them and gave them the same test to take again.\nTurns out, the correlation was strikingly high – .66, to be exact. Those who were at the top of the pack at age 11 also tended to be at the top of the pack at age 80, and those who were at the bottom also tended to stay at the bottom. Equally as interesting, the correlation was far from perfect. A few outliers could be found. One person had an IQ of over 100 at age 11, but scored just over 60 at age 80. There are many possible reasons for this outlier, including dementia. Other folks showed IQ increases as they aged. On average, people’s individual (or absolute) scores on the test taken again at age 80 was much higher (over one standard deviation) than their scores had been at age 11, even though the rank ordering among people stayed roughly the same.\nThese results are illustrative of what psychologists find over and over again. IQ tends to remain relatively stable over the lifespan. The key word here is relative. IQ researchers are interested in explaining differences. Developmentally speaking, an individual’s intelligence is not fixed at birth. Although rank ordering of scores tends to remain stable (relative change), scores within individuals fluctuate quite a bit (absolute change).\n\nIn his analysis, Haufman rightly points out that this is further evidence that IQ is not fixed. However, he hasn’t noted one element of the change in IQ - the heritability of IQ increases through childhood and adolescence. The idiosyncratic environmental influences wash out. Some of the variation is movement reflecting some fixed underlying factors.\nIf the experiment reported changes in IQ between the ages of 20 and 80, I expect the variation would be smaller."
  },
  {
    "objectID": "posts/ips-foolproof-why-safety-can-be-dangerous-and-how-danger-makes-us-safe.html",
    "href": "posts/ips-foolproof-why-safety-can-be-dangerous-and-how-danger-makes-us-safe.html",
    "title": "Greg Ip’s Foolproof: Why Safety Can Be Dangerous and How Danger Makes Us Safe",
    "section": "",
    "text": "Greg Ip’s framework in Foolproof: Why Safety Can Be Dangerous and How Danger Makes Us Safe is the contrast between what he calls the ecologists and engineers. Engineers seek to use the sum of our human knowledge to make us safer and the world more stable. Ecologists recognise that the world is complex and that people adapt, meaning that many of our solutions will have unintended consequences that can be worse than the problems we are trying to solve.\nMuch of Ip’s book is a catalogue of the failures of engineering. Build more and larger levees, and people will move into those flood protected areas. When the levees eventually fail, the damage is larger than it would otherwise have been. There is a self reinforcing link between flood protection and development, ensuring the disasters grow in scale.\nSimilarly, if you put out every forest fire as soon as it pops up, eventually a large fire will get out of control and take advantage of the build up in fuel that occurred due to the suppression of the earlier fires.\nDespite these engineering failures, there is often pressure for regulators or those with responsibility to keep us safe to act as engineers. In Yellowstone National Park, the “ecologists” had taken the perspective that fires did not have to be suppressed immediately, as in combination with prescribed burning they could reduce the build up of fuel. But the economic interests around Yellowstone, largely associated with tourism, fought this use of fire. After all, prescribed burning and letting fires burn for a while is not costless or risk free. But the build up of fuel from failure to bear those short term costs or risks, as much of the pressure was on them to do, results in the long-term risk of a massive fire.\nDespite the problems with engineers, Ip suggests we need to take the best of both the engineering and ecologist approaches in addressing safety. Engineers have made car crashes more survivable. Improved flood protection allows us to develop areas that were previously out of reach. What we need to do, however, is not expect too much of the engineers. You cannot eliminate risks and accidents. Some steps to do so will simply shift, change or exacerbate the risk.\nOne element of Ip’s case for retaining parts of the engineering approach is confidence. People need a degree of confidence or they won’t take any risks. There are many risks we want people to take, such as starting a business or trusting their money with a bank. The evaporation of confidence can be the problem itself, so if you prevent the loss of confidence, you don’t actually need to deploy the safety device. Deposit insurance is the classic example.\nIp ultimately breaks down the balance of engineering and ecology to a desire to maximise the units of innovation per unit of instability. An acceptance of instability is required for people to innovate. This could be through granting people the freedom to take risks, or by creating an impression of safety (and a degree of moral hazard - the taking of risks when the costs are not borne by the risk taker) to retain confidence.\nDespite being an attempt to balance the two approaches, the innovation versus instability formula sounds much like what an engineer might suggest. I agree with Ip that the simple ecologist solution of removing the impression of safety to expunge moral hazard is not without costs. But it is not clear to me that you would ever get this balance right through design. Part of the appeal of the ecologist approach is the acceptance of the complexity of these systems and an acknowledgement to the limits of our knowledge about them.\nAnother way that Ip frames his balanced landing point is that we should accept small risks and the benefits, and save the engineering for the big problems. Ip hints at, but does not directly get to, Taleb’s concept of anti-fragility in this idea. Antifragility would see us develop a system where those small shocks strengthen the system and not simply being a cost we incur to avoid moral hazard.\nThe price of risk\nSome of Ip’s argument is captured by what is known as the Peltzman effect, named after University of Chicago economist Sam Peltzman. Peltzman published a paper in 1975 examining the effect of safety improvements in cars over the previous 10 years. Peltzman found a reduction in deaths per mile travelled for vehicle occupants, but also an increase in pedestrian injuries and property damage.\nPeltzman’s point was that risky driving has a price. If safety improvements reduce that price, people will take more risk. The costs of that additional risk can offsett the safety gains.\nWhile this is in some ways an application of basic economics - make something cheaper and people will consume more - the empirical evidence on the Peltzman effect is interesting.\nOn one level, it is obvious that the Peltzman effect does not make all safety improvements a waste of effort. The large declines in driver deaths relative to the distance travelled over the last 50 years, without fully offsetting pedestrian deaths or other damage, establishes this case.\nBut when you look at individual safety improvements, there are some interesting outcomes. In the case of seat belts, empirical evidence suggests the absence of the Peltzman effect. For example, one study looked at the effects across states as each introduced seatbelt laws and found a decrease in deaths but no increase in pedestrian fatalities.\nIn contrast, anti-lock brakes were predicted to materially reduce crashes, but the evidence suggests effectively no net change. Drivers with anti-lock brakes drive faster and brake harder. While reducing some risks - less front-end collisions - they increase others - such as the increased rear end collisions induced by their hard braking behaviour.\nSo why the difference between seatbelts and anti-lock brakes? Ip argues that the difference depends on what the safety improvement allows us to do and how it feeds back into our behaviour. Anti-lock brakes give a driver with a feeling of control and a belief they can drive faster. This belief is correct, but occasionally it backfires and they have an accident they would not have had otherwise. With seatbelts, most people want to avoid a crash and a car crash remains unpleasant even when wearing a seatbelt. At many times the seatbelt is not even in people’s minds.\nIrrational risk taking?\nOne of the interesting threads through the book (albeit one that I wish Ip had explored in more detail) is the mix of rational and irrational decision making in our approach to risk.\nMuch of this “irrationality” concerns our myopia. We rebuild on sites where hurricanes and storms have swept away or destroyed the previous structures. The lack of personal experience with the disaster leads people to underweight the probability. We also have short memories, with houses built immediately after a hurricane being more likely to survive the next hurricane than those built a few years later.\nA contrasting effect is our fear response to vivid events, which leads us to overweight them in our decision making despite the larger costs of the alternative.\nBut despite the ease in spotting these anomalies, for many of Ip’s real world examples of individual actions that might by myopic or irrational it wouldn’t be hard to craft an argument that the individual might be making a good decision. If the previous building on the site was destroyed by a hurricane, can you still get flood insurance (possibly subsidised), making it a good investment all the same? As Ip points out, there are also many benefits to living in disaster prone areas, which are often sites of great economic opportunity (such as proximity to water).\nIn a similar vein, Ip points to the individual irrationality of “overconfident” entrepreneurs, whose businesses will more often than not end up failing. But as catalogued by Phil Rosenzweig, the idea that these “failed” businesses generally involve large losses is wrong. Overconfident is a poor word to describe these entrepreneurs’ actions (see also here on overconfidence).\nI have a other few quibbles with the book. One was when Ip’s discussion of our response to uncertainty conflated risk aversion with loss aversion, the certainty effect and the endowment effect. But as I say, they are just quibbles. Ip’s book is well worth the read."
  },
  {
    "objectID": "posts/iq-externalities.html",
    "href": "posts/iq-externalities.html",
    "title": "IQ externalities",
    "section": "",
    "text": "Philip Zimbardo’s The Lucifer Effect: Understanding How Good People Turn Evil focuses on a message that the situation is more important than the person’s disposition. Good people can do evil things if placed in the wrong situation.\nOne of my main responses to this message was that the disposition of other people forms part of my situation. Disposition and situation cannot be neatly disentangled.\nThis is similar to the case of IQ and income. As noted by Garett Jones, boost the IQ of a person by two standard deviations and you get an average 30 per cent increase in their wage. Boosting the average IQ of a country’s population by two standard deviations leads to a prediction of a 700 per cent increase in average wages. By this measure, the situation, which is the IQ of the people of the country in which you live, is more important than your own IQ.\nTaking this further, the pay-off to being patient, saving and investing is contingent on the propensity towards violence of those around you. It is contingent on the foresight of those who borrow and invest your savings. It is contingent on the intelligence of the voters and politicians who create the institutional framework that affects the rewards from your work and savings.\nThere are some massive externalities to the behaviour of other people."
  },
  {
    "objectID": "posts/is-biology-easier-than-physics.html",
    "href": "posts/is-biology-easier-than-physics.html",
    "title": "Is biology easier than physics?",
    "section": "",
    "text": "Steve Hsu has pointed out an interesting old interview with Noam Chomsky. Hsu highlights Chomsky’s views on the limits of human intelligence.\n\nIt was possible in the late nineteenth century for an intelligent person of much leisure and wealth to be about as much at home as he wanted to be in the arts and sciences. But forty years later that goal had become hopeless. …\nI think it has happened in physics and mathematics, for example. There’s this idea, which goes back to the French mathematicians known collectively as Bourbaki, that the development of mathematics was originally the exploration of everyday intuitions of space and number. That is probably somewhat true through the end of the nineteenth century. But I don’t think it’s true now. As for physics, in talking to students at MIT, I notice that many of the very brightest ones, who would have gone into physics twenty years ago, are now going into biology. I think part of the reason for this shift is that there are discoveries to be made in biology that are within the range of an intelligent human being. This may not be true in other areas.\n\nWhile I am not convinced that Chomsky’s prediction of bright students going into biology played out (didn’t they all go into finance?), it is an interesting question. Is biology inherently more accessible?\nContrast the current group selection debate, such as that being played out at The Edge followingSteven Pinker’s essay critiquing group selection, with the discussion of the discovery of the Higgs boson. The group selection debate has a range of participants from academic biologists to popular science writers to bloggers. It takes little investment to have an opinion. In contrast, for all but a few physicists, we are passive receivers of information about the Higgs boson.\nHowever, the group selection debate is not necessarily a perfect example of an easily accessible topic. Reading through the responses to Pinker’s essay, it is clear that many of the responders do not have a common understanding of what group selection is. When the statement is made that the inclusive fitness and multi-level selection approaches can be shown to be mathematically equivalent representations, most people do not understand how or why that might be the case. And if we take one of the triggers of the recent escalation in debate, Nowak, Tarnita and Wilson’s Nature paper attacking kin selection, the majority of the debate participants do not fully understand the mathematics that underpins it, including one of the paper’s authors himself. While physics may have progressed to a level such that it is less accessible than biology, some of the accessibility of biology is illusory.\nAs an aside, how quickly would a debate about the Higgs boson would emerge in the blogosphere if its existence had a bearing on politics and whether government should be large or small?"
  },
  {
    "objectID": "posts/is-everyone-the-same.html",
    "href": "posts/is-everyone-the-same.html",
    "title": "Is everyone the same?",
    "section": "",
    "text": "A paper that is getting some attention at the moment is a critique of evolutionary psychology by Bolhuis and colleagues, titled Darwin in Mind: New Opportunities for Evolutionary Psychology. They critique a number of tenets of the “Santa Barbara school of evolutionary psychology”, including the notion that human psychological mechanisms evolved in response to stable ancestral environments or that there is a universal human nature.\nAs there has been some commentary on the general point of the article by some other bloggers (check out John Hawks or Razib at Gene Expression), I won’t spend much time questioning the accuracy of the critique or the lack of concrete suggestions by Bolhuis and colleagues. However, the article does have some interesting points on the speed of human evolution.\nBolhuis and colleagues argue that evolution can occur rapidly. They refer to one study which suggests that a trait can shift by one standard deviation within 25 generations (which I am going to have to read):\n\nEvolutionary biologists have also measured the rate of response to selection in a wide variety of animals, finding that evolutionary change typically occurs much faster than hitherto thought. A recent meta-analysis of 63 studies that measured the strength of natural selection in 62 species, including more than 2,500 estimates of selection, concluded that the median selection gradient (a measure of the rate of change of fitness with trait value) was 0.16, which would cause a quantitative trait to change by one standard deviation in just 25 generations. If humans exhibit equivalent rates, then significant genetic evolution would occur over the course of a few hundred years.\n\nRapid evolution provides the basis for genetic variation between populations, and Bolhuis and colleagues suggest that the brain in particular has been subject to rapid change.\n\nWhile variation within populations accounts for the bulk of human genetic variation, around 5%–7% of genetic differences can be attributed to variation between populations. Some of the significant genetic differences between human populations have arisen from recent selective events. Gene-culture coevolution may well turn out to be the characteristic pattern of evolutionary change in humans over recent time spans. From this perspective, cultural practices are likely to have influenced selection pressures on the human brain, raising the possibility that genetic variation could lead to biases in the human cognitive processing between, as well as within, populations. In summary, there is no uniform human genetic program.\n….\n[H]uman dispersal and subsequent exposure to novel climates, aggregation and exposure to new pathogens, and farming and exposure to new diets are now widely thought to be the source of selection for the spread of many human alleles. Amongst the overrepresented categories in genome-wide scans of recent selection are numerous alleles expressed in the human nervous system and brain. This raises the possibility that complex cognition on which culture is reliant (social intelligence, language, and challenges associated with constructing and adapting to new environmental conditions) have driven human brain evolution. … Gene-culture dynamics are typically faster and stronger and operate over a broader range of conditions than conventional evolutionary dynamics.\n\nWhile this argument for strong recent selection places the claim that humans have a universal nature in some context, it does not appear fatal to the general concept that many of our characteristics are likely to be common through our common ancestors. However, I do like the consideration of variation, and there is another dimension where it is important and often overlooked. That dimensions is whether, within a single population, did diverse traits co-evolve and now co-exist? The example that always comes to my mind is that of the sneaky dung beetles. While some are following an alpha male strategy of growing large and guarding their females, others are sneaking in from the back with their large testicles and long sperm. Similar strategies are found in some species of deer, crickets, cuttlefish and so on. To understand the adaptive benefit of one particular strategy, there should be recognition of the suite of strategies in which it sits. Sometimes those strategies are part of the variation that selection will erase. At other times, that variation might be the equilibrium point, with the success of any individual tied to the presence of others with different traits."
  },
  {
    "objectID": "posts/is-intelligence-at-the-root-of-cooperation.html",
    "href": "posts/is-intelligence-at-the-root-of-cooperation.html",
    "title": "Is intelligence at the root of cooperation?",
    "section": "",
    "text": "From Boyd and Richerson’s The Origin and Evolution of Cultures (references removed):\n\nThe proposal that human intelligence is at the root of human cooperation is difficult to evaluate because of the ambiguity in what we might mean by intelligence in a comparative context. As the Tasmanian Effect [the loss of their toolkit in a small population] illustrates, individual human intelligence is only a part, and perhaps only a small part, of being able to create complex adaptive behaviors. In fact, we think ‘‘intelligence’’ plays little role in the emergence of many human complex adaptations. Instead, humans seem to depend upon socially learned strategies to finesse the shortcomings of their cognitive capabilities. The details of human cognitive abilities apparently vary substantially across cultures because culturally transmitted cognitive styles differ. Although we share the common intuition that humans are individually more intelligent than even our very clever fellow apes, we are not aware of any experiments that sufficiently control for our cultural repertoires to be sure that it is correct. The concept of ‘‘intelligence’’ in individual humans perhaps makes little sense apart from their cultural repertoires: humans are smart in part because they can bring a variety of ‘‘cultural tools’’ (e.g., numbers, symbols, maps, various kinematic models) to bear on problems. A hunter-gatherer would seem an incredibly stupid college professor, but college professors would seem equally dense if forced to try to survive as hunter- gatherers (a few knowledgeable anthropologists aside). Even abilities as seemingly basic as those related directly to visual perception vary across cultures. Second, intelligence implies a means to an end, not an end in itself. Individual intelligence ought to serve the ends of both cooperation and defection. We suspect that actually defection, requiring trickery and deception, is better served by intelligence than cooperation. Game theorists assuming perfect, but selfish, rationality predict that humans should defect in the one-shot anonymous prisoner’s dilemma, just as evolutionary biologists predict that dumb beasts using evolved predispositions will.\n\nI’ve sat on that passage for a while now, contemplating turning it into a larger blog post. But for the moment, the abstract for this paper from Garett Jones points to the crux of my response:\n\nAre more intelligent groups better at cooperating? A meta-study of repeated prisoner’s dilemma experiments run at numerous universities suggests that students cooperate 5–8% more often for every 100-point increase in the school’s average SAT score. This result survives a variety of robustness tests. Axelrod [Axelrod, R., 1984. The Evolution of Cooperation. Basic Books, New York] recommends that the way to create cooperation is to encourage players to be patient and perceptive; experimental evidence suggests that more intelligent groups implicitly follow this advice.\n\nThere is some evidence that patience is enough when the game is not particularly cognitively demanding (noting that patience and cognitive ability are positively correlated in most studies). But beyond a certain point, intelligence and cooperation appear to go hand-in-hand."
  },
  {
    "objectID": "posts/is-it-irrational.html",
    "href": "posts/is-it-irrational.html",
    "title": "Is it irrational?",
    "section": "",
    "text": "Over at Behavioral Scientist magazine my second article, Rationalizing the ‘Irrational’, is up.\nIn the article I suggest that an evolutionary biology lens can give us some insight into what drives peoples’ actions. By understanding someone’s actual objectives, we are better able to determine whether their actions are likely to achieve their goals. Are they are behaving “rationally”?\nAlthough the major thread of the article is evolutionary, in some ways that is not the main point. For me the central argument is simply that when we observe someone else’s actions, we need to exercise a degree of humility in assessing whether they are “rational”. We possibly don’t even know what they are trying to achieve, let alone whether their actions are the best way to achieve it.\nObviously, this new article pursues a somewhat different theme to my first in Behavioral Scientist, which explored the balance between human and algorithmic decision making. After discussing possible topics for my first article with the editor who has been looking after me to date (DJ Neri), I sent sketches of two potential articles. We decided to progress both.\nMy plan for my next article is to return to the themes from the first. I’ve recently been thinking and reading about algorithm aversion, and why we resist using superior decision tools when they are available. Even if the best solution is to simply use the algorithm or statistical output, the reality is that people will typically be involved. How can we develop systems where they don’t mess things up?"
  },
  {
    "objectID": "posts/is-poverty-in-our-genes-from-the-comments.html",
    "href": "posts/is-poverty-in-our-genes-from-the-comments.html",
    "title": "Is poverty in our genes? From the comments",
    "section": "",
    "text": "In response to the critique in Current Anthropology on Ashraf and Galor’s paper on genetic diversity and economic growth, C.W. writes:\n\n\nThe critique of the use of the McEvedy and Jones population density data is (as already noticed by the first comment) not reasonable.\n\nMcEvedy and Jones (1978) is the standard source for cross-country historical population estimates used in dozen of papers by economists, historians and economic historians. No Referee – as I now - did refuse this data source or demand authors to use another. Of course – as with every historical source - the uncertainty is relatively high and increases the farther backwards you go – this is because Ashraf and Galor use that of 1500 AD and put that of the other years in the appendix. Of course, it might be the case that the figures are not correct for some periods or regions. Does this make the source completely unreliable and useless? No, because there always will be measurement error – especially in historical measures. Additionally, Acemoglu et al. (2002) did conduct their empirical estimates based on different population estimates (like that of Bairoch etc.) and found no significant differences at all – at least for their regressions.\n\nPretty much the same holds true for the “timing of agricultural transition” variable from Putterman (2008). This is already a rough measure (nobody would argue that it is possible to figure out the exact date of the agricultural transition) and it might be incorrect for some countries. But, are the measurement errors systematic and how large is this error? As long as these questions cannot be answered appropriately, one should not dismiss this measure.\n\nOf course, I know that actually the measure is criticized also by some economists (Acemoglu and Robinson e.g. in their new book and blog “Why Nations Fail”) and I think, there will be a new measure in future. But today the Putterman (2008) measure – already used in several papers – is the best and far most comprehensive collection of “timing of the Neolithic revolution” estimates available (as far as I know). Therefore it was an understandable choice to use this measure.\n\nAgain, the same is the case for the trust question from the WVS. This is actually the standard measure for generalized trust (i.e. general and unspecific trust of people in other people or strangers) used in almost every empirical study on trust by economist, sociologist or political scientists in the last 20 years.\n\nIt was – and is- heavily criticized by economist and others. As reaction, a significant amount of experimental economic papers test the validity of these question in field or laboratory experiments. Although no clear picture did emerge from these studies (in my opinion) the majority of the papers conclude that the question is valid , i.e. is correlated with actual trusting behavior in experiments. Furthermore, the question clearly is related to the amount of returned wallets in a so called “lost wallet game”.\nOf course, the measure actually is not perfect and there are still economists who do not believe in studies based on this question from WVS. Nevertheless, it is the (only) standard measure established in the literature.\n\nThe author of the critique claim that distance to Addis Ababa would only be a proxy for genetic diversity on a continental scale and does only proxy broad global trends. They wrote:\n\n“Ashraf and Galor’s description of the human pattern of global genetic diversity is consistently inaccurate, leading to concerns that the authors do not understand the data they are attempting to characterize. For example, they repeatedly contend that “migratory distance” to various settlements across the globe affected genetic diversity. This is misleading. The pattern of human genetic diversity they are referring to was primarily affected by the sequential series of founder effects that occurred during the peopling of the world; geographic distance is largely a proxy for these founder effects (Ramachandran et al. 2005). This proxy is accurate for roughly predicting global trends of genetic diversity on a continental scale but does not predict regional genetic diversity within continents. Human populations, stratiﬁed by heterozygosity, can be grouped into just four classes: Africa, West Eurasia, East Eurasia, and a fourth class comprising the remaining populations, nearly all of which have low heterozygosity. This class includes Native American populations. We prefer to use sequence data rather than genotype data to measure heterozygosity, as this avoids ascertainment issues involving the choice of SNPs used. Table S36 of Meyer et al. (2012), which used high coverage sequence data from 11 humans, shows the pattern clearly. In other words, genetic diversity varies on a continental scale, with Africa the most diverse, the Americas the least, and Eurasia having intermediate values. No amount of regression analysis and bootstrapping can alter the fact that, in essence, Ashraf and Galor are working with only four data points: Africa, Europe, Asia, and the Americas. This would be the case even if the raw data of Ashraf and Galor were perfect and free of noise.”\nI read the paper of Ramachandran et al. (2005) and if one sticks to that paper, this is not true (I do not know how accepted and established the findings and methodology of Ramachandran et al. (2005) are within population genetics, but at least Ashraf and Galor stick primarily to them). Ashraf and Galor do extensively discuss and explain the findings and methodology of Ramachandran et al. (2005) and then – in my impression - do exactly the same as Ramachandran et al. (2005). They regress the genetic diversity measure on the distance to Ethiopia taking the migration patterns of humanity into account. This corresponds exactly to what Ramachandran et al. (2005) (Figure 4A) do. Additionally, they found evidence for a clear relation between pairwise genetic distances between the ethnic groups in their sample and the great-circle distance (with waypoints and without) between them (Figure 1 in Ramachandran et al. 2005). So, obviously, at least according to Ramachandran et al. the distance and diversity relation does not only hold for continents or broad global tendencies. Or do I get that wrong (I’m economist not anthropologist or biologist). I think the only point where Ashraf and Galor can be criticized here, is that they do rely on only one paper and do not discuss the probably existing other work in the area. But I do not know whether the other literature come to other conclusions than Ramachandran et al. (2005).\nNevertheless, I think the anthropologists do have some good points. First, the fact that genetic diversity between humans in general is remarkably low. This is a huge problem for all arguments highlighting that genetic diversity between humans might be important for xx or can explain this or that.\nSecond and probably most important, they correctly criticize that the causal story of the paper is really, really weak. To make causal claims about very general and unspecific advantages and disadvantages of genetic diversity within(!) populations and genetic diversity associated with diversity in “Junk DNA” is not very convincing. And then, even worse I do not see enough convincing arguments to link both diversity in “Junk DNA” and very general and broad advantages of genetic diversity (in non-neutral genes!) to trust or to the amount of scientific publications. (And why to choose exactly these two variables why not some other, related to cooperation and creativity?). For me, this is all far too unspecific and speculative.\nIn sum, I do not believe in the story of the paper. But nevertheless I think, nevertheless it is not that bad as postulated by the anthropologist. Actually, the empirical strategy and the arguments in the paper are much more complex, differentiated and careful than in most papers written by economists. They control for almost every imaginable factor, using different variants, measures and adjustments of their variables and data. They worked on the paper for a quite long time and I think they really believe their story. Oded Galor always has unconventional and deep ideas and I think he is one of the most creative and methodologically best trained economist alive. Even more, he is one of the few economists that is willing to do interdisciplinary work – something strongly necessary to solve some the problems mainstream economics have today.\nTo do interdisciplinary research is always a difficult undertaking. Blaming Ashraf and Galor for having the courage of trying it is neither helpful nor fair in my opinion.\n\nMy posts on Ashraf and Galor’s paper on genetic diversity and economic growth are as follows:\n\nA summary of the paper methodology and findings\nDoes genetic diversity increase innovation?\nDoes genetic diversity increase conflict?\nIs genetic diversity a proxy for phenotypic diversity?\nIs population density a good measure of technological progress?\nWhat are the policy implications of the effects of genetic diversity on economic development?\nShould this paper have been published?\n\nOther debate on this paper can also be found here, here and here."
  },
  {
    "objectID": "posts/is-there-a-backfire-effect.html",
    "href": "posts/is-there-a-backfire-effect.html",
    "title": "Is there a “backfire effect”?",
    "section": "",
    "text": "I saw the answer hinted at in a paper released mid last-year (covered on WNYC), but Daniel Engber has now put together a more persuasive case:\n\nTen years ago last fall, Washington Post science writer Shankar Vedantam published an alarming scoop: The truth was useless.\nHis story started with a flyer issued by the Centers for Disease Control and Prevention to counter lies about the flu vaccine. The flyer listed half a dozen statements labeled either “true” or “false”—“Not everyone can take flu vaccine,” for example, or “The side effects are worse than the flu” —along with a paragraph of facts corresponding to each one. Vedantam warned the flyer’s message might be working in reverse. When social psychologists had asked people to read it in a lab, they found the statements bled together in their minds. Yes, the side effects are worse than the flu, they told the scientists half an hour later. That one was true—I saw it on the flyer.\nThis wasn’t just a problem with vaccines. According to Vedantam, a bunch of peer-reviewed experiments had revealed a somber truth about the human mind: Our brains are biased to believe in faulty information, and corrections only make that bias worse.\n…\nThese ideas, and the buzzwords that came with them—filter bubbles, selective exposure, and the backfire effect—would be cited, again and again, as seismic forces pushing us to rival islands of belief.\n\nFast forward a few years:\n\nWhen others tried to reproduce his the research [Ian Skurnik’s vaccine research], though, they didn’t always get the same result. Kenzie Cameron, a public health researcher and communications scholar at Northwestern’s Feinberg School of Medicine, tried a somewhat similar experiment in 2009. … “We found no evidence that presenting both facts and myths is counterproductive,” Cameron concluded in her paper, which got little notice when it was published in 2013.\nThere have been other failed attempts to reproduce the Skurnik, Yoon, and Schwarz finding. For a study that came out last June, Briony Swire, Ullrich Ecker, and “Debunking Handbook” co-author Stephan Lewandowsky showed college undergrads several dozen statements of ambiguous veracity (e.g. “Humans can regrow the tips of fingers and toes after they have been amputated”).  … But the new study found no sign of this effect.\n\nAnd on science done right (well done Brendan Nyhan and Jason Reifler):\n\nBrendan Nyhan and Jason Reifler described their study, called “When Corrections Fail,” as “the first to directly measure the effectiveness of corrections in a realistic context.” Its results were grim: When the researchers presented conservative-leaning subjects with evidence that cut against their prior points of view—that there were no stockpiled weapons in Iraq just before the U.S. invasion, for example—the information sometimes made them double-down on their pre-existing beliefs. …\n…\nHe [Tom Wood] and [Ethan] Porter decided to do a blow-out survey of the topic. Instead of limiting their analysis to just a handful of issues—like Iraqi WMDs, the safety of vaccines, or the science of global warming—they tried to find backfire effects across 52 contentious issues. … They also increased the sample size from the Nyhan-Reifler study more than thirtyfold, recruiting more than 10,000 subjects for their five experiments.\nIn spite of all this effort, and to the surprise of Wood and Porter, the massive replication effort came up with nothing. That’s not to say that Wood and Porter’s subjects were altogether free of motivated reasoning.\nThe people in the study did give a bit more credence to corrections that fit with their beliefs; in those situations, the new information led them to update their positions more emphatically. But they never showed the effect that made the Nyhan-Reifler paper famous: People’s views did not appear to boomerang against the facts. Among the topics tested in the new research—including whether Saddam had been hiding WMDs—not one produced a backfire.\n…\nNyhan and Reifler, in particular, were open to the news that their original work on the subject had failed to replicate. They ended up working with Wood and Porter on a collaborative research project, which came out last summer, and again found no sign of backfire from correcting misinformation. (Wood describes them as “the heroes of this story.”) Meanwhile, Nyhan and Reifler have found some better evidence of the effect, or something like it, in other settings. And another pair of scholars, Brian Schaffner and Cameron Roche, showed something that looks a bit like backfire in a recent, very large study of how Republicans and Democrats responded to a promising monthly jobs report in 2012. But when Nyhan looks at all the evidence together, he concedes that both the prevalence and magnitude of backfire effects could have been overstated and that it will take careful work to figure out exactly when and how they come in play.\n\nRead Engber’s full article. It covers a lot more territory, including some interesting history on how the idea spread.\nI have added this to the growing catalogue of readings on my critical behavioural economics and behavioural science reading list. (Daniel Engber makes a few appearances.)"
  },
  {
    "objectID": "posts/its-a-risky-business-attracting-a-mate.html",
    "href": "posts/its-a-risky-business-attracting-a-mate.html",
    "title": "It’s a risky business attracting a mate",
    "section": "",
    "text": "Last week, ABC’s Catalyst had a story on skateboarders taking extra risks based on the presence of an attractive researcher. This was based on article published earlier in the year (Ronay, R. & von Hippel, W. (2010). The presence of an attractive woman elevates testosterone and physical risk-taking in young men. Social Psychological and Personality Science, 1, 57-64).\nI haven’t been able to access the article yet, but in the Catalyst story, von Hippel proposed that it could be explained through the role risk taking plays as a signal of fitness. It demonstrates skill or (in case of failure) robustness.\nAnother evolutionary explanation, and one that applies particularly to young males, was put forward in 1979 by Rubin & Paul (An Evolutionary Model of Taste for Risk, Economic Inquiry, 17:4).They noted that adolescents, having attracted zero mates, have little to lose from risk seeking activity. By taking the risk, they have a chance of increasing their number of mates from zero. Failure to take the risk leaves them with zero mates with a probability of one. The ‘risky’ activity is not risky from the perspective of the desired result.\nAn extension of the skateboarding experiment to test this other hypothesis could involve using older males or males with long-term partners. It would be interesting to see their testosterone response compared to the young, single cohort.\nIf this hypothesis were true, you would expect to see more risk taking where there were, say, an excess of males or some males monopolising the females. Some cross-society analysis could be interesting."
  },
  {
    "objectID": "posts/janet-brownes-charles-darwin-voyaging.html",
    "href": "posts/janet-brownes-charles-darwin-voyaging.html",
    "title": "Janet Browne’s Charles Darwin: Voyaging",
    "section": "",
    "text": "Having put it in the top ten books I had read in 2010 despite being only halfway through it then, I feel somewhat obliged to offer a review of Janet Browne’s Charles Darwin: Voyaging (or more accurately, some observations). I have now finished it, and I am pleased to say that it can stay in the Top 10 and that I don’t intend to make any late retractions.\nVoyaging is the first volume of a two-part biography. Voyaging covers Darwin’s life to 1856, three years before the Origin of Species was published, which is the point where Darwin has decided to put his theory of natural selection into a book. The second volume, Charles Darwin: The Power of Place, takes off from there.\nThere were a number of features of Voyaging that I particularly enjoyed. Foremost was the way Browne placed Darwin within the context of the day’s science. Victorian science was an interesting mix of gentlemen naturalists and university scientists. From Darwin’s lectures in human anatomy in Edinburgh, consisting of dissections of recently snatched corpses, to his gentlemanly existence as a member of the Royal Society, Linnean Society and the Athenaeum Club, Browne provides reams of insights into the state of the science, the participants and their views. This allowed Browne to place Darwin’s insight into the context of existing debates on evolution and the rapid growth in other sciences such as geology.\nIn this context, Browne takes the reader through the creation of Darwin “the naturalist” as opposed to Darwin the country parson, idle sporting man or doctor. As Browne notes in the introduction, Darwin was no born naturalist. In his late teens and early twenties, he seems to have little direction and even on his return from the Beagle voyage, it seems that he could have easily slipped into a much simpler life than becoming a scientific giant. Through her understanding of Darwin’s correspondence (she was an associate editor of a project putting it together), Browne is able to offer a picture into how Darwin gained enough confidence in his ideas to dedicate a large part of his life to establishing the evidence behind them. Over the course of years, he slowly tested his thoughts with his closet friends (often in the most obtuse ways) as he finally built foundations to his theory. As we well know, it took over 15 years from the first indications to his closest friends of what he was thinking to the publishing of the Origin of Species, and even that was rushed by the letter he had received from Wallace.\nThe way in which Darwin established and used his massive network, largely from the seclusion of Downe house, is nothing short of incredible. Browne paints a beautiful picture of how Darwin sent an extraordinary number of letters to scientists, farmers, gentleman naturalists and anyone else who might be able to help with a litany of requests for samples, evidence and support. The evidence that fills the pages of the Origin of Species was largely sourced through this correspondence, which says something for Darwin’s ability to extract useful information and help from others. He had a gift for this from an early age, with his requests for insects from his friend Herbert carrying little recognition of Herbert’s deformed foot - and yet, Herbert happily complied.\nDarwin does not stay unscathed in the book. It is clear that he is a creature of his times, despite his progressive attitudes towards some issues such as slavery. For example, he largely did not credit his fellow Beagle crew members who made significant contributions to Darwin’s work. For someone who relied so much on help from others, he was sometimes slow to share the credit. However, this did not seem to stem from malice or selfishness, but perhaps unawareness due to his relatively high station in society. When Darwin did feel that he owed something (such as for Herbert’s anonymous gift of a Coddington microscope), his gratitude lasted a lifetime.\nHaving read Volume I, I might take a break before Volume II. A 600 page biography has to contend with the rather large reading list. Interestingly, Volume II seems to get the biggest praise, so I look forward to when it is time to read it."
  },
  {
    "objectID": "posts/jones-on-iq-and-productivity.html",
    "href": "posts/jones-on-iq-and-productivity.html",
    "title": "Jones on IQ and productivity",
    "section": "",
    "text": "The June edition of the Asian Development Review has an article by Garett Jones titled National IQ and National Productivity: The Hive Mind Across Asia (pdf). The abstract is as follows:\n\nA recent line of research demonstrates that cognitive skills—intelligence quotient scores, math skills, and the like—have only a modest influence on individual wages, but are strongly correlated with national outcomes. Is this largely due to human capital spillovers? This paper argues that the answer is yes. It presents four different channels through which intelligence may matter more for nations than for individuals: (i) intelligence is associated with patience and hence higher savings rates; (ii) intelligence causes cooperation; (iii) higher group intelligence opens the door to using fragile, high-value production technologies; and (iv) intelligence is associated with supporting market-oriented policies. Abundant evidence from across ADB member countries demonstrates that environmental improvements can raise cognitive skills is reviewed.\n\nThe article deals nicely with many of the standard objections to the argument that IQ is relevant to national income, including whether IQ tests measure anything meaningful, cultural bias and the direction of causation.\nIn relation to the four channels suggested by Jones, I generally take the first two as given. I am unsure of the importance of the fourth as a direct causative factor, although I do not doubt there is a correlation and support for these policies through self-interest. The channel that I find most interesting at the moment is channel (iii), which is based on Michael Kremer’s paper The O-Ring Theory of Economic Development and Jones’s later paper The O-Ring Sector and the Foolproof Sector: An explanation for cross-country income differences (pdf). If production is conducted through a series of steps, with IQ associated with the rate of error at each stage, small changes in IQ can be responsible for significant differences in productivity.\nI’ll post on Kremer’s and Jones’s papers in the future, but I appreciate this deeper examination of how IQ affects productivity and economic growth. While the correlation between IQ and national income is clear and the case for causation strong, an understanding of the reasons for the causation is important for policy. For example, the O-Ring theory suggests there will be significant issues with attempting to promote high-IQ country production processes in a low-IQ country.\nPolicy would also be enlightened by an understanding of how the distribution of IQ in a population is relevant. Do two populations with identical mean IQ but different distributions have different levels of productivity? Can a small proportion of the population with very high IQ carry a large base of low-IQ people? The O-Ring theory would suggest that this is difficult as the human resources available create problems in the production processes. Conversely, a theory based on the productivity and creativity of an elite that can effectively use the low-IQ human resources available might suggest that such a distribution could be productive.\nMoving beyond IQ, this deeper analysis is relevant for traits such as conscientiousness and agreeableness. What proportion of low conscientiousness people would undermine the general existence of trust is a society? What prevalence of psychopaths or violent people can undermine the trust created by a high-IQ population?\n**As a side note, I am finally off the road and back in Perth. Posting will be more regular than it has been over the last few weeks on the road. However, as my return to Perth means a resumption of my day job, I’ll probably be posting around three times a week compared to the four or five posts per week I was writing in Zurich."
  },
  {
    "objectID": "posts/kahneman-and-tverskys-debatable-loss-aversion-assumption.html",
    "href": "posts/kahneman-and-tverskys-debatable-loss-aversion-assumption.html",
    "title": "Kahneman and Tversky’s “debatable” loss aversion assumption",
    "section": "",
    "text": "Loss aversion is the idea that losses loom larger than gains. It is one of the foundational concepts in the judgment and decision making literature. In Thinking, Fast and Slow, Daniel Kahneman wrote “The concept of loss aversion is certainly the most significant contribution of psychology to behavioral economics.”\nYet, over the last couple of years several critiques have emerged that question the foundations of loss aversion and whether loss aversion is a phenomena at all.\nOne is an article by Eldad Yechiam, titled Acceptable losses: the debatable origins of loss aversion (pdf). Framed in one case as a spread of the replication crisis to loss aversion, the abstract reads as follows:\nA second, The Loss of Loss Aversion: Will It Loom Larger Than Its Gain (pdf), by David Gal and Derek Rucker, attacks the concept of loss aversion more generally (supposedly the “death knell”):\nA third strain of criticism relates to the concept of ergodicity. Put forward by Ole Peters, the basic claim is that people are not maximising the expected value of a series of gambles, but rather the time average. If people maximise the latter, not the former as many approaches assume, you don’t need risk or loss aversion to explain the decisions. (I’ll leave explaining what exactly this means to a later post.)\nI’m as sceptical and cynical about the some of the findings in the behavioural sciences as most (here’s my critical behavioural economics and behavioural science reading list), but I’m not sure I am fully on board with these arguments, particularly the stronger statements of Gal and Rucker. This post is the first of a few rummaging through these critiques to make sense of the debate, starting with Yechiam’s paper on the foundations of loss aversion in prospect theory."
  },
  {
    "objectID": "posts/kahneman-and-tverskys-debatable-loss-aversion-assumption.html#acceptable-losses-the-debatable-origins-of-loss-aversion",
    "href": "posts/kahneman-and-tverskys-debatable-loss-aversion-assumption.html#acceptable-losses-the-debatable-origins-of-loss-aversion",
    "title": "Kahneman and Tversky’s “debatable” loss aversion assumption",
    "section": "Acceptable losses: the debatable origins of loss aversion",
    "text": "Acceptable losses: the debatable origins of loss aversion\nOne of the most cited papers in the social sciences is Daniel Kahneman and Amos Tversky’s 1979 paper Prospect Theory: An Analysis of Decision under Risk (pdf). Prospect theory is intended to be a descriptive model of how people make decisions under risk, and an alternative to expected utility theory.\nUnder expected utility theory, people assign a utility value to each possible outcome of a lottery or gamble, with that outcome typically relating to a final level of wealth. The expected utility for a decision under risk is simply the probability weighted sum of these utilities. The utility of a 50% chance of $0 and a 50% chance of $200 is simply the sum of 50% of the utility of each of $0 and $200.\nWhen utility is assumed to increase at a decreasing rate with each additional dollar of additional wealth - as is typically the case - it leads to risk averse behaviour, with a certain sum preferred to a gamble with an equivalent expected value. For example, a risk averse person would prefer $100 for certain that the 50-50 gamble for $0 or $200.\nIn their 1979 paper, Kahneman and Tversky described a number of departures from expected utility theory. These included:\n\nThe certainty effect: People overweight outcomes that are considered certain, relative to outcomes which are merely probable.\nThe reflection effect: Relative to a reference point, people are risk averse when considering gains, but risk seeking when facing losses.\nThe isolation effect: People focus on the elements that differ between options rather than those components that are shared.\nLoss aversion: Losses loom larger than gains - relative to a reference point, a loss is more painful than a gain of the same magnitude.\n\nLoss aversion and the reflection effect result in the following famous diagram of how people weight losses and gains under prospect theory. Loss aversion leads to a kink in the utility curve at the reference point. The curve is steeper below the reference point than above. The reflection effect results in the curve being concave above the reference point, and convex below.\n\n\nValue Function\n\n\n\nThrough the paper, Kahneman and Tversky describe experiments on each of the certainty effect, reflection effect, and isolation effect. However, as pointed out by Eldad Yechiam in his paper Acceptable losses: the debatable origins of loss aversion, loss aversion is taken as a stylised fact. Yechiam writes:\n\n[I]n their 1979 paper, Kahneman and Tversky (1979) strongly argued for loss aversion, even though, at the time, they had not reported any experiments to support it. By indicating that this was a robust finding in earlier research, Kahneman and Tversky (1979) were able to rely upon it as a stylized fact. They begin their discussion on losses by stating that “a salient characteristic of attitudes to changes in welfare is that losses loom larger than gains” (p. 279), which suggests that this stylized fact is based on earlier findings. They then follow with the (much cited) sentence that “the aggravation that one experiences in losing a sum of money appears to be greater than the pleasure associated with gaining the same amount [17]” (p. 279). Most people who cite this sentence do so without the end quote of Galenter and Pliner (1974). Galenter and Pliner (1974) are, therefore, the first empirical study used to support the notion of loss aversion.\n\nSo what did Galenter and Pliner find? Yechiam writes:\n\nSumming up their findings, Galenter and Pliner (1974) reported as follows: “We now turn to the question of the possible asymmetry of the positive and negative limbs of the utility function. On the basis of intuition and anecdote, one would expect the negative limb of the utility function to decrease more sharply than the positive limb increases… what we have observed if anything is an asymmetry of much less magnitude than would have been expected … the curvature of the function does not change in going from positive to negative” (p. 75).\nThus, our search for the historical foundations of loss aversion turns into a dead end on this particular branch: Galenter and Pliner (1974) did not observe such an asymmetry; and their study was quoted erroneously.\n\nEffectively, the primary reference for the claim that we are loss averse does not support it.\nSo what other sources did Kahneman and Tversky rely on? Yechiam continues:\n\nThey argue that “the main properties ascribed to the value function have been observed in a detailed analysis of von Neumann–Morgenstern utility functions for changes of wealth [14].” (p. 281). The citation refers to Fishburn and Kochenberger’s forthcoming paper (at the time; published 1979). Fishburn and Kochenberger’s (1979) study reviews data of five other papers (Grayson, 1960; Green, 1963; Swalm, 1966; Halter & Dean, 1971; Barnes & Reinmuth, 1976) also cited by Kahneman and Tversky (1979). Summing up all of these findings, Kahneman and Tversky (1979) argue that “with a single exception, utility functions were considerably steeper for losses than for gains.” (p. 281). The “single exception” refers to a single participant who was reported not to show loss aversion, while the remaining one apparently did.\n\nThese five studies all involved very small samples, involving a total of 30 subjects.\nYechiam walks through three of the studies. On Swalm (1966):\n\nThe results of the 13 individuals examined by Swalm … appear at the first glance to be consistent with an asymmetric utility function implying overweighting of losses compared to gains (i.e., loss aversion). Notice, however, that amounts are in the thousands, such that the smallest amount used was set above $1000 and typically above $5000, because it was derived from the participant’s “planning horizon”. Moreover, for more than half of the participants, the utility curve near the origin …, which spans the two smallest gains and two smallest losses for each person, was linear. This deviates from the notion of loss aversion which implies that asymmetries should also be observed for small amounts as well.\n\nThis point reflects an argument that Yechiam and other have made in several papers (including here and here) that loss aversion is only apparent in high-stakes gambles. When the stakes are low, loss aversion does not appear.\nOn Grayson (1960):\n\nA similar pattern is observed in Grayson’s utility functions … The amounts used were also extreme high, with only one or two points below the $50,000 range. For the points above $100,000, the pattern seems to show a clear asymmetry between gains and losses consistent with loss aversion. However, for 2/9 participants …, the utility curve for the points below 100,000 does not indicate loss aversion, and for 2/9 additional participants no loss aversion is observed for the few points below $50,000. Thus, it appears that in Grayson (1960) and Swalm (1966), almost all participants behaved as if they gave extreme losses more weight than corresponding gains, yet about half of them did not exhibit a similar asymmetry for the lower losses (e.g., below $50,000 in Grayson, 1960).\n\nAgain, loss aversion is stronger for extreme losses.\nOn Green (1963):\n\n… Green (1963) did not examine any losses, making any interpretation concerning loss aversion in this study speculative as it rests on the authors’ subjective impression.\n\nThe results from Swalm (1966), Grayson (1960) and Green (1963) covers 26 of the 30 participants aggregated by Fishburn and Kochenberger. Halter and Dean (1971) and Barnes and Reinmuth (1976) only involved two participants each.\nSo what of other studies that were available to Kahneman and Tversky at the time?\n\nIn 1955, Davidson, Siegel, and Suppes conducted an experiment in which participants were presented with heads or tails bets which they could accept or refuse. …\n… Outcomes were in cents and ran up to a gain or loss of 50 cents. The results of 15 participants showed that utility curves for gains and losses were symmetric …, with a loss/ gain utility ratio of 1.1 (far below than the 2.25 estimated by Tversky and Kahneman, 1992). The authors also re-analyzed an earlier data set by Mosteller and Nogee (1951) involving bets for amounts ranging from − 30 to 30 cents, and it too showed utility curves that were symmetric for gains and losses.\nLichtenstein (1965) similarly used incentivized bets and small amounts. … Lichtenstein (1965) argued that “The preference for low V [variance] bets indicates that the utility curve for money is not symmetric in its extreme ranges; that is, that large losses appear larger than large wins.” (p. 168). Thus, Lichtenstein (1965) interpreted her findings not as a general aversion to losses (which would include small losses and gains), but only as a tendency to overweight large losses relative to large gains.\n… Slovic and Lichtenstein (1968) developed a regression-based approach to examine whether the participants’ willingness to pay (WTP) for a certain lottery is predicted more strongly by the size of its gains or the size of its losses. Their results showed that size of losses predicted WTP more than sizes of gains. … Moreover, in a follow-up study, Slovic (1969) found a reverse effect in hypothetical lotteries: Choices were better predicted by the gain amount than the loss amount. In the same study, he found no difference for incentivized lotteries in this respect.\nSimilar findings of no apparent loss aversion were observed in studies that used probabilities that are learned from experience (Katz, 1963; Katz, 1964; Myers & Suydam, 1964).\n\nIn sum, the evidence for loss aversion at the time of the publication of prospect theory was relatively weak and limited to high-stakes gambles.\nAs Yechiam notes, Kahneman and Tversky only turned their attention to specifically investigating loss aversion in 1992 - and even there it tended to involve large amounts.\n\nOnly in 1992 did Tversky and Kahneman (1992) and Redelmeier and Tversky (1992) start to empirically investigate loss aversion, and when they did, they used either very large amounts (Redelmeier & Tversky, 1992) or the so-called “list method” in which one chooses between lotteries with changing amounts up until choices switch from one alternative to the other (Tversky & Kahneman, 1992). This usage of high amounts would come to characterize most of the literature later arguing for loss aversion (e.g., Redelmeier & Tversky, 1992; Abdellaoui et al., 2007; Rabin & Weizsäcker, 2009) as would be the usage of decisions that are not incentivized (i.e., hypothetical; as discussed below).\n\nI’ll examine the post-1979 evidence in more detail in a future post, but in the interim will note this observation from Yechiam on the more recent experiments.\n\nIn a review of the literature, Yechiam and Hochman (2013a) have shown that modern studies of loss aversion seem to be binomially distributed into those who used small or moderate amounts (up to $100) and large amounts (above $500). The former typically find no loss aversion, while the latter do. For example, Yechiam and Hochman (2013a) reviewed 11 studies using decisions from description (i.e., where participants are given exact information regarding the probability of gaining and losing money). From these studies, seven did not find loss aversion and all of them used loss/gain amounts of up to $100. Four did find loss aversion, and three of them used very high amounts (above $500 and typically higher). Thus, the usage of high amounts to produce loss aversion is maintained in modern studies.\n\nThe presence of loss aversion for only large stakes gambles raises some interesting questions. In particular, are we actually observing the effect of “minimal requirements”, whereby a loss would push them below some minimum threshold for, say, survival or other basic necessities? (Or at least a heuristic that operates with that intent?) This is a distinct concept from loss aversion as presented in prospect theory.\nFinally - and a minor point on the claim that Yechiam’s paper was the beginning of the spread of the replication crisis to loss aversion - there is of course no direct experiment on loss aversion in the initial prospect theory paper to be replicated. A recent replication of the experiments in the 1979 paper had positive results (excepting some mixed results concerning the reflection effect). Replication of the 1979 paper doesn’t, however, resolve provide any evidence on the replicability of loss aversion itself, nor the appropriate interpretation of the experiments.\nOn that point, in my next post on the topic I’ll turn to some of the alternative explanations for what appears to be loss aversion, particularly the claims of Gal and Rucker that losses do not loom larger than gains."
  },
  {
    "objectID": "posts/kahnemans-optimistic-view-of-the-mind.html",
    "href": "posts/kahnemans-optimistic-view-of-the-mind.html",
    "title": "Kahneman’s optimistic view of the mind",
    "section": "",
    "text": "In the Gerd Gigerenzer versus Daniel Kahneman wars, most of the projectiles seem to fly one way. Gigerenzer attacks directly, Kahneman expends little effort in defence.\nAs one test of whether my impression was correct, I searched Kahneman’s Thinking, Fast and Slow for how many times Kahneman directly mentions Gigerenzer. The answer is six, once in the index and five times in the notes. Gigerenzer is not alluded to in the main text.\nOf the notes, only one is substantive, but it is an interesting point. In a slight reversal of their usual roles, Kahneman defends the power of the human mind:\n\nAn alternative approach to judgment heuristics has been proposed by Gerd Gigerenzer, Peter M. Todd, and the ABC Research Group, in Simple Heuristics That Make Us Smart (New York: Oxford University Press, 1999). They describe “fast and frugal” formal procedures such as “Take the best [cue],” which under some circumstances generate quite accurate judgments on the basis of little information. As Gigerenzer has emphasized, his heuristics are different from those that Amos and I studied, and he has stressed their accuracy rather than the biases to which they inevitably lead. Much of the research that supports fast and frugal heuristic uses statistical simulations to show that they could work in some real-life situations, but the evidence for the psychological reality of these heuristics remains thin and contested. The most memorable discovery associated with this approach is the recognition heuristic, illustrated by an example that has become well-known: a subject who is asked which of two cities is larger and recognizes one of them should guess that the one she recognizes is larger. The recognition heuristic works fairly well if the subject knows that the city she recognizes is large; if she knows it to be small, however, she will quite reasonably guess that the unknown city is larger. Contrary to the theory, the subjects use more than the recognition cue: Daniel M. Oppenheimer, “Not So Fast! (and Not So Frugal!): Rethinking the Recognition Heuristic,” Cognition 90 (2003): B1–B9. A weakness of the theory is that, from what we know of the mind, there is no need for heuristics to be frugal. The brain processes vast amounts of information in parallel, and the mind can be fast and accurate without ignoring information. Furthermore, it has been known since the early days of research on chess masters that skill need not consist of learning to use less information. On the contrary, skill is more often an ability to deal with large amounts of information quickly and efficiently."
  },
  {
    "objectID": "posts/kasparovs-deep-thinking-where-machine-intelligence-ends-and-human-creativity-begins.html",
    "href": "posts/kasparovs-deep-thinking-where-machine-intelligence-ends-and-human-creativity-begins.html",
    "title": "Garry Kasparov’s Deep Thinking: Where Machine Intelligence Ends and Human Creativity Begins",
    "section": "",
    "text": "In preparation for my recent column in The Behavioral Scientist, which opened with the story of world chess champion Garry Kasparov’s defeat by the computer Deep Blue, I read Kasparov’s recently released Deep Thinking: Where Machine Intelligence Ends and Human Creativity Begins.\nDespite the title and Kasparov’s interesting observations on the computer-human relationship, Deep Thinking is more a history of man versus machine in chess than a deep analysis of human or machine intelligence. Kasparov takes us from the earliest chess program, produced by Alan Turing on a piece of paper in 1952, through to a detailed account of Kasparov’s 1997 match against the computer Deep Blue, and then beyond.\nKasparov’s history provides an interesting sense of not just the process toward a machine defeating the world champion, but also when computers overtook the rest of us. In 1977 Kasparov had the machines ahead of all but the top 5% of humans. From the perspective of the average human versus machine, the battle is over decades before the machine is better than the best human. And even then the competition at the top levels is brief. As Kasparov puts it, we have:\n\nThousands of years of status quo human dominance, a few decades of weak competition, a few years of struggle for supremacy. Then, game over. For the rest of human history, as the timeline draws into infinity, machines will be better than humans at chess. The competition period is a tiny dot on the historical timeline.\n\nAs Kasparov also discusses, his defeat did not completely end the competition between humans and computers in chess. He describes a 1995 competition in what was called “freestyle chess”, whereby people were free to mix humans and machines as they see fit. To his surprise, the winners of this competition were not a grandmaster teamed with a computer, but a pair of amateur Americans using three computers at the same time. As Kasparov puts it, a weak human + machine + better process is superior to a strong human + machine + inferior process. There is still hope for the humans.\nThat hope, however, and the human-computer partnership, is also short-lived.  Kasparov notes that the algorithms will continue to improve and the hardware will get faster until the human partnership adds nothing to the mix. Kasparov’s position does not seem that different to my own.\nOne thing clear through Kasparov’s tale is that he does not consider chess to be the best forum for exploring machine intelligence. This was due to both the nature of chess itself, and the way in which those trying to develop a machine to defeat a world champion (particularly IBM) went about the task.\nOn the nature of chess, chess is just not complex enough. Its constraints - eight by eight board with sixteen pieces a side - meant that it was amenable to algorithms built using a combination of fixed human knowledge and brute force computational power. From the 1970s onward, developers of chess computers realised that this was the case, so much of the focus was on increasing computational power and refining algorithms for efficiency until they inevitably reach world champion standard.\nThe nature of these algorithms is best understood in the context of two search techniques described by Claude Shannon in 1949. Type A search is the process of going through every possible combination of moves deeper and deeper with each pass - one move deep, two moves deep and so on. The Type B search is more human-like, focusing on the few most promising moves and examining those in great depth. The development of Type B processes would provide more insight into machine intelligence.\nThe software that defeated Kasparov, along with most other chess software, used what Kasparov calls alpha-beta search. Alpha-beta search is a Type A approach that stops searching down any particular path whenever a move being examined has a lower value than the currently selected move. This process and increases in computational power were the keys to chess being vulnerable to the brute force attack. Although enormous amounts of work also went into Deep Blue’s openings and evaluation function, another few years would have seen Kasparov or his successor defeated by something far less highly tuned. His defeat was somewhat inevitable.\nIBM’s approach to the contest also did not add much to the exploration of machine intelligence. As became clear to Kasparov in the lead up to the Deep Blue rematch (he had defeated Deep Blue in 1996), IBM was not interested in the science behind the enterprise, but simply wanted to win. It provided great advertising for IBM, but the machine logs of the contest were not made available and Deep Blue was later trashed. It’s an interesting contrast to IBM’s approach with Jeopardy winning Watson, which now seems to be everywhere.\nAs a result, Kasparov sees the AlphaGo project as a more interesting AI project than anything behind the top chess machines. The complexity of Go - a 19 by 19 board and 361 stones - requires the use of techniques such as neural networks. AlphaGo had to teach itself to play.\nEven though Kasparov’s offerings on human and machine on intelligence are relatively thin, the chess history in itself makes the book worth reading. Kasparov’s story differs from some of the “myths” that have spread about that contest over the last 20 years, with Kasparov critical of many commentator’s interpretations of events.\nOne story Kasparov attacks is Nate Silver’s version in The Signal and the Noise (at which time Kasparov also takes a few swings at Silver’s understanding of chess). Silver’s story starts at the conclusion of game 1 of the match. When Kasparov considered his victory near complete, Deep Blue moved a rook in a highly unusual move - a move that turned out the be a “bug” in Deep Blue’s programming. As he did not understand it was a bug, Kasparov saw the move as a sign that the machine could see mate by Kasparov in 20 or more moves, and was seeking to delay this defeat. Kasparov was so impressed by the depth of Deep Blue’s calculations that it affected his play for the rest of the match and was the ultimate cause of his loss.\nAs Kasparov tells in his version, he simply discarded Deep Blue’s move as the type of inexplicable move computers tend to make when lost. Instead, his state of mind suffered most severely when he was defeated in game 2. Through game 2 he played an unnatural (to him) style of anti-computer chess, and overlooked a potential chance to draw the game through perpetual check (he was informed of his missed opportunity the next day). He simply wasn’t looking for opportunities that he thought a computer would have spotted."
  },
  {
    "objectID": "posts/kays-other-peoples-money.html",
    "href": "posts/kays-other-peoples-money.html",
    "title": "Kay’s Other People’s Money",
    "section": "",
    "text": "John Kay’s Other People’s Money is generally an excellent book. Kay argues that the growth in the size of the financial system hasn’t been matched by improvements in the allocation of capital. He proposes that financial services are not as profitable as some headline numbers would suggest. And he suggests that the replacement of  those who are good at meeting clients on the 19th hole with those who were good at solving complex mathematical problems was not always a good thing - sometimes clever people are the problem, particularly in a complex environment.\nI highlighted a lot of passages through the book. Here is a small selection.\nFirst, the chapter on risk in excellent - particularly its treatment of rationality. The point in the following paragraph is in some senses obvious, but often ignored:\n\nIf you don’t behave ‘rationally’, you can be ‘Dutch-booked’ – an offensive phrase (to the Dutch – the origins of the expression seem lost in the mists of time) which means that others can devise strategies that will make money at your expense. Many economists use this argument to insist that people do behave ‘rationally’ – behaviour that does not conform to the model will be abandoned because those who engage in it lose money. I used this reasoning myself with students. But I now see it differently. People do buy lottery tickets, week after week, and they do so for reasons that seem entirely valid to them. People don’t behave – for both good and bad reasons – in line with the economic model of rationality. In consequence others do devise strategies that make money at their expense. That consequence is critical to an understanding of how financial markets operate today.\n\nOne of the most interesting threads in the books is that many of the regulatory mantras are about the financial intermediaries, not the end users. The drives for transparency and liquidity in particular come in for criticism by Kay. First, the demand for transparency is a sign of the problem:\n\nTransparency is the mantra in the modern world of finance. But the demand for transparency in intermediation is a sign that intermediation is working badly, not a means of making it work  well. A happy motorist is one who need never look under the car bonnet. A good lawyer manages our problem; a bad lawyer responds to every issue by asking us what we want to do. When ill, we look for a recommended course of action, not a detailed description of our ailments and a list of references to relevant medical texts.\n\nAnd is this demand for transparency even desirable?\n\nThe primary objective of the Securities and Exchange Commission … was to increase the quality and quantity of information available to the public. The corollary was that trading should take place on the basis of that information alone.\nThe idea has superficial attractions and fundamental flaws. The framework of thought is frequently described through the sporting metaphor of ‘fairness’: the ‘level playing field’ on which all players compete on equal terms. To achieve fairness, a standard template of information should be provided to everyone, whether director of a company, investment banker or day trader with a home computer. Market participants may deal, and may only deal, on the basis of that information. No trader can have better information than any other, and success depends only on skill in interpreting it - or anticipating the interpretations of others.\nOf course, this ‘level playing field’ is not achievable or achieved, and would not be desirable if it were to be achieved. Yet, like the regulators of casinos, the regulators of security markets often describe ‘market integrity’ as their objective; their focus is on the efficient functioning of the market, in a narrow technical sense that is concerned with process rather than outcome. The emphasis on the preoccupations of market participants rather than the interests of market users is deeply embedded in current thinking.\n…\nThe effectiveness of financial intermediation in promoting efficient capital allocation depends on the quality of the information available to market participants. Regulation whose primary purpose is to encourage trading by ensuring no trader has an informational advantage actually gets in the ways of efficient capital allocation, in principle and in practice. Effective information and monitoring are best achieved - perhaps only achieved  - in the context of a trust relationship.\n\nAnd on liquidity:\n\nThe need for extreme liquidity, the capacity to trade in volume (or, at least, to trade) every millisecond, is not a need transmitted to markets from the demands of the end-users of these markets but a need, or a perceived need, created by financial market participants themselves. People who applaud traders for providing liquidity to markets are often saying little more than that trading facilitates trading – an observation which is true, but of very little general interest.\n\nKay also has a subtle shot at the ability of governments to use interest rates to achieve policy outcomes:\n\nIs it desirable for government and its agencies - which have sensibly extricated themselves from the business of controlling most prices - to manipulate interest rates, with a view to managing not just the banking system but the economy as a whole? Electricity is an essential element of the national infrastructure, used by every household and business. It is possible to imagine a government trying to manage the economy by controlling the supply and price of electricity - restraining booms by limiting the availability of new power stations and new connections, or by raising the price of electricity, and tackling recessions with low electricity prices and plentiful power.\nI suspect most people would share my instinctive reaction that this approach would be an extremely bad idea - that the outcome would be inefficiency in the supply and use of electricity, and instability in economic growth. Is the intuition that seems relevant to electricity not equally relevant to the financial sector?\nI think it is.\n\nAnd finally, on the inevitability of crises:\n\nThe organisational sociologist Charles Perrow has studied the robustness and resilience of engineering systems in different contexts, such as nuclear power stations and marine accidents. Robustness and resilience require that individual components of the system are designed to high standards. … More significantly, resilience of individual components is not always necessary, and never sufficient, to achieve system stability. Failures in complex systems are inevitable, and no one can ever be confident of anticipating the full variety of interactions that will be involved.\nEngineers responsible for interactively complex systems have learned that stability and resilience requires conscious and systematic simplification, modularity, which enables failures to be contained, and redundancy, which allows failed elements to be by-passed. None of these features – simplification, modularity, redundancy – characterised the financial system as it had developed in 2008. On the contrary, financialisation had greatly increased complexity, interaction and interdependence. Redundancy – as, for example, in holding capital above the regulatory minimum – was everywhere regarded as an indicator of inefficiency, not of strength."
  },
  {
    "objectID": "posts/kellys-what-technology-wants.html",
    "href": "posts/kellys-what-technology-wants.html",
    "title": "Kelly’s What Technology Wants",
    "section": "",
    "text": "Technology wants increasing efficiency, opportunity, emergence, complexity, diversity, specialisation, ubiquity, freedom, mutualism, beauty, sentience, structure and evolvability. As Kevin Kelly argues in What Technology Wants, these are the same things that life wants. Technology extends evolution’s four billion year path.\nWhether you buy Kelly’s central thesis or not (in general, I don’t) and if you ignore some of Kelly’s near-religious fervour (particularly in the opening and closing chapters), Kelly provides a strong argument that the growth in technology is primarily beneficial, with the major benefit being increased choice. Technology provides the basis for achievement. The technology of vibrating strings provided the opportunity for virtuoso violin players. The technology of film allowed cinematic talents to blossom. And consider the technologies of writing and mathematics.\nKelly is not blind to the potential negative effects of technology. Shipping technology allowed mass slavery. The chemical industry spawns toxins. All technologies have unintended consequences. But in response to those negative elements, Kelly argues that prohibition is pointless. Prohibitions don’t work, don’t last and when they are put in place, are usually gone in the next technological cycle. Rather, we should use new technologies to offer solutions to old technologies, and use our knowledge of the path of technology to understand how to control the negative consequences. When new technologies emerge, they should not be banned but rather tested and actively assessed. Where harms occur, they should be rectified and where problems emerge, the technology should be redirected. The path of technology is inevitable and we cannot stop it.\nThe inevitability of technology is a central plank of Kelly’s argument. If the clock of time was rewound and started again, even with  different initial conditions, we would still end up with a similar path of development and resulting inventions. Kelly points to the examples of similar inventions occurring independently on different continents, such as agriculture. He points to the more recent phenomenon of multiple inventors of the same invention, such as lightbulbs or calculus. These technologies are inevitable, as is the rough order that they appear, as one builds on the other.\nKelly builds on this argument of inevitability by pointing to the (widely disputed) inevitability of biological evolution. Convergent evolution is a similar phenomenon to concurrent invention. Eyes and lactose tolerance evolved on multiple occasions.\nI am not convinced that Kelly’s examples of concurrent invention or convergent evolution provide a strong case of the inevitability of invention or evolution, primarily because we don’t know what the fitness landscape of these technologies or traits looks like. If there is a single, clear peak for fitness, all paths will converge to it. If there is a complex multi-modal fitness landscape with a complex topography, we won’t see many of the possibilities. Within our own little world we will see convergence to a local peak, giving the impression of inevitability, but we might be missing the big picture. There may be an array of possibilities that we cannot get to.\nAnother issue is that we can find examples of one-off inventions or evolutionary solutions. As pointed out in a review by Jerry Coyne, the wheel only appeared in North America when brought by Eurasians. Similarly, bones, feathers and the human brain have only appeared once. How different would Africa or Australia’s path have been, and for how long, if they had been isolated from industrialising Europe?\nI lean towards the view that biology is not repeatable . Small chance events have large effects. Although I am open to the idea that intelligence might be likely to evolve, a one-off example in over 4 billion years is hardly a strong case and doesn’t provide very many sample points.\nIf you pull that theoretical pin out of What Technology Wants, the argument that technology has direction lacks a solid thesis. As Matt Ridley did in The Rational Optimist, Kelly takes a general direction and tries to use evolution to turn it into an iron law. But it is the wrong tool to do so.\nRegardless, I enjoyed the book greatly. It is full of interesting observations and ideas by an astute observer of technology. Just don’t look to it for the all encompassing theory of technology."
  },
  {
    "objectID": "posts/keynes-and-the-solved-economic-problem.html",
    "href": "posts/keynes-and-the-solved-economic-problem.html",
    "title": "Keynes and the solved economic problem",
    "section": "",
    "text": "While many have dusted off Keynes during the last few years and asked “what would Keynes do”, it is fair to question whether Keynes would have done anything at all. If he were here today, he might be writing a mountain of blog posts and opinion pieces, but from the perspective of 1930, it is unclear whether he would consider the developed world to have a problem. In his essay Economic Possibilities for Our Grandchildren (pdf), he figured that over the next 100 years the economic problem might be solved:\n\nAll this means in the long run that mankind is solving its economic problem. I would predict that the standard of life in progressive countries one hundred years hence will be between four and eight times as high as it is to-day. There would be nothing surprising in this even in the light of our present knowledge. It would not be foolish to contemplate the possibility of afar greater progress still.\n\nKeynes’s prediction of massively improved living standards has come true. By that measure the economic problem is solved. Once solved, Keynes foresaw that we could turn our attention to areas other than the economic problem, such as leisure and science. People would no longer need to work endlessly to meet basic needs. However, he did see some constraints:\n\nNow it is true that the needs of human beings may seem to be insatiable. But they fall into two classes –those needs which are absolute in the sense that we feel them whatever the situation of our fellow human beings may be, and those which are relative in the sense that we feel them only if their satisfaction lifts us above, makes us feel superior to, our fellows. Needs of the second class, those which satisfy the desire for superiority, may indeed be insatiable; for the higher the general level, the higher still are they. But this is not so true of the absolute needs-a point may soon be reached, much sooner perhaps than we are all of us aware of, when these needs are satisfied in the sense that we prefer to devote our further energies to non-economic purposes.\n\nThe recognition that many human beings care about relative status fits comfortably with an evolutionary view of humans, whereby higher fitness depends on competition with other humans. However, when Keynes turns to evolution, he misses the core driver of this competition:\n\n[I]f, instead of looking into the future, we look into the past-we find that the economic problem, the struggle for subsistence, always has been hitherto the primary, most pressing problem of the human race-not only of the human race, but of the whole of the biological kingdom from the beginnings of life in its most primitive forms.\nThus we have been expressly evolved by nature-with all our impulses and deepest instincts-for the purpose of solving the economic problem. If the economic problem is solved, mankind will be deprived of its traditional purpose.\n\nEvolution is not only about subsistence and survival. It is also about reproductive success and the raising of viable offspring. While the economic problem is largely solved as it relates to survival, the question of reproduction remains. Keynes’s vision that people would cease to worry about accumulating wealth is unlikely to be realised as long as humans remain human and reproductive success is linked to status, wealth and power."
  },
  {
    "objectID": "posts/kremers-o-ring-theory-of-economic-development.html",
    "href": "posts/kremers-o-ring-theory-of-economic-development.html",
    "title": "Kremer’s O-ring theory of economic development",
    "section": "",
    "text": "The latest issue of the Journal of Economic Behavior and Organization has a new paper by Garett Jones (ungated version here) on the  O-ring theory of economic development. Its been floating around as a working paper for a few years, so its nice to see it get a home. But before I post about that paper, I thought I’d revisit Michael Kremer’s classic 1993 paper on which Jones builds.\nThe name of the theory comes from the cause of the Challenger space shuttle explosion. In that case, a highly complex machine with thousands of components failed because one minor part, an O-ring, failed in the cold conditions. This is despite the remaining components being in order.\nKremer saw a similarity between what occurred in the Challenger case and what may happen in production in the economy. A company with a great product and service may fail due to bad marketing. An otherwise functional good may sell at a much reduced price due to a single defect. Kremer asked, if processes of this nature are the norm, what does this imply for economic development?\nKremer pictured firms that engage in production involving a series of tasks. Workers have different skill levels, which is represented by a probability that they properly perform the task. Even if workers have relatively high completion rates, small differences are costly. A firm employing a production process with 10 tasks and workers who complete their step with 95 per cent probability produces only 60 percent (0.9510) of the output of a firm with perfectly competent workers. It is also disastrous to have a weak link in the chain, as 9 fully competent workers paired with someone who messes up half the time will see their total output halved compared to a firm with the perfect ten.\nThis setup has some interesting implications. First, people will sort by skill as firms will find it worthwhile to employ people of the same competency. Those firms with the best workers will then attract the most capital. This will lead to large wage differentials, with the high-skilled workers paired together being much more productive than the low-skilled. Large differences in wages might also be observed across borders if there are differences in skill between countries.\nIf we assume that the tasks in a process are sequential, there are high costs to messing up at later stages of the production process as the work of everyone before is wasted. Kremer uses the example of Rembrandt, who finished off the face and hands of portraits after his assistants had done the easier work. The result of this assumption is that high skilled workers will be employed later in the production chain. If there is a lack of high-skilled workers, firms will focus on producing goods with short and easy production processes. Kremer suggests this may explain the higher share of primary production in poor countries, while rich countries will specialise in complex products.\nFinally, Kremer asks what happens if firms cannot perfectly assess a worker’s skill. In that case, there will be imperfect matching between firms and workers, and firms will be less efficient. But if workers use education to both signal and increase skill, there are benefits from the improved matching and the significant output increase as mistakes drop. Kremer states that this provides a strong argument for subsidising education, as small increases in education and skill increase the returns to education and skill, causing a virtuous circle.\nKremer’s model also provides an argument for boosting IQ. Any measure that can systematically increase worker quality will have multiplicative effects. This matches the observation that boosting a person’s IQ increases their income, but boost the population’s IQ and the wealth gains are many times higher.\nI tend to see Kremer’s model as a natural counterpart of another story about the benefits of high-quality workers. Modern growth is not primarily generated by people not messing up, but by people coming up with great ideas. If you place a bunch of innovative people together, there can be huge network effects. The cost of the low-skilled does not multiply in the same way as for Kremer’s model, but alternative assumptions about the strength of network effects of innovation for those high-quality people can generate similar disparities in wages. Firms pay the low skilled less simply because they have zero marginal product in those innovative processes.\nJones’s new paper has some relevance to that last point. I’ll post on his paper in the next few days. (That post on Jones’s paper can now be found here.)"
  },
  {
    "objectID": "posts/labelling-cultural-group-selection.html",
    "href": "posts/labelling-cultural-group-selection.html",
    "title": "Labelling cultural group selection",
    "section": "",
    "text": "Steven Pinker’s essay on group selection (my initial post on it here) has now attracted a raft of interesting responses that are well worth reading. While it is hard to stitch together and reconcile the various arguments, in sum they confirm one part of Pinker’s argument. In his essay, Pinker wrote:\n\nThe first big problem with group selection is that the term itself sows so much confusion. People invoke it to refer to many distinct phenomena, so casual users may literally not know what they are talking about. I have seen “group selection” used as a loose synonym for the evolution of organisms that live in groups, and for any competition among groups, such as human warfare. Sometimes the term is needlessly used to refer to an individual trait that happens to be shared by the members of a group; as the evolutionary biologist George Williams noted,“a fleet herd of deer” is really just a herd of fleet deer. And sometimes the term is used as a way of redescribing the conventional gene-level theory of natural selection in different words: subsets of genetically related or reciprocally cooperating individuals are dubbed “groups,” and changes in the frequencies of their genes over time is dubbed “group selection.”\n\nIn the responses, Herb Gintis talks of gene-culture evolution. Jonathan Haidt suggests that to see group selected traits, we should look at groupishness in inter-group competition. Peter Richerson talks of cultural group selection and traits such as language. David Sloan Wilson and David Queller look at the technical alignment between inclusive fitness and multi-level selection, which is a biologically focused approach. Across the responses, most of Pinker’s varieties of group selection are covered, and many responses cannot be reconciled with the others as they are talking about different ideas.\nTo resolve this confusion, there needs to be greater differentiation of the various phenomena that people are trying to describe. A starting point would be removing the label of “group selection” from some of them. Daniel Dennett picks up on this point:\n\nPete Richerson’s comment  articulates the details well, but muddies the water by speaking of cultural group selection. There are reasons for calling these phenomena a variety of group selection, reasons ably recounted by Boyd and Richerson in many publications, but better reasons—in my opinion—for avoiding the label, precisely because it seems to give support to the vague and misguided ideas of group selection that Pinker exposes so effectively. These phenomena consist in the evolution by natural selection (both cultural and genetic) of what might be called groupishness adaptations, dispositions (or traditions) of cooperation and the punishment of defectors, and the like, but not by a process of differential reproduction of groups. What differentially reproduce in these phenomena are groupishness memes, not groups. The establishment of these memes may then enable the genetic evolution of enhancements in hosts—like the lactose toleration that evolved in response to the culturally spread tradition of dairying. This is no more group selection than the differential reproduction of the flora in our guts is group selection.\n\nPersonally, I would accept some of these phenomena being called group selection if they always had the prefix of “cultural” attached. At that point, some biologists will drop their instinctive opposition and the debates will be able to focus on the question of whether cultural group selection was important in the evolution of the trait of interest or is a useful framework for analysing it. As I argued in my initial response to Pinker’s piece, cultural group selection avoids some of the issues associated with “biological” group selection. Peter Richerson also makes this point:\n\nNatural selection on large scale patterns of cultural variation is plausible because the cultural variation between neighboring groups that might compete is typically much larger than the genetic variation between the same groups. The reasons are not hard to see; all human groups are more or less open to immigration. Groups intermarry and intermarriage is a very effective conduit for genes. This is less true of culture. Because culture evolves more rapidly than genes, groups will continue to differ despite migration. A large body of social psychology research has characterized the active mechanisms that damp down variation within groups and protect between group variation from the effects of migration. Human social groups are psychologically very salient entities as Pinker acknowledges."
  },
  {
    "objectID": "posts/lazy-analysis-inequality-edition.html",
    "href": "posts/lazy-analysis-inequality-edition.html",
    "title": "Lazy analysis - inequality edition",
    "section": "",
    "text": "Over at WSJ Real Time Economics, Josh Zumbrun turns the following chart into a claim that “the SAT is just another area in American life where economic inequality results in much more than just disparate incomes.”\n\n\nSAT- Student Affluence Test\n\n\n\nBut what does the chart actually tell us? In a perfect meritocracy, the smartest students will score the highest. But as intelligence is heritable, the smarter kids will tend to have smarter and higher income patterns, giving us the pattern we see in the chart. In an alternative world where parents pay for results, we end up with the same pattern. So that charts tell us nothing. It’s consistent with both worlds.\nI’m not exactly Robinson Crusoe in criticising this article. See also Arnold Kling and James Pethokoukis - although the assortative mating Pethokoukis refers to isn’t necessary to get a graph that looks like this, even though it is almost certainly playing a role.\nHaving picked on this article, the use of a bivariate analysis (a natural result of using a graph) while ignoring other confounding variables is a disappointingly common feature in the increasingly popular “data journalism”."
  },
  {
    "objectID": "posts/levines-is-behavioural-economics-doomed.html",
    "href": "posts/levines-is-behavioural-economics-doomed.html",
    "title": "Levine’s Is Behavioural Economics Doomed?",
    "section": "",
    "text": "David Levine’s Is Behavioural Economics Doomed? is a good but slightly frustrating read. I agree with Levine’s central argument that rationality is underweighted in many applications of behavioural economics, and he provides many good examples of the power of traditional economic thinking. For someone unfamiliar with game theory, this book is in some ways a good introduction (or more particularly, to the concept of Nash equilibrium). And for some of the points, Levine shows a richness in the literature that you don’t often hear about if you only consume pop behavioural economics books.\nBut the book is also littered with straw man arguments. Levine often gives views to behavioural economists which I am not sure they generally hold, and he often picks strange examples. And when it comes to explaining away behaviour that doesn’t fit so neatly with the rational actor model, Levine is not always convincing.\nAs an example, Levine provides an overview of the prisoner’s dilemma, a classic game demonstrating why two people might not cooperate, even though cooperation leads to a better outcome than both players defecting. Levine uses it to argue against those who suffer from the fallacy of composition - inferring something is true of the whole because it is true of the parts - and wonder why we can have war, crime and poverty if people are so rational. But who are these people that Levine is arguing against? I presume not the majority of the behavioural economics profession who are more than familiar with the prisoner’s dilemma game.\nLevine’s introduction to the prisoner’s dilemma is good when he discusses what happens with different strategies or game designs. But when it comes to the players in experiments who don’t conform to the Nash equilibrium - such as those who don’t defect in every period if there is a defined end to the game - he hand waves away their play as “rational and altruistic” rather than seriously exploring whether they made systematic errors.\nSimilarly, when discussing the ultimatum game, Levine simply describes the failure to maximise income as “modest”. He does make the important point that it is rational for first movers to offer more than the minimum if there is a possibility of rejection (and since they don’t have opportunity to learn, they will get this wrong sometimes). But he seems less concerned about the behaviour of player 2 who rejects a material sum. Yes it might be a Nash equilibrium, but the behavioural view might shed some light on why we end up at that particular Nash equilibrium.\nLevine is similarly dismissive of the situations where people make errors in markets. “Behavioural economics focuses on the irrationality of a few people or with people faced with extraordinary circumstances. Given time economists expect these same people will rationally adjust their behaviour to account for new understandings of reality and not simply repeat the same mistakes over and over again.” But given how many major decisions are one-shot decisions with major consequences (purchasing cars, retirement decisions etc.), surely they are worth exploring.\nOne of the more bizarre examples is where Levine addresses the question of why people vote despite having almost no chance of changing the outcome. Levine gives an example of a voting participation game conducted in the lab where he found that participants acted according to the predicted Nash equilibrium, reflecting their costs of voting, the benefits of winning and the probability of their vote swinging the result. But he doesn’t then grapple with the clear problem that this limited experiment doesn’t translate to the real world. Funnily enough, only pages later he cautions “[B]eware also of social scientist [sic] bearing only laboratory results.”\nLevine also  brings out the now classic question of why couldn’t economics predict the economic crisis. He points out that crises must be inherently unpredictable as there is an inherent connection between the forecaster and the forecast. If a model that people believed predicted a collapse in the market of 20% next week, the crash would happen today (Let’s ignore for the moment that there seems to be an economist predicting a crash almost every day).\nIn defence of the economists, Levine pulls out a series of (well-cited) papers that he believes already explained the crisis, such as providing for the possibility of sharp crashes and the effect of fools in the market. Look, the shape of the curve by this random paper is the same! But was that actually what happened? Was that the dominant theory? Levine seems to believe mere existence of literature in which crises are present is an indication that the profession is fine, even if that wasn’t a dominant or widely believed model.\nHaving spend most this post complaining about Levine’s angle of attack, there are many good points. His discussion of learning theory is interesting - people don’t know all information before they undertake an action and learn along the way. Selfish rationality with imperfect learning does a pretty good job of explaining much behaviour. Some of this throwaway lines also make important points. For example, if a task is unpleasant, it can be rational to leave it to the last moment. Uncertainty can make the procrastination even more rational.\nSome of Levine’s critiques of the experimental evidence are also interesting. One I was not aware of was whether the appearance of the endowment effect in some experiments was due to people misunderstanding the Becker-DeGreeot-Marschak elicitation procedure. (People state their willingness to pay or accept and a random draw of the price is made. If the price is lower than the willingness to pay, they pay it.) Levine points to experiments where, if people are trained to understand the procedure, the endowment effect disappears. As I mentioned in a previous post, Levine also points to some interesting literature on anchoring.\nLevine closes with a quote from Loewenstein and Ubel that is worth repeating:\n\n… [behavioral economics] has its limits. As policymakers use it to devise programs, it’s becoming clear that behavioral economics is being asked to solve problems it wasn’t meant to address. Indeed, it seems in some cases that behavioral economics is being used as a political expedient, allowing policymakers to avoid painful but more effective solutions rooted in traditional economics.\nBehavioral economics should complement, not substitute for, more substantive economic interventions. If traditional economics suggests that we should have a larger price difference between sugar-free and sugared drinks, behavioral economics could suggest whether consumers would respond better to a subsidy on unsweetened drinks or a tax on sugary drinks.\nBut that’s the most it can do.\n\nUnderneath Levine’s critique you sense this is what is really bugging him. Despite the critiques, traditional economic approaches still have a lot of power. And for some people, that seems to have been forgotten along the way."
  },
  {
    "objectID": "posts/life-expectancy-and-the-dawn-of-agriculture.html",
    "href": "posts/life-expectancy-and-the-dawn-of-agriculture.html",
    "title": "Life expectancy and the dawn of agriculture",
    "section": "",
    "text": "Relative to their hunter-gatherer counterparts, early Neolithic farmers were short, had poor dental health due to malnutrition, bone lesions suggestive of disease and stunted spines from the back-breaking labour. This comparison underlies Jared Diamond’s claim that agriculture was the worst mistake in the history of the human race.\nHowever, this decline in health was not permanent. For example, the health of Egyptians 12,000 years ago, shortly after the shift to farming from foraging, was poor. But by 4,000 years ago, height had returned to pre-agricultural levels and evidence of malnutrition in tooth enamel was lower than that of their hunter gatherer comparators. Agriculturalists adapted to their new diet and environment.\nThis story plays out in a 2007 working paper by Oded Galor and Omer Moav on how the shift to agriculture could have affected life expectancy, and more importantly, still affects life expectancy today. They make a fairly simple argument. When people shifted to agriculture and faced a greater threat of disease and other hazards associated with dense living, those who were predisposed to make larger investments in health (toward, say, higher investment in immune function or, as they model in their paper, resource transfers to children) would have an evolutionary advantage and increase in prevalence. When those threats to the environment later ease, as has now happened in most advanced economies, that higher investment in health translates into higher life expectancy.\nThe most interesting implication of this argument is the comparison across populations. Those populations that underwent a transition to agriculture earlier will have had more time to adapt to the new high-mortality environment and as a result, will make a larger investment in health. When those same populations transition to a modern low-mortality environment, they have a higher life expectancy than those without the history of agriculture.\nGalor and Moav test this idea by examining how the length of time since the Neolithic Revolution is associated with life expectancy in modern populations. Using a range of controls, including latitude, land arability and levels of income in modern times, they found that an extra thousand years of time elapsed since the transition to agriculture results in an extra 1.5 to 2 years of life expectancy for that population. Using those numbers as a rough guide, the extra 3,000 or so years that European populations have experienced agriculture relative to African populations would result in an extra 5 years life expectancy for the Europeans even if gaps in socioeconomic circumstances were eliminated.\nThere are a couple of implications to this argument. The first is that if mortality risk starts to decline, we would expect investment to start to go the other way, such as focusing on quantity of children. It may be that the couple of hundred years since any population broke the Malthusian shackles is too short for evidence of that rebound. There might also be successive increases in the level of the shock - first with the transition to agriculture, later in the move to high-density disease-ridden cities, with increasing mortality risk playing out right up to the Industrial Revolution.\nThis paper gets a bit of space in Galor’s book Unified Growth Theory, where it ties in quite nicely. Galor advocates a unified growth theory approach as it captures the patterns in technology, income and population (including a demographic transition) within a single analytical framework. The basic mechanism in Galor’s unified growth models is that investments in education (quality) of offspring feeds technological progress, which in turns feeds the incentives to invest in education. This virtuous feedback loop drives the transition to a modern high-growth economy.\nNow take the argument about increasing investment in health when a society has transitioned to agriculture. Those investments form part of the general investment in quality, helping to spur on the growth in human capital. A society with the early transition to agriculture will have higher investment in children, which will in turn drive an earlier transition to a modern growth state than would occur in a society with a short history of agriculture (as is reflected in which populations have made that transition).\nHaving said all the above, I’m not convinced by this particular argument linking longevity and agriculture (although I could be). Investments in health may trade-off with investments in other types of quality, such as education or brain energy consumption. The transition to agriculture also allowed for higher reproduction rates through the sedentary lifestyle, which is investment in quantity not quality. And lastly, does the difference in longevity simply reflect the maladaptation to modern diets among those with a short history of agriculture? As a test of this, what would be the respective lifespans in a hunter-gatherer context of two groups with short and long histories of agriculture?"
  },
  {
    "objectID": "posts/low-social-mobility-equals-success.html",
    "href": "posts/low-social-mobility-equals-success.html",
    "title": "Low social mobility equals success",
    "section": "",
    "text": "At Gene Expression, Razib Khan writes:\n\n[T]he logical end point of a quasi-”blank slate” policy position is the diminution of environmental impacts so that only genes matter\n….\nIf it turns out that the heritability of intelligence is relatively high in the developed world, then it may be that the Left-progressive project of ameliorating class based differences in access to cognitively enhancing environments has succeeded to a large extent. Barring genetic engineering this is the “end of history” for this project. It is a matter of when, not if (i.e., if you reject that the project has hit sharply diminishing marginal returns, logically it should at some point if the Left-progressive project succeeds). Assortative mating and more transparent meritocracy should allow for cleaner sorting within the population, and inter-generational class churn should decrease and stabilize at a basal level dictated by the random environmental variables which no amount of social engineering can squeeze out of the system.** A perfect meritocracy would replace cultural class with biological caste.******\n\nA decrease in social mobility would signify the project’s success. I expect this contrasts with one of the primary measures of success for those who hold the blank slate policy position. Will they use an indicator of their success to argue that opportunity (in an environmental sense) is becoming less equal?"
  },
  {
    "objectID": "posts/male-incentives.html",
    "href": "posts/male-incentives.html",
    "title": "Male incentives",
    "section": "",
    "text": "In the comments to my post last month on the Cato Unbound series New Girl Order: Are men in decline?, there was some suggestion that men were being disincentivised from working hard by the increasing income and resources of women.\nBryan Caplan took up a similar argument when he looked at what was happening through the lens of two markets - labour and mating. He concluded that:\n\nIncome and income potential still matter.  But women now focus more on looks, machismo, coolness, and other “alpha” traits.  Holding charisma constant, working harder just doesn’t attract women the way it used to.  The result: Less desirable men often give up on women altogether - further tilting the effective male/female ratio in favor of the remaining men.  And both kinds of men act like boys: The less desirable men have little to lose, and the more desirable men can get away with it.\n\nWhen Caplan comes to this conclusion, he understates the strength of the effect of increasing female income on the low-status male. This is because there is an additional element of the mating and labour market relationship - labour market outcomes are a signal of quality for the mating market. Woman care not only about the resources from the man’s income, but also about the signal of quality that it provides. This means that high-income men remain attractive even if the woman has considerable resources. When a woman ceases to consider the low-status male as she now has enough resources to find a higher quality mate who won’t support her children, her new mate’s income will act as a signal of quality in the same way that looks or charisma might.\nThe following chart (data from Heard (2011)) of Australian marriage and de-facto rates by age and income shows how strong the benefits of high income remain. The high income 20-24 year old is 4 times more likely to be married or in a de facto relationship than someone from the low-income group. Over 40 per cent of the lowest income 40-44 year old males remain unpaired. While incentives for men at the bottom of the income scales may be declining, there is still plenty of incentive to work hard for most of the male population."
  },
  {
    "objectID": "posts/malthus-and-the-feast.html",
    "href": "posts/malthus-and-the-feast.html",
    "title": "Malthus and the feast",
    "section": "",
    "text": "I have been trying to find an electronic version of Thomas Malthus’s second edition of his An Essay on the Principle of Population. The second edition is significantly expanded and revised from the first, while later editions through to the sixth in 1826 are essentially minor revisions of the second (the sixth part one and part two are here). While I have, to now, been unable to find the text of the second edition, the Wikipedia page for the book notes an interesting paragraph that was included in the second edition, but was omitted from later editions:\n\nA man who is born into a world already possessed, if he cannot get subsistence from his parents on whom he has a just demand, and if the society do not want his labour, has no claim of right to the smallest portion of food, and, in fact, has no business to be where he is. At nature’s mighty feast there is no vacant cover for him. She tells him to be gone, and will quickly execute her own orders, if he does not work upon the compassion of some of her guests. If these guests get up and make room for him, other intruders immediately appear demanding the same favour. The report of a provision for all that come, fills the hall with numerous claimants. The order and harmony of the feast is disturbed, the plenty that before reigned is changed into scarcity; and the happiness of the guests is destroyed by the spectacle of misery and dependence in every part of the hall, and by the clamorous importunity of those, who are justly enraged at not finding the provision which they had been taught to expect. The guests learn too late their error, in counter-acting those strict orders to all intruders, issued by the great mistress of the feast, who, wishing that all guests should have plenty, and knowing she could not provide for unlimited numbers, humanely refused to admit fresh comers when her table was already full.\n\nGarrett Hardin, who I have posted about previously, wrote an essay which blames the general rejection of Malthus’s writings on this single paragraph:\n\nThe shocking Feast was spread before the public in 1803, in the second edition of the Essay. Malthus could hardly have chosen a worse time a sort of “compassion revolution” was then well under way. During the 19th century the English-speaking world made great progress in the humane treatment of animals, in getting rid of slavery, in curtailing child labor, and in (reluctantly) giving a modicum of freedom to women. Bonar has said that ‘for thirty years it rained refutations of Malthus.’ Before Malthus died in 1834, four more editions of the Essay had been published, the last in 1826; but in none of them did the Feast appear for a second time.\n\nThe remainder of Hardin’s essay is typical of many of his later writings on population, but one other passage in particular caught my eye:\n\nConscience, like other individual characteristics, varies. One woman may be satisfied with one child, while another craves four. Intended or not, with no community control of reproduction, a competition in breeding will develop. In all other species of animals there is a genetic component to fertility; but anyone who suggests that genes also influence fertility in the human animal kindles the ire of genophobes - individuals who are intellectually repelled by the idea of genetic differences in humans. Fortunately, in the dispute over population control, it is not necessary to raise the genetic issue. It is enough to assume that there may be a sort of cultural heredity - that the advice and examples set by parents have some influence on the behavior of their children. (Understandably, of course, parents usually crave more than merely ‘some’.) With either genetic heredity or cultural heredity, a variant of Gresham’s Law is set in play since high fertility tends to diminish the monetary wealth of a family, then (focusing only on economics) we must say that, over time, with uninhibited fertility, low living standards drive out high.\n\nAnd if anyone knows where to find an electronic version of the second edition of An Essay on the Principle of Population, please let me know."
  },
  {
    "objectID": "posts/manzi-on-the-abortion-crime-hypothesis.html",
    "href": "posts/manzi-on-the-abortion-crime-hypothesis.html",
    "title": "Manzi on the abortion-crime hypothesis",
    "section": "",
    "text": "My recent reading of David Colander and Roland Kupers’s Complexity and the Art of Public Policy prompted me to re-read James Manzi’s Uncontrolled: The Surprising Payoff of Trial-and-Error for Business, Politics, and Society. I see the two books as riffs on a similar theme.\nI’ll post a review of Uncontrolled later this week, but in the meantime, Manzi provides an interesting take on the Donohue-Levitt abortion-crime hypothesis. Their hypothesis is that abortion reduces crime as unwanted children are more likely to become criminals. As the legalisation of abortion increased access to abortion and decreased the number of unwanted children, decreases in crime through the 1990s and 2000s could be due to this legalisation.\nDonohue and Levitt’s initial paper triggered a raft of responses, including one demonstrating an analytical error, which, once corrected for, resulted in the abortion-crime link disappearing. Donohue and Levitt then redid the work, and showed by recasting a few assumptions, the error could be corrected for and the link re-established. As Manzi states:\n\nThe revealing observation is not that there was an analytical error in the paper (which almost certainly happens far more often than we like to think), but that once it was found and corrected, it was feasible to rejigger the regression analysis to get back to the original directional result through various defensible tweaks to assumptions. If one could rule out either the original assumptions or these new assumptions as unreasonable, that would be better news for the technique. Instead we have a recipe for irresolvable debate.\n\nManzi also points out that Levitt, in his book Freakonomics (with Stephen Dubner), indirectly identified one of the reasons why Donohue and Levitt’s claim is so tenuous:\n\nIn Freakonomics, Levitt and Dubner write that Roe [the Supreme Court decision in _Roe v Wade _establishing a right to abortion] is “like the proverbial butterfly that flaps its wings on one continent and eventually creates a hurricane on another.” But this simile cuts both ways. It is presumably meant to evoke the “butterfly effect”: meteorologist Edward Lorenz’s famous description of a global climate system with such a dense web of interconnected pathways of causation that long-term weather forecasting is a fool’s errand. The actual event that inspired this observation was that, one day in 1961, Lorenz entered .506 instead of .506127 for one parameter in a climate-forecasting model and discovered that it produced a wildly different long-term weather forecast. This is, of course, directly analogous to what we see in the abortion-crime debate and Bartels’s model for income inequality: tiny changes in assumptions yield vastly different results. It is a telltale sign that human society is far too complicated to yield to the analytical tools that nonexperimental social science brings to bear. The questions addressed by social science typically have none of the characteristics that made causal attribution in the smoking–lung cancer case practical."
  },
  {
    "objectID": "posts/marketing-science-ideas-xchange-msix-2015.html",
    "href": "posts/marketing-science-ideas-xchange-msix-2015.html",
    "title": "Marketing Science Ideas Xchange (MSiX) 2015",
    "section": "",
    "text": "The 2015 Marketing Science Ideas Xchange - MSiX - has been announced for 30 July in Sydney. As it says in the blurb, MSiX “is dedicated to exploring how brands can benefit from the interface between behavioural science and marketing.”\nThe headline speaker is Michael Norton, Harvard professor, author of Happy Money: The Science of Happier Spending and developer of the first set of experiments on the IKEA effect (that last point is the reason I knew his name when I heard he would be speaking).\nThe rest of the speakers and further detail on the conference are here - with the speaking line-up including me. I’ll be talking about how behavioural economics (science) could benefit from a good dose of evolutionary biology, and how that evolutionary lens can be valuable in understanding consumer behaviour.\nLast year’s event - headlined by Rory Sutherland (who linked me into this MSiX world) - was a pretty good day, and this year looks promising too.\nAnd here’s Norton doing the TED thing."
  },
  {
    "objectID": "posts/markets-and-morals.html",
    "href": "posts/markets-and-morals.html",
    "title": "Markets and morals",
    "section": "",
    "text": "I enjoyed the responses to Michael Sandel’s critique of markets in the How Markets Crowd Out Morals forum on the Boston Review website. Sandel’s essay follows in the wake of his new book, What Money Can’t Buy: The Moral Limits of Markets.\nMost of the response are worth reading, but I particularly enjoyed the one by Herb Gintis. Some of the more interesting parts of Gintis’s piece were as follows:\n\nThe idea that some valuable things should not be bought and sold on markets has been known for centuries, certainly since the anti-slavery movement in England. All mature economists understand this well. Just because some otherwise-obscure economist can gain ﬁfteen minutes of fame by advocating the suppression of non-monetary gift-giving doesn’t mean we should interpret his claims as an exercise of brilliant economic argument. …\nBy focusing on the marketability of particular things, Sandel misses the larger effect of an economy regulated by markets on the evolution of social morality. Movements for religious and lifestyle tolerance, gender equality, and democracy have ﬂourished and triumphed in societies governed by market exchange, and nowhere else.\nMy colleagues and I found dramatic evidence of this positive relationship between markets and morality in our study of fairness in simple societies—hunter-gatherers, horticulturalists, nomadic herders, and small-scale sedentary farmers—in Africa, Latin America, and Asia. … [W]e measured the degree of market exposure and cooperation in production for each society, and we found that the ones that regularly engage in market exchange with larger surrounding groups have more pronounced fairness motivations. The notion that the market economy makes people greedy, selﬁsh, and amoral is simply fallacious.\n\nAs an aside, an interesting question is the direction of causation between markets and morality. I suspect the emergence of markets is endogenous to the characteristics of the populations in which they emerge."
  },
  {
    "objectID": "posts/maslows-hierarchy.html",
    "href": "posts/maslows-hierarchy.html",
    "title": "Maslow’s hierarchy",
    "section": "",
    "text": "I’ve just read Geoffrey Miller’s Spent, which I enjoyed. There are many interesting threads to the book, which I’ll blog about over the coming weeks.\nIn one of the earlier chapters, Miller discusses Maslow’s hierarchy of needs. The hierarchy, from the base to the top, consists of physiological, safety, love, esteem and self-actualisation needs. It is only on satisfaction of the first four needs that one will focus on self actualisation, which includes (among other things) morality, creativity and discovery.\nMaslow’s hierarchy is often a framing point for discussions about development, concern for the environment and consumerism. The problem is, however, that it often doesn’t fit human behaviour. Miller puts this problem nicely:\n\nIt does not “cut nature at the joints” in terms of the key selection pressures that shaped human behavior: survival and reproduction. Survival includes most of Maslow’s physiological needs (breathing, eating), but also some of the more concrete safety needs (avoiding harm from predators, parasites, sexual rivals, and hostile tribes), social needs (building relations with family, friends, and mates who can help feed, protect, and heal you under adverse conditions), cognitive needs (to learn about survival-increasing opportunities and survival-reducing dangers), and even aesthetic needs (to find a propitious landscape for one’s clan to live in, to make weapons that are serviceably symmetric, strong, and sharp). Reproductive challenges, including finding high-quality sexual partners and raising high-quality offspring, encompass one of the key physiological needs (having sex) and most of the other social, esteem, cognitive, aesthetic, and self-actualization needs.\n\nHe then points out the more serious shortcoming:\n\n[A] branch of evolutionary theory called “life history theory” points out that there are often tough trade-offs between these survival and reproductive priorities. The lower-level needs do not always take priority. For example, male elephant seals will often starve to death during a breeding season while guarding their harems. If elephant seals could talk, … they might explain that they were giving up a physiological need (to eat) for three higher needs: a social need (to feel intimacy and belonging with each of many females), an aesthetic need (to be surrounded by beautiful—that is, fine, fit, fat, fertile—females), and a self-actualization need (to be the best elephant seal one can be, as demonstrated through biting, mauling, bloodying, and excluding all male sexual rivals from one’s beach-front harem). But these last three Maslovian needs can actually be reduced to reproductive benefits. Natural selection crafted social, aesthetic, and self-actualization motivations because they yielded higher reproductive success over thousands of generations of elephant seal evolution. Male elephant seals who were “slackers,” content to fulfill their survival and safety needs without conflict, would have avoided the bloody beach sites where more ambitious “status seekers” fought, copulated, starved, and died. The slacker seals may have been perfectly happy, and might have even turned vegan and ate plankton, but they did not leave any descendants to inherit their easygoing temperaments. Only the male seals that were willing to compete for dominance, status, and harems, even at the cost of their own lives, sired any offspring.\n\nThere is no shortage of people (men) that are willing to significantly risk safety or forgo basic physical needs if they are deprived of reproductive opportunities."
  },
  {
    "objectID": "posts/measurement-error-in-23andme.html",
    "href": "posts/measurement-error-in-23andme.html",
    "title": "Measurement error in 23andme",
    "section": "",
    "text": "As a broad indication of the effects of measurement error in 23andme, ancestry analysis for two identical twins is below. It suggests the decimal point isn’t yet justified. (Their Neanderthal ancestry estimates are also off by 0.1% from each other.)\n\n\nTom ancestry\n\n\n\n\n\nLeo ancestry"
  },
  {
    "objectID": "posts/merton-on-retirement-incomes.html",
    "href": "posts/merton-on-retirement-incomes.html",
    "title": "Merton on retirement incomes",
    "section": "",
    "text": "There is a neat article by Robert Merton (from July last year) in the Harvard Business Review on the shift to defined contribution plans when saving for retirement.\nThere are two major types of retirement savings arrangements. The first is defined benefit pension plans. Under these plans, a set income is paid to retirees based on factors such as years of service and final salary, with the income paid until death. These were once common, but they are now rare.\nToday most of us are in a second type of arrangement, defined contribution plans. Under these, we contribute a proportion of our income to a retirement fund. Our retirement income is dependent upon our contributions and the performance of that fund. In Australia, we are required to contribute 9.5 per cent of our income to a superannuation account.\nWhen I get a statement from my defined contribution fund, it tells me about my contributions, the asset value and the fund returns. There is no mention of the retirement income I might be able to obtain. I also have a choice of asset allocations, which are defined in terms of risk to the asset value. But as Merton points out, this is where things start to go wrong.\n\nThe trouble is that investment value and asset volatility are simply the wrong measures if your goal is to obtain a particular future income. Communicating with savers in those terms, therefore, is unhelpful—even misleading. To see why, imagine that you are a 45-year-old individual looking to ensure a specific level of retirement income to kick in at age 65. Let’s assume for simplicity’s sake that we know for certain you will live to age 85. The safe, risk-free asset today that guarantees your objective is an inflation-protected annuity that makes no payouts for 20 years and then pays the same amount (adjusted for inflation) each year for 20 years. If you had enough money in your retirement account and wanted to lock in that income, the obvious decision is to buy the annuity.\nBut under conventional investment metrics, your annuity would almost certainly look too risky. As interest rates move up and down, the market value of annuities, and other long-maturity fixed-income securities such as U.S. Treasury bonds, fluctuates enormously. In 2012, for instance, there was a 30% range between the highest and lowest market value of the annuity for the 45-year-old over the 12 months. However, the income that the annuity will provide in retirement does not change at all. Clearly, there is a big disconnect about what is and is not risky when it comes to the way we express the value of pension savings.\n\nSo what does that mean for fund managers ?\n\nThe particulars are, of course, somewhat technical, but in general, they should continue to follow portfolio theory: The investment manager invests in a mixture of risky assets (mainly equity) and risk-free assets, with the balance of risky and risk-free shifting over time so as to optimize the likelihood of achieving the investment goal. The difference is that risk should be defined from an income perspective, and the risk-free assets should be deferred inflation-indexed annuities.\n\nThen there is the consumer side of the equation. As a start, financial advisers should be asking what are the consumer’s retirement_ income_ goals and what action would need to be taken to meet them.\nBut rather than suggesting the adviser try to educate the consumer about the technicalities of this decision, Merton suggests more consumer engagement may not be a good thing.\n\nConsumer education is often proposed as a remedy, but to my mind it’s a real stretch to ask people to acquire sufficient financial expertise to manage all the investment steps needed to get to their pension goals. That’s a challenge even for professionals. You’d no more require employees to make those kinds of decisions than an automaker would dump a pile of car parts and a technical manual in the buyer’s driveway with a note that says, “Here’s what you need to put the car together. If it doesn’t work, that’s your problem.”\n\nIt might even backfire.\n\nExperience also suggests that customer engagement in investment management is not necessarily a good thing. People who are induced to open a brokerage account in their IRAs often become very active in investing for their pension, trading stocks around the world on their computers after work. This is far from a good idea; such short-term trading will not improve the savers’ chances of successfully achieving retirement goals—in fact, it will diminish them.\n\nThis is, after all, why we have financial professionals - there is a need for (and benefit to) specialisation and trade.\n\nIt is fair enough to expect people to provide for their retirement. But expecting them to acquire the expertise necessary to invest that provision wisely is not. We wouldn’t want them to. We don’t want a busy surgeon to spend time learning about dynamic immunization trading instead of figuring out how to save lives, any more than we would want skilled finance professionals to spend time learning how to do their own surgery.\n\nIt is interesting that Merton uses automotive and medical examples. People are often wary of trusting mechanics. In medicine, it is hard to asses surgeon quality before an operation, and we receive almost no reliable feedback on whether they did a good job (unless they accidentally leave an instrument in you). If anything, the trend in medicine is for people to become more educated (hello Google) about what services they are about to receive rather than completely letting go and trusting the medical professional."
  },
  {
    "objectID": "posts/michael-mauboussins-more-than-you-know-finding-financial-wisdom-in-unconventional-places.html",
    "href": "posts/michael-mauboussins-more-than-you-know-finding-financial-wisdom-in-unconventional-places.html",
    "title": "Michael Mauboussin’s More Than You Know: Finding Financial Wisdom in Unconventional Places",
    "section": "",
    "text": "Michael Mauboussin’s message in More Than You Know: Finding Financial Wisdom in Unconventional Places is that we need an interdisciplinary toolkit to give us the diversity to make good decisions. This is not diversity in groups, but diversity in thinking. You need diverse cognitive tools to deal with diverse problems.\nThe book is a series of essays that Mauboussin wrote for a newsletter over a dozen years or so when he was at CSFB. Given his background in investment management, there is a heavy focus on investment decisions. However, the tools he discusses are relevant for most decision-making domains, be that as a manager, parent, employee, or so on.\nMauboussin draws his interdisciplinary tools from four main areas, around which the essays in the book are arranged.\nThe first set of essays, on investment philosophy, largely concern probabilistic thinking. Focus on the process, not the outcome. If you judge solely on results, you will be deterred from taking the risks necessary to make the right decision.\nIn this vein, don’t set target prices for shares - an estimate of how you will believe a company will perform. Rather, provide a range of prices with associated probabilities. This allows you to invest knowing the downside probability, and to assess your choices in the knowledge that some decisions will have unfavourable outcomes.\nOne interesting thread to these essays is what amounts to a defence of Mauboussin’s occupation, investment management. Many people (myself included) see investment management performance as largely the outcome of luck. Mauboussin argues for the presence of skill (in at least some cases), with long streaks requiring (to paraphrase Steven Jay Gould) extraordinary luck imposed on great skill.\nOne limb of Mauboussin’s argument is the 15 consecutive years of market out-performance by Bill Miller, a fund manager at Legg Mason (where Mauboussin worked at the time the book was published). Getting 15 consecutive heads when tossing a coin is a one in 32,000 proposition. If your coin has only a 44% chance of coming up heads (the average probability of a fund outperforming the market over that stretch), a streak of 15 has a probability of one in 223,000. That number balloons to one in 2.3 million if you take the average probability of a fund beating the market in each individual year (in a couple of years less than 10% of funds beat the market).\nGiven these odds, Mauboussin argues that it is unlikely that Miller was effectively flipping a coin. Miller’s skills meant the odds were actually less daunting. Yes, he needed luck, but there needed to be skill underneath to realise the streak.\nI’m not sure I buy this argument. This 15 year window is only one of many available. There are many funds. (And I have just found this - someone doing the numbers to get the odds of 3 in 4 of a 15 year streak by someone at some time.)\nA contrast to the Miller story comes later in the book, when Mauboussin notes the trading success of Victor Niederhoffer in a different light. Niederhoffer averaged 35% per year returns from 1972 to 1996 (says Wikipedia), but this all came crashing down to nothing in 1997. He built another fortune to then lose in the global financial crisis. Mauboussin uses Niederhoffer’s story as an example of the fat tails of asset price movements, a pattern of many small changes, and a small but larger than expected number of large changes. To use Nassim Taleb’s framing, Niederhoffer was picking up pennies in front of a steamroller. (And on that point, Miller’s record since his streak is not so great.)\nThe second set of essays draws on psychology. This partly draws on the heuristics and biases program of Daniel Kahneman and friends, but Mauboussin ranges over wider territory. He draws in literature on animal behaviour, such as the herding behaviour of ants and the stress response of animals, and on the literature in naturalistic decision-making. He also has a keen appreciation of the fact that many of these decisions occur in systems, meaning that individual decision making flaws don’t necessarily lead to poor aggregate outcomes.\nThe third set of essays innovation and competition, has a game theory and evolutionary thread. The last set is on complexity, which contains both a warning about seeing cause and effect in complex systems, and a suggestion that some of the work in the complexity field gives a lens to understand the patterns we see.\nSome of these essays deserve posts of their own, so I won’t go into any in-depth except to make a general observation. I am a fan of interdisciplinary approaches to problems, but parts of the book, particularly these latter sections, hint at why they aren’t adopted. Many times you get an interesting angle of looking at an issue, but it is not clear what you should do differently.\nPartly this is a result of the origins of the book. Each essay is around two thousand words (guessing), so each gives a taste of a topic but little depth. One essay tends not to build on another.\nThat said, much of the advice is to effectively do nothing. That is valuable advice. If you want to read media accounts about share market moves, recognise that this is entertainment, not information. Disentangling cause and effect in a complex system like the share market is difficult, if not impossible, so stop telling stories.\nMauboussin also warns in the introduction that some ideas may not be useful right away. Some may never be useful. You are building a toolkit for future problems that you haven’t seen yet.\nAs a closing note, Mauboussin references many other popular science books. Given some of the essays must be 20 years old, I had not heard of most (which might say something for the longevity of popular science books). I’ve added a few to the reading list, but it will be interesting to see how they have held up through time."
  },
  {
    "objectID": "posts/micromotives-and-macrobehavior.html",
    "href": "posts/micromotives-and-macrobehavior.html",
    "title": "Micromotives and macrobehavior",
    "section": "",
    "text": "In a post a couple of months or ago as part of a debate on complexity in aid, I recommended Thomas Schelling’s Micromotives and Macrobehavior as a good starting point for understanding complexity science. The book predates a lot of the language associated with complexity science (in fact, I don’t think it uses the word complexity at all), but it provides an excellent illustration of some of the basic tenets of complexity science. These include that complex behaviour can emerge from simple mathematical models and that individual actions can result in aggregate outcomes that do not reflect the individual intentions.\nHaving recommended but not read the book in over 10 years, I re-read it and it triggered a couple of thoughts. The first is that my recollection of the great illustrations provided by Schelling was correct, although towards the end of the book they can feel a touch laboured as he goes over a few more examples than were needed for me to get the point. Still, some of his classic discussions, such as that on the emergence of segregation despite people having only moderate preferences as to their neighbours, provide real insight. The use of examples like these is typical of a lot of books on complex adaptive systems (such as Miller and Page’s Complex Adaptive Systems), although I rate Schelling’s as the most interesting, and probably the most accessible to someone not familiar with the area.\nOn finishing the book, I found I was asking the same question I always seem to ask after reading a book on complex adaptive systems and agent based modelling (one of the common ways of modelling complex systems). To what extent could complexity science or agent based modelling shed new light on economic policy questions or macroeconomics? The books always give a host of interesting examples that may change the way you think about some phenomena, and often advocate broader use of complexity science, but they are vague on what future uses might be. There seems to be more people interested in the methodology of complexity science and agent based modelling than there are people interested in using it as a tool. I’ve taken a small stake in this question by exploring the use of agent based models in my research as a way of testing the robustness of my models, exploring some extensions (such as adding a spatial element) and allowing me to loosen some assumptions. However, I am not convinced about how much value I am likely to extract.\nOn that note, I’d be happy to receive recommendations of good books that discuss complexity science (or agent based modelling) being applied to an area of economic interest and ideally, showing how it leads to some new insights. And by insights, I am hoping for more than “complexity science shows us it is all too complex” or “we need policies to deal with the system’s complexity”. I have plenty of books I can refer someone to when they wish to know what complexity science is, but none which I am happy to recommend as demonstrating what it is good for."
  },
  {
    "objectID": "posts/millers-spent-sex-evolution-and-consumer-behavior.html",
    "href": "posts/millers-spent-sex-evolution-and-consumer-behavior.html",
    "title": "Miller’s Spent: Sex, Evolution, and Consumer Behavior",
    "section": "",
    "text": "Geoffrey Miller’s main thesis in Spent: Sex, Evolution and Consumer Behavior is that the conspicuous consumption we use to signal traits such as intelligence, agreeableness or conscientiousness is unnecessarily indirect. Instead, we should use our evolved abilities to show these characteristics through humour, communication and interaction with others. Miller summarises his position as follows:\n\nConsumerist capitalism is largely an exercise in gilding the lily. We take wondrously adaptive capacities for human self-display - language, intelligence, kindness, creativity, and beauty - and then forget how to use them in making friends, attracting mates, and gaining prestige. Instead we rely on goods and services acquired through education, work, and consumption to advertise our personal traits to others. These costly signals are mostly redundant or misleading, so others usually ignore them. They prefer to judge us through natural face-to-face interaction. We think our gilding dazzles them, though we ignore their own gilding when choosing our own friends and mates.\n\nI am sympathetic to Miller’s general argument. A few minutes of face-to-face communication can undo many signals, such as our choice of clothing or car. However, status, wealth and power are still qualities that matter to a mate. Given two mates of equal genetic value, the one with the higher resources or status would be expected to be better able to provide for the child. Through human history, parental wealth has had a strong correlation with child mortality (possibly only breaking down in developing countries in recent decades). There will be some rate of trade-off between resources and genetic quality, and we will want to signal our level of resources.\nThere are also limits to the accuracy of face-to-face assessment. While I may seem hard-working and conscientious, the fact I have worked to the top of my business is evidence of this. Can I fake conscientiousness through a few dates more easily than I can through a decade in a top company? Do I then need the BMW to show that I am actually successful in this company as opposed to being at the bottom of the pile?\nMiller is optimistic that people can move away from the consumerist culture. Tongue-in-cheek, he suggests tattoos of our intelligence and big-five personality trait scores. While this may be a more accurate signal, for a signal to be useful we need to consider the recipient. Are they more aroused by the sight of my IQ score, my bank statement or the BMW acquired due to my intelligence and wealth? If they prefer the BMW signal, despite its lower accuracy, it would be a poor strategy on my part to deviate. As Miller notes:\n\nThere seems to be no easy shortcut through our person-perception systems. We have to feed them the kinds of social stimuli that they evolved to expect, and institutionally validated trait tattoos are not among those stimuli.\n\nOne thread of the book I appreciated is Miller’s argument that government should remove any barriers to checking out of consumer culture. In some ways, that is the appeal of the libertarian state - those who wish to check out can. While Miller has this freedom-supporting thread, he is not averse to Pigovian taxes on a raft of products. He writes:\n\nAll negative externalities are, by definition, encroachments on other people’s lives and property. So, even hard-core libertarians who believe that governments should do nothing more than protect people from such encroachments should be willing to accept a consumption tax specifically designed to counteract such encroachments. From this viewpoint, the consumption tax is not paternalistic meddling. Rather, it is a classical “Pigovian tax” designed to correct the negative externalities of market activity. … From that perspective, it seems reasonable that governments should impose consumption taxes designed to neutralize each product type’s externalities. In other words, we should be free to choose what we buy and how we live, as long as we pay the fair price for every harm we do to others in the process.\n\nIf we are going to raise tax revenue, Pigovian taxes are one of the more efficient ways to do it. However, Miller spots my number one concern:\n\nSkeptics might object that to set appropriate product-specific consumption tax rates would require a vast new government bureaucracy. We would need thousands of economists, statisticians, actuaries, and psychologists to measure all the externalities, risks, and costs of every product class. That is true, but that is precisely what we need: good solid data about the true social and environmental costs of the goods and services we buy. If we don’t collect and analyze such data, all arguments about the social and environmental effects of different policies are just blather. They can’t be evidence-based if there is no evidence.\n\nMiller’s view of a product-by-product assessment would seem to offer massive opportunity for companies to lobby for different taxes on competitor’s products as theirs has a longer-life or is more environmentally friendly. And I’m not sure it is required. As an alternative, taxes at the source, such as a carbon tax, would allow the market to allocate the carbon price across products. To address the short life-span of products, you make people pay the costs of their disposal. The bureaucracy could be smaller than it is.\nSome of Miller’s other proposals, such as subsidies for positive externalities like fitting airbags to cars, might also be better addressed by looking at the incentives.  If we put more of the burden of health care of individuals, this increases their incentive to act safely. They can choose their own balance between airbags and speed - or possibly an insurance policy that gives them a discount because they have airbags.\nMiller ends the book with a series of exercise for the reader. You can go through a check-list of the experiences of our paleolithic ancestors, assess what is the purposes of your consumption and consider alternative signalling techniques. It is a useful set of exercises and highlights what is the most useful message of the book - as you go to make each purchase, consider what you are intending to signal by it and whether there is a more effective way to send that message. Are you simply sending the same signal as everyone else? In Miller’s words:\n\nYou anticipate the minor mall adventure: the hunt for the right retail environment playing cohort-appropriate nostalgic pop, the perky submissiveness of sales staff, the quest for the virgin product, the self-restraint you show in resisting frivolous upgrades and accessories, the universe’s warm hug of validation when the debit card machine says “Approved,” and the masterly fulfillment of getting it home, turned on, and doing one’s bidding. The problem is, you’ve experienced all this hundreds of times before with other products, and millions of other people will experience it with the same product. The retail adventure seems unique in prospect but generic in retrospect. In a week, it won’t be worth talking about.\n\nIf your intention was to send a signal with this product, forget it. But if it is useful, that’s another thing.\n\nFollow up posts on Spent can be found here, hereand here."
  },
  {
    "objectID": "posts/modelling-versus-theory.html",
    "href": "posts/modelling-versus-theory.html",
    "title": "Modelling versus theory",
    "section": "",
    "text": "While looking for something completely unrelated, I came across this 2007 Econ Journal Watch paper Model Building versus Theorizing: The Paucity of Theory in the Journal of Economic Theory. From the abstract:\n\nWe argue that a model may qualify as theory only if it purports to answer three questions: Theory of what?, Why should we care?, What merit in your explanation? We examine the 66 regular articles appearing in the 2004 issues of Journal of Economic Theory—“the leading journal in economic theory” —and apply the three requirements. … We find that 27 articles fail the first test (Theory of what?) and 58 articles fail at least one of the three requirements. Thus, 88 percent of the articles do not qualify as theory. … We contend that the journal’s claim to scientific status is doubtful, as well as the very title of the journal. A truer title would be, Journal of Economic Model Building. More generally, we challenge calling model building “theory.”\n\nWhile I would be reluctant to get drawn into an argument about what is theory, Klein and Romero’s argument about the lack of relevance of much economic modelling is important. They go on to say:\n\nAll stakeholders should be concerned that scholarly prestige will be leveraged in a way that feeds mere scholasticism, rather than real contributions to science, learning, and culture. Even if scholastic arts did not distort thought and understanding, they certainly might divert them from the things that matter more. If JET—and many other outlets—consists mainly of crafts that lack integrity as explanation, it does not deserve much prestige within the enterprise we call economics. This article, then, speaks to all stakeholders — elected officials, taxpayers, tuition payers, donors, university administrators, faculty, students, and other citizens concerned about the character and content of economics. …\nOur concern is to challenge the semantics that hold that every model is (or entails) theory. We maintain that scientific culture understands theory to entail requirements of importance and usefulness. Theory must serve real purposes of the science, thus, arguably meriting attention from the scientific community.\n\nOf Klein and Romero’s three questions, I usually worry about the third. Economic models are often not tested and even less commonly discarded. Klein and Romero point to some evidence.\n\nIt is possible that other economists take published models and subsequently supply the commitment to empirical relevance necessary to graduate the models to theory (Hausman 1992, 273). Whether such graduation occurs is a question calling for further research, but investigations by Philip Coelho and James McClure (2005; 2007) suggest that few models graduate to theory. In one investigation, Coelho and McClure identify the JET articles published in 1980 and containing at least five lemmas. They find that there were 12 such articles. They then investigate the articles that cite those 12 papers. As of June 2006, there were 237 articles that cite the 12 JET articles. They report that of 237, only nine utilize data. Of the nine, only two articles attempt a direct empirical assessment of the model’s results, and zero render a judgment of “accept” or “reject.\n\nI sometimes fear that a lot of the work integrating economics and evolutionary biology might fall at this hurdle (particularly the discarding bit), as there are more theorists than experimentalists in this area. But thankfully there is so much relevant empirical work being done in evolutionary biology, anthropology, experimental psychology and behavioural economics that it’s going to be hard to maintain a poor theory in this area without somebody pulling it apart."
  },
  {
    "objectID": "posts/monkey-inequality.html",
    "href": "posts/monkey-inequality.html",
    "title": "Monkey inequality",
    "section": "",
    "text": "Over at Wired, Jonah Lehrer has written a post in which he looks at a couple of lines of evidence about the innate response of humans to inequality. The first line, based on brain scans, is nicely discussed by Jeff at Cheap Talk.\nThe second involves an experiment with capuchin monkeys:\n\nA similar lesson emerges from a classic experiment conducted by Franz de Waals and Sarah Brosnan. The primatologists trained brown capuchin monkeys to give them pebbles in exchange for cucumbers. Almost overnight, a capuchin economy developed, with hungry monkeys harvesting small stones. But the marketplace was disrupted when the scientists got mischievous: instead of giving every monkey a cucumber in exchange for pebbles, they started giving some monkeys a tasty grape instead. (Monkeys prefer grapes to cucumbers.) After witnessing this injustice, the monkeys earning cucumbers went on strike. Some started throwing their cucumbers at the scientists; the vast majority just stopped collecting pebbles. The capuchin economy ground to a halt. The monkeys were willing to forfeit cheap food simply to register their anger at the arbitrary pay scale. &gt; &gt;\n\nThis labor unrest among monkeys illuminates our innate sense of fairness. It’s not that the primates demanded equality — some capuchins collected many more pebbles than others, and that never created a problem — it’s that they couldn’t stand when the inequality was a result of injustice.\n\n\nWhile this experiment shows that problems emerge following arbitrary outcomes, I would not describe the reaction as demonstrating an “innate sense of fairness”. The revolt was one-sided, and the monkeys who received grapes were not handing them back. It is more like an innate dislike of being on the bottom.\nMore importantly, I am not convinced that the experiment fully tested whether fair outcomes could generate unrest. As the stone gathering ability of monkeys is not likely to vary much, particularly when compared to the variation in human ability across different professions, the variation in opportunity for the monkeys was negligible. The variation in outcomes was also relatively small.\nAs a result, if a monkey was hungry, it is an easy task to get some stones. However, if a human needs money, the range of possibly courses of action are large, with the returns widely variable. A cleaner and a brain surgeon earn significantly different rates of return for an hour of effort.\nWhat if the monkeys were paid on the basis that whoever collects the most stones gets the vast majority of the cucumbers or grapes? Or what if effort started to pay compound dividends, meaning that long-term focussed stone gathering delivered much higher returns. The rules might be clear and the victory fair, but I am not sure that the reaction to the significantly different outcomes would be benign.\nLehrer uses this experiment to draw the following conclusion:\n\nWhen the rich do something to deserve their riches, nobody complains; that’s just the meritocracy at work. But when those at the bottom don’t understand the unequal distribution of wealth — when it seems as if the winners are getting rewarded for no reason — they get furious. They doubt the integrity of the system and become more sensitive to perceived inequities. They start camping out in parks. They reject the very premise of the game.\n\nEven if the belief that nobody complains when the rich deserve their riches was true, do the majority of people understand the basis of the distribution of wealth in today’s economy? Has the economy reached a level of complexity that will always generate a certain level of distrust by those at the bottom?"
  },
  {
    "objectID": "posts/more-on-violence.html",
    "href": "posts/more-on-violence.html",
    "title": "More on violence",
    "section": "",
    "text": "Following yesterday’s post on female preference for masculine men, a couple of old articles came to mind.\nThe first (and I am not sure why this did not come into my head yesterday) is the work by Napoleon Chagnon on the Yanomamo. From his paper Life Histories, Blood Revenge, and Warfare in a Tribal Population:\n\nStudies of the Yanomamo Indians of Amazonas during the past 23 years show that 44 percent of males estimated to be 25 or older have participated in the killing of someone, that approximately 30 percent of adult male deaths are due to violence, and that nearly 70 percent of all adults over an estimated 40 years of age have lost a close genetic relative due to violence. Demographic data indicate that men who have killed have more wives and offspring than men who have not killed.\n\nThe reproductive advantage to being unokais (having killed) was significant, with an average of 4.91 children compared to 1.59 for those in Chagnon’s sample who had not killed. To the extent that traits making someone more likely to kill are heritable, they would shortly dominate this population (assuming they do not already).\nThere is a gap between Chagnon’s study and the preference for masculinity that I wrote about yesterday. The unokais were reproductively successful with women who knew that they had killed. There is no evidence that the unokais appear more masculine and that this was behind their reproductive success. However, preference for masculinity in a violent society may be an indicator or proxy of what was Chagnon noted - that in violent societies there is benefit to being aggressive.\nThe second article was by Edward O Wilson, titled Competitive and aggressive behavior. After some very interesting discussion on the rate of behavioural change in humans, the closing paragraph was as follows:\n\nSome degree of aggressiveness in man is nevertheless probably adaptive - that is, genetically programmed by means of natural selection to contribute to fitness in the narrow reproductive sense. This complex trait cannot be assumed to be due to a useless or harmful genetic residue left over from prehistoric times. It is more plausibly viewed as a trait that has been adaptive within the past few hundreds or, at most, thousands of years. Some of its components might have even originated during historical times, since both theoretical considerations and empirical studies on animal populations show that some behavioural traits can evolve significantly within ten generations or less.\n\nIf we accept this point, we might find genetically based variations in aggressiveness across modern populations. Extension of the research into female preferences for masculine males could shed some light on the strength of this adaptive advantage in modern populations and whether a cycle between violence and adaptive advantage for aggressiveness is a feasible scenario."
  },
  {
    "objectID": "posts/more-people-more-ideas-in-the-long-run.html",
    "href": "posts/more-people-more-ideas-in-the-long-run.html",
    "title": "More people, more ideas - in the long run",
    "section": "",
    "text": "More people means more ideas. This concept underlies arguments ranging from Julian Simon’s belief that human living conditions will continue to improve through to Bryan Caplan’s argument that we should have more kids. While I don’t always take this concept to the extent of Simon or Caplan (as I have posted on before), the concept must be right at some level. One person will have more ideas than zero people. One hundred people will have more than one person. You can argue about diminishing returns and so on, but the basic concept must hold.\nFor me, some of the more interesting evidence is over the long-run, which Michael Kremer discusses in a paper titled Population Growth and Technological Change: One Million B.C. to 1990. Kremer bases his argument on the Malthusian concept that population is a measure of technology. In a Malthusian state, the environment and level of technology constrain population. As technology grows, a given area can support a higher population, so technological progress is directly linked to population growth.\nKremer showed that population growth has accelerated over the last million years (although this has slowed the last hundred or so), with population growth faster than exponential growth. This is consistent with the idea that as the population grew, people generated ideas faster and faster, further accelerating the technological growth rate, and hence population growth. The following diagram shows Kremer’s point, with the population growth rate increasing with population size until recent times.\n\nKremer created a number of models to show this point. In one model, he included the concept that research productivity increases with income, which might explain why some large populous countries (China and India) have lower levels of technological development (for the moment). Kremer showed that even though this creates some ambiguity about the relationship between population and technological growth, the concept that larger populations have higher levels of technological progress held.\nAnother concept addressed by Kremer is that productivity might depend on the size of the population. This encompasses positive network effects whereby more people allows more specialisation and service of a larger market, and negative effects such as “stepping on toes” whereby researchers duplicate each others’ efforts. Including these effects in his model did not change the basic finding that technological progress increases roughly in proportion with population.\nKremer tested the model predictions with some basic regressions against long-term population data. Not surprisingly, they all showed a strong correlation between population size and population growth.\nThe paper gets more interesting with some of Kremer’s analysis of inter-regional differences. Taking five successively smaller populations: the old world, the Americas, Australia, Tasmania and Flinders Island. For each of these areas, population density increases with land area, suggesting that higher population are able to sustain a higher level of technology. Kremer writes:\n\nAs the model predicts, in 1500, just after Columbus’ voyage reestablished technological contact, the region with the greatest land area, the Old World, had the highest technological level. The Americas followed, with the agriculture, cities, and elaborate calendars of the Aztec and Mayan civilizations. Mainland Australia was third, with a population of hunters and gatherers. Tasmania, an island slightly smaller than Ireland, lacked even such mainland Australian technologies as the boomerang, fire-making, the spear-thrower, polished stone tools, stone tools with handles, and bone tools, such as needles [Diamond, 1993]. Flinders Island, near Tasmania, has only about 680 square kilometers of land, and according to radiocarbon evidence, its last inhabitants died out about 4000 years after they were cut off by the rising seas-suggesting possible technological regress.\n\nIn some ways, the general message of Kremer’s paper appears obvious. However, it is one of those papers where the feedback relationship between population and technology makes it difficult to confirm the direction of causation (even though I agree with the general concept). Regardless of the source of technological progress, the Malthusian model predicts that population will increase to match it. Higher population levels will always have a higher level of technology, regardless of the source of innovation.\nThe element in the data which lends strength to Kremer’s argument is that technological progress accelerates when there is a larger population. However, the length of time over which Kremer makes his observations raises an important question. Human ancestors only achieved our modern brain size in the last 100,000 or so years (peaking 30,000 years ago). There is evidence of a cultural leap forward around 50,000 years ago. The source of ideas is a very different creature at each end of Kremer’s time series. How much of the acceleration is generated by greater population, and how much by a more productive population?"
  },
  {
    "objectID": "posts/morriss-why-the-west-rules-for-now-part-ii.html",
    "href": "posts/morriss-why-the-west-rules-for-now-part-ii.html",
    "title": "Morris’s Why the West Rules For Now - Part II",
    "section": "",
    "text": "Following yesterday’s post on Ian Morris’s approach to biology in Why the West Rules - for Now, below are my thoughts on the some other elements of the book.\nIn general, I found the book to be an interesting and easy to read description of the history of the West and East, and I will probably use it as a reference for that in the future. I recommend the book for this. At times it felt like Morris was describing “one damned thing after another”, and I found that I was waiting for the theoretical tie-ins. When they did come, they seemed weak, as Morris frames his theoretical explanations with what might be described as slogans, somewhat in the style of Thomas Friedman.\nSo, to Morris’s main points. First, Morris provides his central line that:\n\n[H]istory is made by lazy, greedy, frightened people (who rarely know what they’re doing) looking for easier, more profitable, and safer ways to do things.\n\nMorris takes this to be a universal human characteristic (no argument from me there), although as I suggested yesterday, Morris does not address whether there are differences in the inherent characteristics of the people who seek to meet these goals.\nOn geography, Morris is of the “maps, not chaps” school of thought. In addition to supporting Jared Diamond’s geographic account for the origins of agriculture and why development is higher some parts of the world, Morris uses geography to explain why the West discovered the Americas first and obtained the associated bounty.\nAs Morris describes, the size of the Atlantic and Pacific, island positions and prevailing winds made it far more likely that someone from the West would bump into the Americas before someone from the East. I don’t find that controversial. However, this leaves unexplained why small ships and crews were pouring out of Europe, with various degrees of incompetence, while China had become quiet on this front. For example, Morris describes Columbus’s systematic search for someone to sponsor his voyage to China. Morris suggests that it was inevitable that someone would support Columbus, and there were plenty of other explorers willing to take similar risks.\nGiven the level of exploration by explorers from the West and the absence of explorers from China, if the advantages of geography were reversed, China probably would still not have discovered the Americas, unless Zheng He had discovered it during China’s ocean going days in the early 1400s. Someone from the West, possibly through accident or incompetence, eventually would have. Morris explicitly counters the argument that culture led to this lack of exploration by the Chinese, suggesting that people develop solutions to the problems they face, which in turn creates new problems and so on. Morris argues that China was not engaging in this process of exploration and technological innovation as it did not match the problems faced by it at the time.\nI found this approach by Morris to be a state or whole of culture centred view. The question in my mind is what were the problems faced by the majority of individuals. Whether from the East or West, people in the 1400s (or most eras) would be trying to increase family income and well-being, innovate new ways to increase business profitability and so-on. Morris spent little time considering what problems individuals faced, and most of his time on the state, which I am not sure reveals the true motivations of most of the economic actors.\nFollowing from this concept that people respond to the problems they face, Morris writes of the advantages of backwardness and how social development creates the very factors that undermine it. I would rephrase it slightly to state that the traits, skills and ideas that are most useful at a point will depend on the environment, but the concept provides a useful encapsulation why development is not linear and why there is no steady lead of the West over the East (although as I discussed yesterday, that does not mean that there are no long-term underlying factors to be considered). Morris describes a Red Queen type scenario, where each side needs to run to stay still and over time each will have different advantages.\nThis approach allows Morris to weave in a Malthusian thread to the story. He notes that people can be better off when disaster occurs in per capita sense, such as after the plague, but that this may be negative for social development . At many times in the book I wished his index of development (which consisted of energy usage - which dominated the results until after 1800, war-making, communications and the largest city size) was accompanied by an individual level measure of well-being. Apart from individual well-being being what economists tend to be interested in, it might also have given some shape to the specific problems faced by individuals at any point and their respective motivations.\nFinally, Morris closes his book with some predictions about the future. With a framework that suggests there are advantages to backwardness, you can argue that China will pull ahead. But drawing on Ray Kurzweil’s vision of the singularity and the looming horsemen of the apocalypse, such as climate change, it might all go bad. At this point, the book degenerates a bit into some home-spun wisdom, but I do give credit to Morris for making his predictions with an underlying theme that it is inherently unpredictable."
  },
  {
    "objectID": "posts/msix-marketing-science-ideas-xchange.html",
    "href": "posts/msix-marketing-science-ideas-xchange.html",
    "title": "MSiX: Marketing Science Ideas Xchange",
    "section": "",
    "text": "For those in or near Sydney at the end of July, there’s an interesting conference in the works - the Marketing Science Ideas Exchange. From the blurb:\n\nThe Marketing Science Ideas Xchange (MSiX) is the first event of its type in Australia dedicated to the interface between behavioural science and marketing. The conference will demonstrate why behavioural sciences in general, and behavioural economics in particular, is making such strong headways into advertising. The conference promises to be a mix of theory and practical examples, all housed within a fun and interactive ideas exchange environment.\n\nIt will be interesting to go to a behavioural insights conference full of marketers, rather than the usual economists. Marketers have had to be willing to embrace a relatively realistic understanding of human behaviour and, obviously, have been exploiting decision-making biases for years. But I get the sense that they could be more systematic in their approach and adopt a much richer understanding of human nature.\nHeadlining the conference is Rory Sutherland, the best friend of behavioural science in the ad world (although Rory is happy to keep using the term ’behavioural economics’). Watch the video below for a taste of what is on offer. I endorse Rory’s fast train strategy, and recommend googling around for some of this other presentations.\nhttp://youtu.be/iueVZJVEmEs\nThere are some other interesting speakers in the lineup. I recently saw Alex Gyani from the UK’s Behavioural Insights Team (now seconded in the NSW Department of Premier and Cabinet) speak on the need for evidence based policy, and expect he’ll be continuing that theme. The rest of the program is here."
  },
  {
    "objectID": "posts/my-first-biology-publication.html",
    "href": "posts/my-first-biology-publication.html",
    "title": "My first biology publication",
    "section": "",
    "text": "For pitching in to help my PhD supervisor on a paper, I’ve scored my first biology publication:\n\nSperm use economy of honeybee (Apis mellifera) queens\nAuthors: Boris Baer, Jason Collins, Kristiina Maalaps, Susanne P. A. den Boer\nThe queens of eusocial ants, bees, and wasps only mate during a very brief period early in life to acquire and store a lifetime supply of sperm. As sperm cannot be replenished, queens have to be highly economic when using stored sperm to fertilize eggs, especially in species with large and long-lived colonies. However, queen fertility has not been studied in detail, so that we have little understanding of how economic sperm use is in different species, and whether queens are able to influence their sperm use. This is surprising given that sperm use is a key factor of eusocial life, as it determines the fecundity and longevity of queens and therefore colony fitness. We quantified the number of sperm that honeybee (Apis mellifera) queens use to fertilize eggs. We examined sperm use in naturally mated queens of different ages and in queens artificially inseminated with different volumes of semen. We found that queens are remarkably efficient and only use a median of 2 sperm per egg fertilization, with decreasing sperm use in older queens. The number of sperm in storage was always a significant predictor for the number of sperm used per fertilization, indicating that queens use a constant ratio of spermathecal fluid relative to total spermathecal volume of 2.364 × 10−6 to fertilize eggs. This allowed us to calculate a lifetime fecundity for honeybee queens of around 1,500,000 fertilized eggs. Our data provide the first empirical evidence that honeybee queens do not manipulate sperm use, and fertilization failures in worker-destined eggs are therefore honest signals that workers can use to time queen replacement, which is crucial for colony performance and fitness."
  },
  {
    "objectID": "posts/my-latest-in-behavioral-scientist-simple-heuristics-that-make-algorithms-smart.html",
    "href": "posts/my-latest-in-behavioral-scientist-simple-heuristics-that-make-algorithms-smart.html",
    "title": "My latest in Behavioral Scientist: Simple heuristics that make algorithms smart",
    "section": "",
    "text": "My latest contribution at Behavioral Scientist is up. Here’s an excerpt:\n\nModern discussions of whether humans will be replaced by algorithms typically frame the problem as a choice between humans on one hand or complex statistical and machine learning models on the other. For problems such as image recognition, this is probably the right frame. Yet much of the past success of algorithms relative to human judgment points us to a third option: the mechanical application of simple models and heuristics.\nSimple models appear more powerful when removed from the minds of the human and implemented in a consistent way. The chain of evidence that simple heuristics are powerful tools, that humans use these heuristics, and that these heuristics can make us smart does not bring us to a point where these humans are outperforming simple heuristics or models consistently applied by an algorithm.\nHumans are inextricably entwined in developing these algorithms, and in many cases provide the expert knowledge of what cues should be used. But when it comes to execution, taking the outputs of the model gives us a better outcome.\n\nYou can read the full article here."
  },
  {
    "objectID": "posts/my-year.html",
    "href": "posts/my-year.html",
    "title": "My year",
    "section": "",
    "text": "In the day job, for most of this year I was seconded onto the Australian Government’s Financial System Inquiry. The Inquiry was established to provide a broad review of the Australian financial system, looking at system stability, competition, consumer protection, technological change and whether the system was serving the needs of users.\nThe Inquiry’s final report is now out and available here. It has received a lot of press here - I think my favourite article so far is this one (if you hit the paywall, google “David Murray has gone rogue” and try that link).\nAmong other things, there are recommendations to increase bank capitalisation, introduce new obligations on financial product issuer and distributors, and to hold a review into the ownership and use of customers’ financial data. But given my role in the Inquiry and the stage the Government is at - it is now seeking public comment - it’s not really appropriate for me to say which recommendations I support.\nPossibly the most interesting recommendations are in the retirement income space. Australia has a compulsory superannuation system, where (currently) 9.5 per cent of our income is required to go into retirement savings. But after a lifetime of being forced to save, once we reach what is called the “preservation age”, you can take the money out. You are free to blow it on a holiday, sportcars or pension means-test exempt house, and then receive the pension.\nTo try to change this behaviour, the Inquiry recommended introduction of a default retirement product, which will have some mix of income flow and longevity insurance (so your money doesn’t run out before you die). It will be an interesting exercise to design a system where that default will be an successful anchor. It will require a lot of tax, pension and other social policy settings to stop people from ignoring that default and taking their lump of cash in another way.\nThe other big event of the year was the arrival of twin boys. We think they are identical - four months of confusing who is who is the basis for that - but DNA tests are on the way to confirm. And I’ll be keeping one locked in the cupboard for the next five years to prove to the genetic determinists that environment does matter."
  },
  {
    "objectID": "posts/natural-selection-and-savings.html",
    "href": "posts/natural-selection-and-savings.html",
    "title": "Natural selection and saving",
    "section": "",
    "text": "In the academic literature at the intersection of economics and evolutionary biology, evolution of time preference (patience) is one area that has received much attention. This makes some sense, as most economic models that consider decisions over time include time preference. Time preference is normally included in the model through a discount rate of a fixed value, so an evolutionary analysis might help in determining what that value is.\nOne of the first articles to ask what rate of time preference might be expected to evolve was by Ingemar Hansson and Charles Stuart, who examined the intergenerational rate of time preference. Intergenerational time preference is reflected in the rate of saving of one generation for the benefit of following generations. The first generation sacrifices their own consumption for consumption of their descendants.\nThis paper is another of those that, at least for the headline result, should strike one as obvious. Time preference would evolve such that when the person seeks to maximise utility (in an economic sense), they will also be maximising fitness. In this case, they would follow the “golden rule” of saving, which is the rate of saving that maximises the steady-state level of consumption across generations.\nThis outcome contrasts with empirical evidence that the golden rule tends not to be followed, with savings rate in developed, Western countries well below the golden rule. Savings rates are somewhat closer to (and sometimes argued to be higher) than the golden rule level in Asian countries.  Why there is this departure requires consideration of other factors, such as aggregate risk (as Robson and Samuelson consider in this paper).\nHansson and Stuart determined that, if people followed the golden rule of saving, the discount rate would reflect the long-term population growth rate. This requires that no generation be weighted less than any other generation, which indicates a strong concern about future generations.\nBased on population growth in a number of European countries since 1500, Hansson and Stuart suggested that the discount rate implied by their model would lie between zero and a few per cent per year, or between zero and one per generation. While they selected population growth rates since 1500 largely due to the lack of earlier demographic data, their upper bound estimate implicitly assumes that evolution of time preference can be relatively rapid to reflect recent population growth rates. In this case, it would also suggest that discount rates are increasing. As population growth increases, one should cut saving and focus on consumption today.\nMoving beyond the headline result, an interesting part of the paper is when Hansson and Stuart ask what are the consequences of their model for economic growth. A harsh environment may increase the marginal benefit of consumption (the benefit from each additional unit) and decrease population density, which will in turn increase the level of labour that each person produces. As labour supply increases, this in turn increases the productivity of and the level of capital.\nFollowing this logic, the authors suggest that harsh natural environments select for genotypes that have a stronger preference for saving, leading to an equilibrium with low population density and high per-capita capital. Selected traits include a preference for work and accumulation of physical capital. Hansson and Stuart suggest that this might explain why humans left the Malthusian state first in regions with harsh winters.\nThis results seems intuitive, although it is an interesting contrast with a Malthusian model of the economy. In a Malthusian model, high levels of technology and productivity are reflected in high population densities. To reconcile Hansson and Stuart’s thinly populated, harsh environment with this Malthusian picture, it might be necessary to imply that preferences were shaped when the first populations entered the harsh regions, and that preferences have not significantly changed despite these previously unpopulated environments now having much higher population densities.\nOne interesting comment in the paper refers to a methodological argument by Stigler and Becker, who stated that it is worth assuming that preferences do not differ importantly between people. Hansson and Stuart suggest that this is the case where the population to be modelled consisted of a homogeneous people from a given environment, but that this would not be as applicable in a “melting pot” such as the United States."
  },
  {
    "objectID": "posts/nelson-and-winters-an-evolutionary-theory-of-economic-change.html",
    "href": "posts/nelson-and-winters-an-evolutionary-theory-of-economic-change.html",
    "title": "Nelson and Winter’s An Evolutionary Theory of Economic Change",
    "section": "",
    "text": "Richard Nelson and Sidney Winter’s An Evolutionary Theory of Economic Change is the book on which modern “evolutionary economics” is built. Published in 1982, Nelson and Winter took the ideas expressed by Armen Alchian and Joseph Schumpeter decades earlier and presented a direct evolutionary challenge to mainstream approaches to economic growth, technological progress and competition between firms.\nNelson and Winter’s conception of firms is a collection of heterogeneous organisations guided by routines, the evolutionary economic equivalent of genes. Firms search for innovative (or imitative) solution to improve their profits, with successful firms growing at the expense of the less successful. The process is fundamentally dynamic, as firms interact and create the relative competitive environment that each faces. That firms may not be able to find the best technological solutions, nor seek to optimise profit perfectly, further separates the evolutionary and orthodox approaches.\nWhen Nelson and Winter describe their approach as evolutionary, it is not necessarily “evolution by natural selection” in a strict Darwinian sense. The “evolutionary” label relates to the focus on dynamic change, and even though they make suggestions such as viewing routines as genes, they do not seek to pin their approach precisely to the biology (Geoffrey Hodgson and Thorbjørn Knudsen, who advocate a Darwinian approach, seek to increase this precision). I don’t necessarily think this is problematic, as in a transparently specified model we can see how the dynamics work, be they Darwinian or not. Issues generally arise later when people generalise those results verbally, with a lack of precision then causing confusion.\nThere are a lot of things to like about this book. First, it is an example of a criticism of economics done right (although in some parts the criticism is a bit dated, in others as current as it ever was). Where Nelson and Winter have a specific criticism, they identify the mainstream economic approach, note the outcomes of that approach, undertake the analysis with their own approach and then show how the approaches produce different results. They then argue why their approach is superior. Whether you buy their arguments or not, its transparent and leads to a productive discussion.\nFor example, early in the book they take on Milton Friedman’s claim in “The Methodology of Positive Economics” in which Friedman states:\n\nLet the apparent immediate determinant of business behavior be any­ thing at all - habitual reaction, random chance or what not. When­ ever this determinant happens to lead to behavior consistent with rational and informed maximization of returns, the business will prosper and acquire resources with which to expand; whenever it does not the business will tend to lose resources and can be kept in existence only by the addition of resources from outside. The process of natural selection helps to validate the hypothesis [of maximization of returns] - or, rather, given natural selection, acceptance of the hypothesis can be based largely on the judgment that it sum­marizes appropriately the conditions for survival\n\nTo test this, Nelson and Winter create a basic model of firm interaction, analyse it using an orthodox equilibrium approach as Friedman suggests can be done, and then examine it as a dynamic selection process between firms. They show that the orthodox and selection equilibriums do not correspond with each other, as in the selection approach nonoptimal rules may survive in equilibrium due to path dependency, novel environments may be created as the mix of firms changes (which changes what the optimal rules are, effectively creating an evolutionary mismatch), and firms may fail to discover the optimal rules.\nAnother part of the book I enjoyed was their analysis of skills. In the past, I have been critical of the level at which some evolutionary economic analysis occurs - this being generally at the firm level and not the level of the people within the firm (or even deeper, their genes). But in the chapter on skills, Nelson and Winter spend a lot of time asking how the people in a firm shape the behaviour of that firm. In later chapters and models, the limitations of firms in searching for new technologies or attempting to achieve their objectives are informed by this more atomistic analysis of the people in the firm. Nelson and Winter’s focus on the level of firms is a conscious choice of the most useful level of analysis rather than ignorance that the lower levels might matter.\nTheir discussion of the competition policy implications of some of their models was also interesting. Depending on the assumptions used, monopolies may drive faster technological progress and deliver greater benefits to consumers as they are able to capitalise on any innovations through their large market shares. This is particularly the case if imitation is easy, with imitators eroding the benefits that an innovator can obtain in the market. Conversely, a monopoly might then be the only source of innovation, which is problematic if it satisfices or has other decision rules that limit its investment in innovation when times are good. While producing at times ambiguous results, an evolutionary analysis could form the base for a decent critique of competition policy.\nI’d read parts of the book before, but this was my first time reading it right through. For someone broadly interested in the topic but lacking a desire to play with specific models, I would recommend reading the first five and last two chapters. There is a lot of great material in it. The rest of the book is also useful, although harder work for the rewards delivered.\nAs an endnote, it is interesting how little effect this book has had on mainstream economics, despite the massive influence of this book in evolutionary economic (and to a lesser extent organisational theory) circles. I’ve posted about why this might be the case here."
  },
  {
    "objectID": "posts/new-books-on-the-evolution-of-cooperation.html",
    "href": "posts/new-books-on-the-evolution-of-cooperation.html",
    "title": "New books on the evolution of cooperation",
    "section": "",
    "text": "Diane Coyle has noted the release of three new books on the evolution of cooperation: Wired for Culture by Mark Pagel, Beyond Human Nature by Jesse Prinz and Together by Richard Sennett. Each was reviewed by Robin McKie in the Guardian\nA quick glance at some reviews, such as this by Julian Baggini, suggests that evolution is at the heart of Pagel’s discussion of the development of culture, with a fundamental question being how does human culture affect transmission of genes. In a review in the Telegraph, Tom Chivers pitches Pagel’s book as a response to Dawkins and a corollary to Steven Pinker:\n\n[T]he book could be read as a corollary to Steven Pinker, whose ironically titled The Blank Slate showed that the human mind was anything but – that learning is hugely directed by genes. Pagel says that while that is true of many things, such as language, when it comes to culture, our minds are indeed blank at birth – a Polynesian baby adopted by Westerners would grow up as a Westerner, without any yearning to build an outrigger canoe.\n\nWhile I am not convinced of the outrigger example – replace outrigger with car and there will be some strong similarities between who in the two cultures are interested – the book definitely seems worth a read.\nI am not as keen to read the book by Prinz. McKie quotes Prinz as saying that:\n\nThe vast majority of human behaviours – from pub fights to mental illness – vary in form and frequency from culture to culture. “Our actions are not ingrained,” he states.\n\nThis excerpt from the Amazon review is of a similar vein:\n\nIn this provocative, revelatory tour de force, Jesse Prinz reveals how the cultures we live in - not biology - determine how we think and feel. … He is not interested in finding universal laws but, rather, in understanding, explaining and celebrating our differences.\n\nThere is a case to be made for the power of situation and culture, as shown in Zimbardo’s The Lucifer Effect or evidenced through the decline in violence argued by Pinker, but is there a human culture where pub fights (or their equivalents) are not predominantly the domain of young males? Can any focus on the differences completely ignore the universals?\nI also came across the following review in The Age by Nick Miller. Miller interviewed Geoffrey Miller for some context:\n\n[G]enetic differences account for about 50 per cent of the difference in general intelligence between children. This actually increases to 80 per cent by old age, Miller says, because higher intelligence tends to reinforce itself by driving inquiry but low intelligence will lead us to “watch reality TV, not read books and go to NASCAR races”. …\n“I think the important thing is for parents to recognise that they aren’t at fault [if they happen to] raise a psychopath - though they might have been at fault in choosing the wrong mate,” Miller says. But should this inspire despair? Doesn’t it mean we can’t improve our kids, that the die is set?\n“If you’re single, it’s great news,” he says. You just need to choose the right mate. It also means parents can relax: “You feed them and they will grow … you’re not going to be able to change their basic intelligence or personality very much.”\nIt even has implications for the education system. “I think the US is wasting hundreds of billions of dollars a year trying to give university educations to young people who can’t actually handle university,” Miller says.\n\nThose are two very Bryan Caplan like arguments (with which I agree).\nPrinz responds:\n\n“I think the claims being made are often irresponsible and dangerously so,” he says. “If you really thought there were biologically based differences in intelligence, the thing you should promote is IQ enhancement through education, because this would equalise the differences.”\n\nPrinz goes on to attack twin studies as a tool to assess heritability and argues, in contrast to the evidence with which I am familiar, that parents have a huge effect on outcomes:\n\n“Parents are, in the early years of life, the most important factor in determining intelligence outcomes, because they have so much control over a child’s environment,” Prinz says. “Parents [should] give their children challenging problems, convey an excitement about learning by modelling that enthusiasm, put a child in contact with materials of instruction that are stimulating.”\n\nI try to read a good dose of books that run counter to my understanding of issues, but like to choose the best available. Is Beyond Human Nature worth the effort?"
  },
  {
    "objectID": "posts/nick-chaters-the-mind-is-flat-the-illusion-of-mental-depth-and-the-improvised-mind.html",
    "href": "posts/nick-chaters-the-mind-is-flat-the-illusion-of-mental-depth-and-the-improvised-mind.html",
    "title": "Nick Chater’s The Mind is Flat: The Illusion of Mental Depth and the Improvised Mind",
    "section": "",
    "text": "Nick Chater’s The Mind is Flat: The Illusion of Mental Depth and the Improvised Mind is a great book.\nChater’s basic argument is that there are no ‘hidden depths’ to our minds. The idea that we have an inner mental world with beliefs, motives and fears is just a work of imagination. As Chater puts it:\nThe book represents Chater’s reluctant acceptance that much experimental psychological data can no longer be accommodated by simply extending and modifying existing theories of reasoning and decision making. These theories are built on an intuitive conception of the mind, in which our thoughts and behaviour are rooted in reasoning and built on our deeply held beliefs and desires. As Chater argues, this intuitive conception is simply an illusion. This leads him to take his somewhat radical departure from many theories of perception, reasoning and decision making,\nI have one major disagreement with the book, which turns out to be a fundamental disagreement with Chater’s central claim, but I’ll come to that later."
  },
  {
    "objectID": "posts/nick-chaters-the-mind-is-flat-the-illusion-of-mental-depth-and-the-improvised-mind.html#the-visual-illusion",
    "href": "posts/nick-chaters-the-mind-is-flat-the-illusion-of-mental-depth-and-the-improvised-mind.html#the-visual-illusion",
    "title": "Nick Chater’s The Mind is Flat: The Illusion of Mental Depth and the Improvised Mind",
    "section": "The visual illusion",
    "text": "The visual illusion\nChater starts by examining visual perception. This is in part because visual perception is a (relatively) well understood area of psychology and neuroscience, and in part because Chater sees the whole of thought as being an extension of perception.\nConsider our sense of colour vision. The sensitivity of colour vision falls rapidly outside of the fovea, the area of the retina responsible for our sharp central vision. The rod cells that capture most of our visual field only able to capture light and dark. This means that outside of a few degrees of where you are looking, you are effectively colour blind. Despite this, we feel that our entire visual world is coloured. That is an illusion.\nSimilarly, our visual periphery is fuzzy. Our visual acuity plunges in line with decreasing cone density with the increase in angle. Yet, again, we have a sense that we can capture the entire scene before us.\nThat limited vision is highlighted in experiments using gaze-contingent eye-tracking. In one experiment, participants are asked to read lines of text. Rather than showing the full text, the computer only displayed a window of text where the experimental participants were looking, with all letters outside of that window replaced by blocks of ’x’s.\nWhen someone is reading this text, they feel they are looking at a page or screen full of text. How small can the window of text be before this illusion is shattered? It turns out, the window can be shrunk to around 10 to 15 characters (centred slight right of the fixation point) without the reader sensing anything is amiss. This is despite the page being almost completely covered in ’x’s. The sense that they are looking at a full page of text is an illusion, as most of the text isn’t there.\nChater walks through a range of other interesting experiments showing similar points. For instance, we can only encode one colour or shape or object at a time. The idea we are looking at a rich coloured world, taking in all of the colours and shapes at one, is also an illusion.\nOur brain is not simultaneously grasping a whole, but is rather piecing together a stream of information. Yet we are fooled into believing we are having a rich sensory experience. We don’t actually see a broad, rich multi-coloured world. The sense that we do is a hoax.\nSo show can the mind execute this hoax? Chater suggests the answer is simply because as soon as we wonder about any aspect of the world, we can simply flick our eyes over and instantly provide an answer. The fluency of this process suggests to us that we already had the answers stored, but the experimental and physiological evidence suggests this cannot be the case.\nPut another way, the sense of a rich sensory world is actually just the potential to explore a rich sensory world. This potential is misinterpreted as actually experiencing that world.\nAn interesting question posed by Chater later in the book is why don’t we have any awareness of the brain’s mode of thought. Why don’t we sense the continually flickering snapshots generated by our visual system? His answer is that the brain’s goal is to inform us of the world around us. It is not to inform us about the working of our own mechanisms to understand it."
  },
  {
    "objectID": "posts/nick-chaters-the-mind-is-flat-the-illusion-of-mental-depth-and-the-improvised-mind.html#the-inner-world",
    "href": "posts/nick-chaters-the-mind-is-flat-the-illusion-of-mental-depth-and-the-improvised-mind.html#the-inner-world",
    "title": "Nick Chater’s The Mind is Flat: The Illusion of Mental Depth and the Improvised Mind",
    "section": "The inner world",
    "text": "The inner world\nSo does story change when we move from visual perception to our inner thoughts?\nCharter asks us to think of a tiger as clearly and distinctly as we can. Consider the pattern of stripes on the tiger. Count them. What way do they flow over the body? Along the length or vertically? What about on the legs?\nVisually, we can only grasp fragments at a time, but each visual feature is available on demand, giving the impression that our vision encompasses the whole scene. A similar dynamic is at work for the imaginary tiger. Here the mind improvises the answer as soon as you ask for it. Until you ask the question, those details are entirely absent.\nWhat happens when you compare your answer about the tiger’s stripes with a real tiger? For the real tiger, the front legs don’t have stripes. At the back legs the stripes rotate from horizontal around the leg to vertical around the body. The belly and inner legs are white. Were they part of the image in your mind?\nAs we considered the tiger, we invented the answers to the questions we asked. What appeared to be a coherent image was constructed on the fly in the same way our system of visual perception gives us answers as we need them.\nIn one chapter, Chater also argues that we invent our feelings. He describes experimental participants dosed with either adrenaline or a placebo and then placed in a waiting room with a stooge. The stooge was either manic (flying paper aeroplanes) or angry (reacting to a questionnaire they had to fill in while waiting). Those who had been adrenalised had stronger reactions to both stooges, but in opposite directions: euphoric with the manic stooge and irritated in the presence of the angry stooge. Chater argues that we interpret our emotions in the moment based on both the situation we are in and our own physiological state. By being an act of interpretation, having an emotion is an act of reasoning."
  },
  {
    "objectID": "posts/nick-chaters-the-mind-is-flat-the-illusion-of-mental-depth-and-the-improvised-mind.html#improvising-our-preferences-and-beliefs",
    "href": "posts/nick-chaters-the-mind-is-flat-the-illusion-of-mental-depth-and-the-improvised-mind.html#improvising-our-preferences-and-beliefs",
    "title": "Nick Chater’s The Mind is Flat: The Illusion of Mental Depth and the Improvised Mind",
    "section": "Improvising our preferences and beliefs",
    "text": "Improvising our preferences and beliefs\nThe core of Chater’s argument comes when he turns to our preferences and beliefs. And here he argues that we are still relentless improvisers.\nThe famous split brain research of Michael Gazzaniga provides evidence for the improvisation. A treatment for severe epilepsy is surgical severance of the corpus callosum that links the two hemispheres of the brain. This procedure prevents seizures from spreading from one hemisphere to the other, but also results in the two halves of the cortex functioning independently.\nWhat if you show different images to the right and left halves of the visual field, which are processed in the opposite hemispheres of the brain (the crossover wiring to the brain means that the right hemisphere processes information in the left visual field, and vice versa)? In one experiment Gazzaniga showed two images to a split brain patient, P.S. On the left hand side was a picture of a snowy scene. On the right was a picture of a chicken’s foot. P.S., like most of us, had his language abilities focused in the left hemisphere of the brain, so P.S. could report seeing the chicken foot but was unable to say anything about the snowy scene.\nP.S. was asked to pick one of four pictures associated with each of the images. The right hand, controlled by the left hemisphere, picked a chicken head to match the claw. The left hand picked out a shovel for the snow. And how did P.S. explain the choice of the shovel? ‘Oh that’s simple. The chicken claw goes with the chicken. And you need a shovel to clean out the chicken shed.’ An invented explanation. With no insight into the reason, the left hemisphere invents the explanation.\nThis fluent explanation by split brain patients presents the possibility that after-the-fact explanation might also be the case for people with normal brains. Rather than explanations expressing inner preferences and beliefs, we make up reasons in retrospect to interpret our actions.\nChater proceeds to build his case that we don’t have such inner beliefs and preferences with some of the less convincing research in the book, much of which looks and feels like a lot of what has been questioned during the replication crisis. It is interesting all the same.\nIn one experiment, voters in Sweden were asked whether they intended to vote for the left or right-leaning coalition. They were then given a questionnaire on various campaign topics. When the responses were handed to the experimenter, the experimenter changed some of the responses by a slight of hand. When they were handed back for checking, just under a quarter of voters spotted and corrected the error. But the majority were happy to explain political opinions that moments ago they did not hold.\nChater also reports an experiment where the experimenters got a similar effect when asking people which of two faces they prefer. When the face was switched before asking for the explanation, the fluent explanation still emerged.\nAn interesting twist to this experiment is when people who have been justified a choice of face they didn’t make are asked to choose again. These people tend to choose the face that they didn’t choose previously but were asked to justify. The explanation helped shape future decisions.\nA similar effect occurred in another experiment in which participants took a web-based survey on political attitudes, with half the participants presented with an American flag in corner of screen. The flag caused a shift in political attitudes. But more interestingly, this effect persisted eight months later.\nChater’s interpretation of this experiment is not that Republicans should cover everything with flags. Rather, if people are exposed to a flag at a moment when they are contemplating their political views, this will have a long-lasting effect from the ‘memory traces’ that are laid down at the time.\nWhen I read Chater’s summary of the experiment, my immediate reaction was that this was unlikely to replicate - and my reading of the original paper (PDF) firmed my view. And it turns out there was a replication of the first flag priming experiment in the Many Labs project - no effect. (My reaction to the paper might have been shaped by previously reading the Many Labs paper but not immediately recalling that this particular experiment was included.) So let’s scrub this experiment from the list of evidence in support. If there’s no immediate effect, it’s hard to make a case for an effect eight months later. (Chater should have noted this given the replication was published in 2014.)\nThis isn’t the only experiment reported by Chater with a failed replication in this section, although the other dates from after publication of the book. An experiment by Eldar Shafir that makes an appearance failed to replicate in Many Labs 2.\nOne other piece of evidence called on by Chater is the broad (and strong) evidence of the inconsistency of our risk preferences and how susceptible they are to the framing of the risk and the domain in which they are realised. Present the same gamble in a loss rather than a gain frame, and risk-seeking choices spike.\nBut putting these pieces together, I am not convinced Chater has made his case. The split brain experiments demonstrate our willingness to improvise explanations in the absence of any evidence. But this does not extend to an unequivocal case that we we don’t call on any “hidden depths” that are there. They are variable, but are they so variable that they have no deeper basis at all? Chater thinks so.\n\n[N]o amount of measuring and re-measuring is going to help. The problem with measuring risk preferences is not that measurement is difficult and inaccurate; it is that there are no risk preferences to measure – there is simply no answer to how, ‘deep down’, we wish to balance risk and reward. And, while we’re at it, the same goes for the way people trade off the present against the future; how altruistic we are and to whom; how far we display prejudice on gender or race, and so on.\n\nBut this brings me to my major disagreement with Chater. For all Chater’s sweeping statements about our lack of hidden depths, he didn’t spend much effort trying to find them. Rather, he took a lot of evidence on how manipulable we can be (which we certainly are to a degree) and our willingness to improvise explanations when we have no idea (more robust), and then turned this into a finding that there is no hidden depth.\nOne place Chater could have looked is behavioural genetics. The first law of behavioural genetics is that all behavioural traits are heritable. That is, a proportion of the variation in these characteristics between people are due to genetic variation. These traits include risk preferences, the way we trade off the past and the future, and political preferences. These are among the characteristics that Chater suggests have no hidden depth. If there is no hidden depth, why are identical twins (even raised part) so similar for these traits. Chater is likely right that when asked to explain why we took a certain risky preference we are likely to improvise an explanation with little connection to reality. We rarely point to our genes. But that does not mean the hidden depth is not there."
  },
  {
    "objectID": "posts/nick-chaters-the-mind-is-flat-the-illusion-of-mental-depth-and-the-improvised-mind.html#we-can-only-have-one-thought-at-a-time",
    "href": "posts/nick-chaters-the-mind-is-flat-the-illusion-of-mental-depth-and-the-improvised-mind.html#we-can-only-have-one-thought-at-a-time",
    "title": "Nick Chater’s The Mind is Flat: The Illusion of Mental Depth and the Improvised Mind",
    "section": "We can only have one thought at a time",
    "text": "We can only have one thought at a time\nOnce Chater has completed his argument about our lack of hidden depths, he turns to describing his version of how the mind actually works. And part of that answer is that the brain can only tackle one problem at a time.\nThis inability to take on multiple tasks comes from the way that our brain computes when facing a difficult problem. Computation in the brain occurs through cooperation across the brain, with coordinated neural activity occurring across whole networks or entire regions of the brain. This large cooperative activity between slow neurons means that a network can only work on one problem at a time. And the brain is close to one large network.\nChater turns this idea into an attack on the “myth of the unconscious”. This myth is the idea that our brain is working away in the background. If we step away from a problem, we might suddenly have the answer pop into our head as our unconscious has kept working at the problem while we tend to other things.\nChater argues that for all the stories about scientists suddenly having major breakthroughs in the shower, neuroscience has found no evidence of these hidden processes. Chater summaries the studies in this area as concluding that, first, the effects of breaks either negligible or non-existent, and second, that the explanations for the minor effects of a break involve no unconscious thought at all.\nAs one example of the lack of effect, Chater describes an experiment in which subjects are asked to name both as many food items and as many countries as possible. Someone doing this task might switch back and forth between the two topics, changing to foods when they run out of countries and vice versa. How would the performance of a person able to switch back and forth compare to someone who has to first deal with one category, and only when finished move to the other? Would the former outperform as they could think about the second category in the background before coming back to it? The results suggest that when thinking about countries, there is no evidence that we are also thinking about food. When we switch from one category to the other, the search ceases abruptly.\nSo how did this myth of unconscious thought arise? Chater’s argument is that when we set a problem aside and return to it later, we are unencumbered by the past failures and patterns of thought in which we were trapped before. The new perspective may not be better than the old, but occasionally it will hit upon the angle that we need to solve the problem. So yes, the insight may emerge in a flash, but not because the unconscious had been grinding away at the problem.\nThis lack of unconscious thought is also demonstrated in the the literature concerning inattentional blindness. If people are busy attending to a task, they can miss information that they are not attending to. The classic example of this (at least, before the gorilla experiment) is an experiment by Ulric Neisser, in which participants are asked to watch three people throwing a ball to each other and press a button each time there was a throw. When an unexpected event occurs - in this case a woman with an umbrella walking through the players - less than one quarter of the participants noticed.\nChater takes the inattentional blindness studies as again showing that we can only lock onto and impose meaning on one fragment of sensory information at a time. If our brains are busy on one task, they can be utterly oblivious to other events.\nOne distinction Chater makes that I found useful is how to think about our unconscious thought processes. Chater’s argument is not that there is no processing in the brain outside our conscious knowledge. Rather, we have one type of thought, with unconscious processing resulting a a conscious result. Chater writes:\n\nThe division between the conscious and the unconscious does not distinguish between different types of thought. Instead, it is a division within individual thoughts themselves: between the conscious result of our thinking and the unconscious processes that create it.\nThere are no conscious thoughts and unconscious thoughts; and there are certainly no thoughts slipping in and out of consciousness. There is just one type of thought, and each such thought has two aspects: a conscious read-out, and unconscious processes generating the read-out."
  },
  {
    "objectID": "posts/nick-chaters-the-mind-is-flat-the-illusion-of-mental-depth-and-the-improvised-mind.html#so-where-do-our-actions-come-from",
    "href": "posts/nick-chaters-the-mind-is-flat-the-illusion-of-mental-depth-and-the-improvised-mind.html#so-where-do-our-actions-come-from",
    "title": "Nick Chater’s The Mind is Flat: The Illusion of Mental Depth and the Improvised Mind",
    "section": "So where do our actions come from?",
    "text": "So where do our actions come from?\nSo if there are no hidden depths, what drives us? Chater’s argument is that our thoughts come from memory traces created by previous thoughts and experiences. Each person is shaped by, and in effect unique due to, the uniqueness of their past thoughts and experiences. Thought follows channels carved by previous thoughts.\nThis argument does in some ways suggest that we have an inner-world. But that inner world is a record of the effect of the past cycles of thought. It is not an inner world of beliefs, hopes and fears. As Chater states, the brain operates based on precedents, not principles.\nChater’s first piece of evidence in support of this point comes from chess. What makes grandmasters special? It is not because humans are lightning calculating machines. Rather it is because of their long experience and their ability to find meaning in chess positions with great fluency. They can link the current position with memory traces of past board positions. They do not succeed by looking further ahead, but rather by drawing on a deeper memory bank and then focusing on only the best moves.\nChater argues that this is how perception works more generally. We do not interpret sensory information afresh, but interpret based on memory traces from past experience. He gives the example of “found faces”, where people see faces in inanimate objects. Our interpretation of the inputs finds resonance with memory traces of past inputs. Similarly, recognising a friend, word or tune depend on a link with your memories. Successful perception requires us to deploy the right memory traces when we need them.\nChater’s argument of the role of memory in perception seems sound. But absent the clear case that there there are no other sources of beliefs or motivations, I am not convinced these memory traces are all that there is."
  },
  {
    "objectID": "posts/nick-chaters-the-mind-is-flat-the-illusion-of-mental-depth-and-the-improvised-mind.html#what-this-means-for-intelligence-and-ai",
    "href": "posts/nick-chaters-the-mind-is-flat-the-illusion-of-mental-depth-and-the-improvised-mind.html#what-this-means-for-intelligence-and-ai",
    "title": "Nick Chater’s The Mind is Flat: The Illusion of Mental Depth and the Improvised Mind",
    "section": "What this means for intelligence and AI",
    "text": "What this means for intelligence and AI\nThe final chapter of the book is Chater’s attempt to put a positive gloss on his argument. It feels like the sort of chapter that the publisher might ask for to help with the promotion of the book.\nThat positive gloss is human creativity. Chater writes:\n\nBut the secret of human intelligence is the ability to find patterns in the least structured, most unexpected, hugely variable of streams of information – to lock onto a handbag and see a snarling face; to lock onto a set of black-and-white patches and discern a distinctive, emotion-laden, human being; to find mappings and metaphors through the complexity and chaos of the physical and psychological worlds. All this is far beyond the reach of modern artificial intelligence.\n\nI am not sure I agree. Vision recognition systems regularly make errors through seeing patterns that aren’t there. Are these just the machine version of seeing a face in a handbag? Both are mismatches, but one is labelled as an imaginative leap, the other as an error. Should we endow this overactive human pattern matching with the title of intelligence and call a similar matching errors when done by a computer a mistake? Chess is also instructive here, with a sign of a machine move now often being great creativity.\nThis final chapter is somewhat shallow relative to the rest of the book. Chater provides little in the way of evidence to support his case, although you can piece together some threads supporting Chater yourself from the examples discussed earlier in the book. It ends the book with a nice hook, but for me was a flat ending for an otherwise great book."
  },
  {
    "objectID": "posts/noise.html",
    "href": "posts/noise.html",
    "title": "Noise",
    "section": "",
    "text": "Daniel Kahneman has a new book in the pipeline called Noise. It is to be co-authored with Cass Sunstein and Olivier Sibony, and will focus on the “chance variability in human judgment”, the “noise” of the book’s title.\nI hope the book is more Kahneman than Sunstein. For all Thinking, Fast and Slow’s faults, it is a great book. You can see the thought that went into constructing it.\nSunstein’s recent books feel like research papers pulled together by a university student - which might not be too far from the truth given the fleet of research assistants at Sunstein’s command. Part of the flatness of Sunstein’s books might also come from his writing pace - he writes more than a book a year. (I count over 30 on his Wikipedia page since 2000, and 10 in the last five years.) Hopefully Kahneman will slow things down, although with a planned publication date of 2020, Noise will be a shorter project than Thinking, Fast and Slow.\nWhat is noise?\nKahneman has already written about noise, most prominently with three colleagues in Harvard Business Review. In that article they set out the case for examining noise in decision-making and how to address it.\nPart of that article is spent distinguishing noise from bias. Your bathroom scale is biased if it always reads four kilograms too heavy. If it gives you a different reading each time you get on the scale, it is noisy. Decisions can be noisy, biased, or both. A biased but low noise decision will always be wrong. A biased but high noise decision will be all over the shop but might occasionally get lucky.\nOne piece of evidence for noise in decision-making is the degree to which people will contradict their own prior judgments. Pathologists assessing biopsy results had a correlation of 0.63 with their own judgment of severity when shown the same case twice (the HBR article states 0.61, but I read the referenced article as stating 0.63). Software programmers differed by a median of 71% in the estimates for the same project, with a correlation of 0.7 between their first and second effort. The lack of consistency in decision-making only grows once you start looking across people.\nI find the concept of noise a useful way of thinking about decision-making. One of the main reasons why simple algorithms are typically superior to human decision makers is not because of bias or systematic errors by the humans, but rather the inconsistency of human judgment. We are often all over the place.\nNoise is also a good way of identifying those domains where arguments about the power of human intuition and decision-making (which I often make) fall down. Simple heuristics can make us smart. Developed in the right circumstances, naturalistic decision-making can lead to good decisions. But where human decisions are inconsistent, or noisy, it is often unchallenging to identify better alternatives.\nMeasuring noise\nOne useful feature of noise is that you can measure it without knowing the correct or best decision. If you don’t know your weight, it is hard to tell if the scale is biased. But the fact it differs in measurement as you get on, off, and on again points to the noise. If you have a decision for which there is a large lag before you know if it was the right one, this lag is an obstacle to measuring bias, but not for noise.\nThis ability to measure noise without knowing the right answer also avoids many of the debates about whether the human decisions are actually biased. Two inconsistent decisions cannot both be right.\nYou can measure noise in an organisation’s decision-making processes by examining pairs of decision makers and calculating the relative deviation of their judgments from each other. If one decision maker recommends, say, a price of $200, and the other of $400, the noise is 66%. (They were $200 apart, with the average of the two being $300. 200/300=0.66). You average this noise score across all possible pairs to give you the noise score for that decision.\nThe noise score has an intuitive meaning. It is the expected relative difference if you picked any two decision makers at random.\nIn the HBR article, Kahneman and colleagues report on the noise measurements for ten decisions in two financial services organisations. The noise was between 34% to 62% for the six decisions in organisation A, with an average noise of 48%. Noise was between 46% and 70% for the four decisions in organisation B, with an average noise of 60%. This was substantially above the organisations’ expectations. Experience of the decision makers did not appear to reduce noise.\nReducing noise\nThe main solution proposed by Kahneman and friends to reduce noise is replacing human judgement with algorithms. By returning the same decision every time, the algorithms are noise free.\nRather than suggesting a complex algorithm, Kahneman and friends propose what they call a “reasoned rule”. Here are the five steps in developing a reasoned rule, with loan application assessment an example:\n\nSelect six to eight variables that are distinct and obviously related to the predicted outcome. Assets and revenues (weighted positively) and liabilities (weighted negatively) would surely be included, along with a few other features of loan applications.\nTake the data from your set of cases (all the loan applications from the past year) and compute the mean and standard deviation of each variable in that set.\nFor every case in the set, compute a “standard score” for each variable: the difference between the value in the case and the mean of the whole set, divided by the standard deviation. With standard scores, all variables are expressed on the same scale and can be compared and averaged.\nCompute a “summary score” for each case―the average of its variables’ standard scores. This is the output of the reasoned rule. The same formula will be used for new cases, using the mean and standard deviation of the original set and updating periodically.\nOrder the cases in the set from high to low summary scores, and determine the appropriate actions for different ranges of scores. With loan applications, for instance, the actions might be “the top 10% of applicants will receive a discount” and “the bottom 30% will be turned down.”\n\nThe reliability of this reasoned rule - it returns the same outcome every time - gives it a large advantage over the human.\nI suspect that most lenders are already using more sophisticated models than this, but the strength of a simple approach was shown in Robyn Dawes’s classic article The Robust Beauty of Improper Linear Models in Decision Making (ungated pdf). You typically don’t need a “proper” linear model, such as that produced by regression, to outperform human judgement.\nAs a bonus, improper linear models, as they are less prone to overfitting, often perform well compared to proper models (as per Simple Heuristics That Make Us Smart). Fear of the expense of developing a complex algorithm is not an excuse to leave the human decisions alone.\nUltimately the development of the reasoned rule cannot avoid the question of what the right answer to the problem is. It will take time to determine definitively whether it outperforms. But if the human decision is noisy, there is an excellent chance that it will hit closer to the mark, on average, that the scattered human decisions."
  },
  {
    "objectID": "posts/not-so-irrational.html",
    "href": "posts/not-so-irrational.html",
    "title": "Not so irrational",
    "section": "",
    "text": "In Freeman Dyson’s interesting review of Daniel Kahneman’s Thinking, Fast and Slow, Dyson describes a couple of examples of the biases identified by Kahneman. One of them is as follows:\n\nThe endowment effect is our tendency to value an object more highly when we own it than when someone else owns it. …\nIn poor agrarian societies, such as Ireland in the nineteenth century or much of Africa today, the endowment effect works for evil because it perpetuates poverty. For the Irish landowner and the African village chief, possessions bring status and political power. They do not think like traders, because status and political power are more valuable than money. They will not trade their superior status for money, even when they are heavily in debt. The endowment effect keeps the peasants poor, and drives those of them who think like traders to emigrate.\n\nWhy would a landowner or chief give up their political power and status? There may be an endowment effect in that they value the power and status more when they have it, but if we think in evolutionary terms such as the desire for status and power, and the ability to attract mates, it would not be in the interest of any landowner or chief to step down.\nThis is typical of many uses of behavioural economics where the rationality underlying actions is skipped over in search of potential biases. The bigger picture is missed. People may use less electricity if they are provided real-time data on the use and price, but they will use far less if the price goes up. People eat burgers not because they don’t know they are full of calories, but because they taste good and are cheap. While there is potential for a small behavioural error or bias, the focus on the error masks the fundamentally rational action. And many times a bias is identified when the problem is that we don’t understand what the person’s objective is.\nAs an aside, earlier in the article, Dyson does hit on one of my favourite factoids:\n\nA striking example of availability bias is the fact that sharks save the lives of swimmers. Careful analysis of deaths in the ocean near San Diego shows that on average, the death of each swimmer killed by a shark saves the lives of ten others. Every time a swimmer is killed, the number of deaths by drowning goes down for a few years and then returns to the normal level. The effect occurs because reports of death by shark attack are remembered more vividly than reports of drownings."
  },
  {
    "objectID": "posts/notes-on-a-few-books.html",
    "href": "posts/notes-on-a-few-books.html",
    "title": "Notes on a few books",
    "section": "",
    "text": "The Advertising Effect: How to Change Behaviour by Adam Ferrier\nIf you’ve read a couple of behavioural economics/behavioural science books, it doesn’t take long to become bored with hearing the same experiments and examples over and over again.\nFerrier manages to largely avoid that problem. He works in advertising, so has plenty of new stories to tell, and it’s interesting to hear how advertisers go about their job (and desperately try to win the beer accounts). It also helps that Ferrier is a trained psych, so he brings a bit more psychology to the task than you typically see in the pop behavioural science literature.\nThat said, when The Advertising Effect does stray into those familiar studies, you start to run into the problem that many of them aren’t standing the test of time particularly well (power posing being one example).\nDigital Gold: Bitcoin and the Inside Story of the Misfits and Millionaires Trying to Reinvent Money by Nathaniel Popper\nEven though this book is less than a year old, it already feels like it is missing a chapter or two at the end. Still, it’s an easy and entertaining history of Bitcoin.\nMine-Field: The Dark Side of Australia’s Resources Rush by Paul Cleary\nAs Cleary notes, “regulation is more focused on flora and fauna than on the people affected by mining end energy developments.”\nRadical Chic and Mau-Mauing the Flak Catchers by Tom Wolfe\nNew York society throws a party to raise funds for the Black Panthers. Many great passages - here’s one instance:\n\nOne rule is that nostalgie de la boue – i.e., the styles of romantic, raw-vital, Low Rent primitives – are good; and middle class, whether black or white, is bad. Therefore, Radical Chic invariably favors radicals who seem primitive, exotic and romantic, such as the grape workers, who are not merely radical and ‘of the soil,’ but also Latin; the Panthers, with their leather pieces, Afros, shades, and shoot-outs; and the Red Indians, who, of course, had always seemed primitive, exotic and romantic. …\nRule No. 2 was that no matter what, one should always maintain a proper address, a proper scale of interior decoration, and servants. Servants, especially, were one of the last absolute dividing lines between those truly “in Society,” New or Old, and the great scuffling mass of middle-class strivers paying up to $1,250-a-month rent or buying expensive co-ops all over the East Side. …\nIn the era of Radical Chic, then, what a collision course was set between the absolute need for servants—and the fact that the servant was the absolute symbol of what the new movements, black or brown, were struggling against! How absolutely urgent, then, became the search for the only way out: white servants!\n\nCrime and Punishment by Fyodor Dostoevsky (Richard Pevear and Larissa Volokhonsky translation)\nAnother classic well worth reading."
  },
  {
    "objectID": "posts/nudging-citizens-to-be-risk-savvy.html",
    "href": "posts/nudging-citizens-to-be-risk-savvy.html",
    "title": "Gerd Gigerenzer’s Risk Savvy: How to Make Good Decisions",
    "section": "",
    "text": "I should start this review of Gerd Gigerenzer’s least satisfactory but still interesting book, Risk Savvy: How to Make Good Decisions, by saying that I am a huge Gigerenzer fan and that this book is still worth reading. But there was something about this book that grated at times, especially against the backdrop of his other fantastic work.\nIn part, I continue to be perplexed by Gigerenzer’s ongoing war against nudges (as I have posted about before), despite his recommendations falling into the nudge category themselves. Nudges are all about presenting information and choices in different ways - which is the staple of Gigerenzer’s proposal to make citizens “risk savvy”. Gigerenzer’s use of evidence and examples throughout the book also fall well short of his other work, and this is ultimately the element of the book that left me somewhat disappointed.\nThe need to make citizens risk savvy comes from Gigerenzer’s observation (which matches that of most of Gigerenzer’s faux adversaries - the behavioural scientists) that people misinterpret risks when they are presented in certain ways. If I say that screening reduces the risk of dying from breast cancer by 20 per cent, most people will interpret it to mean that 200 of every 1,000 people will be saved, rather than understanding that it means screening reduces the risk of death from 6 in 1,000 to 5 in 1,000 - effectively saving one out of 1,000.\nGigerenzer’s contribution to this area is to show that if presented in natural frequencies (i.e. tell people about the statistics as proportions of, say, 1,000 people), people are better able to understand the actual risks. This includes doctors, who are equally confused by statistics as everyone else, and who Gigerenzer suggests need training to communicate risks in ways that their patients can understand.\nThis ability to make citizens and experts risk savvy leads Gigerenzer to argue that people do not always need to be at the mercy of their biases. People can be educated to understand risks and experts can present them in ways that others understand. He advocates risk literacy programs in school, showing that simple decision tools can dramatically increase understanding of probability and statistics, although he spends little time discussing how well this education sticks. In making his point, Gigerenzer takes aim at the behavioural science crowd by claiming that natural frequencies wouldn’t have helped if people are subject to cognitive illusions - a strawman argument. As he does at semi-regular intervals through the book, Gigerenzer clouds an interesting argument with an attempt to engage in a battle that doesn’t really exist.\nThat said, I did enjoy this part of the book and have found myself quoting a lot of the examples. His arguments about how to present risk are compelling. Further, it is enjoyable to read Gigerenzer’s evisceration of the presentation of risk by various high-profile cancer organisations.\nThere are parts of the book where Gigerenzer is more pessimistic about the ability to educate the masses, such as when he channels Nassim Taleb and berates the finance industry for not understanding the difference between risk and uncertainty. In a world of uncertainty – where we do not know the probability of events – simple rules often outperform more complex models that are overfitted to past data. This provides a natural entry point to Gigerenzer’s well-established work (and subject of some of his better books) on the accuracy of heuristics. Risk Savvy has plenty of additional advocacy for their use with Gigerenzer arguing that we can be trained to use useful heuristics in making better decisions. Gigerenzer covers areas from marriage (set your aspiration level and choose the first person who meets it) to business to the stability of financial institutions, building on decades of evidence he has accumulated on the accuracy of simple rules.\nGigerenzer’s heuristics don’t always match up with his optimism that we can make people risk savvy. One heuristic he suggests is: “If reason conflicts with a strong emotion, don’t try to argue. Enlist a conflicting and stronger emotion.” He also recognises the limits to education, with heuristics such as “don’t buy financial products you don’t understand.” But given that a lot of people don’t understand compound interest, we might need to rely on the Dunning-Kruger effect to allow people to follow this rule and still make any investments.\nOne interesting point made by Gigerenzer is that there is still a role for experts (and even consultants) in a world where we use simple heuristics. Suppose we replace our complex asset allocation models with a 1/N rule - allocate our assets equally across N choices. This still leaves questions such as the size of N, what we will include in N, or when you should rebalance. For many heuristics, there may be more complex underlying choices - although I imagine heuristics could be developed for many of these too.\nGigerenzer is also a stout defender of gut instinct - again, as covered in his other books. Gigerenzer suggests (and I agree) that data is often gathered due to a culture of defensive decision-making and not because data is the major reason in the decision. This is, however, the weakest area of the book, as Gigerenzer’s stories reek of survivorship bias. Gigerenzer notes that leading figures in business reveal in surveys that they rely on gut instinct and not data in making major decisions. But how many corpses who relied on gut instinct are strewn along the road of entrepreneurship?\nAs another example, Gigerenzer talks of a corporate headhunter who had put a thousand senior managers and CEOs into their positions. The headhunter said that nearly all the time he based his selection on a gut decision. He was now being replaced by tests by psychologists. Gigerenzer puts this down to a negative error culture, with the procedures designed to protect the decision makers. But what is the evidence that the headhunter has been good at their job and could outperform the psychologists armed with tests?  Similarly, Gigerenzer suggests listening to those with good track records in business. Again, survivorship bias could make this a useless exercise. When talking of predictions of exchange rates in other parts of the book, Gigerenzer effectively makes this very same point - the successful people you see in front of you could simply be the lucky survivors.\nHowever, the evidence that Gigerenzer has developed in the past would make it folly for anyone in business to throw gut instinct out the window - or to completely discard Gigerenzer’s arguments. But the way he makes the case through Risk Savvy feels built on anecdote and weak examples.\nThere is one rule I am going to take away from the book - an extension of my usual habit of flipping a coin for decisions about which I’m indifferent. Gigerenzer suggests flipping a coin and as it spins, considering what side you don’t want to come up. He used this example in the context of choosing a partner, but it’s not a bad way to elicit that gut instinct that you can’t otherwise hear."
  },
  {
    "objectID": "posts/o-ring-and-foolproof-sectors.html",
    "href": "posts/o-ring-and-foolproof-sectors.html",
    "title": "O-ring and foolproof sectors",
    "section": "",
    "text": "In my last post, I described Kremer’s O-ring theory of economic development. Kremer’s insight was that if production in an economy consists of many discrete tasks and failure in any one of those tasks can ruin the final output, small differences in skills can drive large differences in output between firms. This can lead to high levels of inequality as the high-skilled work together and are disproportionately more productive.\nIn a new paper in the Journal of Economic Behavior and Organization (although kicking around as a working paper for a few years), Garett Jones tweaks Kremer’s model to capture the observation that measures of worker skill are a stronger predictor of cross-country economic performance than of within-country differences in income.\nJones pictures an economy that comprises two sectors: an O-ring sector of the type described by Kremer; and what Jones calls a foolproof sector. The foolproof sector is not as fragile as the more complex O-ring sector and includes jobs such as cleaning, gardening and other low-skill occupations. The key feature of the foolproof sector is that being of low skill (which Jones suggests relates more to IQ than quantity of education) does not necessarily destroy the final product. It only reduces the efficiency with which it is produced. A couple of low-skill workers can substitute for a high-skill worker in the foolproof sector, but they cannot effectively fill the place of a high-skill O-ring sector worker, no matter how many low-skill workers are supplied.\nIn this economy, low-skill workers will work in the foolproof sector as these firms will pay them more than an O-ring sector firm. High-skill workers are found in both sectors, with their level of participation in each sector such that high-skill workers are paid the same regardless of which sector they work in (the law of one price).\nThus, within a country, firms will pay high-skill workers more than their low-skill counterparts, but not dramatically so. Their wage differential is determined by the difference in their outputs in the foolproof sector.\nAcross countries, however, things are considerably different. The highest skill workers in a country provide labour for the O-Ring sector. If they are low skilled relative to the high-skilled in other countries, their output in that fragile sector will be much lower. This occurs even for relatively small skill differences. Their income will reflect their low output, with wages also lower in the foolproof sector as high-skill workers apportion themselves between sectors such that the law of one price holds. The net result is much lower wages for workers in comparison to another country with a higher-skill elite.\nThis picture does depend on the relative proportions of the low and high skilled in the population. If there are too many low skilled people, the wage differences within a country may be greater as all the high-skilled will work solely in the O-ring sector and wages will not equalise. Some of the low skilled may even engage in a lower output O-ring sector. However, the general picture of a large gap in income between countries remains.\nThere are a couple of interesting things about this model. First, the low skilled can be productive. An additional low skilled person, such as a low-skill immigrant, does not drag everyone down or necessarily destroy a production process. The productivity of those in the O-ring sector remains unchanged. This is somewhat in line with the traditional economic story that everyone can provide value through their comparative advantage. That picture is, of course, incomplete. As Jones points out in other work, there may be other costs to the unskilled, such as lower cooperation or patience and political costs. But this model suggests is that we should look to the quality of the high skilled in the population, and worry less about whether there are too many at the bottom end.\nThe other point worth noting about Jones’s paper, as for Kremer’s, is that small differences matter. Large income differences are not evidence of large skill differences (although, obviously, they are also not evidence against). Conversely, small changes in skills can have large consequences for a nation’s wealth."
  },
  {
    "objectID": "posts/observations-on-happiness-biases-and-preferences.html",
    "href": "posts/observations-on-happiness-biases-and-preferences.html",
    "title": "Observations on happiness, biases and preferences",
    "section": "",
    "text": "This year’s Australian Conference of Economists had a few behavioural/experimental economists among the invited speaker list. This post is a short record of some of the main things I took away from their presentations (which is not necessarily the focus of the presentation).\nFrom Andrew Clarke: Ignore cross-country comparisons of happiness. The word happiness (or whatever term is intended to capture it) is ambiguous enough in survey questions without the added complications of language translations and cross-cultural interpretation.\nFrom Glenn Harrison: Don’t rest on the work done by Kahneman and other behavioural psychology pioneers. Go out and test these biases to ensure that they hold up in incentivised environments. Don’t gives biases a name until they have a theoretical basis - otherwise we are giving too much weight to poorly tested and considered observations. Do decent, robust econometric analysis (some of the examples Harrison gave of work published in high-ranking journals was embarrassing).\nFrom Graham Loomes: Considering preferences as probabilistic and not deterministic (that is, “I will choose the first alternative X per cent of the time”) can shed light on some observations such as preference reversals.\nFrom Robert Sugden (author of one of my favourite papers): Behavioural economists have taken on every challenge thrown down by those who believe in rational choice. Almost every “rational” explanation for these biases has tested and they almost never explain the anomaly, even if they are partially “true”. For example, using the income effect to explain differences between willingness-to-pay and willingness-to-accept explains only a fraction of the observed disparity (this was more from a panel session than Sugden’s main presentation)."
  },
  {
    "objectID": "posts/ongoing-selection-against-violent-behaviour.html",
    "href": "posts/ongoing-selection-against-violent-behaviour.html",
    "title": "Ongoing selection against violent behaviour",
    "section": "",
    "text": "From Mark Pagel, author of Wired for Culture: Origins of the Human Social Mind, in a RSA podcast:\n\nCultural evolution and genetic evolution are still going on. We’re still evolving. Our societies are strongly selecting against violence and antisocial behaviour at a genetic level. People who knock you over the head and steal your wallet get thrown in jail, and it’s hard to have reproductive success in jail. And some societies even kill people who do those things, and so we are still selecting against antisocial behaviour very strongly in our societies. And so we’re still evolving. There’s hope."
  },
  {
    "objectID": "posts/opposing-biases.html",
    "href": "posts/opposing-biases.html",
    "title": "Opposing biases",
    "section": "",
    "text": "From the preface of one print of Philip Tetlock’s Expert Political Judgement (hat tip to Robert Wiblin who quoted this passagein the introduction to an 80,000 hours podcast episode):\n\nThe experts surest of their big-picture grasp of the deep drivers of history, the Isaiah Berlin–style “hedgehogs,” performed worse than their more diffident colleagues, or “foxes,” who stuck closer to the data at hand and saw merit in clashing schools of thought. That differential was particularly pronounced for long-range forecasts inside experts’ domains of expertise.\n…\nHedgehogs were not always the worst forecasters. Tempting though it is to mock their belief-system defenses for their often too-bold forecasts—like “off-on-timing” (the outcome I predicted hasn’t happened yet, but it will) or the close-call counterfactual (the outcome I predicted would have happened but for a fluky exogenous shock)—some of these defenses proved quite defensible. And, though less opinionated, foxes were not always the best forecasters. Some were so open to alternative scenarios (in chapter 7) that their probability estimates of exclusive and exhaustive sets of possible futures summed to well over 1.0. Good judgment requires balancing opposing biases. Over-confidence and belief perseverance may be the more common errors in human judgment but we set the stage for over-correction if we focus solely on these errors and ignore the mirror image mistakes, of under-confidence and excessive volatility.\n\nI can see why this idea of opposing biases makes correction of “biases” difficult.\nBut before we get to the correction of biases, this concept of opposing biases points at a major difficulty with behavioural analyses of decision making. When you have, say, both loss aversion and overconfidence in your bag of explanations for poor decision making, you can explain almost anything after the fact. The gamble turned out poorly? Overconfidence. Didn’t take the gamble? Loss aversion.\nRecently I’ve heard a lot of people talking of action bias. There is also a status quo bias. Again, a pair of biases with which we can explain anything."
  },
  {
    "objectID": "posts/our-visual-system-predicts-the-future.html",
    "href": "posts/our-visual-system-predicts-the-future.html",
    "title": "Our visual system predicts the future",
    "section": "",
    "text": "I am reading John Coates’s thus far excellent The Hour Between Dog and Wolf: How Risk Taking Transforms Us, Body and Mind. There are many highlights and interesting pieces, the below being one of them.\nFirst, we do not see in real-time:\n\nWhen light hits out retina, the photons must be translated into a chemical signal, and then into an electrical signal that can be carried along nerve fibers. The electrical signal must then travel to the very back of the brain, to an area called the visual cortex, and then project forward again, along two separate pathways, one processing the identity of the objects we see, the “what” stream, as some researchers call it, and the other processing the location and motion of the objects, the “where” stream. These streams must then combine to form a unified image, and only then does this stream emerge into conscious awareness. The whole process is a surprisingly slow one, taking … up to one tenth of a second. Such a delay, though brief, leaves us constantly one step behind events.\n\nSo how does our body deal with this problem? How could you catch a ball or dodge a projectile if your vision is behind time?\n\n[T]he brains visual circuits have devised an ingenious way of helping us. The brain anticipates the actual location of the object, and moves the visual image we end up seeing to this hypothetical new location. In other words, your visual system fast forwards what you see.\n\nVery cool concept, but how would you show this?\n\nNeuroscientists … have recorded the visual fast-forwarding by means of an experiment investigating what is called the “flash-lag effect.” In this experiment a person is shown an object, say a blue circle, with another circle inside it, a yellow one. The small yellow circle flashes on and off, so what you see is a blue circle with a yellow circle blinking inside it. Then the blue circle with the yellow one inside starts moving around your computer screen. What you should see is a moving blue circle with a blinking yellow one inside it. But you do not. Instead you see a blue circle moving around the screen with a blinking yellow circle trailing about a quarter of an inch behind it. What is going on is this: while the blue circle is moving, your brain advances the image to its anticipated actual location, given the one-tenth-of-a-second time lag between viewing it and being aware of it. But the yellow circle, blinking on and off, cannot be anticipated, so it is not advanced. It thus appears to be left behind by the fast-forwarded blue circle.\n\nA quick scan of the Wikipedia page on the flash-lag effect suggests there are a few competing explanations, but it’s an interesting idea all the same. It would explain that feeling of disbelief when a batter swings at and misses a ball that moves unexpectedly in the air. They would have seen it in precisely the place they swung.\nThe below video provides a visual illustration.\nhttp://youtu.be/X8RiaNUFIaU"
  },
  {
    "objectID": "posts/overconfident-about-overconfidence.html",
    "href": "posts/overconfident-about-overconfidence.html",
    "title": "Overconfident about overconfidence",
    "section": "",
    "text": "In 1995 Werner De Bondt and Richard Thaler wrote “Perhaps the most robust finding in the psychology of judgment and choice is that people are overconfident.” They are hardly been alone in making such a proclamation. And looking at the evidence, they seem to have a case. Take the following examples:\n\nWhen asked to estimate the length of the Nile by providing a range the respondent is 90% sure contains the correct answer, the estimate typically contains the correct answer only 50% of the time.\nPGA golfers typically believe they sink around 75% of 6 foot putts - some even believe they sink as many as 85% - when the average is closer to 55%.\n93% of American drivers rate themselves as better than average. 25% of high school seniors believe they are in the top 1% in ability to get along with others.\n\nThere is a mountain of similar examples, all seemingly making the case that people are generally overconfident. \nBut despite all being labelled as showing overconfidence, these examples are actually quite different. As pointed out by Don Moore and Paul Healy in “The Trouble with Overconfidence” (pdf), several different phenomena are being captured. Following Moore and Healy, let’s call them overprecision, overestimation and overplacement.\nOverprecision is the tendency to believe that our predictions or estimates are more accurate than they actually are. The typical study seeking to show overprecision asks for someone to give confidence ranges for their estimates, such as estimating the length of the Nile. The evidence that we are overprecise is relatively robust (although I have to admit I haven’t seen any tests asking for 10% confidence intervals).\nOverestimation is the belief that we can perform at a level beyond that which we realistically can (I tend to think of this as overoptimism). The evidence here is more mixed. When attempting a difficult task such as a six foot putt, we typically overestimate. But on easy tasks, the opposite is often the case - we tend underestimate our performance. Whether over or underestimation occurs depends upon the domain.\nOverplacement is the erroneous relative judgment that we are better than others. Obviously, we cannot all be better than average. But this relative judgment, like overestimation, tends to vary with task difficulty. For easy tasks, such as driving a car, we overplace and consider ourselves better than most. But as Phil Rosenzweig points out in his book Left Brain, Right Stuff (which contains a great summary of Moore and Healy’s paper), ask people where they rate for a skill such as drawing, and most people will rate themselves as below average. People don’t suffer from pervasive overplacement. Whether they overplace depends on what the situation is.\nYou might note from the above that we tend to both underestimate and overplace our performance on easy tasks. We can also overestimate but underplace our performance on difficult tasks.\nSo are we both underconfident and overconfident at the same time? The blanket term of overconfidence does little justice to what is actually occurring.\nMoore and Healy’s explanation for what is going on is these situations is that, after performing a task, we have imperfect information about our own performance, and even less perfect information about that of others. As Rosenzweig puts it, we are myopic, which is a better descriptor of what is going on than saying we are biased.\nConsider an easy task. We do well because it is easy. But because we imperfectly assess our performance, our assessment is regressive - that is, it tends to revert to the typical level of performance. Since we have even less information about others, our assessment of them is even more regressive. The net result is we believe we performed worse than we actually did but better than others.\nRosenzweig provides a couple of more intuitive examples of myopia at work. Taking one, we know about our excellent driving record and that there are plenty of people out there who die in car accidents. With a narrow view of that information, it seems logical to place yourself above average.\nBut when considering whether we are an above or below average juggler, the knowledge of our own ineptitude and the knowledge of the existence of excellent jugglers makes for a myopic assessment of being below average. In one example Rosenzweig cites, 94% of students believed they would be below average in a quiz on indigenous Amazon vegetation - hardly a tendency for overplacement, but rather the result of myopic consideration of the outcomes from a difficult task.\nThe conflation of these different effects under the umbrella of overconfidence often plays out in stories of how overconfidence (rarely assessed before the fact) led to someone’s fall. Evidence that people tend to believe they are better drivers than average (overplacement) is not evidence that overconfidence led someone to pursue a disastrous corporate merger (overestimation). Evidence that people tend to be overprecise in estimating the year of Mozart’s birth is not evidence that hubris led the US into the Bay of Pigs fiasco.\nPutting this together, the claims we are systematically overconfident can be somewhat overblown and misapplied. I am not sure Moore and Healy’s labelling is the best available, but recognising the differing forces are at play seems important in understanding how “overconfidence” affects our decisions."
  },
  {
    "objectID": "posts/parental-income-and-sat-scores.html",
    "href": "posts/parental-income-and-sat-scores.html",
    "title": "Parental income and SAT scores",
    "section": "",
    "text": "To make his point that socioeconomic status is a major driver of educational outcomes, Dan Pink made the following chart. SAT scores are on the vertical axis, and family incomes on the horizontal axis.\n\nSidestepping questions of what this correlation actually means, is there any plausible scenario that would result in a different relationship?\nIf we assume, as many are using the chart to assert, that parental income “buys” higher SAT scores, then we will see SAT scores rising with income. The same result would be expected if those at the bottom of the socioeconomic scale receive poor educations.\nHowever, if we assumed the world was a perfect meritocracy and everyone had the same opportunities, we only need mild assumptions about correlations between parent and child talent and personality to get the same result. Those correlations could be through genetic or cultural transmission. If children resemble their parents in any way, and income and SAT scores reflect those underlying dispositions, we will see a positive relationship.\nThe only way we would see no relationship between parental income and SAT scores would be if there was zero correlation between parent and child traits, and we lived in a perfect meritocracy. I don’t believe that anyone can substantiate either of those claims. The question is one of degree.\nSo, in some senses, the chart does not tell us anything, and I am surprised that anyone might have expected any other result."
  },
  {
    "objectID": "posts/paul-ormerod-on-thalers-misbehaving.html",
    "href": "posts/paul-ormerod-on-thalers-misbehaving.html",
    "title": "Paul Ormerod on Thaler’s Misbehaving",
    "section": "",
    "text": "I have been meaning to write some notes on Richard Thaler’s Misbehaving: The Making of Behavioral Economics for some time, but having now come across a review by Paul Ormerod (ungated pdf) - together with his perspective on the position of behavioural economics in the discipline - I feel somewhat less need. Below are some interesting sections of Ormerod’s review.\nFirst, on the incorporation of psychology into economics:\n\nWith a few notable exceptions, psychologists themselves have not engaged with the area. ‘Behavioral economics has turned out to be primarily a field in which economists read the work of psychologists and then go about their business of doing research independently’ (p. 179). One reason for this which Thaler gives is that few psychologists have any attachment to the rational choice model, so studying deviations from it is not interesting. Another is that ‘the study of “applied” problems in psychology has traditionally been considered a low status activity’ (p. 180).\nIt is fashionable in many social science circles to deride economics, and to imagine that if only these obstinate and ideological economists would import social science theories into the discipline, all would be well. All manner of things would be well, for somehow these theories would not only be scientifically superior, but their policy implications would lead to the disappearance of all sorts of evils, such as austerity and even neo-liberalism itself. This previous sentence deliberately invokes a caricature, but one which will be all too recognisable to economists in Anglo-Saxon universities who have dealings with their colleagues in the wider social sciences.\nA recent article in Science (Open Science Collaboration 2015) certainly calls into question whether psychology can perform this role of knight in shining armour. A team of no fewer than 270 co-authors attempted to replicate the results of 100 experiments published in leading psychology journals. … [O]nly 36 per cent of the attempted replications led to results which were statistically significant. Further, the average size of the effects found in the replicated studies was only half that reported in the original studies. …\nEither the original or the replication work could be flawed, or crucial differences between the two might be unappreciated. … So the strategy adopted by behavioural economists of choosing for themselves which bits of psychology to use seems eminently sensible.\n\nOn generalising behavioural economics:\n\nThe empirical results obtained in behavioural economics are very interesting and some, at least, seem to be well established. But the inherent indeterminacy discussed above is the main reason for unease with the area within mainstream economics. Alongside Misbehaving, any economist interested in behavioural economics should read the symposium on bounded rationality in the June 2013 edition of the Journal of Economic Literature. …\nIn a paper titled ‘Bounded-Rationality Models: Tasks to Become Intellectually Competitive’, Harstad and Selten make a key point that although models have been elaborated which incorporate insights of boundedly rational behaviour, ‘the collection of alternative models has made little headway supplanting the dominant paradigm’ (2013, p. 496). Crawford’s symposium paper notes that ‘in most settings, there is an enormous number of logically possible models… that deviate from neoclassical models. In attempting to improve upon neoclassical models, it is essential to have some principled way of choosing among alternatives’ (2013, p. 524). He continues further on the same page ‘to improve on a neoclassical model, one must identify systematic deviations; otherwise one would do better to stick with a noisier neoclassical model’.\nRabin is possibly the most sympathetic of the symposium authors, noting for example that ‘many of the ways humans are less than fully rational are not because the right answers are so complex. They are instead because the wrong answers are so enticing’ (2013, p. 529). Rabin does go on, however, to state that ‘care should be taken to investigate whether the new models improve insight on average… in my view, many new models and explanations for experimental findings look artificially good and artificially insightful in the very limited domain to which they are applied’ (2013, p. 536). …\n… Misbehaving does not deal nearly as well with the arguments that in many situations agents will learn to be rational. The arguments in the Journal of Economic Literature symposium both encompass and generalise this problem for behavioural economics. The authors accept without question that in many circumstances deviations from rationality are observed. However, no guidelines, no heuristics, are offered as to the circumstances in which systematic deviations might be expected, and circumstances where the rational model is still appropriate. Further, the theoretical models developed to explain some of the empirical findings in behavioural economics are very particular to the area of investigation, and do not readily permit generalisation.\n\nOn applying behavioural economics to policy:\n\nIn the final part (Part VIII) he discusses a modest number of examples where the insights of behavioural economics seem to have helped policymakers. He is at pains to point out that he is not trying to ‘replace markets with bureaucrats’ (p. 307). He discusses at some length the term he coined with Sunstein, ‘libertarian paternalism’. …\nWe might perhaps reflect on why it is necessary to invent this term at all. The aim of any democratic government is to improve the lot of the citizens who have elected it to power. A government may attempt to make life better for everyone, for the interest groups who voted for it, for the young, for the old, or for whatever division of the electorate which we care to name. But to do so, it has to implement policies that will lead to outcomes which are different from those which would otherwise have happened. They may succeed, they may fail. They may have unintended consequences, for good or for ill. By definition, government acts in paternalist ways. By the use of the word ‘libertarian’, Thaler could be seen as trying to distance himself from the world of the central planner.\n… And yet the suspicion remains that the central planning mind set lurks beneath the surface. On page 324, for example, Thaler writes that ‘in our increasingly complicated world, people cannot be expected to have the experience to make anything close to the optimal decisions in all the domains in which they are forced to choose’. The implication is that behavioural economics both knows what is optimal for people and can help them get closer to the optimum.\nFurther, we read that ‘[a] big picture question that begs for more thorough behavioral analysis is the best way to encourage people to start new businesses (especially those which might be successful)’ (p. 351). It is the phrase in brackets which is of interest. Very few people, we can readily conjecture, start new businesses in order for them to fail. But most new firms do exactly that. Failure rates are very high, especially in the first two or three years of life. How exactly would we know whether a start-up was likely to be successful? There is indeed a point from the so-called ‘Gauntlet’ of orthodox economics which is valid in this particular context. Anyone who had a good insight into which start-ups were likely to be successful would surely be extremely rich."
  },
  {
    "objectID": "posts/people-should-use-their-judgment-except-theyre-often-lousy-at-it.html",
    "href": "posts/people-should-use-their-judgment-except-theyre-often-lousy-at-it.html",
    "title": "People should use their judgment … except they’re often lousy at it",
    "section": "",
    "text": "My Behavioral Scientist article, Don’t Touch The Computer was in part a reaction to Andrew McAfee and Eric Brynjolfsson’s book The Second Machine Age: Work, Progress, and Prosperity in a Time of Brilliant Technologies. In particular, I felt their story of freestyle chess as an illustration of how humans and machines can work together was somewhat optimistic.\nI have just read McAfee and Brynjolfsson’s Machine, Platform, Crowd: Harnessing Our Digital Future. Chapter 2, titled The Hardest Thing to Accept About Ourselves, runs a line somewhat closer to mine. Here are some snippets:\n\n[L]et people develop and exercise their intuition and judgment in order to make smart decisions, while the computers take care of the math and record keeping. We’ve heard about and seen this division of labor between minds and machines so often that we call it the “standard partnership.”\nThe standard partnership is compelling, but sometimes it doesn’t work very well at all. Getting rid of human judgments altogether—even those from highly experienced and credentialed people—and relying solely on numbers plugged into formulas, often yields better results.\n\nHere’s one example:\n\nSociology professor Chris Snijders used 5,200 computer equipment purchases by Dutch companies to build a mathematical model predicting adherence to budget, timeliness of delivery, and buyer satisfaction with each transaction. He then used this model to predict these outcomes for a different set of transactions taking place across several different industries, and also asked a group of purchasing managers in these sectors to do the same. Snijders’s model beat the managers, even the above-average ones. He also found that veteran managers did no better than newbies, and that, in general, managers did no better looking at transactions within their own industry than at distant ones.\n\nThis is a general finding:\n\nA team led by psychologist William Grove went through 50 years of literature looking for published, peer-reviewed examples of head-to-head comparisons of clinical and statistical prediction (that is, between the judgment of experienced, “expert” humans and a 100% data-driven approach) in the areas of psychology and medicine. They found 136 such studies, covering everything from prediction of IQ to diagnosis of heart disease. In 48% of them, there was no significant difference between the two; the experts, in other words, were on average no better than the formulas. A much bigger blow to the notion of human superiority in judgment came from the finding that in 46% of the studies considered, the human experts actually performed significantly worse than the numbers and formulas alone. This means that people were clearly superior in only 6% of cases. And the authors concluded that in almost all of the studies where humans did better, “the clinicians received more data than the mechanical prediction.”\n\nDespite this victory, it seems a good idea to check the algorithm’s output.\n\nIn many cases … it’s a good idea to have a person check the computer’s decisions to make sure they make sense. Thomas Davenport, a longtime scholar of analytics and technology, calls this taking a “look out of the window.” The phrase is not simply an evocative metaphor. It was inspired by an airline pilot he met who described how he relied heavily on the plane’s instrumentation but found it essential to occasionally visually scan the skyline himself.\n\nBut …\n\nAs companies adopt this approach, though, they will need to be careful. Because we humans are so fond of our judgment, and so overconfident in it, many of us, if not most, will be too quick to override the computers, even when their answer is better. But Chris Snijders, who conducted the research on purchasing managers’ predictions highlighted earlier in the chapter, found that “what you usually see is [that] the judgment of the aided experts is somewhere in between the model and the unaided expert. So the experts get better if you give them the model. But still the model by itself performs better.”\n\nSo, measure which is best:\n\nWe support having humans in the loop for exactly the reasons that Meehl and Davenport described, but we also advocate that companies “keep score” whenever possible—that they track the accuracy of algorithmic decisions versus human decisions over time. If the human overrides do better than the baseline algorithm, things are working as they should. If not, things need to change, and the first step is to make people aware of their true success rate.\n\nAccept the result will often be to defer to the algorithm:\n\nMost of us have a lot of faith in human intuition, judgment, and decision-making ability, especially our own …. But the evidence on this subject is so clear as to be overwhelming: data-driven, System 2 decisions are better than those that arise out of our brains’ blend of System 1 and System 2 in the majority of cases where both options exist. It’s not that our decisions and judgment are worthless; it’s that that they can be improved on. The broad approaches we’ve seen here—letting algorithms and computer systems make the decisions, sometimes with human judgment as an input, and letting the people override them when appropriate—are ways to do this.\n\nAnd from the chapter summary:\n\nThe evidence is overwhelming that, whenever the option is available, relying on data and algorithms alone usually leads to better decisions and forecasts than relying on the judgment of even experienced and “expert” humans.\nMany decisions, judgments, and forecasts now made by humans should be turned over to algorithms. In some cases, people should remain in the loop to provide commonsense checks. In others, they should be taken out of the loop entirely.\nIn other cases, subjective human judgments should still be used, but in an inversion of the standard partnership: the judgments should be quantified and included in quantitative analyses.\n…\nAlgorithms are far from perfect. If they are based on inaccurate or biased data, they will make inaccurate or biased decisions. These biases can be subtle and unintended. The criterion to apply is not whether the algorithms are flawless, but whether they outperform the available alternatives on the relevant metrics, and whether they can be improved over time.\n\nAs for the remainder of the book, I have mixed views. I enjoyed the chapters on machines. The four chapters on platforms and first two on crowds were less interesting, and much could have been written five years ago (e.g. the stories on Wikipedia, Linux, two-sided platforms). The closing two chapters on crowds, which discussed decentralisation, complete contracts and the future of the firm were, however, excellent."
  },
  {
    "objectID": "posts/phd-thesis-passed.html",
    "href": "posts/phd-thesis-passed.html",
    "title": "PhD thesis passed",
    "section": "",
    "text": "A couple of months ago I was notified that my PhD thesis had been passed (full pdf here). I have posted about each chapter before:\n\nThe Evolutionary Foundations of Economics\nEconomic Growth and Evolution: Parental Preference for Quality and Quantity of Offspring\nPopulation, Technological Progress and the Evolution of Innovative Potential\nSexual selection, conspicuous consumption and economic growth\nEvolution, Fertility and the Ageing Population.\n\nHere are links to the three (anonymised) examiner reports - one from an economist at an Australian university, another from a US based economist, and a third from a biologist. A few excerpts follow. First, from the Australian-based economist on the overarching thesis:\n\nWhile there is certainly some truth in the evolutionary hypothesis it is in my view far from the complete story. Culture and institutions certainly also play their part. The industrial revolution in Britain was very much a cultural phenomenon (Morkyr, Jacob) – you do mention that culture get encoded in DNA, but you do not pursue it in depth. As you mention Clark and Galor are into high fertility in the upper class; however, I am not at all compelled by their hypothesis. The landed class was not innovative, it was lazy and land was mostly inherited. Institutions may also have played a role — according to Acemoglu and Robinson this is the only thing that matters.\n\n\nWhile I believe that natural selection forces may have been important in the past they are effectively put out of force in welfare states. It is not exactly the rocket scientists that get numerous kids in today’s advanced countries but often those we don’t want too many of, and that may well lower the innovative activity substantially and lead to an inverse relationship between population growth and productivity growth. The increasingly obese society also appears to go against the fitness hypothesis.\n\nAnd from the biologist:\n\n[P]apers are cited if they have been inspirational for constructing his argument, but in places the message is extrapolated far beyond what the original paper intended to show, while contrary evidence or work emphasizing other aspects (e.g. phenotypic plasticity as opposed to genetic change) is not paid much attention to.\n…\nGiven that we in reality know very little about whether, say, it was genes causally linked to thrift and hard work that caused economies to shift in recent times (a definitely interesting idea [attributed to another scholar cited in the thesis] — but one could also quote more objectionable traits that can documentably lead to getting ahead at the expense of others), or the extent to which demographic transition involves genetic change or, instead, simple behavioural plasticity responding to quality-quantity tradeoffs using old rules but with new outcomes when the environment changes, it would have been refreshing to see what each model produces “as a whole”, together with a discussion of the likelihood of each particular outcome (including but not limited to the best-fitting case). A thorough examination of how robustly the outcomes follow from the assumptions is part of the current standard of eco-evolutionary modelling.\n\nNow that the thesis is complete, I have mixed feelings about the process. I sense I would have learnt more if the process was to produce a book with less focus on formal modelling and more on piecing together the evidence on the overall argument. As an example, I am not sure the model in the conspicuous consumption chapter adds much to an intuitive argument that draws on other theoretical work - although the process of putting the model together did help me come to grips with what was going on.\nI’m not planning to do much more work in the style of academic papers. Partly that reflects my existence outside of the world of academia - I’m not facing any publication incentives. I’ll try to get the as-yet unpublished chapters published, and I have two other papers in progress, but that will likely be it.\nOtherwise, I’m hoping to climb back on to the blogging bandwagon, which has been on and off over the last year for a range of personal reasons. Blogging is a much more satisfying way of communicating than producing academic articles. As a start, my first published paper receives around 400 abstract views and 30 downloads per year (I expect largely due to click-throughs from the blog), compared to an average of several hundred blog visitors each day. And as a medium for collecting and distilling my thoughts, blogging is more effective. So hopefully there will be more posts in the near future."
  },
  {
    "objectID": "posts/pinker-on-violence.html",
    "href": "posts/pinker-on-violence.html",
    "title": "Pinker on violence",
    "section": "",
    "text": "The WSJ has published an essay by Steven Pinker on the decline of violence, which is adapted from his upcoming book The Better Angels of Our Nature: Why Violence Has Declined. Pinker points out six major declines in violence in human history, starting with the shift from the hunter-gatherer life:\n\nThe first was a process of pacification: the transition from the anarchy of the hunting, gathering and horticultural societies in which our species spent most of its evolutionary history to the first agricultural civilizations, with cities and governments, starting about 5,000 years ago.\nFor centuries, social theorists like Hobbes and Rousseau speculated from their armchairs about what life was like in a “state of nature.” Nowadays we can do better. Forensic archeology—a kind of “CSI: Paleolithic”—can estimate rates of violence from the proportion of skeletons in ancient sites with bashed-in skulls, decapitations or arrowheads embedded in bones. And ethnographers can tally the causes of death in tribal peoples that have recently lived outside of state control.\nThese investigations show that, on average, about 15% of people in prestate eras died violently, compared to about 3% of the citizens of the earliest states. Tribal violence commonly subsides when a state or empire imposes control over a territory, leading to the various “paxes” (Romana, Islamica, Brittanica and so on) that are familiar to readers of history.\n\nThis was followed by the “civilising” process in Europe, a humanitarian revolution around the time of the Enlightenment, the respite from major war since World War II, the decline of war worldwide and the rights revolutions which involves a “growing revulsion against aggression on smaller scales”.\nThe facts speak for themselves, so Pinker’s explanations for what lies behind these declines is more interesting. His three major explanatory factors are the pacifying influences of the state, of commerce and of cosmopolitanism. A change in inherent human nature is ruled out:\n\nIs it because violence has literally been bred out of us, leaving us more peaceful by nature?\nThis seems unlikely. Evolution has a speed limit measured in generations, and many of these declines have unfolded over decades or even years. Toddlers continue to kick, bite and hit; little boys continue to play-fight; people of all ages continue to snipe and bicker, and most of them continue to harbor violent fantasies and to enjoy violent entertainment.\nIt’s more likely that human nature has always comprised inclinations toward violence and inclinations that counteract them—such as self-control, empathy, fairness and reason—what Abraham Lincoln called “the better angels of our nature.” Violence has declined because historical circumstances have increasingly favored our better angels.\n\nI look forward to reading Pinker’s reasoning in more detail. While many of the changes in violence occur over the very short-term, the longer term trend has occurred over thousands of years, with ample potential for evolutionary change. Further, the hypothesis of Donohue and Levitt on the legalisation of abortion and crime suggests that changing the population composition can can have large effects in short periods, although it is not clear whether the cohort eliminated in the wake of the Roe v Wade decision would have been criminals due to upbringing or inherent traits."
  },
  {
    "objectID": "posts/please-experiment-on-us.html",
    "href": "posts/please-experiment-on-us.html",
    "title": "Please experiment on us",
    "section": "",
    "text": "Michelle Meyer and Christopher Chabris write:\n\nCompanies — and other powerful actors, including lawmakers, educators and doctors — “experiment” on us without our consent every time they implement a new policy, practice or product without knowing its consequences. When Facebook started, it created a radical new way for people to share emotionally laden information, with unknown effects on their moods. And when OkCupid started, it advised users to go on dates based on an algorithm without knowing whether it worked.\nWhy does one “experiment” (i.e., introducing a new product) fail to raise ethical concerns, whereas a true scientific experiment (i.e., introducing a variation of the product to determine the comparative safety or efficacy of the original) sets off ethical alarms?\nIn a forthcoming article in the Colorado Technology Law Journal, one of us (Professor Meyer) calls this the “A/B illusion” — the human tendency to focus on the risk, uncertainty and power asymmetries of running a test that compares A to B, while ignoring those factors when A is simply imposed by itself.\n….\n[A]s long as we permit those in power to make unilateral choices that affect us, we shouldn’t thwart low-risk efforts, like those of Facebook and OkCupid, to rigorously determine the effects of those choices. Instead, we should cast off the A/B illusion and applaud them.\n\nAmen."
  },
  {
    "objectID": "posts/please-not-another-bias-correcting-the-record.html",
    "href": "posts/please-not-another-bias-correcting-the-record.html",
    "title": "Please not another bias: correcting the record",
    "section": "",
    "text": "In 2015 I gave a presentation titled “Please Not Another Bias! An Evolutionary Take on Behavioural Economics” at the Marketing and Science Ideas Exchange (MSIX) conference. I posted my presentation on this blog, where it had around 100,000 readers in the first month (a lot for this blog). A copy of the post was the most popular post on Evonomics in its first year.\nI still see the post shared, which brings a slight cringe, as its not the article I would write today. I stand by the general argument, but there are several points that I would like to change.\nFirst, I wrote it for a marketing audience. It would be nice if it were more general.\nSecond, I believe the case for better theory in place of lists of biases has only grown, so I would like to make that case more explicitly.\nThird, while I still believe evolutionary biology should be part of the development of better theories of human decision making - and any theory must be consistent with evolutionary theory - I’m also of the belief that a more eclectic selection of disciplines could contribute hypotheses.\nFourth, and the reason for this post, is that much of the experimental evidence I quoted should be considered shaky at best. When I developed the presentation the replication crisis was just warming up. There had been some high-profile failed replications, but the Open Science Collaboration’s first article Estimating the reproducibility of psychological science was still a month away. I was certainly naive about the foundations I was building on.\nFor the remainder of this post I walk through some of my claims and the experimental evidence to support them (not strictly in the order I discussed them then). Which would I trust today? In summary, while I still believe many of the underlying hypotheses, I don’t trust any of the experimental results sufficiently to reference them in a re-write today.\nI opened with a broad claim:\n\nSo, let’s do a quick quiz. Tell me two things about the driver of this Ferrari (I have stolen this example from University of New South Wales evolutionary biologist Rob Brooks).\n\n\nFerrari\n\n\n\nFirst, the driver was male.\nSecond, the driver is likely young (in this case, 25).\n\nI”ll back this being a robust phenomena, but we don’t exactly need a body of science to make that prediction.\nI then supported that point with some storytelling about the different motivations of men and women:\n\nSo why is this the case?\nFemales – and in biology, this is in part how females are defined – produce a large immobile egg. Males produce a smaller gamete – sperm. The egg is the scarce resource. Women are born with a million or so eggs, but they release only one or so a month. Men produce 1,500 sperm a second. Each man in this room will produce enough sperm during this talk to fertilise every egg the women in this room will ever produce.\nThen there is what happens when a sperm and an egg are joined. The woman spends nine months carrying the baby – and is unable to reproduce during that time. She then provides the majority of infant care. Men are less constrained by any such barriers.\n…\nThen, for a few men, the rewards are vast.\nAs one example, approximately 16 million men in central Asia carry the same Y chromosome – the Y chromosome is passed from down the male lineage from father to son. This chromosome originated in Mongolia around 1000 AD with around 8 per cent of the men in the region carrying it (0.5% of the world’s male population) – they all trace their male lineage back to the same man.\nOne possibility is that this chromosome was so successful as it was carried by Genghis Khan and his close relatives. …\nNo woman could ever have that level of success – but for men, the evolutionary rewards to success can be vast.\nThis brings us back to our Ferrari driver. As a male, the risk-reward calculation in evolutionary terms is quite different from women. Men face a higher probability of evolutionary oblivion, and small chance of an evolutionary extravaganza. It makes sense to take risks that may lead to inordinate evolutionary success – or at least to avoid evolutionary oblivion.\n\nThere’s nothing I would say is incorrect here, and it leads to a question of whether we need to build sex more explicitly into models of human behaviour. (Probably not the zeitgeist right now.) But when I then try to add some science^{\\text{TM}} to my case, things become shaky.\nHere’s the first study I would no longer reference (which also happened to be the subject of my first post on this blog over a decade ago):\n\nOne of my favourite examples of this comes from research by Richard Ronay and Bill von Hippel. They got some young male skateboarders to perform tricks, including a difficult trick that they could complete only half the time. Halfway through filming, a woman rated as highly attractive (corroborated by “many informal comments and phone number requests from the skateboarders”) walked onto the scene. Once she appeared, they took more risks and were less likely to bail a trick half-way through, instead riding all the way through to the crash landing (a story on ABC’s Catalyst demonstrates this effect).\n\nIn this experiment there was a small sample of 96 male skateboarders, giving groups of 43 and 53 skateboarders performing ten tricks for the male and female experimenter respectively. There were also have large effect sizes. I wouldn’t claim these effects sizes are impossible using the “effect is too large heuristic”, but they certainly don’t make me feel comfortable. The figure below gives a sense of the change in behaviour.\n\nTo understand one point that makes this study particularly interesting, let me turn to Ulrich Schimmack’s work to develop a replicability index. By comparing the observed power of a set of studies and the rate at which reported results are significant, Schimmack estimates the rate at which studies will replicate in the form of the R-Index. Schimmack uses the R-Index to compare the replicability of journals, authors and departments.\nThe ranking of authors is of relevance here. Bill von Hippel, the second author in this skateboarding paper, was one of those ranked. I’ll use von Hippel’s words from a conversation with Schimmack:\n\nBut your work was made much more personally relevant on January 19, when you wrote a blog on “Personalized p-values for social/personality psychologists.”\nInitially, I was curious if I would be on the list, hoping you had taken the time to evaluate my work so that I could see how I was doing. Your list was ordered from the highest replicability to lowest, so when I hadn’t seen my name by the half-way point, my curiosity changed to trepidation. Alas, there I was – sitting very near the bottom of your list with one of the lowest replicability indices of all the social psychologist’s you evaluated.\n\nOn the ranking table as at 5 July 2022, von Hippel is ranked 416 out of 426.\nAmong the numbers Schimmack generated was the significance threshold we would have to use if we wanted to reduce the false discovery rate for an author’s work below 0.05. (That is the rate at which significant results are false.) For von Hippel, that number is 0.001. He also calculated the expected discovery rate based on the power of the studies (for von Hippel, 12%), which can be compared against the observed discovery rate (for von Hippel, 65%). (I think these precise numbers should be taken with a grain of salt - there are a lot of implicit assumptions in their calculation - but the gross difference is indicative of a problem.) An observed discovery rate materially higher than the expected discovery rate is a sign of publication bias or something going askew in the lab.\nIn this light, I feel I have to place a heavy discount on the skateboarding paper. There’s nothing special about it that should lead us to give it more credence than the broader body of von Hippel’s work.\nBefore going to the next study, I should note that the reason I am highlighting von Hippel is because he took the criticism seriously and sought to understand what he could do better. That conversation with Shimmack excerpted above was part of the process. He then ran a replication of one of the studies he was more confident in as a test of Schimmack’s arguments, and lo and behold, it failed to replicate. Good on von Hippel for taking this on.\nBack to my talk, unfortunately it only gets worse from here. If I had to bet which experiment would have the best (albeit limited) chance of replicating, it would be Ronay and von Hippel’s.\nOne of those less reliable studies is a classic. In some ways it kick-started the body of research that I’m about to pull apart.\n\nIn an experiment by Margo Wilson and Martin Daly – two of the pioneers of evolutionary psychology, and I recommend you read their book Homicide if you haven’t – they exposed men and women to either pictures of attractive faces or pictures of cars before undergoing tests of their degree of present bias.\nThe men who had seen the attractive faces became more severe discounters than those who had seen the cars. They became focused on the present – the mating opportunity.  The women did not become increasingly severe discounters in this experiment – although there may be a smaller effect that the experiment did not have the power to detect.\nSo here, what might be called a very strong present bias has a degree of rationality to it in that the objective of the participants is mating. Obviously, they didn’t have a chance to mate with these pictures – so there we have the issue of mismatch – but you can see the evolutionary foundation of their decision. If they did manage to capitalise on that moment and manage to mate, their evolutionary future is set.\n\nThere might be something to the hypothesis underlying this experiment but if you wanted a study to have all the hallmarks of one that won’t replicate, this is it. The men who rated “hot” women had a significant increase in discount rate. But, there were only 21 of them. This study simply comes from an era where sample sizes were so small that the experiments couldn’t be expected to find a realistic effect size even if they existed. Then throw into the mix the fact that this is effectively a priming study - and we’ve all seen how that’s fared the last decade - and absent a large replication I think we can redeuce the weight of this experiment to zero.\nBefore moving onto the body of research that Wilson and Daly inspired, I do want to note that priming lies at the heart of the problem with many of these evolutionary studies. I suspect many of the hypothesised effects exist in the wild. People behave differently around highly attractive potential partners. But to do a nice, cheap, easy lab experiment you need to use a weak prime to generate the phenomena, and that weak prime is the weak link in the chain. (This is why I have less slightly less scepticism about the skateboarding example - the experimental subjects weren’t just reading about an attractive lab assistant.)\nDaly and Wilson’s experiment seems highly unlikely to replicate. The next one didn’t. I wrote:\n\nTake a group of men and show them pictures of attractive women and then ask them what they will do with their money. The mating prime makes men more likely to engage in conspicuous consumption or conspicuous charitable donation, but has no effect on inconspicuous consumption.\nWomen can also be affected by mating primes, although in that particular experiment their change in behaviour in response to pictures of attractive men was an desired increase in volunteering in a public way (but no increase in private benevolence).\nThe difference reflects the different traits each are communicating – men are communicating resources and the traits required to accumulate them, women their conscientiousness.\n\nDavid Shanks and friends ran eight replication experiments, the first five of which were all twists on the experiments reported by Griskevicius and friends that formed the basis of the above excerpt. The result? As you can see in this figure, there’s nothing there.\n\nEven more striking is Shanks and friends’ funnel plot of results published on this topic. The black dots represent published results. The white triangles represent the results of the replication experiments.\n\nIt looks like we have a hideous case of publication bias.\nThat said, a subset of the authors whose studies were replicated responded, claiming that that an alternative approach to the meta-analysis suggested there was no evidence of p-hacking, and that the replications had numerous issues. Shanks and Vadillo responded in turn. I won’t tackle the back-and-forth in detail here, but will say that I believe the conclusion that romantic priming is on shaky grounds holds. Much of the critique of the replications is the often-seen handwaving that the replication methods were not close enough to the originals. But that’s a sign of a flaky phenomena at best, and more likely a sign that the effect doesn’t exist. The critique of the meta-analysis only serves to highlight the gross lack of statistical power across the literature.\nThere’s also another body of evidence relevant here. Following the replications and meta-analysis by Shanks and friends, Ulrich Schimmack put a larger set of romantic priming papers through the R-index wringer. Perhaps most damming, a histogram of results showed evidence of severe selection bias and, Schimmack argued, evidence of questionable research practices. The papers also scored low on his R-Index, suggesting a low probability of replication across the whole literature.\nNow onto the next paper referenced in my talk:\n\n… show one group of people the movie The Shining, the other half a romantic movie starring Ethan Hawke. Then manipulate the ads they see during the movies to either accentuate the uniqueness of the product, or its popularity.\nThose watching The Shining are more likely to prefer popular products – safety in numbers as their danger avoidance personality is triggered. For those watching the romantic movie, they wanted unique products so that they would stand out from the crowd. Their mating motives have been triggered. You effectively get a change in preferences based on which movie they are watching and which self is answering the questions about the products. The effectiveness of social proof varied with context.\n\nGiven everything I’ve said about similar studies above, I can’t give this one much weight. There’s actually two subtle effects that need to work here. First there is a fear/romantic response sought to be generated by the movie (or in a second experiment, a story). The “popular” or “unique” products were then actually the same thing, but advertised slightly differently. For example, in experiment 1a the popular product was a museum with a tagline of “Visited by over a Million People Each Year” whereas the unique product was the same museum with the tagline “Stand Out from the Crowd”. So the experiment is relying on both the initial prime and then a response to these subtle changes in messaging for effectively the same product. Not likely.\nAnd now for the final study:\n\n[I]n one study men and women were shown pictures of members of the opposite sex in either a red Ford Fiesta or a silver Bentley. Unfortunately the photos in the paper are provided in black and white – as shown in this slide – but these indicate the types of images the experimental subjects were shown.\n\n\n\nDunn et al\n\n\n\n\nThe result – the expensive car made the male more attractive to the females, whereas there was no effect on male perception of the female drivers. The increase in male’s attractiveness was equivalent to around 1 point on a scale of 1 to 10.\n\nI’d give this result a decent chance of replicating, maybe 60%. The proposed mechanism is fairly direct, albeit involving a photo in a lab. There are other similar results. But, I’m not comfortable giving it much weight absent a pre-registered replication.\nSummarising the above, I didn’t reference a single experiment in that speech that I would include today. Not a great result."
  },
  {
    "objectID": "posts/population-and-the-tragedy-of-the-commons.html",
    "href": "posts/population-and-the-tragedy-of-the-commons.html",
    "title": "Population and the tragedy of the commons",
    "section": "",
    "text": "Like all economists, I am familiar with the concept of the tragedy of the commons. However, possibly like most economists, I had not read Garrett Hardin’s 1968 article from where we derive the phrase - that is, until yesterday. As a result, I did not understand the extent to which overpopulation concerns underpinned Hardin’s writing (HT: Daniel Rankin).\nMuch of Hardin’s career focussed on overpopulation. He wrote one article called Living on a Lifeboat in which he argued that the lives saved by food aid would only make life worse for later generations (a Malthusian world). He was also an advocate of allowing people to choose their own time to die - he committed suicide with his wife at age 88. Given the debate on fertility at Cato Unbound and the release of Bryan Caplan’s Selfish Reasons to Have More Kids, what Hardin wrote in The Tragedy of the Commons about population is now particularly topical.\nHardin’s concern about overpopulation stemmed from the existence of the commons. For example, in a “dog eat dog” world, if someone had too many children, it would not be a matter of public concern as these parents would leave less descendants than others. This is because they would be unable to adequately care for a large number of children. He wrote:\n\nIf each human family were dependent only on its own resources; if the children of improvident parents starved to death; if, thus, overbreeding brought its own “punishment” to the germ line—then there would be no public interest in controlling the breeding of families.\n\nHardin felt, however, that these conditions did not hold as society was committed to one form of commons, the welfare state. In some ways, this is a direct response to Bryan Caplan’s recent statement that:\n\nAs long as parents are financially responsible for their children, any negative effect of population on living standards is internal to the family. ….. [T]he negative externalities, if any, are intra-family.\n\nThe question is, thus, to what extent does Caplan’s condition that “as long as parents are financially responsible for their children” hold?\nBeyond the welfare state example of Hardin’s, some other negative externalities come to mind. These include the raft of unpriced commons to which people still have access, positional goods, limited land (think beach front real estate) or where desperate conditions result in property rights breaking down (say, Rwanda in 1994 as family plot sizes dropped below that which could sustain a family). The high wages after the Black Death induced population plunge in the 1300s also suggest that externalities extend beyond the family.\nWhen it comes to solving the problem of the tragedy of the commons, Hardin generally favours property rights, although he did consider that the system of private property plus inheritance was unjust. He simply did not see any other alternative.\nFor the issue of overpopulation and fertility, however, he was less clear. He notes the uselessness of appealing to conscience to cut fertility, as those who continue to breed despite the appeal to conscience will then dominate the population. He considered that we needed to abandon the commons in breeding and relinquish the freedom to breed. On the United Nations Declaration on Human Rights, which allows freedom of choice of family size for the family, Hardin wrote that “If we love the truth we must openly deny the validity of the Universal Declaration of Human Rights”.\nHow precisely Hardin proposed to curtail the right to breed, however, is not addressed in his tragedy of the commons article. I understand that he addressed specific population control measures in more detail in his later work, which might be the subject of a later post."
  },
  {
    "objectID": "posts/population-genetics-and-economic-growth.html",
    "href": "posts/population-genetics-and-economic-growth.html",
    "title": "Population genetics and economic growth",
    "section": "",
    "text": "The title of this post comes from a 2002 paper by Paul Zak and Kwang Woo Park. The title is mildly deceptive, as the paper has many elements and ideas crammed into it beyond population genetics. The model described by the authors includes working, consumption, saving, marriage, genetic diversity, sexual selection, intelligence, beauty, education, the Flynn effect, family size effects and more. While many of these elements deserve consideration, this is ultimately the paper’s weakness. Even though most of the assumptions are reasonable and well supported in the literature, the resulting mix is hard to disentangle, with a few factors dominating the results.\nZak and Park built an age-structured model in which agents’ cognitive ability and beauty are genetically determined, with human capital a function of cognitive ability and education. The model agents are paired with potential partners over a series of rounds, and decide whether they will marry their potential partner based on their beauty and human capital (together called “pizzazz”). In each round, they weigh the potential benefits to marriage, including increased income if their partner has higher human capital, the joy of marriage due to their partner’s pizzazz and the potential partners in future matching rounds. If they agree to marry, they then decide family size, trading off consumption (in both their young adulthood and old-age) and children. These interactions drive the population dynamics and economic output in the economy.\nThe agents do not seek to maximise biological fitness directly. As I have posted before, this is a workable choice if you can offer a link between the factor that the agent seeks to maximise and their fitness. However, where agents gain utility from a basket of outcomes that are linked to fitness to varying degrees, this creates a fitness trade-off between the activities. In the case of Zak and Park’s model, the agents trade-off marriage, children and consumption of goods. As a result, an agent with lower preference for consumption of goods relative to children would have a biological advantage and could invade the population.\nZak and Park run their agents through a number of simulated scenarios. In the baseline scenario we see a typical evolutionary biology result - a preference for pizzazz increases the accumulated human capital and beauty of the population, reducing its variance. Low pizzazz people are rejected, causing them to disappear from the gene pool. Beauty increases towards its upper bound, while human capital can continue to accumulate. It is the increase in human capital that drives long-term growth. The base-line simulation generated a one per cent growth in human capital per generation over 40 generations, which the authors suggest is a reasonable approximation of the last 800 years.\nIn an increased inequality scenario, higher variance in traits for half of the population results in low growth as there are low marriage rates and reproduction. The increased inequality results in less pairings where each agent meets the others’ criteria. The population shrinks, with output bottoming out and eventually picking up as agents become less choosy and start to reproduce. Total output is largely a function of population size. A similar effect is seen when there is a bimodal distribution in beauty. Initially there are low marriage and reproduction rates as high pizzazz individuals reject those with low pizzazz. This causes lower population, with associated reduced total output, which recovers once people become less choosy. Greater genetic diversity also decreases marriage rates, population and output.\nIn each of these scenarios, output is largely tied to population size and the degree to which agents are willing to mate with whomever they are paired. As a result, the authors’ discussion of economic output is effectively a discussion of population size. Only in the pandemic scenario, where different segments of the population are eliminated, is any commentary on per capita income made. In that case, the authors note that there is little effect of the pandemic on the output of survivors.\nThe last scenario explored by the authors is love. Love increases the probability that agents agree to marry despite more dissimilar levels of pizzazz. This scenario makes for massively increased output with increased population. The authors see this as illustrating the benefits of the right balance  between diversity an assortive mating. Assortive mating drives an increase in average pizzazz, but if too strict, population plummets as no one is willing to mate with the agent they are randomly paired. They call this balance the “The Goldilocks Principle”.\nThis conclusion is interesting, but I am not sure that is the world we are in.  Many women may be increasing their standards as their income increases, but per capita income is also increasing despite lower marriage rates. The demographic revolution was not primarily due to lower pairing, but due to the timing of pairing and lower fertility within pairs. Assortive mating is relatively easy in our assorted world. A university educated person is likely surrounded by other people of similar quality.\nApart from a near identical paper that Zak and Park released in 2006 (with an extra section on inequality), there has been little further work in the footsteps of this model. I suspect this is because the paper has too many elements, and that there is more reward in using simpler models to explore each element in-depth."
  },
  {
    "objectID": "posts/positive-eugenics.html",
    "href": "posts/positive-eugenics.html",
    "title": "Positive eugenics",
    "section": "",
    "text": "In Forbes, Jon Entine discussesthe rise of “positive eugenics”:\n\nScientists offered what they considered to be a progressive solution: “positive eugenics,” which would encourage society’s healthiest citizens to have more children—the founder of Planned Parenthood, Margaret Sanger, was an eager proponent of eugenics—and more tentatively, “negative eugenics.”The “negative” wing of eugenics prevailed, however, which for the most part meant restricting the mentally ill, poor, immigrants and non-whites from propagating. It served as an inspiration and justification for Nazism and the “Final Solution,” which led to the discrediting of the entire movement.\nNow, eugenics is back in vogue with a clear focus on the positive role that genetics can and is playing in medicine and health. …\n[M]odern eugenics aspirations aren’t about top-down measures promoted by the Nazis or the forced sterilizations of the past, as Comfort points out. Instead of being driven by a desire to improve the species, new eugenics is being driven largely by the individual’s personal desire to be as healthy, intelligent and attractive as possible—and for our children to be so.\n\nEntine focuses on medical interventions, such as the testing of foetuses, and hints at the ethical debates about more controversial areas such as choice of sex and genetic enhancement. While he does not go into enhancement in detail, Entine links to an interesting post at Gene Expression, where Razib asks how “positive eugenics” might  be implemented for traits such as intelligence. As a huge number of gene variants affect intelligence, genetic screening for intelligence is not an easy task. Razib argues that this will cause the initial focus to be on mutation load. If intelligence is highly vulnerable to mutations, with variation in intelligence largely due to negative mutations, reducing mutation load will probably be a long-term approach too.\nInterestingly, some of the “positive eugenics” discussed by Entine does not reduce the frequency of harmful alleles in the population, but rather affects the frequency in which they express themselves in the population. For example, the incidence of Tay Sachs disease has been reduced due to screening in Jewish populations. As the disease only manifests when someone possesses both disease-related alleles, the prevention of marriage between carriers of the alleles stops their children from having the disease. But the disease-related allele can still be harmlessly present in the next generation. Further, if there were a positive effect from carrying only one of these alleles, which is suggested by the spread of the allele despite the high costs to the disease, screening could actually increase the prevalence.\nFinally, as I have posted before, a positive eugenics program has been ongoing for millions of years through our selection of partners with whom we wish to have children. We are still some way from using modern technologies to shape out genetic future to the extent that sexual selection already does."
  },
  {
    "objectID": "posts/predicting-replication.html",
    "href": "posts/predicting-replication.html",
    "title": "Predicting replication",
    "section": "",
    "text": "The Behavioural Economics Replication Project:\n\nThis project will provide evidence of how accurately peer prediction markets can forecast replication of scientific experiments in economics.\nIn order to incentivize prediction market activity, and collect evidence on actual replication, eighteen (18) prominently published studies in experimental economics were chosen for trading in prediction markets, followed by replication. They are laboratory studies, using student participants, that were published in the American Economic Review (AER) or in the Quarterly Journal of Economics (QJE) in the years 2011 to 2014, testing specific hypotheses using between-subjects designs.\n\nIt is neat that a prediction market will be opened up to allow experimental economists to bet on which studies will be replicated. How much faith do those familiar with the workings of academia have in these studies?\nLooking at the 18 studies, I suspect there will be a higher rate of replication than occurred in last year’s special issue of Social Psychology, but it wouldn’t surprise me to see a majority fail to replicate."
  },
  {
    "objectID": "posts/publishing-on-genetic-diversity-and-economic-growth.html",
    "href": "posts/publishing-on-genetic-diversity-and-economic-growth.html",
    "title": "Publishing on genetic diversity and economic growth",
    "section": "",
    "text": "Should Ashraf and Galor’s paper The ‘Out of Africa’ Hypothesis, Human Genetic Diversity, and Comparative Economic Development have been published?\nThe major critique about whether the paper should have been published comes from the conclusion of an article in Current Anthropology:\n\nSocial scientists seeking to explain economic behavior through genetics must exercise particular caution. As Benjamin et al. (2012:656) point out, “researchers in this field hold a special responsibility to try to accurately inform the media and the public about the limitations of the science,” especially in studies intended for “social-scientific interventions” (Benjamin 2010:1). Without proper methodology and data analysis standards, false positives are likely to be misunderstood as facts, and these can then be mobilized in the political arena. Ashraf and Galor’s (2013) paper is based on a fundamental scientific misunderstanding, bad data, poor methodology, and an uncritical theoretical framework. While the attempt to create interdisciplinary studies that link anthropology, genetics, and economics is laudable, economists should consult with specialists in those fields to avoid making such uninformed blunders. The same should be true of the peer-review process for such interdisciplinary articles.\nMore egregiously, this study has the potential to cause serious harm. By claiming a causal link between the degree of genetic heterogeneity and economic development, their thesis could be interpreted to suggest that increasing or decreasing a nation’s genetic (or ethnic) diversity would promote prosperity. Ultimately, this can provide fodder to those looking to justify policies ranging from mistreatment of immigrants to ethnic cleansing (especially by groups with real political power, e.g., Golden Dawn in Greece).\nWe are not concerned here with the authors’ own social or political attitudes. Rather, we wish to emphasize the irresponsibility of bad science. In the social sciences, scientific methods are an extremely powerful tool for analyzing trends in an empirically demonstrable manner and thus have the important opportunity to guide political action. When used improperly or when it is of dubious quality, however, science can become a justification for reactionary policy. The dismal nature of economics is often appealed to when facts contradict a desired reality. However, we are not arguing a case for blissful ignorance. What we see in Ashraf and Galor’s study is the worst of all worlds: something false and undesirable.\n\nThis argument has two major threads. First is the responsibility of the authors to consult with specialists in other fields and for the peer review process to do likewise. I don’t know who reviewed this article for the American Economic Review, but Ashraf and Galor acknowledge five anonymous referees. The acknowledgements section of the paper also lists participants from 33 seminars, 13 conferences and 3 lectures. They list 24 people by name, including the population geneticist whose data they use. The list does not include any anthropologists, but this is not through hiding the paper. They posted the paper on IDEAS in May 2010, on the Social Science Research Network in January 2008 and again in May 2011 (with regular updates to those submissions), and as an NBER working paperin July 2011. This paper epitomises open peer review.\nThe issue seems to be that it was not noticed by some people until it was mentioned in Science late last year. If not for the Science mention, it may have been published without any fuss. This is indicative of a broader problem that I am not sure how to solve. In working on a paper of this nature, how do you get it seen by everyone who might object and want to comment? My own experience of seeking a broad variety of feedback on my working papers is that it is not easy. Everyone is busy and has limited time. It is only when accepted by a major journal that people become interested, by which time it is too late.\nThe second argument by the authors of the Current Anthropology piece concerns the potential for harm. But for this argument to hold much weight, we need to assume that those who would misuse such research are not gathering evidence from other sources. A quick google search will show the depth and breadth of discussion on the topic of “human biodiversity”. Some of the discussion is serious and data rich (some of it is horrible). Barring work that implies genetic differences between populations does not stop these discussions. All it prevents is the ability to examine openly whether the hypotheses have merit and to build evidence against those that don’t. As an example, research by Arthur Jensen, who faced similar claims about the responsibility of his research, triggered James Flynn’s important work on IQ. As Flynn wrote at the end of his recent book:\n\nPsychologists should thank Jensen for pursuing his life-long mission, against great odds, to clarify the concept of g. In addition to intellectual eminence, he had the courage to face down opposition often political rather than scientific. If I have made a significant contribution to the literature, virtually every endeavor was in response to a problem set by Arthur Jensen.\n\nA further issue arises where the work is empirically strong. Should it then be published? If you base your defence against unethical use of the hypothesis on the hypothesis not being true, your defence falls away as soon as the evidence mounts in support of the hypothesis. An ethical framework of human equality or liberty, whatever the outcomes of the genetic research, is a stronger place to build the barricades if you are worried about the ethical implications of research such as this.\nAs a reading of my other posts on this paper suggest, I don’t believe that the conclusions drawn by Ashraf and Galor are correct. But scientific progress is not made by publishing the perfect answer the first time. We should not see peer-reviewed economics articles as truth handed down from above (if you do, you would become confused very quickly). A serious effort can provide a springboard for other work, even if you consider it flawed.\nMy posts on Ashraf and Galor’s paper on genetic diversity and economic growth are as follows:\n\nA summary of the paper methodology and findings\nDoes genetic diversity increase innovation?\nDoes genetic diversity increase conflict?\nIs genetic diversity a proxy for phenotypic diversity?\nIs population density a good measure of technological progress?\nWhat are the policy implications of the effects of genetic diversity on economic development?\nShould this paper have been published? (this post)\n\nEarlier debate on this paper can also be found here, here, here and here."
  },
  {
    "objectID": "posts/quantifying-children.html",
    "href": "posts/quantifying-children.html",
    "title": "Quantifying children",
    "section": "",
    "text": "In the New York Times profile of Justin Wolfers and Betsey Stevenson, Motoko Rich writes:\n\nStill, data alone can’t explain everything in life. Before Matilda arrived, Ms. Stevenson reviewed research on children and their effect on adult happiness. “I was really put off by the fact that people with kids were less happy,” she said.\nBut at their home last month, their delight in their daughter was clear.\nMr. Wolfers has written about the joys of fatherhood: “It’s visceral; it’s real; it’s hormonal and it’s not in our economic models.”\n\nOn the back of this profile, some people are suggesting that there are circumstances where we need to throw out the equations. KJ Dell’Antonia writes:\n\nMs. Stevenson and Mr. Wolfers are a relief: finally, academic recognition that not every human action is, or needs to be, rational. Of course our children don’t necessarily repay us for our efforts every minute of every day (witness the laptop shooting father in my last post). But whatever indefinable thing those children do offer (joy, love, purpose) resists quantification.\n\nAre you only rational when you maximise what the economists say you should?\nMore importantly, much of what children do offer, a vehicle for 50 per cent of your genes, is easily quantifiable. Evolutionary biologists have been quantifying the results of reproduction for a century.\nIf we accept that “joy, love, purpose” resist quantification, does that intangible joy we feel when we buy a new iPhone mean that we cannot quantify the purchase (consumption) of it? Of course not – the purchase is clearly quantifiable. In the same way, the hard to measure drivers behind our urge to have children do not erase the very real, quantifiable results. And there is a growing science on those urges.\nUltimately, Wolfers identifies the problem when he states, “it’s not in our economic models”. It is time to start putting the biology in."
  },
  {
    "objectID": "posts/rats-in-a-casino.html",
    "href": "posts/rats-in-a-casino.html",
    "title": "Rats in a casino",
    "section": "",
    "text": "From Adam Alter’s Irresistible: Why We Can’t Stop Checking, Scrolling, Clicking and Watching:\n\nJuice refers to the layer of surface feedback that sits above the game’s rules. It isn’t essential to the game, but it’s essential to the game’s success. Without juice, the same game loses its charm. Think of candies replaced by gray bricks and none of the reinforcing sights and sounds that make the game fun. …\nJuice is effective in part because it triggers very primitive parts of the brain. To show this, Michael Barrus and Catharine Winstanley, psychologists at the University of British Columbia, created a “rat casino.” The rats in the experiment gambled for delicious sugar pellets by pushing their noses through one of four small holes. Some of the holes were low-risk options with small rewards. One, for example, produced one sugar pellet 90 percent of the time, but punished the rat 10 percent of the time by forcing him to wait five seconds before the casino would respond to his next nose poke. (Rats are impatient, so even small waits register as punishments.) Other holes were high-risk options with larger rewards. The riskiest hole produced four pellets, but only 40 percent of the time—on 60 percent of trials, the rat was forced to wait in time-out for forty seconds, a relative eternity.\nMost of the time, rats tend to be risk-averse, preferring the low-risk options with small payouts. But that approach changed completely for rats who played in a casino with rewarding tones and flashing lights. Those rats were far more risk-seeking, spurred on by the double-promise of sugar pellets and reinforcing signals. Like human gamblers, they were sucked in by juice. “I was surprised, not that it worked, but how well it worked,” Barrus said. “We expected that adding these stimulating cues would have an effect. But we didn’t realize that it would shift decision making so much.”\n\nI’ll post some other thoughts on the book later this week."
  },
  {
    "objectID": "posts/recent-selection-for-height.html",
    "href": "posts/recent-selection-for-height.html",
    "title": "Recent selection for height",
    "section": "",
    "text": "As noted by Steve Hsu and Razib Khan, a new paper in Nature Genetics reports evidence of recent selection on existing variation in height in European populations. The paper’s authors summarise as follows:\n\nIn summary, we have provided an empirical example of widespread weak selection on standing variation. We observed genetic differences using multiple populations from across Europe, thereby showing that the adult height differences across populations of European descent are not due entirely to environmental differences but rather are, at least partly, genetic differences arising from selection. Height differences across populations of non-European ancestries may also be genetic in origin, but potential nongenetic factors, such as differences in timing of secular trends, mean that this inference would need to be directly tested with genetic data in additional populations. By aggregating evidence of directionally consistent intra-European frequency differences over many individual height-increasing alleles, none of which has a clear signal of selection on its own, we observed a combined signature of widespread weak selection. However, we were not able to determine whether this differential weak selection (either positive or negative) favored increased height in Northern Europe, decreased height in Southern Europe or both.\n\nAlthough it is not clear what the nature of the selection was, this paper provides yet another example of human evolution since the origin of agriculture. Add it to growing list.\nThis finding reflects some of the material in Cochran and Harpending’s book The 10,000 Year Explosion: How Civilization Accelerated Human Evolution, where they argued that selection on variation in existing functional genes, as opposed to new mutations, would have played a major role in the initial response to agriculture. Selection could have been particularly strong in the case of balanced polymorphisms, which occur where a population maintains two different alleles of a gene. In such a case, balanced polymorphisms can respond quickly to the environmental change as the alleles are already at a substantial prevalence in the population. In comparison, a new mutation present in a single person may take thousands of years (if it spreads at all) to reach the higher frequencies where the rate of spread will be largest.\nHowever, Cochran and Harpending also suggested that as population increased, the importance of new mutations increased:\n\nHuman genetic variation was limited in the days before agriculture, in part because populations were small, and it was often not useful, since many of the changes that were favored among agriculturalists would actually have been deleterious among their hunter-gatherer ancestors. This means that some of the alleles with the right effects in farmers would have been extremely rare or nonexistent in their hunter-gatherer ancestors.\nTherefore, new mutations must have played a major role in the evolutionary response to agriculture—and as luck would have it, there was a vast increase in the supply of those mutations just around this time because of the population increase associated with agriculture. … Increased population size increased the supply of beneficial mutations just as buying many lottery tickets increases your chance of winning the prize.\nBy the beginnings of recorded history some 5,000 years ago, new adaptive mutations were coming into existence at a tremendous rate, roughly 100 times more rapidly than in the Pleistocene.\n\nRegardless, it is positive to see the Nature Genetics paper as a sign of increasing research on selection of standing variation across large numbers of loci as a complement to the search for novel mutations."
  },
  {
    "objectID": "posts/replicating-anchoring-effects.html",
    "href": "posts/replicating-anchoring-effects.html",
    "title": "Replicating anchoring effects",
    "section": "",
    "text": "The classic Ariely, Loewenstein, and Prelec experiment (ungated pdf) ran as follows. Students are asked to think of the last two digits of their social security number - essentially a random number - as a dollar price. They are then asked whether they would be willing to buy certain consumer goods for that price or not. Finally, they are asked what is the most they would be willing to pay for each of these goods.\nThe result was that those with a higher starting price - that is, a higher last two digits on their social security number - were willing to pay more for the consumer goods. That random number “anchored” how much they were willing to pay.\nReading David Levine’s Is Behavioural Economics Doomed? (review to come soon), Levine mentions the following attempted replication:\n\nOn the Robustness of Anchoring Effects in WTP and WTA Experiments (ungated pdf)\nDrew Fudenberg, David K. Levine, and Zacharias Maniadis\nWe reexamine the effects of the anchoring manipulation of Ariely, Loewenstein, and Prelec (2003) on the evaluation of common market goods and find very weak anchoring effects. We perform the same manipulation on the evaluation of binary lotteries, and find no anchoring effects at all. This suggests limits on the robustness of anchoring effects.\n\nAnd from the body of the article:\n\nOur first finding is that we are unable to replicate the results of ALP [Ariely, Loewenstein, and Prelec]: we find very weak anchoring effects both with WTP [willingness to pay] and with WTA [willingness to accept]. The Pearson correlation coefficients between the anchor and stated valuation are generally much lower than in ALP, and the magnitudes of the anchoring effects (as measured by the ratio of top to bottom quintile) are smaller. Repeating the ALP procedure for lotteries we do not find any anchoring effects at all.\nUnlike ALP, we carried out laboratory rather than classroom experiments. This necessitated some minor changes—discussed below—from ALP’s procedures. It is conceivable that these changes are responsible for the differences in our findings; if so the robustness of their results is limited.\n…\nOur results do not confirm the very strong anchoring effects found in ALP. They are more in agreement with the results of Simonson and Drolet (2004) and Alevy, Landry, and List (2011). Simonson and Drolet (2004) used the same SSN-based anchor as ALP, and found no anchoring effects on WTA, and moderate anchoring effects on WTP for four common consumer goods. Alevy, Landry, and List (2011) performed a field experiment, eliciting the WTP for peanuts and collectible sports cards, and they found no anchoring effects. Bergman et al. (2010) also used the design of ALP for six common goods, and found anchoring effects, but of smaller magnitude than in ALP.\nTufano (2010) and Maniadis, Tufano, and List (2011) also failed to confirm the robustness of the magnitude of the anchoring effects of ALP, using hedonic experiences, rather than common goods. Tufano (2010) used the anchoring manipulation to increase the variance in subjects’ WTA for a bad-tasting liquid, but the manipulation had no effect. Notice that this liquid offers a simple (negative) hedonic experience, like the “annoying sounds” used in Experiment 2 of ALP. Maniadis, Tufano, and List (2011) replicated Experiment 2 of ALP and found weaker (and nonsignificant) anchoring effects. Overall our results suggest that anchoring is real—it is hard to reconcile otherwise the fact that in the WTA treatment with goods the ratios between highest and lowest quintile is always bigger than one—but that quantitatively the effect is small. Additionally our data supports the idea that anchoring goes away when bidding on objects with greater familiarity, such as lotteries."
  },
  {
    "objectID": "posts/returns-to-self-control-unemployment-edition.html",
    "href": "posts/returns-to-self-control-unemployment-edition.html",
    "title": "Returns to self control - unemployment edition",
    "section": "",
    "text": "A new paper in Psychological Science by Michael Daly and friends:\n\nChildhood Self-Control and Unemployment Throughout the Life Span: Evidence From Two British Cohort Studies\nThe capacity for self-control may underlie successful labor-force entry and job retention, particularly in times of economic uncertainty. Analyzing unemployment data from two nationally representative British cohorts (N = 16,780), we found that low self-control in childhood was associated with the emergence and persistence of unemployment across four decades. On average, a 1-SD increase in self-control was associated with a reduction in the probability of unemployment of 1.4 percentage points after adjustment for intelligence, social class, and gender. From labor-market entry to middle age, individuals with low self-control experienced 1.6 times as many months of unemployment as those with high self-control. Analysis of monthly unemployment data before and during the 1980s recession showed that individuals with low self-control experienced the greatest increases in unemployment during the recession. Our results underscore the critical role of self-control in shaping life-span trajectories of occupational success and in affecting how macroeconomic conditions affect unemployment levels in the population.\n\nHT: Stirling Behavioural Science Blog"
  },
  {
    "objectID": "posts/ridleys-the-rational-optimist.html",
    "href": "posts/ridleys-the-rational-optimist.html",
    "title": "Matt Ridley’s The Rational Optimist",
    "section": "",
    "text": "A common recommendation for an addition to my evolution and economics reading list is Matt Ridley’s The Rational Optimist: How Prosperity Evolves. These thoughts are designed, in part, to explain why I don’t plan to add it.\nThe core theme of The Rational Optimist is that exchange is the major driver of human progress. Exchange allows specialisation and division of labour, which results in people doing the tasks they do best.\nExhange also increases innovation, for which, as Ridley points out, there are cumulative returns. He captures this concept in the phrase “ideas having sex”. Ideas can be combined into new ideas, further increasing their value.\nI agree with Ridley’s general argument concerning the importance of exchange and innovation, but when Ridley puts it in the context of evolution, he creates an implication of progress that evolution does not have. For example, Ridley writes:\n\nI have tried to show that, just as sex made biological evolution cumulative, so exchange made cultural evolution cumulative and intelligence collective, and that there is therefore an inexorable tide in the affairs of men and women discernible beneath the chaos of their actions. A flood tide, not an ebb tide.\n\nThrough the book, Ridley generally takes this tide to be one way unless government gets in the way. But while biological evolution has a tendency towards greater complexity, there are extinctions and crashes. Ridley turns a general tendency into an iron law.\nThis habit manifests itself through the book, as noted in an excellent review by Bill Easterly. Suppositions and thought experiments often become statements of fact in the space of a couple of pages. I suspect many of them are right, but some arguments are a stretch.\nPart of the reason why Ridley takes the combination and accumulation of ideas to be a forward march is that the ideas which are the focus of Ridley’s discussion are those which drive technological progress. But ideas also appear as religions, urban myths, political philosophies and conspiracies. So when he writes of the Darwinian selection that occurs between ideas, he does not consider whether the idea will be good for the hosts. It may generally be, but again there is no iron law that this is the case.\nRidley also skims over human evolution, attributing change over the last thirty thousand years to cultural evolution. While he occasionally contemplates the role of more recent evolution, such as his suggestion that humans have evolved to have high oxytocin receptivity associated with trade, his exploration in that area sells it short. Willingness to trade and foresight would have been greatly rewarded in recent times.\nDespite my discomfort with the direction that Ridley places on evolution, I like The Rational Optimist. It is an entertaining read, and despite the selective use of facts and arguments that come with an advocacy book, it places a light on human progress and the benefits to trade that most people don’t appreciate. I also believe that Ridley’s optimism concerning human wellbeing is well placed over the timeframes in which we can predict. In 2050, despite climate change, the people of the world will be richer, live longer, have better health, have access to more goods and services and experience less extreme poverty, on average, than they do now. Over the last 200 years, technological progress has been very good to us.\nDespite Ridley’s claims that he is no Panglossian, he sometimes veers in that direction. Worried about overpopulation? Don’t worry, fertility is falling. Worried about underpopulation and an ageing population? Don’t worry, fertility is going back up again. His approach to the ecological effects of climate change reflect a similar tendency.\nI would have liked Ridley to discuss in more detail how he sees his expectation of increasing wilderness areas and protection for ecosystems coming to fruition. He argues that organic farming is land intensive and notes that by adopting more intensive forms of agriculture, we could return land to wilderness. If organic farming were as inefficient in its use of space as Ridley suggests, and was demanded by an increasingly rich population, what would Ridley propose we do about it? Ban it? Many of Ridley’s targets for criticism are the product of the choices of rich people, and there are going to be more rich people.\nUltimately, I also take Ridley’s plea for optimism with a grain of salt. Contrast Ridley’s plea for optimism to Julian Simon’s suggestion that it is worry that actually delivers the benefits.\n\nI have never said that we don’t need to worry about anything. We need to worry about everything, in the same sense that you had to worry whether you’d get here on time, whether there’ll be enough food in your kitchen for next week, and so on. The world needs the best effort of all of us. I’m saying that the result of all this worry – and of your constructive work, of your throwing your life into trying to do good things for the world and for other people – is that on balance you will create more than you will use in your lifetime, and you will leave the world a little better than before, on average.\n\nIf there is no iron law that everything will get better, a bit of worry is useful to increase the chance it will.\nOverall, Ridley’s case is strongest on the economics. Specialisation and trade have played a massive part in human progress. More people mean more ideas, so larger populations have generated greater technological progress. But as Ridley argued in his book The Red Queen: Sex and the Evolution of Human Nature (a book I should add to my recommended reading list), evolution is not about constant progress. Rather, it is about running as fast as you can to avoid falling behind."
  },
  {
    "objectID": "posts/robert-sapolskys-why-zebras-dont-get-ulcers.html",
    "href": "posts/robert-sapolskys-why-zebras-dont-get-ulcers.html",
    "title": "Robert Sapolsky’s Why Zebra’s Don’t Get Ulcers",
    "section": "",
    "text": "Before tackling Robert Sapolsky’s new book Behave: The Biology of Humans at Our Best and Worst, I decided to read Sapolsky’s earlier, well-regarded book Why Zebra’s Don’t Get Ulcers. I have been a fan of Sapolsky’s for some time, largely through his appearance on various podcasts. (This discussion with Sam Harrisis excellent.)\nWhy Zebra’s Don’t Get Ulcers is a wonderful book. Sapolsky is a great writer, and the science is interesting. That Sapolsky did not sugarcoat the introduction to every chapter with a cute story, as seems to be a common formula today, made the book a pleasant contrast to a lot of my recent reading.\nThe core theme of the book is that chronic stress is bad for your health. It can lead to cardiovascular disease, destroy your sleep, age you faster, and so on. The one positive (relative to common beliefs) is that stress probably doesn’t cause cancer (with the possible exception of colon cancer).\nThe story linking stress with these health problems largely revolves around the hormones that trigger the stress response. I’ll give a quick synopsis of this story, as it helps give context to some of the snippets below.\nWhen the stressor first arises, CRH (corticotropin releasing hormone) is released from the hypothalamus in the brain. CRH helps to turn on the sympathetic nervous system, with the nerve endings of the sympathetic nervous system releasing adrenaline (called epinephrine through the book). This all leads to increased heart rate, vigilance and arousal. It triggers the cessation of many bodily functions, such as digestion, repair and reproductive processes, and suppresses immunity, mobilising the body’s resources to solve the stressor at hand.\nFast forward 15 seconds, and the CRH has triggered the pituitary at the base of the brain to release ACTH (also known as corticotropin). A few minutes later the ACTH in turn triggers the release of glucocorticoids by the adrenal gland. The glucocorticoids increase the stress response, further arousing the sympathetic nervous system and raising circulating glucose. The glucocorticoids are also involved in recovery and the preparation for the next stressor. For instance, they stimulate appetite.\nMany of the costs of stress arise through the actions of these hormones when the stress is intermittent or chronic. CRH is cleared from the body a couple of minutes after the end of the stressor. It can take hours for glucocorticoids to be cleared. Continued intermittent or chronic stressors results in permanently elevated glucocorticoid levels, subjecting the body to a stress response without pause. For instance, the stress response makes the heart work harder. If you are in chronic stress, this increased work effort is constant, leading to high blood pressure, and wearing out your blood vessels.\nThere are a raft of other hormones and processes involved in the stress response, each with their own roles, costs and benefits, but this basic picture, particularly the cost of ongoing high levels of glucocorticoids, forms the books central thread.\nAlthough this sounds like a somewhat mechanical process, an important theme in the book is that the cost of stress is not just a mechanical equation, whereby stress causes a bodily response with various costs. The book balances a reductive view of biology, in which you can trace everything back to physical factors such as bacteria, viruses, genes, hormones and so on, with another view that is more psychologically grounded. In that latter view, stress can be purely psychological, affected by someone’s sense of control and so on.\nThe one part of the book that I found mildly unsatisfying was the chapter on the link between stress, poverty and health. Naturally, poverty and poor health are closely linked, with poverty associated with greater stress. Sapolsky asks about direction of causality: does poverty harm health, or does poor health lead to poverty. But (as he does in some other chapters), Sapolsky does not delve deeply into whether there might be other causal factors. I felt that that chapter deserves another book.\nMore generally, I don’t have the subject expertise to critique the book, but I highlighted a lot of interesting passages. Below is a selection.\nOn sex differences in stress response:\n\nTaylor argues convincingly that the physiology of the stress-response can be quite different in females, built around the fact that in most species, females are typically less aggressive than males, and that having dependent young often precludes the option of flight. Showing that she can match the good old boys at coming up with a snappy sound bite, Taylor suggests that rather than the female stress-response being about fight-or-flight, it’s about “tend and befriend”—taking care of her young and seeking social affiliation.\n…\nA few critics of Taylor’s influential work have pointed out that sometimes the stress-response in females can be about fight-or-flight, rather than affiliation. For example, females are certainly capable of being wildly aggressive (often in the context of protecting their young), and often sprint for their lives or for a meal (among lions, for example, females do most of the hunting). Moreover, sometimes the stress-response in males can be about affiliation rather than fight-or-flight. This can take the form of creating affiliative coalitions with other males or, in those rare monogamous species (in which males typically do a fair amount of the child care), some of the same tending and befriending behaviors as seen among females. Nevertheless, amid these criticisms, there is a widespread acceptance of the idea that the body does not respond to stress merely by preparing for aggression or escape, and that there are important gender differences in the physiology and psychology of stress.\n\nOn stress making us both eat more and less:\n\nThe official numbers are that stress makes about two-thirds of people hyperphagic (eating more) and the rest hypophagic. Weirdly, when you stress lab rats, you get the same confusing picture, where some become hyperphagic, others hypophagic. So we can conclude with scientific certainty that stress can alter appetite. Which doesn’t teach us a whole lot, since it doesn’t tell us whether there’s an increase or decrease. …\nThe confusing issue is that one of the critical hormones of the stress-response stimulates appetite, while another inhibits it. … CRH inhibits appetite, glucocorticoids do the opposite. Yet they are both hormones secreted during stress. Timing turns out to be critical. …\nSuppose that something truly stressful occurs, and a maximal signal to secrete CRH, ACTH, and glucocorticoids is initiated. If the stressor ends after, say, ten minutes, there will cumulatively be perhaps a twelve-minute burst of CRH exposure (ten minutes during the stressor, plus the seconds it takes to clear the CRH afterward) and a two-hour burst of exposure to glucocorticoids (the roughly eight minutes of secretion during the stressor plus the much longer time to clear the glucocorticoids). So the period where glucocorticoid levels are high and those of CRH are low is much longer than the period of CRH levels being high. A situation that winds up stimulating appetite. In contrast, suppose the stressor lasts for days, nonstop. In other words, days of elevated CRH and glucocorticoids, followed by a few hours of high glucocorticoids and low CRH, as the system recovers. The sort of setting where the most likely outcome is suppression of appetite. The type of stressor is key to whether the net result is hyper-or hypophagia. …\nTake some crazed, maze-running rat of a human. He sleeps through the alarm clock first thing in the morning, total panic. Calms down when it looks like the commute isn’t so bad today, maybe he won’t be late for work after all. Gets panicked all over again when the commute then turns awful. Calms down at work when it looks like the boss is away for the day and she didn’t notice he was late. Panics all over again when it becomes clear the boss is there and did notice. So it goes throughout the day. … What this first person is actually experiencing is frequent intermittent stressors. And what’s going on hormonally in that scenario? Frequent bursts of CRH release throughout the day. As a result of the slow speed at which glucocorticoids are cleared from the circulation, elevated glucocorticoid levels are close to nonstop. Guess who’s going to be scarfing up Krispy Kremes all day at work?\nSo a big reason why most of us become hyperphagic during stress is our westernized human capacity to have intermittent psychological stressors throughout the day.\n\nOn the link between the brain and immunity:\n\nThe evidence for the brain’s influence on the immune system goes back at least a century, dating to the first demonstration that if you waved an artificial rose in front of someone who is highly allergic to roses (and who didn’t know it was a fake), they’d get an allergic response. … [T]he study that probably most solidified the link between the brain and the immune system used a paradigm called conditioned immunosuppression.\nGive an animal a drug that suppresses the immune system. Along with it, provide, à la Pavlov’s experiments, a “conditioned stimulus”—for example, an artificially flavored drink, something that the animal will associate with the suppressive drug. A few days later, present the conditioned stimulus by itself—and down goes immune function. … The two researchers experimented with a strain of mice that spontaneously develop disease because of overactivity of their immune systems. Normally, the disease is controlled by treating the mice with an immunosuppressive drug. Ader and Cohen showed that by using their conditioning techniques, they could substitute the conditioned stimulus for the actual drug—and sufficiently alter immunity in these animals to extend their life spans.\n\nDoes acupuncture rely on a placebo effect?\n\n[S]cientists noted that Chinese veterinarians used acupuncture to do surgery on animals, thereby refuting the argument that the painkilling characteristic of acupuncture was one big placebo effect ascribable to cultural conditioning (no cow on earth will go along with unanesthetized surgery just because it has a heavy investment in the cultural mores of the society in which it dwells).\n\nOn the anticipatory stress when you set an early alarm:\n\nIn the study, one group of volunteers was allowed to sleep for as long as they wanted, which turned out to be until around nine in the morning. As would be expected, their stress hormone levels began to rise around eight. How might you interpret that? These folks had enough sleep, happily restored and reenergized, and by about eight in the morning, their brains knew it. Start secreting those stress hormones to prepare to end the sleep. But the second group of volunteers went to sleep at the same time but were told that they would be woken up at six in the morning. And what happened with them? At five in the morning, their stress hormone levels began to rise. This is important. Did their stress hormone levels rise three hours earlier than the other group because they needed three hours less sleep? Obviously not. … Their brains were feeling that anticipatory stress while sleeping, demonstrating that a sleeping brain is still a working brain.\n\nOn the importance of having outlets for stress, even if that outlet is someone else:\n\nAn organism is subjected to a painful stimulus, and you are interested in how great a stress-response will be triggered. The bioengineers had been all over that one, mapping the relationship between the intensity and duration of the stimulus and the response. But this time, when the painful stimulus occurs, the organism under study can reach out for its mommy and cry in her arms. Under these circumstances, this organism shows less of a stress-response. …\nTwo identical stressors with the same extent of allostatic disruption can be perceived, can be appraised differently, and the whole show changes from there. …\nThe subject of one experiment is a rat that receives mild electric shocks (roughly equivalent to the static shock you might get from scuffing your foot on a carpet). Over a series of these, the rat develops a prolonged stress-response: its heart rate and glucocorticoid secretion rate go up, for example. For convenience, we can express the long-term consequences by how likely the rat is to get an ulcer, and in this situation, the probability soars. In the next room, a different rat gets the same series of shocks—identical pattern and intensity; its allostatic balance is challenged to exactly the same extent. But this time, whenever the rat gets a shock, it can run over to a bar of wood and gnaw on it. The rat in this situation is far less likely to get an ulcer. You have given it an outlet for frustration. Other types of outlets work as well—let the stressed rat eat something, drink water, or sprint on a running wheel, and it is less likely to develop an ulcer. …\nA variant of Weiss’s experiment uncovers a special feature of the outlet-for-frustration reaction. This time, when the rat gets the identical series of electric shocks and is upset, it can run across the cage, sit next to another rat and… bite the hell out of it. Stress-induced displacement of aggression: the practice works wonders at minimizing the stressfulness of a stressor.\n\nOn how predictability can make stressors less stressful:\n\nDuring the onset of the Nazi blitzkrieg bombings of England, London was hit every night like clockwork. Lots of stress. In the suburbs the bombings were far more sporadic, occurring perhaps once a week. Fewer stressors, but much less predictability. There was a significant increase in the incidence of ulcers during that time. Who developed more ulcers? The suburban population. (As another measure of the importance of unpredictability, by the third month of the bombing, ulcer rates in all the hospitals had dropped back to normal.)\n\nOn the link between low SES and poor health - it is more about someone’s beliefs than their actual level of poverty:\n\n[T]he SES/ health gradient is not really about a distribution that bottoms out at being poor. It’s not about being poor. It’s about feeling poor, which is to say, it’s about feeling poorer than others around you. …\nInstead of just looking at the relationship between SES and health, Adler looks at what health has to do with what someone thinks and feels their SES is—their “subjective SES.” Show someone a ladder with ten rungs on it and ask them, “In society, where on this ladder would you rank yourself in terms of how well you’re doing?” Simple. First off, if people were purely accurate and rational, the answers across a group should average out to the middle of the ladder’s rungs. But cultural distortions come in—expansive, self-congratulatory European-Americans average out at higher than the middle rung (what Adler calls her Lake Wobegon Effect, where all the children are above average); in contrast, Chinese-Americans, from a culture with less chest-thumping individualism, average out to below the middle rung. …\nAmazingly, it is at least as good a predictor of these health measures as is one’s actual SES, and, in some cases, it is even better."
  },
  {
    "objectID": "posts/rosenzweigs-left-brain-right-stuff-how-leaders-make-winning-decisions.html",
    "href": "posts/rosenzweigs-left-brain-right-stuff-how-leaders-make-winning-decisions.html",
    "title": "Rosenzweig’s Left Brain, Right Stuff: How Leaders Make Winning Decisions",
    "section": "",
    "text": "I was triggered to write my recent posts on overconfidence and the illusion of control - pointing to doubts about the pervasiveness of these “biases” - by Phil Rosenzweig’s entertaining Left Brain, Right Stuff: How Leaders Make Winning Decisions. Part of the value of Rosenzweig’s book comes from his examination of some classic behavioural findings, as those recent posts show. But much of Rosenzweig’s major point concerns the application of behavioural findings to real-world decision making.\nRosenzweig’s starting point is that laboratory experiments have greatly added to our understanding about how people make decisions. By carefully controlling the setup, we are able to focus on individual factors affecting decisions and tease out where decision making might go wrong (replication crisis notwithstanding). One result of this body of work is the famous catalogue of heuristics and biases where we depart from the model of the perfectly rational decision maker.\nSome of this work has been applied with good results to areas such as public policy, finance or forecasting political and economic events. Predictable errors in how people make decisions have been demonstrated, and in some cases substantial changes in behaviour have been generated by changing the decision environment.\nBut as Rosenzweig argues  - and this is the punchline of the book - this research does not easily translate across to many areas of decision-making. Laboratory experiments typically involve choices from options that cannot be influenced, involve absolute payoffs, provide quick feedback, and are made by individuals rather than leaders. Change any of these elements, and crude applications of the laboratory findings to the outside world can go wrong. In particular, we should be careful not to compare predictions of an event with scenarios where we can influence the outcomes and will be in competition with others.\nLet’s take the first, whether outcomes can be influenced. Professional golfers believe they sink around 70% of their 6 foot putts, compared to an actual success rate closer to 55%. This is typically labelled as overconfidence and an error (although see my recent post on overconfidence).\nNow, is this irrational? Not necessarily suggests Rosenzweig, as holding that belief can influence the outcome. Thinking you are better at sinking six-foot putts than you actually are will increase the chance that you will sink them.\nIn one experiment, participants putted toward a hole that was made to look bigger or smaller by using lighting to create an optical illusion. Putting from a little less than six feet, the (amateur) participants sank almost twice as many putts when putting toward the larger looking hole. They were more likely to sink the putts when it appeared an easier task.\nThis points to the question of whether we want to ward off biases. Debiasing might be good practice if you can’t influence the outcome, but if it’s up to you to make something happen, that “bias” might be an important part of making it happen.\nMore broadly, there is evidence that positive illusions allow us to take action, cope with adversity and persevere in the face of competition. Positive people have more friends and stronger social bonds, suggesting a “healthy” person is not necessarily someone who sees the world exactly as it is.\nConfidence may also be required to lead people. If confidence is required to inspire others to succeed, it may be necessary rather than excessive. As Rosenzweig notes, getting people to believe they can perform is the supreme act of leadership.\nA similar story about the application of laboratory findings is the difference between relative and absolute payoffs. If the competition is relative, playing it safe may be guaranteed failure. The person who comes out ahead will almost always be the one who takes the bigger risk, meaning that an exaggerated level of confidence may be essential to operate in some areas - although as Rosenzweig argues, the “excessive” risk may be calculated.\nOne section of the book focuses on people starting new ventures. New businesses have failure rates of around 50% after five years (depending on your study). As a result, it is common for entrepreneurs to be described overconfident or naive. Alternatively, their “reckless ambition” and “blind faith” is praised as necessary for the broader economic benefits that flow from new business formation. (We rarely hear people lamenting that we aren’t starting enough businesses).\nRosenzweig points evidence that calls this view into question. He argues that entrepreneurs are persistent tinkerers rather than bold arrogant visionaries, and that the losses they suffer are typically constrained in the event of failure. While there are certainly some wildly overconfident entrepreneurs, closure of their business should not always be taken as failure and overconfidence as the cause. There are many types of errors - calculation, memory, motor skills, tactics etc. - and even good decisions sometimes turn out badly. Plus, as many as 92% firms close with no debt. 25% close with a profit.\nRosenzweig also notes evidence that, at least in an experimental setting, entrepreneurs enter at less than optimal rates. As noted in my recent post on overconfidence, people tend to overplace themselves relative to the rest of the population for easy tasks (e.g. most drivers believe they are above average). But for hard tasks, they underplace. In experiments by Don Moore and friends on firm entry, they found a similar effect - excess entry when the industry appeared an easy one in which to compete, but too few entered when it appeared difficult. Hubristic entrepreneurs didn’t flood into all areas, and myopia about one’s own and competing firms’ abilities appears a better explanation for what is occurring than being the result of the actions of overconfident entrepreneurs.\nThere is the occasional part of the book that falls flat with me - the section on the limitations of mathematical models and some of the story telling around massive one-off decisions - but it’s generally a fine book.\nListen to Russ Roberts interview Rosenzweig on Econtalk for a summary of some of the themes from the book."
  },
  {
    "objectID": "posts/rotten-kids-and-altruism.html",
    "href": "posts/rotten-kids-and-altruism.html",
    "title": "Rotten kids and altruism",
    "section": "",
    "text": "Gary Becker’s 1976 article Altruism, Egoism and Genetic Fitness: Economics and Sociobiology is an article that I cite often. Becker’s closing paragraph has one of the earliest statements of the benefits of combining economics and sociobiology. He wrote:\n\nI have argued that both economics and sociobiology would gain from combining the analytical techniques of economists with the techniques in population genetics, entomology, and other biological foundations of sociobiology. The preferences taken as given by economists and vaguely attributed to “human nature” or something similar - the emphasis on self-interest, altruism toward kin, social distinction, and other enduring aspects of preferences - may be largely explained by the selection over time of traits having greater genetic fitness and survival value.\n\nDespite my regular references to the article, I have not spent much time dwelling on the article’s substance, an analysis of altruism as an extension of Becker’s “rotten-kid” theorem. At the recent conference Social Decision Making: Bridging Economics and Biology, Alan Grafen addressed Becker’s paper in his plenary presentation. This triggered me to revisit the paper.\nBecker’s “rotten-kid” theorem, first laid out in a 1974 paper, works on the following premise. Suppose there is a parent who cares about the welfare of their children and will give them wealth and other gifts. Suppose further that one of the kids is rotten and he would like to harm his fellow siblings. Becker argued that if the parent redirected money to the hurt sibling when the rotten kid acted on his impulses, the rotten kid would have an incentive not to harm his siblings as it would cost him in the form of lost transfers from his parent. This induces to rotten child to act benevolently.\nIn his 1976 article, Becker extended this argument to general altruistic behaviour. Suppose there is a single altruist in a large group that cares about the welfare of all the members of the group. They are willing to transfer resources to other group members to increase their welfare. Becker argued that any egoist in the group would refrain from harming the altruist or others in the group to the extent that the harm would cut transfers to them from the altruist. This altruism exists through interaction, not kinship, so does not need the altruists or egoists to be related.\nAs well as analysing from the perspective of consumption, Becker examines the fitness of the altruist and egoist and shows that a similar result holds. The only added element that a consideration of fitness requires is that transfers may contribute to the fitness of the altruists and egoists at different rates.\nBecker provides an example where there is a single egoist and altruist and the egoist could increase his income by $800 at the cost of $5,000 to the altruist. As the altruist cares about both his own and the egoists welfare, the net result of the egoist’s action is that the egoist will receive a lower transfer from the altruist and on net, will suffer a loss due to his egoism. As the egoist will not take the selfish action, the altruist’s altruism could be said to have saved him $5,000 in income.\nThis example, while providing a useful illustration, indicates the weakness in the model. The egoist will only be restrained to the extent that the transfer lost would be more than the gain from acting selfishly. If we reversed the above the numbers, the egoist would unequivocally take the action. The equation becomes even less tenable as a group becomes large. Suppose there is one altruist in a group of 10. The size of the transfer any egoist would receive from the altruist would be expected to be very small, and unlikely to be of a comparable size to any gain from cheating. This is particularly the case if the gain for the egoist approaches the size of the loss to the altruist (the technical term for these types of situation is a corner solution).\nOne of the interesting points Becker makes is the distinction between actual and simulated altruism. In his model, altruists are actual altruists because they care about the welfare or fitness of others. Egoists are simulated altruists because it is in their own personal interest to act altruistic, despite their egoist underpinnings. From an evolutionary point of view, I am not sure this distinction exists.\nI should note, finally, that there is no shortage of other literature that addresses the “rotten-kid” theorem and this paper, and shows the restrictive assumptions that need to be applied to the model for it to work (such as by Bergstrom)."
  },
  {
    "objectID": "posts/saads-the-evolutionary-bases-of-consumption.html",
    "href": "posts/saads-the-evolutionary-bases-of-consumption.html",
    "title": "Saad’s The Evolutionary Bases of Consumption",
    "section": "",
    "text": "Over the last three to four decades, the social sciences have been subject to increasing examination under an evolutionary framework. Leading the charge into consumer and marketing theory has been Gad Saad, a pioneer of evolutionary consumer psychology who was responsible for the first evolutionary psychology papers to appear in any consumer and marketing journals.\nThe Evolutionary Bases of Consumption is Saad’s “academic book”, in which he argues that introducing an evolutionary framework into the analysis of consumer behaviour and marketing theory can provide an explanation for consumption patterns that cultural critiques struggle to provide. It is a convincing and thorough effort, and full of interesting references.\nFor someone familiar with the evolutionary psychology literature, many of Saad’s arguments do not come as a surprise. The value in Saad’s book is the detailed cataloguing of the literature behind many of the claims, rather than providing an easy front to back read. The description of it as an “academic book” is how I expect I will use it, as a source of arguments and references.\nSaad uses consumption in the broad sense that an economist would, with it encompassing almost all human activities. He maps consumption across four Darwinian modules: reproduction, survival, kin selection and reciprocation. For reproduction, mating itself is a consumption choice, while consumption also serves as a signal to mates. Consumption underpins survival of both oneself and ones kin, with humans having evolved preferences for fatty foods, sugar and sharing with relatives. Consumption also forms the basis of many reciprocal relationships.\nWhile Saad does not explicitly put the text into an economic framework, the analysis is ripe for economic applications. Consumption is shaped by preferences shaped by evolution. Saad has provided much material that could be used in examining those preferences, and the book provides many interesting hypotheses that economists might find value in testing.\nGiven the depth of the literature already in place in these fields, much of the book involves a critique of findings developed under the standard social science model, which involves assumptions such as the malleability of human nature and a belief that culture cannot be broken down into components. For example, in Saad’s discussion of advertising, it is noted that women in advertisements tend to be young and attractive, the men older than the women, the men taller than the women and so on. This pattern is present across societies, and any cultural explanation concerning trained gender roles suffers from trying to note why there are universal patterns across such seemingly diverse cultures.\nMy main critique of the book is related to a mismatch between what I expect were Saad’s objectives in writing the book and what I wanted to get out of it. For an outsider with no familiarity with the literature in the consumer behaviour and marketing fields, there was questionable value to having the existing academic approaches explained only for them to be pulled down again. It was important for Saad to address these academic approaches, reflecting years of battles to have evolutionary psychology accepted in the consumer and marketing fields. However, it means that the evolutionary framework gets less space than it should.\nThe evolutionary explanations also get less space than might be expected simply due to the state of the field in 2007. Many of the arguments made by Saad, while logical under an evolutionary framework, had not been fully tested at the time of writing. Even though the book is only five years old, it already feels like there is much new literature that could be included to bolster the arguments. Many of Saad’s conjectures have now been tested (often by himself) and the book would benefit from their inclusion. Due to this, I hope that there is second edition of this book planned. This might also provide the opportunity for the book to be more forward looking and set its own agenda without having to spend so much time dealing with the failings of the consumer and marketing fields.\nI should also note that I have not yet read Saad’s The Consuming Instinct: What Juicy Burgers, Ferraris, Pornography, and Gift Giving Reveal About Human Nature. Released last year and benefiting from much of the more recent research, I have been told it is worth the read."
  },
  {
    "objectID": "posts/sam-bowles-on-the-death-of-homo-economicus.html",
    "href": "posts/sam-bowles-on-the-death-of-homo-economicus.html",
    "title": "Sam Bowles on the death of ‘Homo Economicus’",
    "section": "",
    "text": "A few straw men are burnt along the way, but interesting all the same.\n\nI’ve posted about some of Bowles’s work before - a review of A Cooperative Species, and some thoughts on his work on income and IQ."
  },
  {
    "objectID": "posts/seabrights-the-war-of-the-sexes.html",
    "href": "posts/seabrights-the-war-of-the-sexes.html",
    "title": "Seabright’s The War of the Sexes",
    "section": "",
    "text": "When measured against his fantastic The Company of Strangers, Paul Seabright’s new book The War of the Sexes: How Conflict and Cooperation Have Shaped Men and Women from Prehistory to the Present was always facing a tough task. The War of the Sexes contains some interesting insights, and it is accessible and easy to read. However, some parts of the book feel flat.\nPart of my reaction stems from the first half of the book, titled “Prehistory”, which contains little new for someone who is well-read in evolutionary biology. It is a great introduction to sexual selection, signalling, sexual conflict and the evolutionary basis for male-female differences, but it contains few surprises for someone who might have read, say, Geoffrey Miller’s The Mating Mind and Spent, or Matt Ridley’s The Red Queen.\nOne novel thread in the first half concerns Seabright’s views on the change in the relative power of women as societies transitioned from hunter-gathering to agriculture to modern society. Seabright considers that men in agricultural societies had an increased ability to confine women relative to what they had previously. Unfortunately, we only get a taste of this idea because, as noted in reviews by Arnold Kling and John Whitfield, Seabright spends little time on the transition.\nMy equivocal feelings extended to the next two chapters where Seabright discusses whether talent or occupational preferences might explain differences in outcomes for men and women. Seabright implicitly rejects these explanations. I say implicitly as Seabright does not state this as a direct conclusion, but instead focuses on undermining assumptions that might be made for the other side. For example, he notes the greater prevalence of men at the very high end of mathematics performance and states that this should not be assumed to immediately carry over into economic value, rather than seeking to address whether it might be true. Similarly, when he notes that men and women have different occupational preferences, his major area of exploration is their respective preferences for competition. But what of winner take all professions or scalability? Or desire for power and influence? Maybe they are not the answer, but Seabright prefers to chip at particular points rather than taking on the whole issue and drawing strong conclusions.\nI expect Seabright’s approach is partly driven by a desire to be even-handed, which he is gallantly so in an area where many authors are happy to march through with bold, unsubstantiated claims. This will likely have the benefit of reaching an audience who are turned off by discussions of male-female differences. But given the mountain of literature in the area, which Seabright samples but does wholeheartedly embrace, he could have more robustly engaged with the positions of others. Seabright rarely names another author and takes them to task, which leaves parts of the book some distance from the cutting edge of debate.\nThe two most interesting chapters of the book relate to networking and charm. Men tend to have larger networks, and when it comes to positions of power such as top executive positions, those larger networks give men a material return. Seabright also considers that technological progress is raising the need for charm. When the internet can pull in thousands of responses to a job advertisement, it takes something special to get on the shortlist for that job - and networks and charm are an important part of that. Seabright hypothesises that when women take time out of the workforce to have children, their networks suffer as they are no longer on the charm offensive. When they return, everyone who remained in the workforce was busy building and strengthening networks that the woman now no longer has.\nOnce he has fingered networks as being a factor in the disparate economic outcomes, Seabright puts forward two ideas to address them. His first is that short lists for positions should be subject to quotas. This brings women into the mix for consideration when their networks may not get them through the first cut. They are then assessed on merit against the other short-listed candidates. If networks, not talent, are standing in the way of higher female representation, this may present a solution. However, as Seabright notes, if this is a good idea, companies should do this without any push from outside as it would be in their own interest to find the most talented staff. The other question is what happens to less social but talented men, who Seabright notes often suffer the same penalties from non-conformist life choices.\nSeabright’s other idea, compulsory paternity leave, is likely to be the more controversial. Seabright proposes that men should have to take the same breaks as women, which should then remove the signalling and networking cost to women of taking time off work.\nBeyond my natural apprehension at a compulsory scheme of that nature, the question at the forefront of my mind was whether networks have merit in themselves. Employers care about signalling as it contains information. And within a job, networks can lead to sales, a real benefit to the employer.\nApart from his policy recommendations, Seabright’s closing chapter has an interesting take on “the model relationship”. Given the balance of conflict and cooperation that a relationship entails, Seabright argues that is unreasonable to expect our relationships to be conflict free, consistently loving and free from jealousy. If a politician slips up, it probably says nothing about their ability to manage government. If we slip up, it may not mean that we no longer love our partner. We should not measure others or ourselves against an unrealisable ideal.\nSeabright’s assault on the model relationship is interesting, but I am not convinced that recognising its flaws would allow us to escape relationship straight jackets. Evolved preferences, however unreasonable, underlie the expectations of relationships as much as social norms."
  },
  {
    "objectID": "posts/selection-for-aggression.html",
    "href": "posts/selection-for-aggression.html",
    "title": "Selection for aggression",
    "section": "",
    "text": "Masculine appearance in a man is an indicator of their health, which in turn leads to more viable offspring. On this basis, one might assume that women prefer masculine men. However, empirical research into whether women prefer men with more masculine physical features has not shown the strong positive preference we might expect. While masculine appearance is linked to health, that masculine partner may be less interested in a long-term relationship and be unlikely to provide for the child over the long-term. In that case, a woman is likely to weigh up the benefits of a healthy child against the likelihood of provision of care by the father of that child.\nAs reported in The Economist last month, there has been an interesting exchange in the Proceedings of the Royal Society about what factors may influence this trade-off and lead to variations in preferences for masculinity. In the first paper,DeBruine et al proposed that where health in a country is poor, a woman’s preference for masculinity will be strong as it is important to have healthier offspring. Across about 4,800 women resident in 30 developed countries, they found a significant relationship between health and preference for masculinity that was robust to controls for age and wealth. However, they suggested that other factors such as violence in the society should be researched.\nBrooks et al took up this suggestion and used data from the first study to examine whether women are attracted to more masculine men in environments where there are greater benefits to dominance. Using income inequality as an indicator of violence, they found that income inequality may be a better predictor than health of preferences for masculinity. They did add the proviso, however, that health and inequality may be correlated, and that other correlates may mediate the relationship. Brooks et al also tested murder rates as an explanatory variable and found that if they included both health and murder rates in the regression, only the murder rate was significant (although income inequality was a stronger predictor than the murder rate).\nDeBruine et al responded that the finding by Brooks et al that male-male competition is responsible for the preference for masculinity was reliant on income being excluded from the murder rate and health regressions. Using new United States data, they also called into question the finding that income inequality was a better predictor than health.\nLeaving aside which analysis of the data we should prefer, the relationship suggested by Brooks et al could have some interesting dynamics. If a society is violent, Brooks et al would suggest that more masculine, dominant men would have a fitness advantage. If that were the case, they would come to form a larger part of the population. Does a larger proportion of more masculine men in a population make it inherently more violent? If so, a cycle could emerge where violent societies have a larger proportion of violent men and stay violent. Conversely, peaceful societies would stay peaceful with characteristics linked to greater propensity to violence not being rewarded.\nTo look at whether that scenario might hold, greater analysis of the strength of the preference variation would be required. Is the fitness advantage large enough that it could have dynamic implications? To analyse this we would need to expand the analysis to countries with greater extremes of violence, with the DeBruine et al dataset restricted to people in developed countries who had identified their ethnicity as white. What is the relative strength of the preference in Iraq or Sudan? Another source of data would be the level of masculinity of men within a population. Is there a higher proportion of masculine men in populations with a long history of violence?"
  },
  {
    "objectID": "posts/self-evident-but-unexplored-how-genetic-effects-vary-over-time.html",
    "href": "posts/self-evident-but-unexplored-how-genetic-effects-vary-over-time.html",
    "title": "Self evident but unexplored - how genetic effects vary over time",
    "section": "",
    "text": "A new paper in PNAS reports on how the effect of a variant of a gene called FTO varies over time. Previous research has shown that people with two copies of a particular FTO variant are on average three kilograms heavier than those with none. But this was not always the case. I’ll let Carl Zimmer provide the background:\n\nIn 1948, researchers enlisted over 5,000 people in Framingham, Mass., and began to follow their health. In 1971, the so-called Framingham Heart Study also recruited many of the children of original subjects, and in 2002, the grandchildren joined in. In addition to such data as body mass index, the researchers have been gathering information on the genes of their subjects.\nThe scientists compared Framingham subjects with the risky variant of FTO to those with the healthy variant. Over all, the scientists confirmed the longstanding finding that people with the risky FTO variant got heavier.\nPeople born before the early 1940s were not at additional risk of putting on weight if they had the risky variant of FTO. Only subjects born in later years had a greater risk. And the more recently they were born, the scientists found, the greater the gene’s effect.\nSome change in the way people lived in the late 20th century may have transformed FTO into a gene with a big impact on the risk of obesity, the researchers theorized.\n\nThis result is unsurprising. It is standard knowledge that genetic effects can vary with environment, and when it comes to factors likely to influence obesity such as diet, there have been massive changes to the environment over time. As the authors of the PNAS paper note:\n\nThis idea, that genetic effects could vary by geographic or temporal context is somewhat self-evident, yet has been relatively unexplored and raises the question of whether some association results and genetic risk estimates may be less stable than we might hope.\n\nIt is great that the authors have provided a particular example of this change. And also useful, the study provides another response to the claim genetics is not relevant to increases in obesity because there has been limited genetic change since levels of obesity took off. The high heritability of obesity has always pointed to the relevance of genetics, but this paper strengthens the case.\nIn his NY Times piece, Carl Zimmer quotes study co-author Nicholas Christakis on whether the changing role of genes may be a more general phenomenon:\n\nDr. Nicholas A. Christakis, a sociologist and physician at Yale University and a co-author of the new study, suggested that the influence of many other genes on health had waxed and waned over the past century. Reconstructing this history could drastically influence the way doctors predict disease risk. What might look like a safe version of a gene today could someday become a risk factor.\n“The thing we think is fixed may not be fixed at all,” said Dr. Christakis.\n\nI have written before about another example of the changing effect of genes over time - the effect of genes on fertility.\nBefore the demographic transition when fertility rates plunged in the world’s developed countries, the heritability of fertility was around zero. This is unsurprising as any genetic variation in fitness is quickly eliminated by natural selection.\nBut when you look at the heritability of fertility after the demographic transition, things have changed. The heritability of fertility as derived from twin studies is around 0.2 to 0.4. That is, 20 to 40 per cent of the variation in fertility is due to genetic variation. People with different genes have responded to changes in environment in different ways.\nThe non-zero heritability of fertility has some interesting implications for long-term fertility. My working paper outlines the research on the heritability of fertility in discussing these long-term implications. I have posted on the working paper here."
  },
  {
    "objectID": "posts/sexual-selection-and-entrepreneurship.html",
    "href": "posts/sexual-selection-and-entrepreneurship.html",
    "title": "Sexual selection and entrepreneurship",
    "section": "",
    "text": "From Neil Niman’s article Sexual Selection and Economic Positioning:\n\nIf economic agents earn the market clearing wage or the normal rate of return, it becomes difficult for an individual to stand out relative to one’s peers. Yet it is the ability to distinguish one’s prosperity and prospects that lead to reproductive success—a level of success that ultimately determines which genes are passed along from generation to generation. Thus, if the economic agent is to maximize the probability for survival of the gene through time, it cannot be a passive ‘price taker,’ but rather must be an active seeker of relative success. For employees, relative success often comes from actions designed to gain above average wage increases within a particular organization. In the case of entrepreneurs, market power is created by either holding back the forces of competition within an existing market, or as the result of inventive activity that ultimately results in the creation of entirely new ones.\n\nThe typical economic story of competitive markets is that only the lowest cost producers survive in the long-run. This tends to result in a uniformity in the market. However, pulling the other way are the efforts of entrepreneurs and other market players (ultimately, all of us) who are trying to distinguish themselves to achieve greater status and reproductive success. Niman describes the tension between these forces:\n\nThe market as a social institution is designed to maximize the continuation of society by ensuring the survival of its members through a process that promotes uniformity in an effort to achieve an efficient allocation of resources. Individual action on the other hand, in so far as it is motivated by the desire to maximize the survival potential of the gene, in many instances is designed to disrupt the workings of the market to ensure a non-level playing field so that relative position can be preserved or enhanced. Thus individual decision-making at the level of the survival of the gene may lead to choices that foster the achievement of social status (Castronova 2004) at the expense of social well-being.\n\nI would describe the process somewhat differently, as it is the actions of the entrepreneurs that develop the low-cost efficient solutions that will eventually spread to all market participants. It is the disruptive activity of the individual agents that makes markets so powerful. I’m also somewhat reluctant to ascribe a purpose of “the continuation of society” to markets. However, I like Niman’s characterisation of entrepreneurs being engaged in a Red Queen process where they must constantly run to stay ahead.\n\nThe successful entrepreneur, able to earn an above normal profit, gains not only a superior economic position, but also a relative advantage in the sexual selection process. However, success is often only temporary as the forces of competition work aggressively to undermine the superior position of the entrepreneur. Hence the entrepreneur is forced to constantly reinvent both his product and his firm in an effort to hold back the forces of competition or fundamentally change the rules to forestall the need to compete. This creates its own ‘Red Queen’ process where the degree of relative success depends upon how well one can constantly raise the bar for success and stay ahead of competitors who are constantly nipping at one’s heels. Therefore the same ‘Red Queen’ process that drives the need for ever newer combinations of genes to ensure the survival of the species provides the dynamic force that motivates the actions of the entrepreneur who creates new combinations that drive entire economies."
  },
  {
    "objectID": "posts/sexual-selection-conspicuous-consumption-and-economic-growth.html",
    "href": "posts/sexual-selection-conspicuous-consumption-and-economic-growth.html",
    "title": "Sexual selection, conspicuous consumption and economic growth",
    "section": "",
    "text": "Around ten years ago, I was rummaging through books in a bargain bookshop under Sydney’s Central Station when I came across a $2 copy of Geoffrey Miller’s The Mating Mind. It turned out to be a good use of my $2, as The Mating Mind is one of the most important books in shaping my thinking, and it was one of the first books I put on my economics and evolutionary biology reading list.\nMiller’s basic argument was that sexual selection shaped the human mind. Whether through runaway selection or the brain acting as a fitness indicator, female choice led to increasing mental capacity and shaped our propensity to be humorous, create art or engage in other displays of mental fitness.\nAs I read the Mating Mind, it occurred to me that the growing mental capability and tendency to display it would have direct economic effects. It would be possible to argue that sexual selection shapes economic growth. Ten years after that idea, my latest working paper (co-authored with my supervisors Boris Baer and Juerg Weber) seeks to flesh out one element of it. The working paper provides a theoretical model for the hypothesis that sexual selection and the resulting propensity to engage in conspicuous consumption has economic effects, and in particular, the desire to engage in conspicuous consumption is one of the pillars underlying the emergence of modern economic growth.\nThe concept behind the hypothesis is relatively simple. Men who signal their quality through conspicuous consumption have higher reproductive success, as the conspicuous consumption provides a reliable signal of their quality to potential mates. To engage in conspicuous consumption takes effort by the men – whether in the form of art, humour or entering the labour force to acquire resources to consume conspicuously. As the prevalence of males who conspicuously consume increases, the total level of these activities also increases. The increased participation in productive activities results in a scale effect, whereby the greater number of people involved in creative and productive activities results in increased technological progress, which underlies economic growth.\nThe evolutionary part of the model is more interesting than the economic as there is minimal feedback from the economy back into the evolutionary dynamics. The lack of feedback also means that it is not very representative of modern society, as conspicuous consumption in modern societies is of limited threat to survival. Still, the model provides a starting point and I have a few ideas to take it further.\nI have been introducing my talks on the paper with an example from Robert Frank’s Luxury Fever, in which Frank held up Patek Philippe’s Calibre 89 watch as an example of conspicuous consumption. Only four were made, with the first selling for $2.5 million and the latest auction price being over $5 million. Frank mocks the watch for its need for a tourbillon, a mechanism to account for the earth’s rotation, when his cheap quartz watch does not require such a mechanism, as gravity does not affect the vibrations of the crystal.\nNow consider the innovation and thought that went into the Patek Philippe watch, including that tourbillon. This watch has 1728 components, gives you the date of Easter each year, and unlike most mechanical watches, will not record the years 2100, 2200 and 2300 as leap years while still recording 2400 as one (as per the order of Pope Gregory XIII in 1582). If you look at Patek Philippe’s list of patents, you get a feel for the innovation involved in making watches for what is largely conspicuous consumption.\nWhen you also consider the innovation undertaken by the potential buyers as they seek to amass the wealth necessary to obtain such a watch, the positive angle to conspicuous consumption grows. As a result, curbing conspicuous consumption may have costs (although, I still prefer taxing consumption to income). If nothing else, we should appreciate the historical role of conspicuous consumption – competition for sexual partners is a driving force for many productive activities, and one generation’s conspicuous consumption is another generation’s day-to-day tool."
  },
  {
    "objectID": "posts/shaping-the-brain-and-humans-as-complex-systems.html",
    "href": "posts/shaping-the-brain-and-humans-as-complex-systems.html",
    "title": "Shaping the brain and humans as complex systems",
    "section": "",
    "text": "I linked to this interview with Robert Sapolsky a couple of weeks ago, but after glancing through it again, I felt it worth highlighting two paragraphs (both for your interest and so I can find them again). First, on the evolutionary purpose of the teenage brain:\n\nWhat I’ve been thinking might actually be going on is that adolescence is something unavoidable that emerges not because it’s so cool and adaptive, but because the adaptive thing is wait a long, long time before you have fully wired up your frontal cortex. Why might that be the case? Alright, so we’re born with our genome, the combination of your mother and father’s genes, that wind up in that first fertilized egg and that’s it. That’s your genetic legacy. Every cell in your body is destined to have that exact same genome. That turns out not to be true in all sorts of interesting ways, but what that also means is that when you’re thinking about what genes have to do with the brain behavior, by definition critically, if the frontal cortex is the last part of the brain to develop it’s the part of the brain least shaped by genes, and most sculpted by the environment and experience. And I think basically the only way you can have a species that is as complex and socially resilient and socially context dependent and all those amazing things we do, the only way you can pull that off is to have a frontal cortex whose development just bears the imprint of everything you experienced along the way—in effect, that’s been freed from whatever extent the genes are deterministic, which is not very. I think ironically what the evolution of the frontal cortex has been about is genetic evolution to free it as much as possible from the straight jacket of genes.\n\nSecond, on reductionism in neurobiology:\n\n[R]eductionism doesn’t actually tell you a whole lot about how this stuff works. I mean reductionism is perfect for like telling you why your clock is broken. What you do is you break it down to its component parts. You find the part that’s got a tooth missing from the gear. I guess there’s not a clock on earth that works this way anymore, but your Renaissance clock. You fix the missing tooth, you put it back, you add the pieces back together and it works. The way to understand a complicated system is to understand its component parts. The way in which that steps away from the ideology is the component parts of the genes and the nerve transmitters and the hormones and the early experience. Okay, so that’s a more sophisticated version of reductionism. You got to be reductive about lots of different domains. But nonetheless, even that more multidisciplinary version of reductionism isn’t going to work because that’s not how complex systems work and humans are a complex system. You got these emergent non-linear chaotic properties. What’s that another way of saying? If you knew every individual’s genome and exactly which gene was active at which point, are you going to be able to predict who’s going to do what next? Absolutely not. If you added in knowing the levels of every hormone in their body at that point, if you added in… it doesn’t work that way. The reductionism breaks down because the reductionism breaks down in the same way that like a cloud that isn’t producing enough rain during a drought or something, the solution isn’t to study half the cloud and then get a research grant to study a quarter of the cloud and smaller, smaller pieces and finally understand the reductive basis of the non-rain and add it up together. That’s not how clouds work when they don’t rain. Humans are more like clouds than they are like clocks. We’re not reductive in that way, which is the case for any complex system.\n\nAnd if you haven’t read the full interview, do it."
  },
  {
    "objectID": "posts/shrinking-brains-and-intelligence.html",
    "href": "posts/shrinking-brains-and-intelligence.html",
    "title": "Shrinking brains and intelligence",
    "section": "",
    "text": "Average human brain size has declined for 20,000 years, a stark contrast to the steady increase over the preceding millions. While I would argue that translation of this reduced brain size into lower intelligence would have a negative result, I am not convinced that intelligence has dropped over that time.\nEighteen months ago, Discover magazine published an article by Kathleen McAuliffe on the shrinking brain phenomenon and what this means for intelligence. On the idiocracy side of the debate was David Geary and Drew Bailey:\n\n[T]he Missouri team used population density as a proxy for social complexity, reasoning that when more people are concentrated in a geographic region, trade springs up between groups, there is greater division of labor, the gathering of food becomes more efficient, and interactions among individuals become richer and more varied.\nBailey and Geary found population density did indeed track closely with brain size, but in a surprising way. When population numbers were low, as was the case for most of our evolution, the cranium kept getting bigger. But as population went from sparse to dense in a given area, cranial size declined, highlighted by a sudden 3 to 4 percent drop in EQ starting around 15,000 to 10,000 years ago. “We saw that trend in Europe, China, Africa, Malaysia—everywhere we looked,” Geary says.\nThe observation led the researchers to a radical conclusion: As complex societies emerged, the brain became smaller because people did not have to be as smart to stay alive. As Geary explains, individuals who would not have been able to survive by their wits alone could scrape by with the help of others—supported, as it were, by the first social safety nets.\n\nIn a Malthusian world where incomes hover around subsistence levels, population density is proportional to the level of technology available to the population. Population density is not something that simply arises, but it is enabled by the technology which allows for the required level of subsistence to be produced for a larger population in a given area. As a result, I am not convinced by Bailey and Geary’s conclusion. If we need a higher level of technology to support higher population density, and average intelligence of the population is related to technology, you would expect higher intelligence populations to have higher population densities.\nAs for their concerns about the dysgenics of the first social safety nets (I wonder what their views are about the modern ones), I am not convinced that higher populations delivered such survival benefits to the dumb. With a larger, more dense society comes trade, specialisation and more complex social arrangements, all of which are likely to favour those with higher intelligence. Plus, if there was such a social net, why wasn’t it also supporting those huge brained people who were burning too much energy to survive? If we throw sexual selection into the mix, the benefits to intelligence appear larger.\nJohn Hawks puts a different spin on Bailey and Geary’s data, which may reconcile the Malthusian assumptions with the brain size observations:\n\nThe organ is such a glutton for fuel, he says, that it gobbles up 20 percent of all the calories we consume. “So although a bigger brain can presumably carry out more functions, it takes longer to develop and it uses more energy.” Brain size probably depends on how those opposing forces play out.\nThe optimal solution to the problem, he suggests, “is a brain that yields the most intelligence for the least energy.” For evolution to deliver up such a product, Hawks admits, would probably require several rare beneficial mutations—a seeming long shot. But a boom in the human population between 20,000 and 10,000 years ago greatly improved the odds of such a fortuitous development. He cites a central tenet of population genetics: The more individuals, the bigger the gene pool, and the greater the chance for an unusual advantageous mutation to happen. …\nHawks notes that such changes would be consistent with the many brain-related DNA mutations seen over the past 20 millennia. He speculates that the organ’s wiring pattern became more streamlined, the neurochemistry shifted, or perhaps both happened in tandem to boost our cognitive ability.\n\nWhile bigger brains lead to greater intelligence (there is a positive correlation in modern populations), intelligence is not all about size. More efficient arrangements may result in a better energy-intelligence trade-off, delivering more efficient, smaller but smarter brains. The continued strong selection of alleles associated with brain size is also suggestive of this.\nAs an end note, brain size has been back on an increasing trend for the last 200 years, and there is some suggestion that this is not all environmental:\n\nSince evolution does not happen overnight, one would assume this sudden shift (much like the increase in height and weight) is unrelated to genetic adaptations. Hawks, for instance, says the explanation is “mostly nutrition.” Jantz agrees but still thinks the trend has “an evolutionary component because the forces of natural selection have changed so radically in the last 200 years.” His theory: In earlier periods, when famine was more common, people with unusually large brains would have been at greater peril of starving to death because of gray matter’s prodigious energy requirements. But with the unprecedented abundance of food in more recent times, those selective forces have relaxed, reducing the evolutionary cost of a large brain.\n\nOf course, as Geary may point out, the survival cost of a small brain’s lack of intellectual horsepower is also well down."
  },
  {
    "objectID": "posts/simons-models-of-my-life.html",
    "href": "posts/simons-models-of-my-life.html",
    "title": "Simon’s Models of My Life",
    "section": "",
    "text": "Herbert Simon’s autobiography is probably not the best introduction to his work (I would suggest other starting points), but below are two paragraphs that caught my eye.\nFirst, describing Chapter 15 of Models of Man:\n\nBracketing satisficing with Darwinian may appear contradictory, for evolutionists sometimes talk about survival of the fittest. But in fact, natural selection only predicts that survivors will be fit enough, that is, fitter than their losing competitors; it postulates satisficing, not optimizing. The paper showed how relatively simple choice mechanisms could enable an organism, searching through its life maze, to survive in an uncertain environment in which several incommensurable needs had to be met. It depicted a procedural rationality for organisms that was squarely based on satisficing rather than optimizing.\n\nOn whether the natural sciences are exact sciences:\n\nAs soon as they have to cope with the messiness of real world problems, as contrasted with the sometimes neat and simple laboratory problems, they become at least as inexact as the social sciences (which only rarely can retreat to the laboratory). I no longer have any patience with natural scientists who imagine that they have some kind of patent on exactness which they have not licensed to their social science brethren. …\nThe true line is not between “hard” natural science and “soft” social sciences, but between precise science limited to highly abstract and simple phenomena in the laboratory and inexact science and technology dealing with complex problems in the real world."
  },
  {
    "objectID": "posts/six-signs-youre-reading-good-criticism-of-economics.html",
    "href": "posts/six-signs-youre-reading-good-criticism-of-economics.html",
    "title": "Six signs you’re reading good criticism of economics",
    "section": "",
    "text": "After reading Chris Auld’s 18 signs you’re reading bad criticism of economics (I agree with most, although by viewing them as “signs” with exceptions), I was thinking about what signs someone should someone look for in a decent critique. So, here are my thoughts. Some are twists on Auld’s points, or could easily be turned into additional reasons that a criticism is likely bad, but they’re a useful initial filter.\n1. The criticism is by an economist\nThere are a lot of exceptions to this rule in both directions (see Auld’s sign #18). Some of the best and most groundbreaking critiques of particular areas of economics have come from outside (just look at the list of Economics Nobel winners who aren’t economists). But as an initial sign, knowing whether the critic is an economist does a pretty good job, particularly for any screed that declares “here’s what wrong in economics”. There simply seems to be a base level of knowledge of economics required to give a decent critique. That knowledge is so rare outside of economics.\nTake the common critique that all economists assume that people are self-interested rational automatons. There is a mountain of work in economics that relaxes this assumption, and most non-economists (and a few economists) have almost no awareness of it.\nAnd sorry economists, this sign also works the other way.  There have been some great economic analyses of other fields, but if you’re reading an economic critique of another academic field - “if only they were more statistically rigorous like us” - there’s a good chance it’s breaching the equivalent 18 signs for that field.\n2. They know the difference between academic economists, economic consultants, business, bureaucrats and politicians\nWhen a criticism of economics actually identifies work of an academic economist and then seeks to pull it apart, that’s normally a good sign. If they quote Alan Greenspan, Ron Paul, Ayn Rand, a random McKinsey consultant or Jamie Dimon before tearing apart the current state of economics, its unlikely they understand the current state of economics.\nOf course, if the critic does follow this rule, there is a good chance that the critique is not actually of economics but rather of some hack’s use of economics for their particular means. “Our stadium will produce 20,000 jobs!” We don’t hold evolutionary biologists responsible for eugenics do we?\n3. They distinguish “good for business” and “good economics” \nWhat is good for (existing) business is not necessarily good economic policy. There aren’t many economists who are happy about how the major financial players in the GFC emerged relatively unscathed. And conversely, opposing certain types of banking regulation does not mean that an economist simply wants to protect the banks. What’s good for business and good economics may align, but sometimes it doesn’t.\nWhen an economic critique can distinguish between the two, they tend to do a better job at addressing the underlying argument made by the economist. The assumption that the economist is a “corporate shill” is never a good start for a rational debate (even if they are a shill).\n4. They criticise a particular, clearly defined area or use of economics \nEconomics is full of ideas and sub-fields that could do with a good beating. A critique that takes on a particular idea or field has a chance of hitting on the core issues. Believe that economists should be better macroeconomic forecasters? Take on the economists who research macroeconomic forecasting, point out what is wrong with their approach and suggest how it could be improved. Don’t expand a single issue to all of economics, unless you have a coherent reason as to why the failure of macroeconomic forecasters implies larger problems (as John Quiggin tries to do).\nSimilarly, there is a big difference between generally criticising economics for the use of rational actors in a model, and criticising an economist using rational actors in a particular model or context. Much of the time, a rational self-interested actor is a simple assumption that does a good job. However, there are some contexts where it is inappropriate and should be called as such.\n5. They criticise a specific economist\nThis is a twist of sign 4. Of course, if that person happens to be Adam Smith, Friedrich Hayek or Milton Friedman, then this sign no longer applies. Chances are that they’ll go on to breach sign 4 by implying that economics is built purely on the back of the alleged problem child they have identified (and likely misinterpreted and possibly never even read).\n 6. They recognise that economics and values cannot be untangled, no matter who is doing the analysis\nEconomics draws heat as it deals with areas where people have strong opinions, regardless of whether they are an economist. And economists can be biased. We can push our favoured theories while ignoring evidence to the contrary, always supporting “our team”. But to claim that “if only evolutionary biologists or anthropologists or physicists were pulling the levers we’d hit on the rights answers” suggests a lack  of self-awareness (which is unfortunately also lacking for many economists).\n\nIf the above signs are present, it’s more likely to be a coherent critique that will draw an interesting response instead of turning into a higher-level flame war that these types of discussion usually trigger. And of course, there is the possibility that you’ll get a poor defence from the economist, but the signs of a poor defence will have to be the subject of another list (Unlearningecon has put together a list that could serve this purpose, of which I’d agree with around two-thirds if I read them sympathetically and as “signs”).\nHaving said the above, it pays to be humble. One day one of these screed writers might trigger the overthrow a dominant economic paradigm from outside, and people will laugh at what we today call economics. It’s worth looking past the signs that you are reading a bad criticism to see if there might be a charitable way of reading the argument and taking it on board."
  },
  {
    "objectID": "posts/social-decision-making-bridging-economics-and-biology.html",
    "href": "posts/social-decision-making-bridging-economics-and-biology.html",
    "title": "Social Decision Making: Bridging Economics and Biology",
    "section": "",
    "text": "I am at the Social Decision Making: Bridging Economics and Biology conference (the abstracts of which can be downloaded here). As the name suggests, the basic idea behind the conference is to pull together economists and evolutionary biologists to develop new collaborations and examine how their respective approaches to social decision-making might be useful to each other.\nSo far, the most surprising observation (to me) is how many of the evolutionary biologists are working in the behavioural economics area and conducting experiments with human subjects. That is certainly a good thing, as behavioural economics could do with an evolutionary framework.\nDuring the opening presentations there was a slight flavour of “bash the economists”, but the targets have generally been fair enough - I just wish that individual economists’ positions would not be taken to be the entire profession’s position. When you consider the evolutionary approaches of the economists whose work has been mentioned, such as Gary Becker or Herbet Gintis, they are poles apart and not necessarily indicative of modern approaches.\nThe other thing that stands out is that there is little discussion of what economists can offer evolutionary biologists. As is a central theme of this blog, I believe that economics could be much improved by considering humans as evolved (evolving) animals. But what insights by economists should evolutionary biologists be considering in their work that they aren’t now? Once you move past methodologies (such as the raft of experimental experience in behavioural economics), I am not sure that I can name a central insight that might have a significant effect on evolutionary biology.\nI’m not sure how much I’ll be posting during the rest of the conference or over the following Easter break, but I’ll be posting on the conference content over the next few weeks."
  },
  {
    "objectID": "posts/socioeconomic-status-versus-fitness.html",
    "href": "posts/socioeconomic-status-versus-fitness.html",
    "title": "Socioeconomic status versus fitness",
    "section": "",
    "text": "One common explanation for fertility declines over the last 200 years is that parents have shifted to investing in quality of children, rather than quantity. What is often not made clear is that this quality-quantity trade-off has two dimensions. The first trade-off relates to socioeconomic status (SES), with greater numbers of children resulting in less investment in education and resource dilution. The second trade-off relates to fitness, as a short-term increase in children may reduce fertility in future generations.\nThese two dimensions may not line up in modern settings. If an increased investment in quality increases both SES and fitness, there must be some point at which that investment pays off through more children. If not, despite the socioeconomic gains, the trade-off will not be delivering increased fitness.\nA new article in the Proceedings of the Royal Society B: Biological Sciences by Goodman and colleagues presents some empirical evidence on this issue. Analysing a cohort of 14,000 Swedes born between 1915 and 1929 and their descendants, they showed that low fertility and high SES for parents increased child SES (as measured by school marks, university attendance and income) for as much as four generations into the future. However, this SES did not translate into higher fitness.\nThis result lends support to models which suggest that fertility is reduced for socioeconomic gains, but puts into doubt biologically based models in which reduced fertility is an adaptive trade-off for long-run fitness. The high SES people in the study had lower fertility than optimal, so the strategy pursued by these people is maladaptive.\nOne way in which SES affected fertility was the time between generations. While most of the low-SES people in the sample had four generations of children since the first study members were born, the higher SES members often had only three. In a growing population, you can have the same number of children per generation but lower fitness, as the shorter generation time of competitors allows them to increase in population more quickly.\nThe study did not analyse in-depth whether SES transmission is due to genetic inheritance, parental decisions to invest in quality over quantity, or resource transfers such as inheritance. As I am skeptical about the returns to investments in quality above a basic threshold (particularly in a developed country such as Sweden), I lean towards the genetic and resource transfer explanations."
  },
  {
    "objectID": "posts/some-podcast-recommendations.html",
    "href": "posts/some-podcast-recommendations.html",
    "title": "Some podcast recommendations",
    "section": "",
    "text": "What I’ve been listening to recently:\n\nShane Parrish’s blog Farnam Street is a favourite of mine. His podcast The Knowledge Project is also worth a listen. I recommend the episodes featuring Michael Mauboussin (1 and 2), Rory Sutherland (if you’ve seen Rory speak before, the half hour gap between Shane’s first attempt to wind up the conversation and the end of the episode will come as no surprise), Susan Cain (my review of Quiet), Adam Grant (I disagree with his perspective on the replication crisis) and Chris Voss (I recommend Voss’s book, Never Split the Difference).\nI turned to Sam Harris’s podcast Waking Up after reading the book of the same title (which I need to read again if I am going to write anything about it). There are plenty of episodes worth listening to, including interviews with David Krakauer of the Santa Fe Institute, Stuart Russell on the threats of AI, Tristan Harris on what technology is doing to us, and Max Tegmark on the future of intelligence. I’ve generally avoided the episodes on politics, free speech and the culture wars.\nRuss Roberts’s Econtalk is always worth listening to. I particularly enjoyed the episode with Tim O’Reilly. Here’s one great section (in turn pulling from O’Reilly’s book:\n\n\nRuss Roberts: You say,\nIf you think with the 20th century factory mindset, you might believe that the tens of thousands of software engineers in companies like Google, Amazon, and Facebook spend their days grinding out products just like their industrial forebears, only today they are producing software rather than physical goods. If instead you step back and view these companies with a 21st century mindset, you realize that a large part of what they do–delivering search results, news and information, social network status updates, relevant products for purchase, drivers on demand–is done by software programs and algorithms. These programs are workers; and the programmers who create them are their managers. Each day, these managers take in feedback about their workers’ performance, as measured in real-time data from the marketplace. And if necessary, they give feedback to the workers in the form of minor tweaks and updates to the program or the algorithm.\nEnd of quote. … And, as you point out a number of times in the book, and as you just said: It’s hard to talk about where the human and where the technology start and end. They are just intertwined. They are augmenting each other.\nTim O’Reilly: Yeah. And you pick a key word here, which is ‘augmenting.’ … just as the technology is the 18th, 19th, and 20th century were about augmenting our muscles, from the 20th into the 21st century we were really about augmenting our minds. And, you augment in a word to increase our capabilities.\n\n\nFrank Conway’s Economic Rockstar. I’ve only listened to a couple of episodes, but the conversation with Greg Davies is excellent. After listening to the episode, watch the below.\n\nhttps://youtu.be/WOT00xBTy7s"
  },
  {
    "objectID": "posts/sports-stars-born-early-in-the-year.html",
    "href": "posts/sports-stars-born-early-in-the-year.html",
    "title": "Sports stars born early in the year",
    "section": "",
    "text": "One of the more interesting pieces of evidence in the nature or nurture debate is the that athletes on professional sports teams tend to have a higher proportion of players born early in the year. Malcolm Gladwell documented this phenomenon for ice hockey players in his book Outliers. The basic idea is that when young, those born earlier in the year are bigger and faster than their peers and, as a result, tend to get more game time, are selected for further development and so on. This ongoing cycle amplifies the original difference. (The precise time of the year can change if the age cut-off is based on another date, but the same concept still holds.)\nThis morning I went through the current player lists of the Australian Rules Football teams and saw a similar result. Around 30 per cent more players were born in the first quarter of the year than the final quarter (although as a quick google discovered, I was not the firstto have done this). Still, there is nothing like playing with the data and seeing it with your own eyes.\nThis is clearly evidence in favour of the nurture side of the debate. There are also a few possible policy responses. Gladwell talks of setting up parallel sports leagues for children, with one having a different birthday cut-off (say, mid year). Another suggestion might be to spend less time trying to pick future stars at such a young age. Within a single country, there is probably not much benefit to these policies, but for international sports, could this give a country an edge by ensuring that talented individuals born later in the year have a chance?\nThere is some evidence for this effect in academic pursuits, with the difference in test results between children born earlier and later in the year enough to differentiate who might be selected in a gifted academic program. The longer term effects of this is not so clear however. What would be a good dataset to test whether someone born later in the year is under-represented in an academic field?\nTo me, whether this all matters depends on whether this effect extends beyond the top end of the bell curve. Whether a few dozen professional sports players make it to the big league or not is of no great social consequence. If this effect was throughout all levels of society, with those born later in the year being invested in less than the early birds, there may be some serious misallocation of resources and sub-optimal use of human resources.\nI have one further hypothesis regarding this scenario: the long term effect is larger in sport than in other areas. This is based on two ideas. Firstly, it comes from some scepticism on my part about the benefits of “gifted child” programs and the like. Secondly, in the area of sports, if you are shorter or slower, you tend not to be picked for the sports team or not given the ball. In academic areas, you still have to do your math problems, sit through the test and undergo most of the academic training that the “gifted children” do.  I’m not fully convinced of my hypothesis, but if there was a dataset to examine the birth effect for academic related outcomes in adulthood, it would be worth a look."
  },
  {
    "objectID": "posts/status-signalling-and-the-handicap-principle.html",
    "href": "posts/status-signalling-and-the-handicap-principle.html",
    "title": "Status, signalling and the handicap principle",
    "section": "",
    "text": "Robin Hanson writes:\n\nZahavi’s seminal book on animal signaling tells how certain birds look high status by forcing food down the throat of other birds, who thereby seem low status. While this “altruism” does help low status birds survive, they rightly resent it, as their status loss outweighs their food gain.\nIn our society, “sympathy” by high status folks for low status folks usually functions similarly — it affirms their high status while giving little net benefit to the low status.\n\nI would frame the babbler (the type of bird) example slightly differently. The babblers are not solely trying to appear high-status by dumping on low-status birds - status is determined through these activities by providing an accurate signal of their quality to their potential mates. Some of the relevant passages from Zahavi’s The Handicap Principle: A Missing Piece of Darwin’s Puzzle are illustrative:\n\nNot only are babblers, by all accounts, at least as altruistic as other group-breeding birds; close, detailed observation shows that babblers actually compete with one another for the “right” to be altruistic. Instead of expecting their partners to return tit for tat, they attempt to prevent them from doing their share. The theory of reciprocal altruism cannot explain why individuals compete for the chance to help other members of the group, let alone why they prevent others from helping in return. …\nWhen there is a large difference in age between the feeder and the one being fed, the latter is sometimes eager to accept the food, and may even approach the feeder, making begging sounds. But in some 15 percent of cases, babblers try to avoid being fed by another bird. When a guard sees a higher-ranking comrade approaching it with food, to feed it and replace it on sentinel duty, it may sacrifice its guard post to avoid being fed. In other cases, the sentinel may close its beak tightly and refuse to accept the food being offered, even though it is hungry; it eagerly accepts a dry crumb of bread from us immediately after refusing a juicy insect from another babbler.\n\nSympathy of high-status people towards those of lower-status has little influence on their rank relative to those to whom they show sympathy. In the same way, derision of high-status people by low-status people does not change their relative pecking order. Sympathy might be a signal to obtain status, but the competition is not between the sympathiser and the recipient of their sympathy.\nConversely, the forcing of food and rejection of it by other babblers has a direct effect on rank in the eyes of the potential mate who receives the signal - for both the feeder and the bird that is fed. It is also likely to be a reliable signal. Giving food away is costly and can only be done by a babbler with the spare resources. Similarly, most birds accept the food, as rejecting it has a significant cost.\nThe giving and receipt of food by babblers is more akin to charity. Charitable giving is costly and a reliable signal of wealth. Competition to buy an artwork at a charity auction is similar to the competition between babblers to give food.\nOf course, the charity auction example lacks the lower status person as an unwilling recipient. Examples with that feature might include insisting on paying a restaurant bill, or that someone accept $50 in a time of need. “I insist”. I expect there are many government interventions that also have this characteristic.\nAs a side note and following my recent post on biologists being important players in economics, I rate The Handicap Principle: A Missing Piece of Darwin’s Puzzle as one of the best economics books."
  },
  {
    "objectID": "posts/subsidise-the-rich-for-the-good-of-our-species.html",
    "href": "posts/subsidise-the-rich-for-the-good-of-our-species.html",
    "title": "Subsidise the rich for the good of our species",
    "section": "",
    "text": "From Michael Shermer’s review of Robert Frank’s The Darwin Economy:\n\n[S]exual selection may very well account for most of characteristics that we so admire about our species: art, music, humor, literature, poetry, fashion, dance and, more generally, creativity and intelligence. Science itself may be a byproduct of the cognitive process of trying to impress others in order to gain status and mates by making breakthrough discoveries and formulating important new theories. …\nThus, contrary to what Frank argues, a viable case can be made that the evolutionary arms races he so detests—men’s suits, women’s high heels, McMansion homes, and elaborate coming of age parties—are products of a larger system that drives our species to be so successful. By carrying out the biological analogy into political economy, if anything we should be rewarding the most ostentatious displays of power, prestige, wealth, creativity, health, vigor and intelligence with tax breaks and even subsidies! At the very least one could argue that a consumption tax on the rich could very well backfire and reduce the reproductive success of our species by attenuating the creative productivity that has given us so much of our culture that we cherish.\n\nFrank’s proposal of a consumption tax to curb conspicuous consumption does not change the relative ranking of the signallers. As Shermer notes, some signalling is socially optimal, but an arms race that does not change rank order is not. Signals are honest by virtue of their cost. By adding a cost in the form of the tax, a smaller signal will require as much creative effort and will be as hard to fake. The assortment based on signalling will continue.\nSo does the activity itself – the conspicuous consumption – have benefits through requiring innovation or creativity. It may be possible to make this argument if we assume that innovation occurs in the race to produce goods to consume. As Porsche adds new features to its car each year, or as Rolex develops more complex watches with higher precision (a precision far beyond that which any human cares), what are the spill over effects?\nThen there is the question as to whether conspicuous consumption is actually sending the signals we are intending to send. As asked by Geoffrey Miller in Spent, are our consumption instincts tuned to our current environment? Are signals using our intelligence and creativity a forgotten but better signal of our qualities? Regardless of the tax, displays of intelligence and creativity will still be freely available.\nWhile Shermer takes on Frank’s proposal to curb conspicuous consumption, unfortunately Shermer does not address some of the more pernicious effects of the competition for positional goods. In the introduction to his review, Shermer concedes he was relieved when helmets in cycling races were made compulsory. I would have liked to have heard his views on collective action problems such as competition for positional goods driving down worker demand for safety standards, instead of the softer target of the size of government.\nHowever, Shermer’s focus on the size of government reveals the shortcoming in Frank’s pitch to libertarians. Frank does not want to decrease government spending or coercion. Throughout The Darwin Economy, Frank laments drops in government expenditure and the deteriorating social infrastructure that results.\nA better pitch to libertarians would have involved a shift in the base of taxation, without associated increases. A consumption tax makes sense, with Milton Friedman among those who have conceded that it is a preferred way to raise money. A shift of the taxation base towards consumption, carbon, congestion and the like (Pigovian taxes anyone?) and away from labour and capital could appeal to libertarians. But by also suggesting we need more government expenditure, Frank has lost most libertarians’ interest and dropped us into the usual debate on whether government should be bigger or smaller. When Frank wants to change and shrink the basis of taxation, he might gain some libertarians’ interest.\nThere were some other interesting points to Shermer’s review. Shermer poses the argument that in considering the evolutionary features of the economy, we should be thinking of corporations as species. This reflects analysis by Paul Ormerod, who showed that extinction patterns of corporations were similar to that of species. Ormerod’s analysis provides an argument that there is more blind luck behind corporation success than skilful management.\nThis level of analysis provides some interesting outcomes, but using the corporation as the unit of analysis has limitations. If we want to understand why an investment bank behaves the way it does, should we look at the bank as a whole, or the actions of the highly incentivised individuals within it? In the same way, should we examine the actions of government by looking at government as a single entity, or are we better off analysing the incentives of those within it? Public choice theory has already provided the answer to that question.\nAs a final note, Shermer also pitches his long stated argument that both evolution and economics involve bottom up systems, and as a result, we should not feel the need for a top down economic designer. Shermer writes:\n\nRobert Frank is not a socialist and yet the design conceit is there nonetheless. Even when gussied up in economic jargon with Darwinian overtones, hints of the totalitarian mind from millennia past creep into our thoughts and reach for the controls. …. It’s counterintuitive to think bottom up instead of top down. It is why so many people struggle to truly grasp the deep meaning of evolutionary theory, and it is why so many people fail to see that economic order is the product not of human design but of human action.\n\nJust because something is a bottom up system does not imply that you cannot or do not want to have rules imposed from the top. What evolution show us is that bottom up systems do not always deliver good results, and as Shermer rightly notes, that top down design is fraught with difficulty. Looking at some of Shermer’s other writings, Shermer is not short of top down design wishes in his libertarian world:\n• The rule of law. • Property rights. • Economic stability through a secure and trustworthy banking and monetary system. • A reliable infrastructure and the freedom to move about the country. • Mass education. • A robust military for protection of our liberties from attacks by other states. • A potent police for protection of our freedoms from attacks by other people within the state. • A viable legislative system for establishing fair and just laws. • An effective judicial system for the equitable enforcement of those fair and just laws.\nWhile I generally agree with where Shermer draws the boundary, it is apparent that the question is not whether there should be top down interference in an economy, but rather what the nature of that interference should be."
  },
  {
    "objectID": "posts/take-the-evolutionary-economics-pill.html",
    "href": "posts/take-the-evolutionary-economics-pill.html",
    "title": "Take the evolutionary economics pill",
    "section": "",
    "text": "Frances Wooley writes:\n\nEconomists’ policy recommendations - our ideas about which policies enhance economic efficiency and which ones detract from efficiency - are all based on the idea that individuals know what’s best for themselves. …\nBut if people’s demands are just a product of framing, salience, and the public prominence of hurty-elbow syndrome, how can we use them to infer the marginal benefits to consumers of consuming health care? How can we make statements like ‘the marginal benefits of health care are less than the marginal costs’? …\nIf people’s choices are not a reliable guide to their well-being, you have to turn to something else. Ask people how happy they are and measure well-being in terms of happiness. Evaluate health care spending by looking at objective measures of health, such as mortality, morbidity, or survival rates. Chuck out the entire elegant theoretical framework of welfare economics.\nThat idea has me, for one, reaching for the blue pill - after all, people aren’t stupid, so standard economic analysis isn’t a bad approximation of the real world, is it?\nBut I can’t find one. I can’t get all of that behavioural stuff outside of my head.\nI want a purple pill - a merging of the red and the blue - that would allow me to merge behavioural insights into a coherent model of economic behaviour.\n(Evolutionary economics - and other research programs that explain why humans behave the way they do - might have some promise as a purple pill).\n\nBehavioural economics is going to continue to be attacked as a discipline until more of the results are placed in a conceptual framework. One of my favourite wikipedia pages is the list of cognitive biases - yet the nature of this list hints at the underlying failure to develop the framework in which they fit.\nHT: Arnold Kling"
  },
  {
    "objectID": "posts/teacher-expectations-and-self-fulfilling-prophesies.html",
    "href": "posts/teacher-expectations-and-self-fulfilling-prophesies.html",
    "title": "Teacher expectations and self-fulfilling prophesies",
    "section": "",
    "text": "I first came across the idea of teacher expectations turning into self-fulfilling prophesies more than a decade ago, in Steven Covey’s The 7 Habits of Highly Effective People:\n\nOne of the classic stories in the field of self-fulfilling prophecies is of a computer in England that was accidently programmed incorrectly. In academic terms, it labeled a class of “bright” kids “dumb” kids and a class of supposedly “dumb” kids “bright.” And that computer report was the primary criterion that created the teachers’ paradigms about their students at the beginning of the year.\nWhen the administration finally discovered the mistake five and a half months later, they decided to test the kids again without telling anyone what had happened. And the results were amazing. The “bright” kids had gone down significantly in IQ test points. They had been seen and treated as mentally limited, uncooperative, and difficult to teach. The teachers’ paradigms had become a self-fulfilling prophecy.\nBut scores in the supposedly “dumb” group had gone up. The teachers had treated them as though they were bright, and their energy, their hope, their optimism, their excitement had reflected high individual expectations and worth for those kids.\nThese teachers were asked what it was like during the first few weeks of the term. “For some reason, our methods weren’t working,” they replied. “So we had to change our methods.” The information showed that the kids were bright. If things weren’t working well, they figured it had to be the teaching methods. So they worked on methods. They were proactive; they worked in their Circle of Influence. Apparent learner disability was nothing more or less than teacher inflexibility.\n\nI tried to find the source for this story, and failed. But what I did find was a similar concept called the Pygmalion effect, and assumed that Covey’s story was a mangled or somewhat made-up telling of that research.\nWhat is the Pygmalion effect? It has appeared in my blog feed twice in the past two weeks. Here’s a slice from the first, by Shane Parrish at Farnam Street, describing the effect and the most famous study in the area:\n\nThe Pygmalion effect is a psychological phenomenon wherein high expectations lead to improved performance in a given area. Its name comes from the story of Pygmalion, a mythical Greek sculptor. Pygmalion carved a statue of a woman and then became enamored with it. Unable to love a human, Pygmalion appealed to Aphrodite, the goddess of love. She took pity and brought the statue to life. The couple married and went on to have a daughter, Paphos.\n…\nResearch by Robert Rosenthal and Lenore Jacobson examined the influence of teachers’ expectations on students’ performance. Their subsequent paper is one of the most cited and discussed psychological studies ever conducted.\nRosenthal and Jacobson began by testing the IQ of elementary school students. Teachers were told that the IQ test showed around one-fifth of their students to be unusually intelligent. For ethical reasons, they did not label an alternate group as unintelligent and instead used unlabeled classmates as the control group. It will doubtless come as no surprise that the “gifted” students were chosen at random. They should not have had a significant statistical advantage over their peers. As the study period ended, all students had their IQs retested. Both groups showed an improvement. Yet those who were described as intelligent experienced much greater gains in their IQ points. Rosenthal and Jacobson attributed this result to the Pygmalion effect. Teachers paid more attention to “gifted” students, offering more support and encouragement than they would otherwise. Picked at random, those children ended up excelling. Sadly, no follow-up studies were ever conducted, so we do not know the long-term impact on the children involved.\n\nThe increases in IQ were 8 IQ points for the control group, and 12 points for those who were “growth spurters”. (The papers describing the study - from 1966 (pdf) and 1968 (pdf) - are somewhat thin on the experimental methodology, but it seems the description used in the study was “growth spurters” or high scorers in a “test for intellectual blooming”).\nI always took the Pygmalion effect with a grain of salt. Most educational interventions have little to zero effect - particularly over the long-run - even when they involve far more than giving a label.\nAs it turns out, the story is not as clean as Parrish and others typically tell it. There have been battles over the Pygmalion effect since the original paper, with failed replications, duelling meta-analyses and debates about what the Pygmalion effect actually is.\nBob C-J discusses this at The Introduction to the New Statistics (HT: Slate Star Codex - the second appearance of the Pygmalion effect in my feed). Here is a cut of Bob C-J’s summary of these battles:\n\nThe original study was shrewdly popularized and had an enormous impact on policy well before sufficient data had been collected to demonstrate it is a reliable and robust result.\nCritics raged about poor measurement, flexible statistical analysis, and cherry-picking of data.\nThat criticism was shrugged off.\nReplications were conducted.\nThe point of replication studies was disputed.\nDirect replications that showed no effect were discounted for a variety of post-hoc reasons.\nAny shred of remotely supportive evidence was claimed as a supportive replication.  This stretched the Pygmalion effect from something specific (an impact on actual IQ) to basically any type of expectancy effect in any situation…. which makes it trivially true but not really what was originally claimed.  Rosenthal didn’t seem to notice or mind as he elided the details with constant promotion of the effect. …\nMultiple rounds of meta-analysis were conducted to try to ferret out the real effect; though these were always contested by those on opposing sides of this issue.  …\nEven though the best evidence suggests that expectation effects are small and cannot impact IQ directly, the Pygmalion Effect continues to be taught and cited uncritically.  The criticisms and failed replications are largely forgotten.\nThe truth seems to be that there are expectancy effects–but:\n\nthat there are important boundary conditions (like not producing real effects on IQ)\nthey are often small\nand there are important moderators (Jussim & Harber, 2005).\n\n\nThe Jussim and Harber paper (pdf) Bob C-J references provides a great discussion of the controversy. (Bob C-J also recommends Social Perception and Social Reality: Why Accuracy Dominates Bias and Self-Fulfilling Prophecy by Lee Jussim). Here’s a section of the abstract:\n\nThis article shows that 35 years of empirical research on teacher expectations justifies the following conclusions: (a) Self-fulfilling prophecies in the classroom do occur, but these effects are typically small, they do not accumulate greatly across perceivers or over time, and they may be more likely to dissipate than accumulate; (b) powerful self-fulfilling prophecies may selectively occur among students from stigmatized social groups; (c) whether self-fulfilling prophecies affect intelligence, and whether they in general do more harm than good, remains unclear, and (d) teacher expectations may predict student outcomes more because these expectations are accurate than because they are self-fulfilling.\n\nThat paper contains some amusing facts about the original Rosenthal and Jacobson study. Some students had pre-test IQ scores near zero, others near 200, yet “the children were neither vegetables nor geniuses.” Exclude scores outside of the range 60 to 160, and the effect disappears. Five of the “bloomers” had increases of over 90 IQ points. Again, exclude these five and the effect disappears. The original study is basically worthless. While there is something to the effect of teacher expectations on students, the gap between the story telling and reality is rather large."
  },
  {
    "objectID": "posts/tetlock-and-gardners-superforecasting-the-art-and-science-of-prediction.html",
    "href": "posts/tetlock-and-gardners-superforecasting-the-art-and-science-of-prediction.html",
    "title": "Tetlock and Gardner’s Superforecasting: The Art and Science of Prediction",
    "section": "",
    "text": "Philip Tetlock and Dan Gardner’s Superforecasting: The Art and Science of Prediction doesn’t quite measure up to Tetlock’s superb Expert Political Judgment (read EPJ first), but it contains more than enough interesting material to make it worth the read.\nThe book emerged from a tournament conducted by the Intelligence Advanced Research Projects Activity (IARPA), designed to pit teams of forecasters against each other in predicting political and economic events. These teams included Tetlock’s Good Judgment Project (also run by Barbara Mellers and Don Moore), a team from George Mason University (for which I was a limited participant), and teams from MIT and the University of Michigan.\nThe result of the tournament was such a decisive victory by the Good Judgment Project during the first 2 years that IARPA dropped the other teams for later years. (It wasn’t a completely open fight - prediction markets could not use real money. Still, Tetlock concedes that the money-free prediction markets did pretty well, and there is scope to test them further in the future.)\nTetlock’s formula for a successful team is fairly simple. Get lots of forecasts, calculate the average of the forecast, and give extra weight to the top forecasters - a version of wisdom of the crowds. Then extremize the forecast. If the forecast is a 70% probability, bump up to 85%. If 30%, cut it to 15%.\nThe idea behind extremising is quite clever. No one in the group has access to all the dispersed information. If everyone had all the available information, this would tend to raise their confidence, which would result in a more extreme forecast. Since we can’t give everyone all the information, extremising is an attempt to simulate what would happen if you did. To get the benefits of this extremising, however, requires diversity. If everyone holds the same information there is no sharing of information to be simulated.\nBut the book is not so much about why the Good Judgment Project was superior to the other teams. Mostly it is about the characteristics of the top 2% of the Good Judgment Project forecasters - a group that Tetlock calls superforecasters.\nImportantly, “superforecaster” is not a label given on the basis of blind luck. The correlation in forecasting accuracy for Good Judgment Project members between one year and next was around 0.65. 70% of superforecasters stay in the top 2% the following year.\nSome of the characteristics of superforecasters are to be expected. Whereas the average Good Judgment participant scored better than 70% of the population on IQ, superforecasters were better than about 80%. They were smarter, but not markedly so.\nTetlock argues much of the differences lies in technique, and this is where he focused. When faced with a complex question, superforecasters tended to first break it into manageable pieces. For the question of whether French or Swiss inquiries would discover elevated levels of polonium in Yasser Arafat’s body (had he been poisoned?), they might ask whether polonium (which decays) could be found in a man dead for years, what ways could polonium have made its way into his body etc. They don’t jump straight to the implied question of whether Israel poisoned Arafat (which the question was technically not about).\nSuperforecasters also tended to take the outside view for each of these sub-questions. What is the base rate of this event? (Not so easy for this Arafat question) It is only then that they take the “inside view” by looking for information idiosyncratic to that particular question.\nThe most surprising finding (to me) was that superforecasters were highly granular in their probability forecasts and granularity predicts accuracy. People who stick to tens (10%, 20%, 30% etc) are less accurate than those who stick to fives (5%, 10%, 15% etc), who are less accurate than those who use ones (35%, 36%, 37% etc). Rounding superforecaster estimates reduces their accuracy, although this has little effect on regular forecasters. A superforecaster will distinguish between 63% and 65%, and this makes them more accurate.\nPartly this granularity is reflected in the updates they make when new information is obtained (although they are also more accurate on their initial estimate). Being a superforecaster requires monitoring the news, and reacting the right amount. There are occasional big updates - which Tetlock suggests superforecasters can make because they are not tied to their forecasts like a professional pundit - but most of the time the tweaks represent an iteration toward an answer.\nTetlock suggests such fine-grained distinctions would not come to people naturally, as making them would not have been evolutionarily favourable. If there is a lion in grass, there are three likely responses - yes, no, maybe - not 100 shades of grey. But the reality is there needs to be a threshold for each, and evolution can act on fine distinctions. A gene that leads people to apply “run” with 1% greater accuracy over many generations will spread.\nSuperforecasters also suffer less from scope insensitivity. People will pay roughly the same amount to save 2,000 or 200,000 migrating birds. Similarly, when asked whether an event will occur in the next 6 or 12 months, regular forecasters would predict approximately the same probability of the event occurring. Conversely, superforecasters tend to spot the difference in timeframes and adjust their probabilities so, although they did not exhibit perfect scope insensitivity. I expect an explicit examination of base rates would help in reducing that scope insensitivity as it will tend to relate to a timeframe.\nA couple of the characteristics Tetlock gives to the superforecasters seem a bit fluffy. Tetlock describes them as having a “growth mindset”, although the evidence presented simply suggests that they work hard and try to improve.\nSimilarly, Tetlock labels the superforecasters as having “grit”. I’ll just call them conscientious.\nBeyond the characteristics of superforecasters, Tetlock revisits a couple of themes from Expert Political Judgment. As a start, there is a need to apply numbers to forecasts, or else they are fluff. Tetlock relates the story of Sharman Kent asking intelligence officers what they took the words “serious possibility” in a National Intelligence estimate to mean (this wording relating to the possibility of a Soviet invasion of Yugoslavia in 1951). The answer turned out to be anything between a 20% and an 80% probability.\nThen there is a need for scoring against appropriate benchmarks - such as no change or the base rate. As Tetlock points out, lauding Nate Silver for picking 50 of 50 states in the 2012 Presidential election is a “tad overwrought” if compared to the no-change prediction of 48.\nOne contrast with the private Expert Political Judgment project was that forecasters in the public IARPA tournament were better calibrated. While the nature of the questions may have been a factor - the tournament questions related to shorter timeframes to allow the tournament to deliver results in a useful time - Tetlock suggests that publicity creates a form of accountability. There was also less difference between foxes and hedgehogs in the public environment.\nOne interesting point buried in the notes is where Tetlock acknowledges the various schools of thought around how accurate people are, such as the work by Gerd Gigerenzer and friends on the accuracy of our gut instincts and simple heuristics. Without going into a lot of detail, Tetlock declares the “heuristics and biases” program is the best approach to bring error rates in forecasting down. The short training guidelines - contained in the Appendix to the book and targeted to typical biases - improved accuracy by 10%. While Tetlock doesn’t really put his claim to the test by comparing all approaches (What would a Gigerenzer led team do?), the evidence of the success of the Good Judgment team makes it hard, at least for the moment, to argue with."
  },
  {
    "objectID": "posts/thaler-and-sunsteins-nudge.html",
    "href": "posts/thaler-and-sunsteins-nudge.html",
    "title": "Thaler and Sunstein’s Nudge",
    "section": "",
    "text": "In the process of listening to audio versions of some of the less arduous books on my reading list, I have just listened to Richard Thaler and Cass Sunstein’s Nudge.\nThe ideas in the book have been discussed in the public realm often enough that the book didn’t contain any surprises, although the emphasis in the book is a touch different from that in public debate. That difference in emphasis largely relates to the first word of Thaler and Sunstein’s philosophy, “libertarian paternalism”, and how nudges could be used to increase freedom.\nUnlike public policy discussion of nudges which always seem to be on top of existing regulation, Sunstein and Thaler discuss how nudging could be used to wind back government involvement and increase freedom. They cover, among other things, school choice, getting the state out of marriage, allowing waiver of the ability to sue for medical negligence and allowing people to ride motorcycles without a helmet if they choose to opt out after suitable warnings.\nTheir discussion of how nudges could apply to marriage was the most interesting part of the book. They suggest that marriage should be the domain of private institutions, with the role of the state to set default rules that apply if there is no agreement to the contrary. If a couple splits, how much support should someone provide to their partner who has low career prospects due to the time they spent raising children? Appropriate default rules would give protection as they would likely stick (particularly as most people do not expect to divorce) and if they are amended, there will need to be transparent agreement between the parties. This nudge could provide protection to the vulnerable member of a couple and get the government out of marriage.\nI’m not convinced by the “slippery slope” argument against nudges, as the slope toward regulation is already well oiled, and would like to see nudges used as a push toward sliding the other way. What about legalising a range of drugs with warnings? Or an ability to opt out of government provided services in exchange for tax breaks after you are nudged through, say, financial advice requirements (Bryan Caplan makes some of these arguments)? Libertarians should be more aggressive in seeing how nudges can wind back hard regulation.\nAnd as an end note, this is another book that I’m glad I did via audiobook. If you’re familiar with much of the behavioural science literature, the book covers a lot of stuff you already know. And when you come to the chapters on money where Thaler and Sunstein tell “econs” to skip those sections, do it. You really won’t learn anything. That said, I am sure there are a lot of bureaucrats and politicians who, among others, should read the whole book."
  },
  {
    "objectID": "posts/the-1-n-portfolio-versus-the-optimal-strategy.html",
    "href": "posts/the-1-n-portfolio-versus-the-optimal-strategy.html",
    "title": "The 1/N portfolio versus the optimal strategy: Does a simple heuristic outperform?",
    "section": "",
    "text": "Gerd Gigerenzer is fond of telling a story about Harry Markowitz, modern portfolio design pioneer and winner of the 1990 Nobel Memorial Prize in Economic Sciences.\nHere’s one version of the story, from Risk Savvy:\n\nAssume you have a chunk of money and want to invest it. You do not want to put all your eggs into one basket and are considering a number of stocks. You want to diversify. But how?\nHarry Markowitz won a Nobel Prize in economics for solving this problem. The solution is known as the mean-variance portfolio. The portfolio maximizes the gain (mean) for a given risk or minimizes the risk (variance) for a given return. Many banks rely on this and similar investment methods and warn customers against relying on their intuitions instead.\nBut when Markowitz made his own investments for his retirement, he did not use his Nobel Prize–winning method. Instead, he employed a simple rule of thumb called 1/N:\nAllocate your money equally to each of N funds.\nWhy did he rely on a hunch instead of crunching the numbers? In an interview, Markowitz said he wanted to avoid regrets: “I thought, ‘You know, if the stock market goes way up and I’m not in, I’ll feel stupid. And if it goes way down and I’m in it, I’ll feel stupid.’ So I went 50-50.” He did what many investors do: Make it simple. And 1/N is not only simple, it is the purest form of diversification. (Gigerenzer’s source is an interview of Markowitz by Bruce Bower.)\nHow good is this rule of thumb? In a study, it was compared to mean-variance and a dozen other complex investment methods. Seven situations were analyzed, such as investing in ten American industry funds. The mean-variance portfolio made use of ten years of stock data, while 1/N did not need any. What was the result? In six of the seven tests, 1/N scored better than mean-variance in common performance criteria. Moreover, none of the other twelve complex methods was consistently better at predicting the future value of the stocks.\nDoes this mean that the Nobel Prize–winning method is a sham? No. It is optimal in an ideal world of known risks, but not necessarily in the uncertain world of the stock market, where so much is unknown. To use such a complex formula requires us to estimate a large number of parameters based on past data. Yet as we have seen, ten years is too short a time to get reliable estimates. Say you invested in fifty funds. How many years of stock data would be needed before the mean-variance method finally does better than 1/N? A computer simulation provides the answer: about five hundred years!\nThat means that in the year 2500, investors can turn from the simple rule to the high-level math of the mean-variance model and hope to win. But this holds only if the same stocks—and the stock market—are still around.\n\nIt’s a great story about the power of simple heuristics. But if you follow the academic trail behind it, the story gets messier.\nThe study that Gigerenzer references is a well-cited 2007 paper by Victor DeMiguel, Lorenzo Garlappi and Raman Uppal (pdf). They described a comparison of performance between the 1/N rule and 14 other asset allocation models. These included Markowitz’s famous model and a number of its derivatives. The result across seven empirical datasets was that none of the 14 models consistently outperformed the 1/N rule when tested out-of-sample; that is, tested against new data that hadn’t been used to develop the model.\nBut if you follow Gigerenzer’s footnote referencing the DeMiguel and friends study, you also see the following note:\n\nThe conditions under which 1/N is superior to optimization methods are still debated; see Kritzman, Page, and Turkington 2010.\n\nSo what do Kritzman and friends say?\n\nThe ostensible superiority of the 1/N approach arises not from limitations in optimization but, rather, from reliance on rolling short-term samples for estimating expected returns. This approach often yields implausible expectations. By relying on longer-term samples for estimating expected returns or even naively contrived yet plausible assumptions, optimized portfolios outperform equally weighted portfolios out of sample.\n\nThe rolling short-term samples referred to relate to DeMiguel and friends’ use of a rolling 60- or 120-month history to determine the input parameters for the optimisation exercise.\nKritzman and friends argue that the use of this rolling average leads to problems. For example, in February 2009, the 60-month trailing return of the S&P 500 Index was –5.66 percent. The return of a major bond index was around 3 percent. If you took this previous 5-years as your future expectation, the observed stock and bond holdings implies that most people prefer more risk to less risk for the same return. Why would you otherwise hold the riskier stocks if you expected a negative return when a safer positive return was available? As input assumptions for an optimisation exercise, they are implausible. Kritzman and friends state that in such a situation no investor would simply take the recent historial return as their future expectation.\nKritzman and friends argued that the consequence of these implausible assumptions could be demonstrated by inputting some arbitrarily decided but reasonable sounding expected returns instead of using the recent short-term history. In that case, the superiority of the optimisation approaches re-emerges.\nI have some sympathy for this argument. As Kritzman and friends point out, Markowitz did not intend for a naive extrapolation of past returns to form the basis of expectations for the optimisation exercise. Rolling averages make the exercise mechanical and feasible, reducing the number of decisions by the researchers. But simplifying the exercise in this way has a cost.\nAlas, the story goes further. A quick Google Scholar search of the papers citing Kritzman and friends shows that this has turned into a huge academic debate.\nHere’s a quick taste of some of the other papers that I found. I haven’t read them in detail and this is out of my area of expertise, so assume I haven’t get everything right here:\n\nBehr and friends (2013) (pdf) developed a “constrained minimum-variance strategy” that outperformed a 1/N policy across 6 datasets.\nJacobs and friends (2014) (pdf) argue that “prominent Markowitz extensions do not outperform several heuristic weighting schemes (1/N heuristic, market value-weighting and GDP-weighting).” While Jacobs and friends extend the range of simple heuristics that are competitive against optimised portfolios, they appear to rely on the same 60-month rolling average to develop input parameters that was criticised by Kritzman and friends.\nValeriy Zakamulin (2017) suggested that those studies which find superior performance of optimised portfolios are doing so because of the particular datasets that tend to be used. The “Kenneth French datasets” tend to have a “low volatility effect”, an effect whereby low volatility stocks tend to outperform. The optimisation approaches capitalise on the low volatility effect. I am not convinced that this is a problematic argument. If the optimisation approaches capitalise on an anomaly or established factor premium to outperform, so be it. It suggests there is a way to beat the simple heuristic. But on the other hand, it points to possible alternative heuristics that can capitalise on the anomaly without mean-variance optimisation.\nHsu and friends (2018) suggest that nearly all papers claiming a superiority for optimisation approaches suffer from “data snooping”. Data snooping captures publication bias, p-hacking, the garden of forking paths and other phenomena that yield unrepresentative published results. Get rid of “data snooping” and most of the outperformance of optimised methods disappears. This paper is a good reminder that when people claim good “out-of-sample” results, they tell us about the exercise only after they had seen the results themselves. “Out-of-sample” would be far more impressive if that was a future sample.\nDavid Allen and friends (2019) find that if investors have “modest” forecasting ability, mean-variance optimisation is superior. I’m somewhat sceptical of the concept that people have forecasting ability in the sense defined. The paper makes a fairly token attempt to make the case that they do. That said, Allen and friends also provide another take-down of the DeMiguel and friends paper, arguing their result is the result of their modelling approach. Loosen some constraints and optimisation outperforms even without forecasting ability. (This paper itself spawned a debate, with Michaud and friends claiming they omitted certain forms of error, and Allen and friends responding that they did not and that they use richer, higher-frequency data that many other comparisons.)\n\nAs said above, messy. And that is without also exploring the approach to measuring outperformance. I’m not convinced the measures used answer the question at heart. But that’s an issue for another day.\nAbsent a deeper dive into these papers, and as someone who has often told the Markowitz story in a similar manner to Gigerenzer - with a reference to DeMiguel and friends paper and all - I am updating the story I tell somewhat:\n\nDeMiguel and friends demonstrated that 1/N performed on par with more complex optimisation approaches under a fairly limited set of assumptions and the result may be a peculiarity of their model. I should stop stating that their model establishes the need for 500-years of data.\nHowever, no-one has clearly shown that more complex optimisation methods are better. In particular, “data snooping” brings many of the results claiming outperformance into question.\nI don’t believe it is possible to definitively settle this debate with historic data. Some future oriented bets would be more informative and could substantially shift my beliefs, but even then it may not be conclusive. You can get a high Sharpe ratio by picking up pennies in front of a steamroller if the period of measurement doesn’t include you getting hit by the steamroller. I am sure I have read (somewhere) Nassim Taleb claiming that high Sharpe ratios are a sign that someone will blow up.\n\nDeep down, I still believe that a 1/N heuristic can be sound even when compared to more sophisticated approaches. This is not a question of getting enough data - as appears implied by Gigerenzer in the quoted passage - but one of of robustness in the face of uncertainty and an unstable future with the potential for structural changes. This belief is buttressed by the broader evidence on the power of simple heuristics.\nBut as a starting point I should be weakening - at least slightly - my belief about the 1/N heuristic given the specific paper I have regularly quoted in support of that claim isn’t as general or robust as I thought it was.\nOne factor that constrains that weakening is an observation from this latest perusal of the literature. The 1/N heuristic appears to have a degree of respect. It is often used as the benchmark against which portfolio optimisation techniques are assessed. Most of the papers I found exploring the performance of 1/N were using it as a benchmark rather than genuinely exploring its robustness.\nFinally, with all this said, there remains one issue that looms large over this exercise. If deciding a 1/N strategy, what should N be?\nIf there are two bonds and three stocks, does N equal two (stocks and bonds) or five. What of an N of two with crypto- and non-crypto-assets? Should we group domestic and international stocks? What of different sectors?\nMost of the papers mentioned above have relatively sophisticated people picking what comprises N. What of the average investor? How does their portfolio hold up with a 1/N rule?\nIn Risk Savvy, Gigerenzer is sensitive to this question of choosing N. He discussed a time when an organisation’s head of investment asked whether his customers might just invest themselves if all they need is a 1/N rule. Gigerenzer’s writes:\n\nI reassured him that there are plenty of open questions, such as how large N should be, what kind of stocks, when to rebalance, and most important, to figure out when and where 1/N is a successful strategy.\n\nThat’s not an easy task. But maybe there’s another simple heuristic by which we choose N?"
  },
  {
    "objectID": "posts/the-beauty-of-self-interest.html",
    "href": "posts/the-beauty-of-self-interest.html",
    "title": "The beauty of self interest",
    "section": "",
    "text": "In my review of E.O. Wilson’s The Social Conquest of Earth, I quoted this passage which captures Wilson’s conception of the origin of cooperation in humans.\n\nSelection at the individual level tends to create competitiveness and selfish behaviour among group members - in status, mating, and the securing of resources. In opposition, selection between groups tends to create selfless behavior, expressed in greater generosity and altruism, which in turn promote stronger cohesion and strength of the group as a whole.\n\nThis passage from Matt Ridley strikes at the heart of Wilson’s dichotomy between selfishness and generosity:\n\n“Group selection” has always been portrayed as a more politically correct idea, implying that there is an evolutionary tendency to general altruism in people. Gene selection has generally seemed to be more of a right-wing idea, in which individuals are at the mercy of the harsh calculus of the genes.\nActually, this folk understanding is about as misleading as it can be. Society is not built on one-sided altruism but on mutually beneficial co-operation.\nNearly all the kind things people do in the world are done in the name of enlightened self-interest. Think of the people who sold you coffee, drove your train, even wrote your newspaper today. They were paid to do so but they did things for you (and you for them). Likewise, gene selection clearly drives the evolution of a co-operative instinct in the human breast, and not just towards close kin.\n\nYou can read the full article here."
  },
  {
    "objectID": "posts/the-behavioural-economics-guide-2016-with-an-intro-by-gerd-gigerenzer.html",
    "href": "posts/the-behavioural-economics-guide-2016-with-an-intro-by-gerd-gigerenzer.html",
    "title": "The Behavioural Economics Guide 2016 (with an intro by Gerd Gigerenzer)",
    "section": "",
    "text": "The Behavioural Economics Guide 2016 is out (including a couple of references to yours truly), with the introduction by Gerd Gigerenzer. It’s nice to see some of the debate in the area making an appearance.\nHere are a few snippets from Gigerenzer’s piece. First, on heuristics:\n\nTo rethink behavioral economics, we need to bury the negative rhetoric about heuristics and the false assumption that complexity is always better. The point I want to make here is not that heuristics are always better than complex methods. Instead, I encourage researchers to help work out the exact conditions under which a heuristic is likely to perform better or worse than some fine-tuned optimization method. First, we need to identify and study in detail the repertoire of heuristics that individuals and institutions rely on, which can be thought of as a box of cognitive tools. This program is called the analysis of the adaptive toolbox and is descriptive in its nature. Second, we need to analyze the environment or conditions under which a given heuristic (or complex model) is likely to succeed and fail. This second program, known as the study of the ecological rationality of heuristics (or complex models), is prescriptive in nature. For instance, relying on one good reason, as the hiatus rule does [If a customer has not made a purchase for nine months or longer, classify him/her as inactive, otherwise as active], is likely to be ecologically rational if the other reasons have comparatively small weights, if the sample size is small, and if customer behavior is unstable.\n\nAnd the “bias bias”:\n\nThe bias bias is the tendency to diagnose biases in others without seriously examining whether a problem actually exists. In decision research, a bias is defined as a systematic deviation from (what is believed to be) rational choice, which typically means that people are expected to add and weigh all information before making a decision. In the absence of an empirical analysis, the managers who rely on the hiatus heuristic would be diagnosed as having committed a number of biases: they pay no attention to customers’ other attributes, let alone to the weight of these attributes and their dependency. Their stubborn refusal to perform extensive calculations might be labeled the “hiatus fallacy” – and provide entry number 176 in the list on Wikipedia. Yet many, including experts, don’t add and weigh most of the time, and their behavior is not inevitably irrational. As the bias–variance dilemma shows, ignoring some information can help to reduce error from variance – the error that arises from fine-tuned estimates that produce mostly noise. Thus, a certain amount of bias can assist in making better decisions.\nThe bias bias blinds us to the benefits of simplicity and also prevents us from carefully analyzing what the rational behavior in a given situation actually is. I, along with others, have shown that more than a few of the items in the Wikipedia list have been deemed reasoning errors on the basis of a narrow idea of rationality and that they can instead be easily justified as intelligent actions (Gigerenzer et al., 2012).\n\n\nA recent Spectator article on an interview with Richard Thaler - a contributor the 2016 Guide - opened with the following:\n\n‘For ten years or so, my name was “that jerk”,’ says Professor Richard Thaler, president of the American Economics Association and principal architect of the behavioural economics movement. ‘But that was a promotion. Before, I was “Who’s he?”’\n\nOn hearing that Gigerenzer had written the introduction to the Guide, Thaler tweeted:\n\nI suppose Thaler is now the establishment and Gigerenzer is “that jerk”."
  },
  {
    "objectID": "posts/the-benefit-of-uncertainty.html",
    "href": "posts/the-benefit-of-uncertainty.html",
    "title": "The benefit of uncertainty",
    "section": "",
    "text": "John Coates writes:\n\n[W]e tend to view financial risk taking as a purely intellectual activity. But this view is incomplete. Risk is more than an intellectual puzzle — it is a profoundly physical experience, and it involves your body. Risk by its very nature threatens to hurt you, so when confronted by it your body and brain, under the influence of the stress response, unite as a single functioning unit. This occurs in athletes and soldiers, and it occurs as well in traders and people investing from home. The state of your body predicts your appetite for financial risk just as it predicts an athlete’s performance. …\nOur challenge response, and especially its main hormone cortisol (produced by the adrenal glands) is particularly active when we are exposed to novelty and uncertainty. If a person is subjected to something mildly unpleasant, like bursts of white noise, but these are delivered at regular intervals, they may leave cortisol levels unaffected. But if the timing of the noise changes and it is delivered randomly, meaning it cannot be predicted, then cortisol levels rise significantly.\n\nSo what does this mean for financial markets?\n\n\nWhen opportunities abound, a potent cocktail of dopamine — a neurotransmitter operating along the pleasure pathways of the brain — and testosterone encourages us to expand our risk taking, a physical transformation I refer to as “the hour between dog and wolf.” One such opportunity is a brief spike in market volatility, for this presents a chance to make money. But if volatility rises for a long period, the prolonged uncertainty leads us to subconsciously conclude that we no longer understand what is happening and then cortisol scales back our risk taking. In this way our risk taking calibrates to the amount of uncertainty and threat in the environment.\n\n\nIn this light, the Federal Reserve has another parameter to consider.\n\n\nTHE Fed, however, through its control of policy uncertainty, has in its hands a powerful tool for influencing risk takers. But by trying to be more transparent, it has relinquished this control. …\n\n\nAs uncertainty in fed funds declined, one of the most powerful brakes on excessive risk taking in stocks was released.\n\n\nDuring their tenures, in response to surging stock and housing markets, both Mr. Greenspan and Mr. Bernanke embarked on campaigns of tightening, but the metronome-like ticking of their rate increases was so soothing it failed to dampen exuberance.\n\n\nThere are times when the Fed does need to calm the markets. After the credit crisis, it did just that. But when the economy and market are strong, as they were during the dot-com and housing bubbles, what, pray tell, is the point of calming the markets? Of raising rates in a predictable fashion? If you think the markets are complacent, then unnerve them. Over the past 20 years the Fed may have perfected the art of reassuring the markets, but it has lost the power to scare. And that means stock markets more easily overshoot, and then collapse.\n\n\nThe Fed could dampen this cycle. It has, in interest rate policy, not one tool but two: the level of rates and the uncertainty of rates.\n\n\nI like this analysis, but I’m not sure about the conclusion. Reserve banks operate under an illusion of control, acting as though they are behind the wheel of a finely tuned sports car. The reality is closer to 19th century medicine. Even if Coates’s analysis is right, I’m wary of the Fed or other reserve banks trying to generate the “right level” of uncertainty."
  },
  {
    "objectID": "posts/the-benefits-of-chinese-eugenics.html",
    "href": "posts/the-benefits-of-chinese-eugenics.html",
    "title": "The benefits of Chinese eugenics",
    "section": "",
    "text": "Edge’s annual question for 2013 - What should we worry about? - has generated a bunch of interesting responses. First in the list is Geoffrey Miller’s response, Chinese eugenics. Miller writes:\n\nWhen I learned about Chinese eugenics this summer, I was astonished that its population policies had received so little attention. China makes no secret of its eugenic ambitions, in either its cultural history or its government policies. …\nThe BGI Cognitive Genomics Project is currently doing whole-genome sequencing of 1,000 very-high-IQ people around the world, hunting for sets of sets of IQ-predicting alleles. I know because I recently contributed my DNA to the project, not fully understanding the implications. These IQ gene-sets will be found eventually—but will probably be used mostly in China, for China. Potentially, the results would allow all Chinese couples to maximize the intelligence of their offspring by selecting among their own fertilized eggs for the one or two that include the highest likelihood of the highest intelligence. Given the Mendelian genetic lottery, the kids produced by any one couple typically differ by 5 to 15 IQ points. So this method of “preimplantation embryo selection” might allow IQ within every Chinese family to increase by 5 to 15 IQ points per generation. After a couple of generations, it would be game over for Western global competitiveness.\n\nSupposing that the Chinese are engaging in a eugenic exercise to boost IQ, and ignoring the potential moral implications such as coercion, should we be worried? Miller suggests it might be the end of Western global competitiveness, but off the top of my head I can see the following benefits:\n\nInnovation would increase with the greater number of high-IQ people. As ideas are non-rivalrous, that innovation will benefit other countries.\nSavings would increase (IQ is correlated with time preference), creating a greater capital stock. This capital stock can be invested in Western countries.\nGiven the strong link between IQ and economic growth, China’s economy would grow, creating a larger trading partner and greater demand for Western goods and services.\nChina would be a source of high-IQ immigrants.\n\nOf course, we are already reaping these benefits from China. East Asians already have an average IQ above Western populations and China is a growing source of ideas, capital, demand for Western goods and services and high-IQ immigrants. If anything, we would be worried if Chinese IQ were dropping.\nMiller notes at the end, however, that his real concern is the Western response:\n\nThe most likely response, given Euro-American ideological biases, would be a bioethical panic that leads to criticism of Chinese population policy with the same self-righteous hypocrisy that we have shown in criticizing various Chinese socio-cultural policies. But the global stakes are too high for us to act that stupidly and short-sightedly. A more mature response would be based on mutual civilizational respect, asking—what can we learn from what the Chinese are doing, how can we help them, and how can they help us to keep up as they create their brave new world?\n\nThe Western response will be interesting, but I do not expect that the response to actions in China will be the most important in this area. Rather, it will be the response to “positive eugenics” within Western Countries’ own borders as people increasingly take their genetic future into their own hands. If someone wishes to select the embryo with the highest predicted IQ, will they be allowed?"
  },
  {
    "objectID": "posts/the-benefits-of-competition.html",
    "href": "posts/the-benefits-of-competition.html",
    "title": "The benefits of competition",
    "section": "",
    "text": "I recently came across a review of Robert Frank’s The Darwin Economy by Ted Bergstrom. Frank’s argument is largely based on the concept that a person is made worse off when they respond to someone else’s consumption choices, as it often turns into a winner takes all arms race. Bergstrom makes an important point that people may wish for someone else to increase their level of conspicuous consumption or competitive output. Bergstrom writes:\n\nSuppose, for example, that neighbors A and B each consume two goods, x whose consumption is publicly observed and y whose consumption is not. Suppose that A gets an income windfall and buys more x. Shortly thereafter, we observe that B buys more x, although his income hasn’t changed. Can we conclude that B has been made worse off by A’s good fortune? We know that B now chooses a bundle that he rejected before A’s windfall, which would be bad for him if he were indifferent about A’s consumption. But, in fact, he is not indifferent about A’s consumption. It is not hard to construct “realistic” vignettes in which B would be pleased to see the increase in A’s consumption of x and pleased to increase his own x in response. For example, A if paints his house, or improves the appearance of his garden, B might enjoy the neighborhood improvement and although he could still afford his old combination of x and y he would prefer to complement his neighbor’s action by his own home improvements. Other examples come from athletic endeavors. Suppose that A and B frequently play tennis together and traditionally win about equally often. For some reason, A’s game improves, and he begins to win more than half the time. This induces B to play harder or perhaps purchase costly tennis lessons so that once again they win about equally often. Are we to conclude that B is worse off and A is no better off than before the improvement of their games? Not necessarily. Both may be evolutionarily programmed by our hunter-gather past to enjoy the challenge of succeeding at a difficult task. After all, they do not play each other for prize money, they play for the pleasure of competing.\n\nI have some sympathy for Bergstrom’s argument, although I wonder how many examples of this type might be explained by competition in other domains. For example, if someone is pleased that their neighbour renovates their house and then increases the upkeep on theirs, is this because there are people besides their neighbour with whom they are engaging in positional competition? Does it increase the prestige of their neighbourhood?\nI am also not convinced that the pleasure of competing is materially improved through your playing partner in social tennis becoming better than you, particularly given experimental evidence on changes in testosterone and cortisol from losing “fun” competitions. Being at the bottom of the pile is bad for your health. Is the additional training simply intended to increase the probability of winning against the improved player? Or is the training against the better competitor yielding benefits in more success against other potential competitors?\nDespite my doubt about whether these examples are useful or representative, I am hesitant to ignore them in drawing policy conclusions that may affect competition. As I suggested recently, there may be spillovers from competitive activity that yield broader benefits. Plus, do I have the required level of insight about someone’s enjoyment of tennis to be making assessments on their behalf? It’s a reasonable assumption that the mismatch between private and social benefits means that private investment in winner takes all competition exceeds the socially optimal level, but quantifying or controlling that mismatch with imperfect information about a third party’s intentions is a difficult task."
  },
  {
    "objectID": "posts/the-best-books-i-read-in-2012.html",
    "href": "posts/the-best-books-i-read-in-2012.html",
    "title": "The best books I read in 2012",
    "section": "",
    "text": "As is normally the case, my annual list comprises the best books I have read in the past year, irrespective of their date of release. I read fewer books this year than usual, so I’m drawing from a smaller pool than for the last couple of years (2010 and 2011). Here are my favourite six for 2012 - links are to reviews:\nPassions Within Reason: The Strategic Role of the Emotions by Robert Frank: A book I should have read a long time ago. I particularly appreciated Frank’s use of path-dependent evolution to develop his model of human behaviour.\nThe Righteous Mind: Why Good People Are Divided by Politics and Religion by Jonathan Haidt: At the top of many lists for good reason.\nAdapt: Why Success Always Starts with Failure by Tim Harford: Apart from being interesting and full of reasonable advice, Harford demonstrates a deep understanding of evolutionary processes, which is not often the case in books of this nature.\nThinking, Fast and Slow by Daniel Kahneman: Magnificent. The clear and accessible way that each chapter illustrates a bias or heuristic makes it the best book on rationality and decision-making that I have read.\nMoby Dick by Herman Melville: The best classic I read this year. Although I could have skipped some of the detours, many are fascinating.\nDarwinian Politics: The Evolutionary Origin of Freedom by Paul Rubin: A strong argument for political institutions that maximise freedom.\nThere are a few books that I read this year that have me in two minds, so I haven’t included them in the above list. I enjoyed Ridley’s The Rational Optimist: How Prosperity Evolves, but despite his claims to the contrary, Ridley stretched the evolutionary metaphor too far in drawing a Panglossian case. Robert Trivers’s The Folly of Fools: The Logic of Deceit and Self-Deception in Human Life would have required a much more thorough editing to make this list. I also enjoyed Kevin Kelly’s What Technology Wants, despite not buying the central argument.\nOf the books I have in my reading pile, I still haven’t got to Pinker’s The Better Angels of Our Nature: Why Violence Has Declined, and I intend to read Flynn’s Are We Getting Smarter?: Rising IQ in the Twenty-First Century over the Christmas break. Hopefully they will crack next year’s list."
  },
  {
    "objectID": "posts/the-biology-of-boom-and-bust.html",
    "href": "posts/the-biology-of-boom-and-bust.html",
    "title": "The biology of boom and bust",
    "section": "",
    "text": "John Coates’s excellent The Hour Between Dog and Wolf: Risk Taking, Gut Feelings and the Biology of Boom and Bust tells the story of the effect of hormones on decision making in finance. By the end of the book, the idea that traders are rational calculating machines driven by their brains is torn apart.\nAs Coates shows, the divide between body and mind is not as Descartes or economists would have us believe. Signals travel both ways. The body can influence the brain. Physiological reactions triggered by pre-conscious regions of the brain affect emotion and mood. New to me was the existence of the enteric nervous system, which comprises around 100 million neurons in our gastrointestinal lining. It can operate autonomously of the central nervous system. Messages flow back and forth between the brain and enteric nervous system via the vagus nerve, the decisions of one affecting the decisions of the other.\nThe basic dynamic of decision making described by Coates involves the hormones testosterone, adrenalin, cortisol and dopamine. In anticipation of an opportunity on the trading floor, a trader’s hormones kick into action. Testosterone levels increase, bringing with it increased oxygen carrying capacity, confidence and appetite for risk. Adrenalin surges, quickening reactions and tapping into glucose deposits in the liver, which provides energy for the upcoming challenge.\nCortisol is also produced. Unlike the short-term action of adrenalin, cortisol acts over the longer term and stops metabolically expensive functions such as growth, reproduction, digestion or immune function. The initial release of cortisol also stimulates the release of dopamine, delivering a rush. Dopamine rewards us when we take actions that result in an unexpected reward. It makes us want to repeat and crave these actions (as a result, animals would rather work for food than simply be given it). Traders crave the rush of the floor.\nWhen the trader has a win, his testosterone shoots up further. This testosterone infused trader will then take more risks. On average, more risk means more reward, so he earns higher profits. In fact, his testosterone levels in the morning are predictive of his afternoon profit. Hormones also make an appearance when this trader has a loss. His cortisol levels increase, decreasing his risk appetite and causing him to see danger everywhere.\nThe effects of biology are not only important for the people or firms involved, but can have systemic effects. Hormones exacerbate the market cycle. In a bull market, testosterone surges through the population of traders. Each takes larger and larger risks, pushing markets to new highs and triggering further cascades of testosterone. Irrational exuberance has a chemical base.\nSimilarly, in bear markets, cortisol levels peak. At the very time it might be best to buy, the market dries up as tentative traders retreat into their shells. Over the longer term, excessive cortisol impairs memory and causes anxiety.\nAs I mentioned in a previous post on an article by Coates, central banks could take this knowledge and use it to curb market cycles. In a bubble or crash, the population of traders could even enter a clinical state under the influence of pathologically elevated hormone levels. If that occurs, they could become insensitive to interest rates or other attempts by regulators to curb or control their activities.\nCoates extends his idea to some interesting speculation on market cycles. Testosterone levels fluctuate over the year. In humans, they rise until autumn and fall through to spring. The drop in testosterone in autumn can cause males to suffer from ‘irritable male syndrome’. Given most major market crashes have occurred in October, is it autumn moodiness that takes stock markets down? Similarly, does ’seasonal affective disorder, possibly also affected by testosterone, underlie underperformance between the autumn equinox and winter solstice?\nFor those in the industry, Coates offers advice on how to apply these findings, some relatively futuristic. We  can already record a range of physiological features, including hormone levels. Why not test them in the morning and set traders’ tasks or risk limits based on those measurements? We already have consumer products that perform real-time health monitoring. It is simply adding hormone levels to the suite of measures - although other measures such as heart rate, sweat levels and the like could also be useful indicators.\nThese physiological measures are probably better indicators than simply asking the traders how they are feeling. Whereas Coates found that hormone levels closely tracked the volatility of trading results and uncertainty in the market, surveys of these same traders about how stressed they were had almost no relationship to trading conditions, volatility or whether they were losing money.\nToward the end of the book, Coates includes some interesting material on how we might train our stress responses. One simple suggestion is exposure to acute stress, with those who have experienced moderate but short-lived stressors being toughened. In one study of rats, those rats exposed to stress when young had larger adrenal glands but a more muted response to stress. This was reflected in trader stress responses, with experienced traders having higher initial stress responses to events, but being able to quickly return to normal. However, once those stressors shift from being acute to chronic, problems begin. Exercise might also offer some protection, with sports science a potential source of new ideas.\nOne interesting piece of speculation is whether cold weather might provide useful training. Rats exposed to cold water have a quick arousal but quick recovery, with the stress response based more on adrenalin than cortisol. To the extent this occurs in humans, cold weather or water could be part of our training regime. Even more speculatively, Coates asks if the shift to more climate controlled environments has prevented a toughening of our psychological mechanisms, unlike that experienced by our ancestors.\nAn alternative to training could be to simply hire more women and older men who are less susceptible to testosterone feedback loops. However, I am not sure whether firms would want to implement this solution, with higher testosterone and risk taking leading to higher profits. The costs are across society when the crash comes and government steps in to lend a hand. Coates indicates this misalignment of incentives through the book, which suggests more than hiring policies are required.\nOne other interesting idea - only loosely linked to the major thesis - concerns fatigue. Fatigue might be seen as simply the result of running out of energy. But Coates points out a new model in neuroscience that suggests fatigue is a signal that the benefit from our current activity has dropped below its metabolic cost. It is a signal to stop the current search and start elsewhere. As a result, the cure for fatigue is a new task, not rest. Coates points to research suggesting overtime leads to hypertension and heart disease if we have no control over our attention, but otherwise it is not a problem. Flexibility in work could be as good as a vacation.\nIf I were to highlight one weakness of the book (more due to the state of the field than any fault of the author), it is that the foundation of studies on which it is built is fairly small, and largely based on data from a couple of trading floors. It would be great to see longitudinal data across a range of market participants during a number of cycles. Another potentially interesting extension would be to look at the hormone cycle in politics. Politicians can experience rational exuberance or appear to be exhibiting a constant state of panic. Are these the same biologically driven problems that Coates found in traders? Looking in new arenas such as this could provide a substantial contribution to our understanding of how humans make decisions."
  },
  {
    "objectID": "posts/the-bright-tax.html",
    "href": "posts/the-bright-tax.html",
    "title": "The bright tax",
    "section": "",
    "text": "The Smithsonian magazine has an interview with James Flynn about his book Are We Getting Smarter? (HT: Annie Murphy Paul) First, Flynn on the “bright tax:\n\nThe wisdom always was that the brighter you were, the less your mental abilities declined in old age. I found that was an oversimplification. It is true of verbal intelligence. The brighter you are, the more you get a bonus for verbal skills. I call that a “bright bonus.” Your vocabulary declines at a much less steep rate in old age than an ordinary or below average person. But to my amazement I found that for analytic abilities it was just the reverse. There is a “bright tax.” The brighter you are, the quicker after the age of 65 you have a downward curve for your analytic abilities. For a bright person, you go downhill faster than an average person.\nThis raises an interesting question. Is it something to do with the aging brain, or does it have to do with environment? It could be that a good analytic brain is like a high performance sports car; it just requires more maintenance, and in old age, the body can’t give it. That would be a physiological explanation; the bright brain requires sustenance from the body, which as the body ages is no longer forthcoming. The environmental explanation would be that we use our analytic abilities mainly at work. That means that if a bright person is in a cognitively demanding profession, they are like an athlete; they build up a big exercise advantage over the average person, who has a humdrum job. Then, retirement would be a leveler. That is, if you give up work at 65, you are like an athlete who is retired from competition. You no longer have that exercise advantage of your analytic abilities that work affords. We don’t really know which of these things is true. It could be that they are both true to some degree.\n\nAlso, a prediction of the next 100 years:\n\nIn my book, I study six developing nations. Kenya is undergoing explosive IQ gains. Brazil and Turkey are undergoing quite profound gains. Nations like Saudi Arabia and the Sudan are not, but the Sudanese keep having civil wars and the Saudis are really just living off of oil revenue. They are not industrializing in any real sense. Dominica is the sixth case. There, they are making IQ gains, but their infrastructure is wiped out about every 10 years by hurricanes, earthquakes and tsunamis. I predict that Brazil, Turkey and Kenya will industrialize over the next century and begin to rival the Western world for IQ."
  },
  {
    "objectID": "posts/the-consequences-of-shrinking-brains.html",
    "href": "posts/the-consequences-of-shrinking-brains.html",
    "title": "The consequences of shrinking brains",
    "section": "",
    "text": "Matt Ridley writes on the fossil evidence that human brains have shrunk from around 1,500 cubic centimetres to 1,350 cubic centimetres over the last 20,000 years:\n\nThis neither worries nor surprises me. We ceased relying upon individual brain power tens of thousands of years ago. Our civilization now gets all its inventive and creative power from the linking of brains into networks. Our future depends on being clever not individually, but collectively.\n\nRidley’s reason for not worrying is at odds with evidence that the return to an increase in an individual persons IQ is lower than that for an equivalent increase in average IQ for a population.\nTake this factoid from Garett Jones’s website (I have blogged about work by Jones before, including here and here):\n\nA two standard deviation rise in an individual person’s IQ predicts only about a 30% increase in her wage.  But the same rise in a country’s average IQ score predicts a 700% increase in the average wage in that country.\n\nThere are massive positive externalities to a population having a higher average IQ. Ridley is right to point to the networked nature of intelligence, but those networks result in large benefits to being surrounded by high IQ people. If I considered that intelligence were declining along with brain size (I’m not, which is a subject for a later post), I would be worried."
  },
  {
    "objectID": "posts/the-death-of-defaults.html",
    "href": "posts/the-death-of-defaults.html",
    "title": "The death of defaults?",
    "section": "",
    "text": "Late last year I went to a presentation by Schlomo Benartzi on how people think differently when they are using a screen. The punchline was that many of the classic behavioural biases do not play out as expected in digital mediums.\nOne example Benartzi gave involved defaults. The standard understanding is that defaults are powerful ways to influence behaviour - people will tend to stick to them. But Benartzi spoke of digital experiments with pre-populated checkboxes where people went out of their way to untick the box. The default backfired.\nWhy does this occur? I suggest a starting point should be our experience with defaults. Online retailers know the power of defaults, and regularly pre-populate checkboxes to join their mailing list or buy add-ons such as insurance. Generally, the default is a crap option. (Look at Dark Patterns for a pile of examples.) So what does someone with experience do? You scan every pre-populated checkbox to see whether you are being lumped with something you don’t want. If unsure, uncheck it.\nAs we are moving to a world where most interactions with government will be digital, will the power of defaults be lost? Will we untick the “register as an organ donor” or “save 3 per cent of you salary” boxes due to a newly acquired habit? And what other “nudges” will we resist when we learn that many nudgers don’t have our best interests at heart?"
  },
  {
    "objectID": "posts/the-deep-roots-of-development.html",
    "href": "posts/the-deep-roots-of-development.html",
    "title": "The deep roots of development",
    "section": "",
    "text": "Enrico Spolaore and Romain Wacziarg have put out a nice review article on long-term economic growth and the intergenerational transmission of development. Below are some of the more interesting parts.\nThey note two important papers (which I intend to write more detailed posts about at some stage) by Louis Putterman and David Weil, and by Comin, Easterly and Gong. They write:\n\n[Putterman and Weil] examine explicitly whether it is the historical legacy of geographic locations or the historical legacy of the populations currently inhabiting these locations that matters more for contemporary outcomes. …\nPutterman and Weil’s results strongly suggest that the ultimate drivers of development cannot be fully disembodied from characteristics of human populations. When migrating to the New World, populations brought with them traits that carried the seeds of their economic performance. This stands in contrast to views emphasizing the direct effects of geography or the direct effects of institutions, for both of these characteristics could, in principle, operate irrespective of the population to which they apply. A population’s long familiarity with certain types of institutions, human capital, norms of behavior or more broadly culture seems important to account for comparative development. …\nThe deep historical roots of development are at the center of Comin, Easterly and Gong (2010). They consider the adoption rates of various basic technologies in 1000 BC, 1 AD, and 1500 AD. in a cross-section of countries defined by their current boundaries. They find that technology adoption in 1500, but also as far back as 1000 BC, is a significant predictor of income per capita and technology adoption today.\n\nSpolaore and Wacziarg also note Easterly and Levine’s paper on the link between European settlement and economic growth (a later version of which I mentioned last week).\nSpolaore and Wacziarg were the authors of The Diffusion of Development, about which I have previously posted. In that paper, they proposed that genetic distance acts as a barrier to technology transfer, so that when one population becomes more technologically advanced, even if through luck, the barrier to transfer results in differences in economic development. In this new paper, they explain the mechanism as follows:\n\n[T]he mechanism need not be a direct effect of those traits (whether culturally or genetically transmitted) on income and productivity. Rather, divergence in human traits, habits, norms, etc. have created barriers to communication and imitation across societies. While it is possible that intergenerationally transmitted traits have direct effects on productivity and economic performance (for example, if some parents transmit a stronger work ethic to their children), another possibility is that human traits also act to hinder development through a barrier effect: more closely related societies are more likely to learn from each other and adopt each other’s innovations. It is easier for someone to learn from a sibling than from a cousin, and easier to learn from a cousin than from a stranger. Populations that share more recent common ancestors have had less time to diverge in a wide range of traits and characteristics - many of them cultural rather than biological - that are transmitted from a generation to the next with variation. Similarity in such traits facilitates communication and learning, and hence the diffusion and adaptation of complex technological and institutional innovations.\n\nSpolaore and Wacziarg also mention the other (few) core papers or books in the field of biology and economic growth - by Gregory Clark, Galor and Moav and Ashraf and Galor. It is still a short list.\nOne of their conclusions for this review is that greater focus should be on populations and not locations:\n\nThe importance of controlling for populations’ ancestry highlights the second message from this literature: long-term persistence holds at the level of populations rather than locations. A focus on populations rather than locations helps us understand both persistence and reversal of fortune, and sheds light on the spread of economic development. The need to adjust for population ancestry is at the core of Putterman and Weil’s (2010) contribution, showing that current economic development is correlated with historical characteristics of a population’s ancestors, including ancestors’ years of experience with agriculture, going back, again, to the Neolithic transition. The overall message from Comin, Easterly and Gong (2010), Putterman and Weil (2010) and several other contributions covered in this article is that long-term historical factors predict current income per capita, and that these factors become much more important when considering the history of populations rather than locations.\n\nThe article also has some interesting thoughts on gene-culture coevolution (is there anything else?) and is generally worth the read."
  },
  {
    "objectID": "posts/the-difference-between-knowing-the-name-of-something-and-knowing-something.html",
    "href": "posts/the-difference-between-knowing-the-name-of-something-and-knowing-something.html",
    "title": "The difference between knowing the name of something and knowing something",
    "section": "",
    "text": "In an excellent article over at Behavioral Scientist (read the whole piece), Koen Smets writes:\n\nA widespread misconception is that biases explain or even produce behavior. They don’t—they describe behavior. The endowment effect does not _cause _people to demand more for a mug they received than a mug-less counterpart is prepared to pay for one. It is not _because of _the sunk cost fallacy that we hang on to a course of action we’ve invested a lot in already. Biases, fallacies, and so on are no more than labels for a particular type of observed behavior, often in a peculiar context, that contradicts traditional economics’ simplified view of behavior.\n\nA related point was made by Owen Jones in his paper Why Behavioral Economics Isn’t Better, and How it Could Be:\n\n[S]aying that the endowment effect is caused by Loss Aversion, as a function of Prospect Theory, is like saying that human sexual behavior is caused by Abstinence Aversion, as a function of Lust Theory. The latter provides no intellectual or analytic purchase, none, on why sexual behavior exists. Similarly, Prospect Theory and Loss Aversion – as valuable as they may be in describing the endowment effect phenomena and their interrelationship to one another – provide no intellectual or analytic purchase, none at all, on why the endowment effect exists. …\n[Y]ou can’t provide a satisfying causal explanation for a behavior by merely positing that it is caused by some psychological force that operates to cause it. That’s like saying that the orbits of planets around the sun are caused by the “orbit-causing force.” …\n[L]oss aversion rests on no theoretical foundation. Nothing in it explains why, when people behave irrationally with respect to exchanges, they would deviate in a pattern, rather than randomly. Nor does it explain why, if any pattern emerges, it should have been loss aversion rather than gain aversion. Were those two outcomes equally likely? If not, why not?\n\nAnd here’s Richard Feynman on the point more generally (from What Do You Care What Other People Think):\n\nWe used to go to the Catskill Mountains, a place where people from New York City would go in the summer. The fathers would all return to New York to work during the week, and come back only for the weekend. On weekends, my father would take me for walks in the woods and he’d tell me about interesting things that were going on in the woods. When the other mothers saw this, they thought it was wonderful and that the other fathers should take their sons for walks. They tried to work on them but they didn’t get anywhere at first. They wanted my father to take all the kids, but he didn’t want to because he had a special relationship with me. So it ended up that the other fathers had to take their children for walks the next weekend.\nThe next Monday, when the fathers were all back at work, we kids were playing in a field. One kid says to me, “See that bird? What kind of bird is that?”\nI said, “I haven’t the slightest idea what kind of a bird it is.”\nHe says, “It’s a brown-throated thrush. Your father doesn’t teach you anything!”\nBut it was the opposite. He had already taught me: “See that bird?” he says. “It’s a Spencer’s warbler.” (I knew he didn’t know the real name.) “Well, in Italian, it’s a Chutto Lapittida. In Portuguese, it’s a Bom da Peida. In Chinese, it’s a Chung-long-tah, and in Japanese, it’s a Katano Tekeda. You can know the name of that bird in all the languages of the world, but when you’re finished, you’ll know absolutely nothing whatever about the bird. You’ll only know about humans in different places, and what they call the bird. So let’s look at the bird and see what it’s doing—that’s what counts.” (I learned very early the difference between knowing the name of something and knowing something.)\n\nKnowing the name of a “bias” such as loss aversion isn’t zero knowledge - at least you know it exists. But knowing something exists is a very shallow understanding.\nAnd back to Koen Smets:\n\nLearning the names of musical notes and of the various signs on a staff doesn’t mean you’re capable of composing a symphony. Likewise, learning a concise definition of a selection of cognitive effects, or having a diagram that lists them on your wall, does not magically give you the ability to analyze and diagnose a particular behavioral issue or to formulate and implement an effective intervention.\nBehavioral economics is not magic: it’s rare for a single, simple nudge to have the full desired effect. And being able to recite the definitions of cognitive effects does not magically turn a person into a competent behavioral practitioner either. When it comes to understanding and influencing human behavior, there is no substitute for experience and deep knowledge. Nor, perhaps even more importantly, is there a substitute for intellectual rigor, humility, and a healthy appreciation of complexity and nuance."
  },
  {
    "objectID": "posts/the-end-of-women.html",
    "href": "posts/the-end-of-women.html",
    "title": "The end of women",
    "section": "",
    "text": "The Economist has an amusing reductio ad absurdum in its regular Daily Charts section. At current fertility rates, The Economist predicts it will take 25 generations for Hong Kong’s population to drop from its current 3.25 million to just one - which by their calculations will occur in the year 2798. China and Japan will be down to their last woman around 3500 AD, with Canada holding out until almost 4500 AD.\nIf you browse through any official projections of population in developed countries, they tend to suggest that the current below replacement levels of fertility are here to stay. But would anyone argue that fertility rate in developed countries will not, at some point, increase?"
  },
  {
    "objectID": "posts/the-evolution-institute.html",
    "href": "posts/the-evolution-institute.html",
    "title": "The Evolution Institute",
    "section": "",
    "text": "In the first of a series of blog posts by David Sloan Wilson on economics and evolution (which I will blog about in the coming weeks as the posts contain some interesting ideas), Wilson introduced The Evolution Institute, a think tank that seeks to apply evolutionary theory to modern policy problems.\nI had not heard of the Institute before, but naturally I consider that integration of evolutionary thinking into any policy framework can bring value. It will be interesting to see whether the Institute is be used as a vehicle to push pre-established economic views, or for a genuine exploration about how evolutionary biology might add something to current economic thought. From my brief look through the Institute’s website, it does not seem that the Institute has produced any tangible outcomes yet, but there are plenty of statements of intent. It will be interesting to see what actually emerges.\nThe Institute frames its economics focal topic around the need to have an economics that reflects how humans actually act, with behavioural economics as the immediate focus of integrating evolution into economics. Interestingly, the objective of the topic is not to introduce more behavioural economics findings into mainstream economic theory, but to give a framework to behavioural economics itself. The website states:\n\nThere is widespread agreement that economic theory must become based on a more accurate conception of human nature to successfully guide public policy. That is the objective of behavioral economics, which has become prominent within the larger field of economics. However, behavioral economics needs to become more broadly based in the human behavioral sciences, which in turn must be grounded in evolutionary theory. The purpose of our project is to properly ground behavioral economics in evolutionary science, including the study of proximate mechanisms in a more fully rounded sense.  We intend to provide the most accurate conception of human nature possible based on current scientific knowledge, oriented toward the formulation of economic theory and public policy.\n\nIf a project of this type succeeds, behavioural economics would be much more useful. It might actually extend beyond being a catalogue of paradoxes and biases and offer a framework under which economic decisions might be understood.\n*My four posts on David Sloan Wilson’s Economics and Evolution as Different Paradigms can be found here, here, here and here."
  },
  {
    "objectID": "posts/the-evolution-of-cornets.html",
    "href": "posts/the-evolution-of-cornets.html",
    "title": "The evolution of cornets",
    "section": "",
    "text": "In my recent review of Paul Ormerod’s Why Most Things Fail, I asked if Ormerod’s comparison between the extinction of species and the death of firms was the right analogy. One reason for my question was that species are typically defined due to their reproductive isolation, preventing gene transfer between species. In contrast, the unit of selection for firms, business plan modules (the name used by Eric Beinhocker), can spread freely spread between firms.\nInterestingly, I have just picked up Kevin Kelly’s What Technology Wants, 18 months after my post speculating on Kelly’s approach, where he raises a similar issue. Kelly notes that whereas the passage of biological traits is limited to the passing of traits to offspring, technology transmission can occur horizontally. He writes:\n\n[N]ature can’t plan ahead. It does not hoard innovations for later use. If a variation in nature does not provide an immediate survival advantage, it is too costly to maintain and so over time it disappears. But sometimes a trait advantageous for one problem will turn out to be advantageous for a second, unanticipated problem. … These inadvertent anticipatory inventions are called exaptations in biology. We don’t know how common exaptations are in nature, but they are routine in the technium. The technium is nothing but exaptations, since innovations can be easily borrowed across lines of origin or moved across time and repurposed.\nNiles Eldredge is the cofounder (with Stephen Jay Gould) of the theory of punctuated, stepwise evolution. … Once Eldredge applied his professional taxonomic methods to his collection of 500 cornets, some dating back to 1825. He selected 17 traits that varied among his instruments—the shape of their horns, the placement of the valves, the length and diameter of their tubes—very similar to the kinds of metrics he applies to trilobites. When he mapped the evolution of cornets using techniques similar to those he applies to ancient arthropods, he found that the pattern of the lineages were very similar in many ways to those of living organisms. As one example, the evolution of cornets showed a stepwise progress, much like trilobites. But the evolution of musical instruments was also very distinctive. The key difference between the evolution of multicellular life and the evolution of the technium is that in life most blending of traits happens “vertically” in time. Innovations are passed from living parents down (vertically) through offspring. In the technium, on the other hand, most blending of traits happens laterally across time—even from “extinct” species and across lineages from nonparents. Eldredge discovered that the pattern of evolution in the technium is not the repeated forking of branches we associate with the tree of life, but rather a spreading, recursive network of pathways that often double back to “dead” ideas and resurrect “lost” traits.\n\nKelly goes as far as suggesting that no species of technology ever goes extinct. I’m not sure that calling a technology a species is the right approach - is the species the horn and the feature of the horn the unit of selection? - but the general point has some significant consequences for the biology-technology analogy. Similarly, the firm-species analogy is not a perfect fit due to this horizontal transfer of business plan modules, and possibility that individual modules never go extinct. In the case of Ormerod’s analysis however, I’m not yet sure what the consequences of that difference is.\nThe article by Tëmkin and Eldredge that charts the evolution of cornets can be found in Current Anthropology (ungated version here)."
  },
  {
    "objectID": "posts/the-evolution-of-technology.html",
    "href": "posts/the-evolution-of-technology.html",
    "title": "The evolution of technology",
    "section": "",
    "text": "In Kevin Kelly’s recent appearance on Econtalk, he talked about his new book What Technology Wants. I have not read the book but some of Kelly’s statements about human evolution are worth comment.\nKelly notes that, to a degree, humans created our own humanity. Kelly talked about how humans are the first domesticated animal, in that we used our minds to domesticate ourselves. While we are a continuation of the primate line, we have added with our minds things other primates don’t have. For example, by inventing cooking, humans effectively developed an external stomach to digest food that we were not normally able to digest. This additional nutrition changed the size of our teeth, the shape of our jaw and the enzymes in our stomachs. When humans domesticated livestock, we developed lactose tolerance. As a result, we are self-created - both the creator and the created.\nMy first thought on Kelly’s observations is around the idea of us evolving a trait such as lactose tolerance. The way it happens is that those with the lactose tolerance gene have higher fitness than those without and so, their descendants come to form a larger part of the population. So, when “we” domesticated milk producing animals, it was the end of the genetic line for many humans.\nMy second thought is a more substantive criticism. Through sexual selection, many animals are both the creator and created. Choice by peahens created the peacock’s tail. Choice by female bower birds has led male bower birds to evolve a preference for constructing elaborate bowers. Taking a human example, Geoffrey Miller argues that female choice shaped the human brain in the first place. To isolate humans as special in being both the creator and created is to ignore an important evolutionary force.\nOn a speculative note, Kelly is drawing a long bow when using evolution as part of the argument that technology is progressive. Evolution is not uni-directional and while there is a general tendency towards complexity (when compared to the earliest life forms), there is no law pushing it in one direction. I will need to read the book before unequivocally accusing Kelly of misusing the evolution analogy but if he were to do so, he certainly would not be alone - just read Matt Ridley’s The Rational Optimist: How Prosperity Evolves."
  },
  {
    "objectID": "posts/the-gell-mann-amnesia-effect.html",
    "href": "posts/the-gell-mann-amnesia-effect.html",
    "title": "The Gell-Mann amnesia effect",
    "section": "",
    "text": "I spotted this in a tweet from Abe List yesterday, and love the idea. The original source is a speech by Michael Crichton (which is worth reading in itself).\n\nMedia carries with it a credibility that is totally undeserved. You have all experienced this, in what I call the Murray Gell-Mann Amnesia effect. (I refer to it by this name because I once discussed it with Murray Gell-Mann, and by dropping a famous name I imply greater importance to myself, and to the effect, than it would otherwise have.)\nBriefly stated, the Gell-Mann Amnesia effect is as follows. You open the newspaper to an article on some subject you know well. In Murray’s case, physics. In mine, show business. You read the article and see the journalist has absolutely no understanding of either the facts or the issues. Often, the article is so wrong it actually presents the story backward—reversing cause and effect. I call these the “wet streets cause rain” stories. Paper’s full of them.\nIn any case, you read with exasperation or amusement the multiple errors in a story, and then turn the page to national or international affairs, and read as if the rest of the newspaper was somehow more accurate about Palestine than the baloney you just read. You turn the page, and forget what you know.\nThat is the Gell-Mann Amnesia effect. I’d point out it does not operate in other arenas of life. In ordinary life, if somebody consistently exaggerates or lies to you, you soon discount everything they say. In court, there is the legal doctrine of falsus in uno, falsus in omnibus, which means untruthful in one part, untruthful in all. But when it comes to the media, we believe against evidence that it is probably worth our time to read other parts of the paper. When, in fact, it almost certainly isn’t. The only possible explanation for our behavior is amnesia.\n\nMy general news consumption is deliberately low due to a combination of mistrust and a desire to read and think about things that matter."
  },
  {
    "objectID": "posts/the-gender-reading-gap-and-love-of-learning.html",
    "href": "posts/the-gender-reading-gap-and-love-of-learning.html",
    "title": "The gender reading gap and love of learning",
    "section": "",
    "text": "Two interesting education snippets.\nFirst, Brookings has released a new report that looks at the gender gap in reading:\n\nGirls outscore boys on practically every reading test given to a large population. And they have for a long time. A 1942 Iowa study found girls performing better than boys on tests of reading comprehension, vocabulary, and basic language skills. Girls have outscored boys on every reading test ever given by the National Assessment of Educational Progress (NAEP)—the first long term trend test was administered in 1971—at ages nine, 13, and 17. The gap is not confined to the U.S. Reading tests administered as part of the Progress in International Reading Literacy Study (PIRLS) and the Program for International Student Assessment (PISA) reveal that the gender gap is a worldwide phenomenon. In more than sixty countries participating in the two assessments, girls are better readers than boys.\nPerhaps the most surprising finding is that Finland, celebrated for its extraordinary performance on PISA for over a decade, can take pride in its high standing on the PISA reading test solely because of the performance of that nation’s young women. With its 62 point gap, Finland has the largest gender gap of any PISA participant, with girls scoring 556 and boys scoring 494 points (the OECD average is 496, with a standard deviation of 94). If Finland were only a nation of young men, its PISA ranking would be mediocre.\n\nAnd where does love of learning come from? From a new twin study:\n\nLittle is known about why people differ in their levels of academic motivation. This study explored the etiology of individual differences in enjoyment and self-perceived ability for several school subjects in nearly 13,000 twins aged 9–16 from 6 countries. The results showed a striking consistency across ages, school subjects, and cultures. Contrary to common belief, enjoyment of learning and children’s perceptions of their competence were no less heritable than cognitive ability. Genetic factors explained approximately 40% of the variance and all of the observed twins’ similarity in academic motivation. Shared environmental factors, such as home or classroom, did not contribute to the twin’s similarity in academic motivation. Environmental influences stemmed entirely from individual specific experiences."
  },
  {
    "objectID": "posts/the-genetic-architecture-of-economic-and-political-preferences.html",
    "href": "posts/the-genetic-architecture-of-economic-and-political-preferences.html",
    "title": "The genetic architecture of economic and political preferences",
    "section": "",
    "text": "Evidence from twin studies implies that economic and political traits have a significant heritable component. That is, some of the variation between people is attributable to genetic variation.\nDespite this, there has been a failure to demonstrate that the heritability can be attributed to specific genes. Candidate gene studies, in which a single gene (or SNP) is examined for its potential influence on a trait, have long failed to identify effects beyond a fraction of one per cent. Further, many of the candidate gene results fail to be replicated in studies with new samples.\nAn alternative approach to genetic analysis is now starting to address this issue. Genomic-relatedness-matrix restricted maximum likelihood (GREML - the term used by the authors of the paper discussed below) is a technique that looks to examine how the variance in traits can be explained by all of the SNPs simultaneously. This approach has been used to examine height, intelligence, personality and several diseases, and has generally shown that half of the heritability estimated in twin studies can be attributed to the sampled SNPs.\nA new paper released in PNAS seeks to apply this approach to economic and political phenotypes. The paper by Benjamin and colleagues shows that around half the heritability in economic and political behaviour observed in behavioural studies could be explained by the array of SNPs.\nThe authors used the results of recent surveys of subjects from the Swedish Twin Registry, who had their educational attainment, four economic preferences (risk, patience, fairness and trust) and five political preferences (immigration/crime, foreign policy, environmentalism, feminism and equality, and economic policy) measured. The GREML analysis found that for one economic preference, trust, the level of variance explained by the SNPs was statistically significant, with an estimate of narrow heritability of over 0.2. Two of the political preferences, economic policy and foreign policy, had narrow heritability that was statistically significant, with heritability estimates above 0.3 for each of these. The authors noted that as the estimates are noisy and GREML provides a lower bound, the results are consistent with low to moderate heritability for these traits.\nEducational attainment was also found to have a statistically significant result, although the more precise measurement of educational attainment and the availability of this data across all subjects made that result more likely.\nThis result is corroboration of the evidence from twin studies and provides a basis for believing that molecular genetic data could be used to predict phenotypic traits. However, one interesting feature of the GREML method of analysis is that after conducting this analysis with one sample, the data obtained does not assist in predicting the traits for someone out of the sample. This technique shows the potential of molecular genetic data without directly realising those results.\nAs a comparison, the authors examined whether any individual SNPs might predict economic or political preferences, but found none that met the significance test standard of 5x10-8. Such a high level of significance is required to reflect the huge number of SNPs that are being tested.\nThe authors also conducted the standard comparison between monozygotic (identical) and dizygotic (fraternal) twins, which resulted in heritability estimates consistent with the existing literature, although with a much larger sample than typically used. Looking through the supplementary materials, the major surprise to me was that the twin analysis suggests that patience has low heritability, with a very low correlation between twins and almost no difference between monozygotic and dizygotic twins (in fact, for males, dizygotic twins were more similar).\nThe authors draw a few conclusions from their work, many which reflect the argument in a Journal of Economic Perspectives article from late last year. The first and most obvious is that we should treat all candidate gene studies with caution. Hopefully some journals that insist on publishing low sample size candidate gene studies will pay attention to this. Where they are going to be conducted, you need very large samples, and significantly larger than are being used in most studies being published.\nMeanwhile, they are still hopeful that there can be a contribution from genetic research, particularly if the biological pathways between the gene and trait can be determined. This might include using genes as instrumental variables or as control variables in non-genetic empirical work. The use as instrumental variables does require, however, some understanding of the pathways through which the gene acts as it may have multiple roles (that is, it is pleiotropic). They also suggest that the focus be turned to SNPs for which there are known large effects and the results have been replicated.\nOn element of analyses of political and economic preferences that makes me slightly uncomfortable is the loose nature of these preferences. For one, the manner in which they are elicited from subjects can vary substantially, as can the nature of the measurement. Take the 2005 paper by Alford and colleagues on political preferences, which canvassed 28 political preferences. Many of the views are likely to change over time and be highly correlated with each other. And why stop at 28?\nAs a result, it may be preferable to take a step back and ensure that data on higher level traits are collected. I generally consider that IQ and the big five personality traits (openness, conscientiousness, agreeableness, extraversion and stability) are a good starting point and are likely to capture much of the variation in political and economic preferences. For example, preferences such as patience are likely to be reflected in IQ, while openness captures much of the liberal-conservative spectrum of political leaning. Starting from a basis such as this may also give greater scope for working back to the biological pathways.\nThe Social Science Genetics Association Consortium is doing some work in harmonising phenotypes across large samples. Hopefully their work will lead in this direction."
  },
  {
    "objectID": "posts/the-growth-of-atheism.html",
    "href": "posts/the-growth-of-atheism.html",
    "title": "The growth of atheism",
    "section": "",
    "text": "Nigel Barber of The Daily Beast (Psychology Today) has posted on a forthcoming article in which he shows that the level of atheism increases with the quality of life. Barber explains the trend as follows:\n\nThe reasons that churches lose ground in developed countries can be summarized in market terms. First, with better science, and with government safety nets, and smaller families, there is less fear and uncertainty in people’s daily lives and hence less of a market for religion. At the same time many alternative products are being offered, such as psychotropic medicines and electronic entertainment that have fewer strings attached and that do not require slavish conformity to unscientific belief.\n\nBarber pulls out some interesting evidence in support of his claim. For example, he argues in the article that superstition increases in the face of uncertainty:\n\nGmelch (1974) found that different positions in baseball evoke varying levels of superstitious behavior: When players fielded they were less superstitious than when batting or pitching, and this difference was attributed to the fact that fielding errors are rare whereas even good batters miss the ball more often than they hit it. Recently Burger and Lynn (2005) supported the uncertainty hypothesis by finding that major league baseball players engaged in more superstitious behavior the more that they believed the outcome was determined by luck. Interestingly, men and women become more religious when they view pictures of attractive same-sex mating competitors, another possible case of uncertain outcomes (Li, Cohen, Weeden, & Kenrick, 2010).\n\nThis is one of those articles where I am happy to agree with the results. However, on Barber’s longer term prediction of increasing atheism as developing countries develop, Barber did not discuss the higher fertility of those with religion and the heritability of religious belief.\nTake the United States. Religiosity has undergone significant long-term decline, consistent with Barber’s findings. However, among the remaining religious people in the population, fertility rates are higher. With religiosity heritable, it is open to ask which dynamic will win in the long-term - the decline due to higher quality of life or the higher fertility of those who maintain religious beliefs despite the increase in quality of life. As the security dynamic has largely played out in most people’s lives in developed countries, my money is on the latter.\nOne way of putting this might be to consider the increase in quality of life as a shock. Some people respond to this shock by dropping their religious belief. As they form the majority of the population, the rate of religious belief initially drops. However, those who are immune to the shock maintain their religious belief. As those with religious belief have higher fertility, they eventually come to form the larger proportion of the population - as Rowthorn suggested in a paper I previously posted about.\nFrom this, we would support Barber’s prediction that as the quality of life in developing countries increases, religiosity might decrease. In developed countries however, the trend will start to head the other way.\nAs a final thought, Barber finds higher levels of atheism where there is less income inequality and higher taxation rates. While Barber also puts this down to the security hypothesis, I wonder how much of this can be attributed to state involvement in religion. In European countries with high taxation rates, the church is often state sponsored, with limited competition between religious offerings. In such an environment, it is no surprise that there are not more takers for the moribund religious alternatives."
  },
  {
    "objectID": "posts/the-human-benchmark-is-typically-unimpressive.html",
    "href": "posts/the-human-benchmark-is-typically-unimpressive.html",
    "title": "The human benchmark is typically unimpressive",
    "section": "",
    "text": "If you ever read a claim of an AI outperforming a human, dig into the performance data to check out the human benchmark. The mediocrity of the human is often more salient than the competence of the AI. The AI has an easy job.\nHere’s an example from a paper by Ayers and friends (2023). They sampled 195 question and response exchanges on Reddit’s r/AskDocs and entered those same questions into ChatGPT 3.5. Responses by the Reddit physicians and ChatGPT were then independently rated for quality and empathy.\nChatGPT responses were rated as higher quality, with 78% of ChatGPT responses rated as good or very good quality. 22% of physician responses had that rating. Similarly, 45% of ChatGPT responses were rated empathetic or very empathetic, compared to 5% for the physicians.\n\n\nDistribution of Average Quality and Empathy Ratings for Chatbot and Physician Responses to Patient Questions\n\n\n\nI downloaded the csv file of the responses. The most striking feature was the brevity and curtness of the humans (52 words average compared to 211 for ChatGPT). Here are a few of the human responses.\nA friend’s uninsured mother needs a liver transplant. What should she do?\n\nIts not just a work around the insurance, she will need lifelong expensive meds after surgery as well. Why does she not have insurance?\n\nShould they be worried about arm pain when they sneeze?\n\nThe answer simply is no..\n\nAre they crazy for thinking they have pancreatic or some other cancer given a list of symptoms?\n\nWhy do you think you have pancreatic cancer?\n\nWhere can someone from a family of anti-vaxers get information on vaccines?\n\nSee your general practitioner. They’ll understand and will sort you out bro.\n\nDoes smelly poop and less frequent bowel movements indicate cancer?\n\nThis does not sound like cancer. I would advise stop googling.\n\nIn contrast, ChatGPT gave long, involved answers that inevitably began with a statement of concern for the medical condition. “I’m sorry to hear about your friend’s mom.” Funnily enough, that verbosity annoys me the most in using LLMs. One person’s verbosity is another’s empathy.\nI don’t mean the above to be read as a criticism of AI performance. (This is ChatGPT 3.5 after all!) But there’s an interesting contrast between performance benchmarking and discussions of the role of AI. In benchmarking exercises, we use actual human performance as the standard. But when discussing whether AI should replace or augment human decisions, we often compare AI to an idealised human. We have a mental image of a high-quality physician with excellent bedside manner, not the distribution of skills and empathy we actually have. Scrutinising the performance data for some of these benchmarking exercises is a good reminder that AI doesn’t always have to be great to be better.\n\n\n\n\nReferences\n\nAyers, J. W., Poliak, A., Dredze, M., Leas, E. C., Zhu, Z., … Smith, D. M. (2023). Comparing physician and artificial intelligence chatbot responses to patient questions posted to a public social media forum. JAMA Internal Medicine, 183(6), 589–596. https://doi.org/10.1001/jamainternmed.2023.1838"
  },
  {
    "objectID": "posts/the-illusion-of-the-illusion-of-control.html",
    "href": "posts/the-illusion-of-the-illusion-of-control.html",
    "title": "The illusion of the illusion of control",
    "section": "",
    "text": "In the spirit of my recent post on overconfidence, the illusion of control is another “bias” where imperfect information might be a better explanation for what is occurring.\nThe illusion of control is a common finding in psychology that people believe they can control things that they cannot. People would prefer to pick their lottery numbers than have them randomly allocated - being willing to even pay for the privilege. In laboratory games, people often report having control over outcomes that were randomly generated.\nThis effect was labelled by Ellen Langer as the illusion of control (for an interesting read about Langer’s other work, see here). The decision making advice that naturally flows out of this - and you will find in plenty of books building on the illusion of control literature - is that we need to recognise that we can control less than we think. Luck plays a larger role than we believe.\nBut when you ask about people’s control of random events, which is the typical experimental setup in this literature, you can only get errors in one direction - the belief that they have more control than they actually do. It is not possible to believe you have less than no control.\nSo what do people believe in situations where they do have some control?\nIn Left Brain, Right Stuff, Phil Rosenzweig reports on research (pdf) by Francesca Gino, Zachariah Sharek and Don Moore in which people have varying degrees of control over whether clicking a mouse would change the colour of the screen. For those that had no or little control (clicking the mouse worked 0% or 15% of the time), the participants tended to believe they had more control than they did - an illusion of control.\nBut when it came to those who had high control (clicking the mouse worked 85% of the time), they believed they had less control than they did. Rather than having an illusion of control, they failed to recognise the degree of control that they had. The one point where there was accurate calibration was when there was 100% control.\nThe net finding of this and other experiments is that we don’t systematically have an illusion on control. Rather, we have imperfect information about our level of control. When low, we tend to overestimate. When high (but not perfect), we tend to underestimate.\nThat the illusion of control was previously seen to be largely acting in one direction was due to experimental design. When people have no control and can only err in one way, that is naturally what will be found. Gino and friends term this problem as the illusion of the illusion of control.\nSo when it comes to decision making advice, we need to be aware of the context. If someone is picking stocks or something of that nature, the illusion of control is not helpful. But in their day-to-day life where they have influence over many outcomes, underestimating control could be a serious error.\nShould we be warning against underestimating control? If we were to err consistently in one direction, it is not clear to me that having an illusion of control is of greater concern. Maybe we should err on the side of believing we can get things done.\n*As an aside, there is a failed replication (pdf) of one of Langer’s 1975 experiments from the paper for which the illusion is named."
  },
  {
    "objectID": "posts/the-intergenerational-transmission-of-economic-development.html",
    "href": "posts/the-intergenerational-transmission-of-economic-development.html",
    "title": "The intergenerational transmission of economic development",
    "section": "",
    "text": "In my last post, I noted one of the major themes of a new Journal of Economic Literature paper, How Deep Are the Roots of Economic Development (ungated pdf). Enrico Spolaore and Romain Wacziarg reviewed some literature that makes a strong case that it is population, not institutions, that underlies long-term economic growth. This post turns to the focus of the second half of the article - the genetic and cultural intergenerational transmission of development. If it is populations that underlie development, how are the traits that affect development passed through the generations?\nSpolaore and Wacziarg start their analysis by slicing the analysis of intergenerational transmission into two dimensions. First, there is the genetic and cultural dimension, which comprises three types of transmission: genetic, cultural and gene-culture coevolution (often called dual inheritance theory). Under gene-culture coevolution, both genes and cultures are transmitted across generations in a unified framework.\nThe second dimension concerns whether the intergenerationally transmitted traits directly affect productivity and economic performance, or act as a barrier to the flow of technological and institutional innovations. It is this second pathway that is the subject of the Spolaore and Wacziarg’s paper The Diffusion of Development, which analyses the effect of genetic distance on economic development.\nWith these dimensions, we have six possible combinations into which we can classify this type of research. I am not a fan of this taxonomy, largely because of the biological-cultural dimension. First, there is no case where there is not gene-culture interaction. Where a paper tends to focus generally relates to whether the question being asked warrants one of these factors being held fixed for the analysis. Further, it is difficult to make a model of economic development that is exclusively genetic as they inherently contain culture - the economic part of the model. For example, Spolaore and Wacziarg classify Galor and Moav’s paper on the inheritance of the preference for high or low quality children as being biological, even though technology progresses due to the genetically predispositions of the agents, and technological progress causes changes in the relative fitness of the model agents. There is two-way feedback - a gene-culture interaction.\nFunnily enough, Spolaore and Wacziarg also do not seem enamoured with their distinction between biological, cultural and gene-culture evolution, although for different reasons to me. They conclude that:\n\n[T]he most recent scientific literature suggests that it may be conceptually very difficult, or even meaningless, to separate biological and cultural mechanisms, given the coevolution of biological traits. Consequently, a more productive approach, from an empirical perspective, is to focus on whether intergenerationally transmitted traits - whether biological or cultural - operate directly or as barriers to the diffusion of innovations.\n\nBut are they seriously saying that the dairy example they offer - coevolution of the milking of livestock and lactose tolerance - has a meaningless distinction between culture and genetics? In parts of the article they seem to have taken the idea that nature and nurture interact and inferred that this means the distinction cannot be made. But it can, just as in the dairy example. What would be incorrect is to say that the dairying example is purely a genetic or cultural process.\nAnother factor that may have triggered their assessment about the nature-nurture distinction is that in articles and books analysing genetic factors underlying economic development, the author is often equivocal about whether the transmission mechanism is biological or cultural. In Galor and Moav’s paper I referred to above, they note in a footnote that transmission could equally be cultural. In Greg Clark’s A Farewell to Alms, Clark often appears to be wavering between whether the transmission of traits was cultural or genetic (although he appears to have moved more to the genetic side).\nWhile there is equivocation from those who suggest there may be genetic factors, there is usually no such uncertainty expressed by those who suggest there is transmission of cultural factors underlying economic development. For example, Spolaore and Wacziarg refer to a paper by Doepke and Zilibotti in which altruistic parents shape their children’s preferences, ruling out genetic factors on the basis that the timescale covers “at most, a few centuries”, despite it being a timescale similar to that considered by Clark. A paper by Fernandez and Fogli on second generation Americans and the explanatory power of preferences in their home country could similarly be placed in the genetic basket, as could Algan and Cahuc’s paper on the inheritance of trust. Even where there is an explicitly cultural transmission, such as Francois and Zabojnik’s analysis of trust, the persistent trait sought to be explained often has material heritability in modern environments, suggesting a genetic factor is relevant.\nHaving said the above, the way I would frame the taxonomy is to consider it all as gene-culture coevolution, and within that framework ask what genetic and cultural factors are and how they are evolving and driving the economic outcomes.\nI find Spolaore and Wacziarg’s second dimension of direct and barrier effects more useful. But towards the close of their article, they suggest that analysis of the direct effects of intergenerationally transmitted traits may be too hard, making a focus on barrier effects more likely to yield results.\n\nFor instance, while barrier effects can explain how the Industrial Revolution spread across different societies over time and space, it is much harder to identify which intergenerationally transmitted traits, if any, are responsible for the original onset of such a major technological and institutional change. This difficulty is due to at least two reasons. Firstly, phenomena such as the Industrial Revolution are, almost by definition, unique and exceptional, and therefore one cannot build a data set of different and independent Industrial Revolutions to test alternative theories of onset. Secondly, such a complex phenomenon is likely to be the outcome of a vast set of forces and causes, including historical accidents and contingencies.\n\nTrue, it’s a hard question. But it is strange to close an article on the deep roots of development that tracks those roots to thousands of years before the Industrial Revolution with a statement that we may not be able to figure it out. More importantly, classing observed effects as barrier effects without analysing alternative hypotheses such as the direct effect of intergenerational transmission runs the risk of misspecification of models and incorrect attribution of causal relationships.\nUltimately, I wonder how much Spolaore and Wacziarg’s position is driven by the academic barriers to open discussion on genetic direct effects on economics development. It’s not a comfortable area of analysis and can attract critical attention, as Ashraf and Galor’s work did earlier this year.\nStill, having spent most of this post complaining about a couple of parts of it, Spolaore and Wacziarg’s article is excellent. And as I noted in my last post, I’ve added the paper to my evolutionary biology and economics reading list."
  },
  {
    "objectID": "posts/the-invisible-hand-of-jupiter.html",
    "href": "posts/the-invisible-hand-of-jupiter.html",
    "title": "The invisible hand of Jupiter",
    "section": "",
    "text": "I’m note sure how I hadn’t come across this before (one need only read the Wikipedia entry “invisible hand”), but Adam Smith used the phrase “invisible hand” three times. It is used once in The Theory of Moral Sentiments (1759) and The Wealth of Nations (1776) - both of those I knew. The third time comes from a posthumously published (1795) essay The History of Astronomy, written before The Theory of Moral Sentiments. Smith wrote:\n\nFor it may be observed, that in all Polytheistic religions, among savages, as well as in the early ages of heathen antiquity, it is the irregular events of nature only that are ascribed to the agency and power of the gods. Fire burns, and water refreshes; heavy bodies descend, and lighter substances fly upwards, by the necessity of their own nature; nor was the invisible hand of Jupiter every apprehended to be employed in those matters. But thunder and lightning, storms and sunshine, those more irregular events, were ascribed to his favour, or his anger. Man, the only designing power with which they were acquainted, never acts but either to stop, or to alter the course, which natural events would take, if left to themselves. Those other intelligent beings, whom they imagined, but knew not, were naturally supposed to act in the same manner; not to employ themselves in supporting the ordinary course of things, which went on of its own accord, but to stop, to thwart, and to disturb it. And thus, in the first ages of the world, the lowest and most pusillanimous superstition supplied the place of philosophy.\n\nIn this case, the invisible hand of Jupiter is the explanation that “savages” and those in “the early ages of heathen antiquity” apply to otherwise unexplainable irregular events. It isn’t much help in interpreting the other two uses.\nHT: Jag Bhalla"
  },
  {
    "objectID": "posts/the-law-of-laws-leverage.html",
    "href": "posts/the-law-of-laws-leverage.html",
    "title": "The law of law’s leverage",
    "section": "",
    "text": "Last week I posted on Owen Jones’s 2000 article Time-Shifted Rationality and the Law of Law’s Leverage: Behavioral Economics Meets Behavioral Biology and his argument that behavioural economics (and law) requires the theoretical backbone of evolutionary biology.\nThe second half of that article has a neat idea - what Jones calls the law of law’s leverage. The basic idea is that the effectiveness of laws will vary with the adaptiveness (in ancestral environments) of the behaviour the law is trying to change. Jones describes it as follows:\n\nThe law of law’s leverage predicts that less legal intervention will be necessary to shift a behavior in ways that tended to increase reproductive success in ancestral environments than will be necessary to shift behavior in ways that tended to decrease reproductive success in ancestral environments. Put another way, the slope of the demand curve for historically adaptive behavior that is now deemed to be socially (in some cases even individually) undesirable will be far steeper than the slope of the demand curve for behavior that was comparatively less adaptive in ancestral environments. Importantly, this relationship between the slopes will hold, even when the costs that an individual actually and foreseeably incurs in behaving in an historically adaptive way will exceed presently foreseeable benefits of such behavior.\n\nAs an example:\n\nEvolutionary analysis predicts that, and explains why, the slope of the demand curve for adulterous behavior is likely to be comparatively steep (as is the slope for most sexual behavior) and thus comparatively insensitive to the imposition of legal prohibitions (or other costs, such as effect on career). Evolutionary analysis also predicts that, and may help explain why, marriage, separation, divorce, and remarriage behavior will be less sensitive to legal changes than will be many other forms of behavior. Because, as we know, natural selection disfavors inbreeding among close relatives, evolutionary analysis also and separately predicts that it will be far less costly to discourage incest among parents and their natural children, and between siblings reared together, than among stepparents and stepchildren, and among stepchildren. Because we know that natural selection favors discriminative parental solicitude rather than indiscriminate parental solicitude (that is, it generally favors psychological mechanisms that bias resources toward offspring over non-offspring), we can explain and anticipate that the cost of reducing child abuse will be greater, per capita, for stepparent households than for non-stepparent households. Similarly, we can predict that men under court-order to provide child support payments for a child they know or suspect they did not father will be less likely to comply than will biological fathers.\n\nAnother nice example brings the point home strongly:\n\nTake, for example, a hypothetical legal rule that required an adult, in a crisis situation involving both her children and the children of others, to save children in order of their ranked intelligence (or any other desirable characteristic), irrespective of her own relatedness to each. We know that such a legal rule would be absurd. But why? It is not because the rule would lead to inefficient outcomes. To the contrary, the outcome might increase social wealth compared to the alternative.\nIt is not enough to say that powerful social norms would generate irresistible emotions in the woman to save her own children, because it so happens that we would expect the same behavior from parents all over the world, regardless of the many vicissitudes of culture. We know the rule would be absurd because we intuitively sense that the preference to save one’s own child would be insensitive to variations in legal costs we might impose in an effort to shift the behavior—all over the planet, in every human culture. The theoretical basis for that sense of relative inelasticity of the demand for certain behaviors, in certain contexts, is not simply acculturation alone, but the law of law’s leverage, as derived from the effects of evolution on human behavior-biasing psychological predispositions.\n\nThe concept of the law of law’s leverage reminded me of a section in Steven Pinker’s The Blank Slate: The Modern Denial of Human Nature, when he pointed out that evidence of a biological basis to crime could be used both by advocates of stronger punishment and advocates of weaker punishment. Evidence of a biological disposition could be argued to reduce culpability. Alternatively, a strong disposition needs to be countered by even stronger incentives.\nAs such, the law of law’s leverage may lead us to look at a steep demand curve for a particular behaviour and acknowledge that we can’t change it. Or, we may want to massively change the price."
  },
  {
    "objectID": "posts/the-lipstick-effect.html",
    "href": "posts/the-lipstick-effect.html",
    "title": "The lipstick effect",
    "section": "",
    "text": "Sarah Hill has posted at Scientific American on a new paper (pdf) that she (and colleagues) has written on the lipstick effect. The lipstick effect is a phenomena where sales of beauty products increase in times of recession, in contrast to the reduction in purchases for most other goods. The authors suggest that this reflects the desire of women to reproduce more quickly, and competition between women for access to the relatively scarcer resource-rich men. Part of the abstract reads:\n\nFindings revealed that recessionary cues—whether naturally occurring or experimentally primed—decreased desire for most products (e.g., electronics, household items). However, these cues consistently increased women’s desire for products that increase attractiveness to mates—the first experimental demonstration of the lipstick effect. Additional studies show that this effect is driven by women’s desire to attract mates with resources and depends on the perceived mate attraction function served by these products.\n\nThe most interesting element of this paper is the argument that the lipstick effect is not driven solely by resource needs. One of the five studies reported in the paper directly addressed this point:\n\nThe second goal of Study 4 was to rule out an alternative hypothesis derived from social roles theory (see Eagly & Wood, 1999)—specifically, that the lipstick effect may reflect women’s greater resource need in a recession. On this view, because resources tend to be controlled by men, economic recessions should prompt women to attract wealthy mates specifically as a means to obtaining these rarified resources. In contrast, our evolutionary model predicts that economic resource scarcity should lead women to invest more effort in mate attraction effort because such conditions heighten reproductive goal immediacy and signal diminished access to high-quality mates, both of which prompt greater mate attraction efforts. …\nFrom a social roles perspective, which would predict that the lipstick effect reflects women’s increased resource needs in a recession, women’s own resource access (e.g., her socioeconomic status [SES]) should be the driver of the lipstick effect. That is, the effect should be driven primarily by lower SES women, whose resource need is greatest. In contrast, our evolutionary model predicts that uncertain economic climates should lead women to heighten mate attraction effort—and to do so irrespective of their own resource need.\n\nStudy 4 supported the authors’ hypothesis:\n\n[C]onsistent with our model based on theories in evolutionary psychology, a robust lipstick effect was found in women across levels of SES. … Furthermore, that each of our studies revealed evidence of a robust lipstick effect, despite the fact that many of the women in our sample came from backgrounds implying low resource need, indicates additional evidence that the lipstick effect does not emerge exclusively in response to objective resource need. These results provide  support for the idea that social roles, by themselves, are unlikely to provide a complete explanation for the lipstick effect.\n\nI would be interested in examining this from a slightly different angle, focusing on the uncertainty associated with recessions and not the low resource availability that accompanies them. If we think of an r/K selection framework, unstable environments tend to result in strategies of rapid reproduction when possible, whereas stable but resource constrained environments lead to greater investment in quality of offspring. The lipstick effect is a response to the uncertainty. If these experiments were run with women with children who must face a quality-quality trade-off, and not the young, college attending women used in the experiments in this paper, we could  examine this point further."
  },
  {
    "objectID": "posts/the-macrogenoeconomics-of-comparative-development.html",
    "href": "posts/the-macrogenoeconomics-of-comparative-development.html",
    "title": "The Macrogenoeconomics of Comparative Development",
    "section": "",
    "text": "Oded Galor has pointed me to his forthcoming article with Quamrul Ashraf in The Journal of Economic Literature.\n\nThe Macrogenoeconomics of Comparative Development\nA vibrant literature has emerged in recent years to explore the influences of human evolution and the genetic composition of populations on the comparative economic performance of societies, highlighting the roles played by the Neolithic Revolution and the prehistoric “out of Africa” migration of anatomically modern humans in generating worldwide variations in the composition of genetic traits across populations. The recent attempt by Nicholas Wade’s “A Troublesome Inheritance: Genes, Race and Human History” to expose the evolutionary origins of comparative economic development to a wider audience provides an opportunity to review this important literature in the context of his theory.\n\nA couple of paragraphs from the introduction:\n\nWade advances a modified evolutionary theory of long-run economic development, based on regional variation in the intensity of positive selection of traits that are conducive to growth-enhancing institutions. His theory suggests that variation in the duration of selective pressures on genetic traits across regions form the basis of differences in social behaviors across racial groups, thereby shaping variations in the nature of institutions and, thus, the level of economic development across the globe. Although at the outset, the broad outline of this argument appears plausible and largely consistent with existing evolutionary theories of comparative development, there is currently no compelling evidence for supporting the actual mechanisms proposed by Wade. …\nThe two fundamental building blocks of Wade’s theory are rather speculative. In particular, his narrative relies on unsubstantiated selection mechanisms and on empirically unsupported conjectures regarding the determinants of institutional variation across societies. … Rather than subjecting his hypothesized mechanism to the scrutiny of evolutionary growth theory, Wade follows the speculative supposition of Clark (2007), merely positing that in historically densely populated regions of the world that were characterized by early statehood, there existed a class of rich elites, endowed with genetic traits (e.g., nonviolence, cooperation, and thrift) conducive to growth-enhancing institutions, whose evolutionary advantage increased the prevalence of these favorable traits in the populations of those regions over time. It is far from evident, however, that the traits emphasized by Wade necessarily generated higher incomes in a Malthusian environment and were, thus, necessarily favored by the forces of natural selection. Moreover, Wade provides no evidence on how variations across societies in their geographical setting or historical experience could have given rise to differential selective pressures on these traits and, thus, generated variation in the growth-promoting genetic makeup of their populations. Furthermore, there is currently little scientific consensus on the extent to which the key behavioral traits of nonviolence, cooperation, and thrift, as emphasized by Wade’s theory, are genetically determined.\nThe second building block of Wade’s theory that links genetic traits to institutions is equally speculative. In particular, there is little evidence to support the claim that the variation in institutions across societies is driven by differences in their endowment of specific genetic traits that might govern key social behaviors."
  },
  {
    "objectID": "posts/the-marshmallow-test-held-up-ok.html",
    "href": "posts/the-marshmallow-test-held-up-ok.html",
    "title": "The marshmallow test held up OK",
    "section": "",
    "text": "A common theme I see on my weekly visits to Twitter is the hordes piling onto the latest psychological study or effect that hasn’t survived a replication or meta-analysis. More often than not, the study deserves the criticism. But recently, the hordes have occasionally swung into action too quickly.\nOne series of tweets suggested that loss aversion had entered the replication crisis. A better description of the two papers that triggered the tweets is that they were the latest salvos in a decade-old debate about the interpretation of many loss aversion experiments. They have nothing to do with replication. (If you’re interested, the papers are here (ungated) and here. I have sympathy with parts of the arguments, and some other critiques of the concept of loss aversion. I’ll discuss these papers in a later post.)\nAnother set of tweets concerned a conceptual replication of the marshmallow test. Many of the comments suggested that the replication was a failure, and that the original study was rubbish. My view is that the original work has actually held up OK, although the interpretation of the result and some of the story-telling that followed the study is challenged.\nFirst, to the original paper by Shoda, Mischel, and Peake, published in 1990 (pdf). In that study, four-year old children were placed at a table with a bell and a pair of “reward objects”. The pair of regard objects might be one marshmallow and two marshmallows, or one pretzel and two pretzels, and so on.\nThe children were told that the experimenter was going to leave the room, and that if they waited until the experimenter came back, they could have their preferred reward (the two marshmallows). Otherwise, they could call the experimenter back earlier by ringing the bell, but in that case they could only have their less preferred reward (one marshmallow). (Could a truly impatient child just not ring the bell and eat all three marshmallows?) The time until the children rang the bell, up to a maximum of 15 to 20 minutes, was recorded.\nThe headline result was that the time to ring the bell was predictive of future achievement in the SAT. Those who delayed their gratification had higher achievement. The time waited correlated 0.57 with SAT math scores and 0.42 with SAT verbal scores.\nThe new paper discusses a “conceptual replication”. It doesn’t copy the experimental design and replicate it precisely, but relies on a similar experimental design and a measure of academic achievement based on a composite of age-15 reading and math scores.\nThe main point to emerge from this replication is that there is an association between the delay in gratification and academic achievement, but the correlation (0.28) is only half to two-thirds of that found in the original study.\nAnyone familiar with the replication literature will find this reduction in correlation unsurprising. One of the headline findings from the Reproducibility Project was that effect sizes in replications were around half of those in the original studies. Small sample sizes (low experimental power) also tend to result in Type M errors, whereby the effect size is exaggerated. (The original study only had 35 children in the baseline condition for which they were able to get the later academic results.)\nShoda and friends recognised this possibility (although perhaps not the reasons for it). As they wrote in the original paper:\n\n[G]iven the smallness of the sample, the obtained coefficients could very well exaggerate the magnitude of the true association. For example, in the diagnostic condition, the 95% confidence interval for the correlation of preschool delay time with SAT verbal score ranges from .10 to .66, and with SAT quantitative score, the confidence interval ranges from .29 to .76. The value and importance given to SAT scores in our culture make caution essential before generalizing from the present study; at the very least, further replications with other populations, cohorts, and testing conditions seem necessary next steps.\n\nThe differences between the experiments could also be behind the difference in size of correlation. Each study used different measures of achievement. The marshmallow test in the replication had a maximum wait of only 7 minutes, compared to 15 to 20 minutes in the original (although most of the predictive power in the new study was found to be in the first 20 seconds). The replication created categories for time waited (e.g. 0 to 20 seconds, 20 seconds to 2 minutes, and so on), rather than using time as a continuous variable. It also focused on children with parents who did not have a college education - too many of the children with college-educated parents waited the full seven minutes. The original study drew its sample from the Stanford community.\nGiven the original authors’ notes about effect size, and the differences in study design, the original findings have held up rather well. For a simple diagnostic, the marshmallow test still has a surprising amount of predictive power. Delay of gratification at age 4 predicts later achievement. Some of the write-ups of this new work have stated that the marshmallow test may not be as strong a predictor of future outcomes as previously believed, but how strong did you actually believe it to be in the first place?\nThe other headline from the replication is that the predictive ability of the marshmallow test disappears with controls. That is, if you account for the children’s socioeconomic status, parental characteristics and a set of measures of cognitive and behavioural development, the marshmallow test does not provide any further information about that future achievement. It’s no surprise that controls of this nature do this. It simply suggests that the controls are better predictors. The original claim was not that the marshmallow test was the best or only predictor.\nWhat is called into question are the implications that have been drawn from the marshmallow test studies. Shoda and friends suggested that the predictive power of the test might be related to the meta-cognitive strategies that the children employed. For instance, successful children might divert themselves so that they don’t just sit and stare at the marshmallows. If that is the case, we could teach children these strategies, and they might then be better able to delay gratification and have higher achievement in life. This has been a common theme of discussion of the marshmallow test for the last 30 years.\nIn the replication data, most of the predictive power of the marshmallow test was found to lie in the first 20 seconds. There was not a lot of difference between the kids who waited more than 20 seconds and those that waited the full seven minutes. It is questionable whether meta-cognitive strategies come into play in those first few seconds. If not, there may be little benefit in teaching children strategies to enable them to delay gratification. It seems less a problem of developing strategies for gratification, and more one of basic impulse control. To increase future achievement, broader behaviour and cognitive change might be required."
  },
  {
    "objectID": "posts/the-next-decade-of-behavioural-science-a-call-for-intellectual-diversity.html",
    "href": "posts/the-next-decade-of-behavioural-science-a-call-for-intellectual-diversity.html",
    "title": "The next decade of behavioural science: a call for intellectual diversity",
    "section": "",
    "text": "Behavioral Scientist put out the call to share hopes, fears, predictions and warnings about the next decade of behavioral science. Here’s my contribution:\n\nAs behavioral scientists, we’re not exactly a diverse bunch. We’re university educated. We live in major cities. We work in academia, tech, consulting, banking and finance. And dare I say it, we’re rather liberal. Read the twitter streams or other public outputs of the major behavioral science institutions, publications and personalities, and the topics of interest don’t stray too far from what a Democratic politician (substitute your own nation’s centre-left party) would discuss in a stump speech.\nIn that light, we need to think more broadly about both the questions we tackle and the answers we “like”. We need to ask what problems matter to the large swathes of the population that we don’t encounter in our day-to-day. We need to be self critical, open to being wrong, and not cheerleaders of our own narrow conception of the world. We must find and listen those who don’t share our points of view. We must question our orthodoxies.\nIn practice, that’s not easy. But its vital to our relevance and to our intellectual foundations.\n\nI had a few stabs at the ~200 words. Here’s another attempt on a similar theme:\n\nThrough the replication crisis, some prominent concepts in behavioural science have been challenged. The priming literature is in ruins. The concept of willpower as a finite resource is scarcely alive. Experiments in areas from disfluency to scarcity have failed to replicate.\nThe shaking of the behavioural foundation is not over. More tenets of behavioural science are going to bite the dust. Many will be exposed through ongoing replication attempts. They are built on the same foundations as those that have already crumbled: publication bias, the garden of forking paths, among other things. New findings that continue to be built on those same foundations will also tumble down.\nI suspect there will be dismay when some ideas crumble, as they align with core beliefs of the behavioural science community. Yet those beliefs will be at the core of the weakness. Ideas of a different alignment would have faced a more serious challenge. We accept too many ideas because we “like” them.\nThankfully, I am confident that infrastructure is being built that will allow us to challenge even cherished ideas. Let us just make sure that when the scientific foundation no longer exists, we are willing to let them go.\n\nYou can read contributions from the broader behavioural science community here."
  },
  {
    "objectID": "posts/the-origins-of-savings-behaviour.html",
    "href": "posts/the-origins-of-savings-behaviour.html",
    "title": "The Origins of Savings Behaviour",
    "section": "",
    "text": "Bryan Caplan points out a paper by  Henrik Cronqvist and Stephan Siegel on the genetic and parental influences on savings behaviour. The first part of the abstract reads:\n\nAnalyzing identical and fraternal twins matched with data on their savings propensities, we find that genetic variation explains about 33 percent of the variation in savings behavior across individuals. Parenting effects on savings behavior are strong for those in their twenties but decay to zero by middle age, i.e., parents do not have a lifelong non-genetic impact on their children’s savings. The family environment when growing up and an individual’s socioeconomic status later in life moderate genetic effects, so that more supportive environments result in a stronger genetic expression of savings behavior.\n\nThere are no surprises in these results. The proportion of the variance in savings explained by genetic variation is typical of that for many other social traits. And once opportunity is equalised, genetics becomes more important.\nThe evidence that parental influence fades out for older subjects and disappears by age 45, compared to the relatively constant genetic effects, is interesting. The break down of effects by age is not a regular feature of studies such as these (it comes at the cost of sample size). The authors write:\n\nOur interpretation of this evidence is that social transmission from parents to their children affects children’s savings behavior early on in life, but unlike genetic effects, parenting does not have a lifelong impact on an individual’s savings behavior. These results are broadly consistent with research in behavioral genetics which has found a significant effect of the common family environment in early ages on, e.g., personality, but also shown that such effects approach zero in adulthood\n\nIt is a pity that savings behaviour is only exhibited without significant constraints in adulthood, as if we could also get data for the first twenty years of life, we might also see the genetic influence increase as happens for IQ.\nIn the conclusion, the authors link the observed behaviour to time preference and self-control. Those with lower savings rates are probably also suffering the consequences of impatience and poor self-control in many aspects of their lives.\n\nOne explanation for why savings behavior is genetic appears to be that an individual’s time preferences are partly genetic. Our evidence of a significant positive genetic correlation between an individual’s savings and income growth supports such an explanation. Some individuals are born to be more patient, and this affects these individuals’ savings behavior, as well as other outcomes, e.g., the choice of income process. Moreover, the negative and significant genetic correlation between savings rate and both smoking and body weight suggests that behavioral factors such as lack of self-control may also affects savings behavior. For example, to the extent that a high BMI and obesity may be interpreted as an expression of lack of self-control, we conclude that lack of savings correlates with lack of self-control, and this correlation is mainly found to be genetic.\n\nFor those looking for more on the subject, there is a lecture by Cronvqist on this paper on Vimeo:\n[vimeo http://vimeo.com/16490797]\nAs an end note, in the closing of Caplan’s post, he notes that there is still much variation that is not explained by either parental effects or genes. Caplan falls back on his old free will argument, but there is a lot to be said for noise. This is a subject I hope to come back to in the near future."
  },
  {
    "objectID": "posts/the-out-of-africa-hypothesis-human-genetic-diversity-and-comparative-economic-development.html",
    "href": "posts/the-out-of-africa-hypothesis-human-genetic-diversity-and-comparative-economic-development.html",
    "title": "The ‘Out of Africa’ Hypothesis, Human Genetic Diversity, and Comparative Economic Development",
    "section": "",
    "text": "Although the debate it triggered has been going for a few months (see here, here, here and here.), Quamrul Ashraf and Oded Galor’s paper The ‘Out of Africa’ Hypothesis, Human Genetic Diversity, and Comparative Economic Development has been published in the February edition of the American Economic Review (for the latest ungated version, go here - although you can download the data and supplementary materials from the AER site without a subscription).\nOver the next few weeks I will dissect parts of Ashraf and Galor’s argument, and look at some of the criticisms that people have made. As a start, however, I’ll present a basic description of the method and findings.\nAshraf and Galor’s hypothesis is that genetic diversity affects economic development through two pathways. First, genetic diversity has a positive role in development as it expands a population’s production possibility frontier. That is, the wider mix of  traits available in the population means that there are more likely to be traits present that can advance and implement new technologies.\nThe second is a negative effect of genetic diversity, whereby heterogeneity increases distrust, thereby reducing cooperation. This increases the chance of conflict and generally reduces the level of social order in the population.\nAshraf and Galor use expected heterozygosity as their measure of genetic diversity, which is a measure of the probability that two randomly selected people from the population differ with respect to a given gene, averaged over the measured genes. Genetic diversity is affected by what is known as the founder effect. When a new population emerges from a larger population, such as when a group of humans migrate, they take only a subset of the genetic diversity available in the initial population. As humans migrated out of Africa and spread across the world, each new migration took a smaller set of the available diversity. Diversity tends to decline as we move from Africa to Europe to the Americas.\nDepending on the relative strengths of the negative and positive effects of genetic diversity on economic development, this pattern may result in a hump-shaped relationship between the two. Populations with more extreme levels of diversity may suffer from insufficient diversity for pushing out the production frontier, or high levels of conflict due to dissimilar individuals.\nAshraf and Galor tested this hypothesis using genetic data from the Human Genome Diversity Cell Line Panel, which comprises 53 ethnic groups, each of which are believed to be native to the area in which they are found and relatively isolated from gene flow from other groups. As a result, they represent a reasonable measure of genetic diversity in those areas before modern-day mobility.\nAshraf and Galor recognised that this dataset is not very big. For example, it comprises only 2 populations from Oceania and 4 populations from the Americas. As a result, they also developed an index of predicted genetic diversity based on migratory distance for a larger group of 145 countries.\nAs their initial analysis is for 1500 CE, Ashraf and Galor use population density as the measure of development. In a Malthusian world, any improvements in technology that might improve living standards are quickly swallowed by population increases. Population grows to its carrying capacity. The Industrial Revolution was the first time in human history where this pattern was broken. Thus, per person income is a poor measure of technology in a Malthusian world as everyone is at subsistence. Technology only changes how many people can live on subsistence in a given area - hence the use of population density. Population density is also used for their analysis of 1 CE and 1000 CE (which is contained in the Web Appendix). For their analysis of 2000 CE they use income per person.\nAshraf and Galor ran regressions of genetic diversity against economic development using a range of control variables, including latitude, the percentage of arable land and the suitability of land for agriculture. They also use continent fixed effects as part of the controls, which should account for any unobserved continent specific factors. The interpretation of the use of continent fixed effects is that the findings hold within the continents.\nTheir first set of results using the smaller 53 ethnic groups found a hump-shaped relationship between genetic diversity and development, as would be predicted by the opposing costs and benefits to diversity. The size of the effect is such that a 1 percentage point increase in diversity for the least diverse society would increase population density by 58 per cent. A 1 percentage point decrease in diversity for the most diverse society would increase population density by 23 per cent. Ashraf and Galor also ran some tests to show that it is diversity and not migratory distance that is affecting development.\nThe headline results from their larger predicted diversity set is again a hump-shaped relationship between diversity and development for 1500 CE. A one percentage point increase in diversity for the least diverse society would increase population density by 36 per cent, while a one percentage point decrease for the most diverse society would increase population density by 29 per cent.\nThe analysis is then done for 2000 CE, with country diversity calculated by examining the mix of ethnicities that make up the country. Again, the hump shaped pattern holds. Given countries can be identified, these results have attracted some of the most attention. Ashraf and Galor summarise them as follows:\n\nThe direct effect of genetic diversity on contemporary income per capita, once institutional, cultural, and geographical factors are accounted for, indicates that (i) increasing the diversity of the most homogenous country in the sample (Bolivia) by 1 percentage point would raise its income per capita in the year 2000 CE by 41 percent; (ii) decreasing the diversity of the most diverse country in the sample (Ethiopia) by 1 percentage point would raise its income per capita by 21 percent; (iii) a 1 percentage point change in genetic diversity (in either direction) at the optimum level of 0.721 (that most closely resembles the diversity level of the United States) would lower income per capita by 1.9 percent; (iv) increasing Bolivia’s diversity to the optimum level prevalent in the United States would increase Bolivia’s per capita income by a factor of 5.4, closing the income gap between the United States and Bolivia from a ratio of 12:1 to 2.2:1; and (v) decreasing Ethiopia’s diversity to the optimum level of the United States would increase Ethiopia’s per capita income by a factor of 1.7 and thus close the income gap between the United States and Ethiopia from a ratio of 47:1 to 27:1. Moreover, the partial R2 associated with diversity suggests that residual genetic diversity explains about 16 percent of the cross-country variation in residual log income per capita in 2000 CE, conditional on the institutional, cultural, and geographical covariates in the baseline regression model.\n\nThe paper closes with a brief look at the evidence for the costs and benefits of genetic diversity, such as various measures of trust and innovation within countries. Ashraf and Galor show that there is some evidence that these relationships are in the right direction. I’ll delve into that evidence in more detail in later posts.\nMy posts on Ashraf and Galor’s paper on genetic diversity and economic growth are as follows:\n\nA summary of the paper methodology and findings (this post)\nDoes genetic diversity increase innovation?\nDoes genetic diversity increase conflict?\nIs genetic diversity a proxy for phenotypic diversity?\nIs population density a good measure of technological progress?\nWhat are the policy implications of the effects of genetic diversity on economic development?\nShould this paper have been published?\n\nOther debate on this paper can also be found here, here, here and here."
  },
  {
    "objectID": "posts/the-paradox-of-trust.html",
    "href": "posts/the-paradox-of-trust.html",
    "title": "The Paradox of Trust",
    "section": "",
    "text": "In a chapter of Robert Sugden’s The Community of Advantage: A Behavioural Economist’s Defence of the Market, he makes some interesting arguments about how we should interpret the results of the trust game.\nFirst, what is the trust game:\n\nThe ‘Trust Game’ was first investigated experimentally by Joyce Berg, John Dickhaut, and Kevin McCabe (1995). … In Berg et al.’ s game, two players (A and B) are in separate rooms and never know one another’s identity. Each player is given $10 in one-dollar bills as a ‘show up fee’. A puts any number of these bills, from zero to ten, in an envelope which will be sent to B; he keeps the rest of the money for himself. The experimenter supplements this transfer so that B receives three times what A chose to send. B then puts any number of the bills she has received into another envelope, which is returned to A; she keeps the rest of the money for herself. The game is played once only, and the experiment is set up so that no one (including the experimenter) can know what any other identifiable person chooses to do. The game is interesting to theorists of rational choice because it provides the two players with an opportunity for mutual gain, but if the players are rational and self-interested, and if each knows that this is true of the other, no money will be transferred. (It is rational for B to keep everything she is sent; knowing this, it is rational for A to send nothing.)\n\nThere is a sizeable body of empirical evidence that player A often does send money and B often returns money. How can this be explained? One option is to draw on the concept of reciprocity.\n\nIn this literature, it is a standard modelling strategy to follow Matthew Rabin (1993) in characterizing intentions as kind or unkind. … The greater the degree to which one player benefits the other by forgoing his own payoffs, the kinder he is. Rabin’s hypothesis is that individuals derive utility from their own payoffs, from being kind towards people who are being kind to them, and from being unkind towards people who are being unkind to them.\n\nBut if you think this hypothesis through, there is a problem, which Sugden calls the Paradox of Trust.\n\n[I]t seems that any reasonable extension of Rabin’s theory will have the following implication for the Trust Game: It cannot be the case that A plays send, expecting B to play return with probability 1, and that B, knowing that A has played send, plays return. To see why not, suppose that A chooses send, believing that B will choose return with probability 1.\nA has not faced any trade-off between his payoffs and B’s, and so has not had the opportunity to display kindness or unkindness.\n…\nSince Rabin often describes positive reciprocity as ‘rewarding’ kind behaviour (and describes negative reciprocity as ‘punishing’ unkind behaviour), the idea seems to be that B’s choice of return is her way of rewarding A for the goodness of send. But if A’s action was self-interested, it is not clear why it deserves reward.\nIt may seem paradoxical that, in a theory in which individuals are motivated by reciprocity, two individuals cannot have common knowledge that they will both participate in a practice of trust. Nevertheless, this conclusion reflects the fundamental logic of a modelling strategy in which pro-social motivations are represented as preferences that are acted on by individually rational agents. It is an essential feature of (send, return), understood as a practice of trust, that both players benefit from both players’ adherence to the practice. If A plays his part in the practice, expecting B to play hers, he must believe and intend that his action will lead to an outcome that will in fact benefit both of them. Thus, if pro-sociality is interpreted as kindness—as a willingness to forgo one’s own interests to benefit others—A’s choice of send cannot signal pro-social intentions, and so cannot induce reciprocal kindness from B. I will call this the Paradox of Trust.\n\nIs there an alternative way of seeing this problem? Sugden turns to the idea of mutually beneficial exchange.\n\nThe escape route from the Paradox of Trust is to recognize that mutually beneficial cooperation between two individuals is not the same thing as the coincidence of two acts of kindness. When A chooses send in the Trust Game, his intention is not to be kind to B: it is to play his part in a mutually beneficial scheme of cooperation, defined by the joint action (send, return). … If A is completely confident that B will reciprocate, and if that confidence is in fact justified, A’s choice of send is in his own interests, while B’s choice of return is not in hers. Nevertheless, both players can understand their interaction as a mutually beneficial cooperative scheme in which each is playing his or her part.\n\nThis interpretation has implications for how we should view market exchange.\n\nTheorists of social preferences sometimes comment on the fact that behaviour in market environments, unlike behaviour in Trust and Public Good Games, does not seem to reveal the preferences for equality, fairness and reciprocity that their models are designed to represent. The explanation usually offered is that people have social preferences in all economic interactions, but the rules of the market are such that individuals with such preferences have no way of bringing about the fair outcomes that they really desire.\n…\nCould it be that behaviour in markets expresses the same intentions for reciprocity as are expressed in Trust and Public Good Games, but that these intentions are misrepresented in theories of social preference?"
  },
  {
    "objectID": "posts/the-perfection-of-man.html",
    "href": "posts/the-perfection-of-man.html",
    "title": "The perfection of man",
    "section": "",
    "text": "From 100 years ago, Scientific American calls for more research into human evolution:\n\nMendelian principles have no doubt long been followed by professional animal breeders in an empirical way, but only within recent years have enough data been accumulated to show that they apply with equal force to human beings. We know enough about the laws of heredity, we have enough statistics from insane asylums and prisons, we have enough genealogies, to show that, although we may not be able directly to improve the human race as we improve the breed of guinea pigs, rabbits or cows, because of the rebellious spirit of mankind, yet the time has come when the lawmaker should join hands with the scientist, and at least check the propagation of the unfit. Prizes have been offered to crack trotters for beating their own record, $10,000 for a fifth of a second, all for the purpose of evolving a precious two-minute horse. Yet we hear of no prizes which are offered for that much worthier object, the physically and intellectually perfect man."
  },
  {
    "objectID": "posts/the-power-of-heuristics.html",
    "href": "posts/the-power-of-heuristics.html",
    "title": "The power of heuristics",
    "section": "",
    "text": "Gerd Gigerenzer is a strong advocate of the idea that simple heuristics can make us smart. We don’t need complex models of the world to make good decisions.\nThe classic example is the gaze heuristic. Rather than solving a complex equation to catch a ball, which requires us to know the ball’s speed and trajectory and the effect of the wind, a catcher can simply run to keep the ball at a constant angle in the air, leading them to the point where it will land.\nGigerenzer’s faith in heuristics is often taken to be based on the idea that people have limited processing capacity and are unable to solve the complex optimisation problems that would be needed in the absence of these rules. However, as Gigerenzer points out in Rationality for Mortals: How People Cope with Uncertainty, this is perhaps the weakest argument for heuristics:\n\n[W]e will start off by mentioning the weakest reason. With simple heuristics we can be more confident that our brains are capable of performing the necessary calculations. The weakness of this argument is that it is hard to judge what complexity of calculation or memory a brain might achieve. At the lower levels of processing, some human capabilities apparently involve calculations that seem surprisingly difficult (e.g., Bayesian estimation in a sensorimotor context: Körding & Wolpert, 2004). So if we can perform these calculations at that level in the hierarchy (abilities), why should we not be able to evolve similar complex strategies to replace simple heuristics?\n\nRather, the advantage of heuristics lies in their low information requirements, their speed and, importantly, their accuracy:\n\nOne answer is that simple heuristics often need access to less information (i.e. they are frugal) and can thus make a decision faster, at least if information search is external. Another answer - and a more important argument for simple heuristics - is the high accuracy they exhibit in our simulations. This accuracy may be because of, not just in spite of, their simplicity. In particular, because they have few parameters they avoid overfitting data in a learning sample and, consequently, generalize better across other samples. The extra parameters of more complex models often fit the noise rather than the signal. Of course, we are not saying that all simple heuristics are good; only some simple heuristics will perform well in any given environment.\n\nAs the last sentence indicates, Gigerenzer is careful not to make any claims that heuristics generally outperform. A statement that a heuristic is “good” is ill-conceived without considering the environment in which it will be used. This is the major departure of Gigerenzer’s ecological rationality from the standard approach in the behavioural sciences, where the failure of a heuristic to perform in an environment is taken as evidence of bias or irrationality.\nOnce you have noted what heuristic is being used in what environment, you can have more predictive power than in a well-solved optimisation model. For example. an optimisation model to catch a ball will simply predict that the catcher will be at the place and time where the ball lands. Once you understand that they use the gaze heuristic to catch the ball, you can also predict the path that they will take to get to the ball - including that they won’t simply run in a straight line to catch it. If a baseball or cricket coach took the optimisation model too seriously, they would tell the catcher that they are running inefficiently by not going straight to where it will land. Instructions telling them to run is a straight line will likely make their performance worse."
  },
  {
    "objectID": "posts/the-rationale-of-the-family.html",
    "href": "posts/the-rationale-of-the-family.html",
    "title": "The rationale of the family",
    "section": "",
    "text": "John Kay writes:\n\nA narrow focus is characteristic of scientific method but gets in the way of understanding social phenomena. …\nThe economists who argue that the rationale of the family is found in cost savings have a point. Two together can live more cheaply than two separately, if not as cheaply as one. But anyone who thinks the quest for scale economies is the primary explanation of the human desire for family life is strangely deficient in observational capacity, as well as common sense.\nThe “economics of the family” is a prime example of an economic imperialism that seeks to account for all behaviour through a distorted concept of rationality, an extreme example of economists’ notorious physics envy.\n\nRationality is a powerful tool in the economic toolkit, and the lens of natural selection is a powerful rationalising agent. But what is the objective we are rationally trying to achieve in forming a family? Likely not the minimisation of costs. And what powers do we have to achieve that objective? Not perfect foresight and calculating ability. The application of economic tools with an understanding of the agent’s objectives and the bounded nature of human rationality is the more fruitful exercise."
  },
  {
    "objectID": "posts/the-return-of-group-selection.html",
    "href": "posts/the-return-of-group-selection.html",
    "title": "The return of group selection",
    "section": "",
    "text": "In 2010, Martin Nowak, Corina Tarnita and Edward O Wilson had their paper The evolution of eusociality published in Nature. They argued that inclusive fitness could not explain eusociality, and that competition between groups was required as an explanatory factor. The anti-group selection forces were quick to mobilise. Apart from the many blog posts and column inches, Nature published a response with 137 signatories defending the concept of inclusive fitness. One paragraph by Carl Zimmer captures (for me) where the argument is at:\n\nNowak et al respond to all the criticism and don’t budge in their own stand. They claim that their critics have misinterpreted their own argument. And they claim that sex allocation does not require inclusive fitness. Oddly, though, they never explain why it doesn’t, despite the thousands of papers that have been published on inclusive fitness and sex allocation. They don’t even cite a paper that explains why.\n\nDespite the lack of traction for group selection in the evolutionary biology world, there appears to be a resurgence in the social sciences and popular press. For example, Jonathan Haidt, author of the recently released The Righteous Mind: Why Good People Are Divided by Politics and Religion, argues that humans have been subject to and shaped by group selection. Haidt is getting plenty of relatively unimpeded coverage in the popular press.\nThis resurgence is about to hit a new high with the upcoming release of The Social Conquest of Earth by Edward O Wilson. While I am sure it will receive a skeptical reaction from most of Wilson’s fellow biologists, it is already getting a welcome reception in parts of the media and from some social scientists. Take this piece by Jonathan Gottschall in the Huffington Post. Gottshall writes:\n\nOf course, it would be a great distortion to suggest that people are – like ants – selfless all of the time. But the vision of rigid selfishness that arose from biology’s rejection of group selection was an equally great distortion. The real picture is more complex. Natural selection occurs at the level of groups and individuals. Between-group competition favors selfless genes while competition inside groups favors selfish genes. As Wilson and a colleague wrote, “Selfishness beats altruism within groups. Altruistic groups beat selfish groups. Everything else is commentary.”\n\nMy perception is that there is a growing gap between the relative standing of group selection in the social science and evolutionary worlds. This is particularly the case for discussion of the evolution of cooperation and altruism in humans. How wide will this gap grow before a serious response emerges from the evolutionary sciences to the claim that human cooperation is shaped by group selection? There have been some good papers (pdf) on this in the past, but these types of arguments are not getting much column space."
  },
  {
    "objectID": "posts/the-short-wingman-do-humans-use-visual-illusions-to-attract-a-mate.html",
    "href": "posts/the-short-wingman-do-humans-use-visual-illusions-to-attract-a-mate.html",
    "title": "The short wingman - do humans use visual illusions to attract a mate?",
    "section": "",
    "text": "An ABC news article last week reported a study by Professor John Endler on the use of visual tricks by male bower birds to make themselves look larger to females. By placing small objects at the front and larger objects at the back of their bower, the court in which the male bower bird is viewed appears smaller, which makes the male look larger. Endler noted, however, that it was not clear whether the male had any empathy for the females perspective or if the male simply arranged the objects based on his own preferences.\nIt seems certain that humans use illusions to attract mates. I have some difficulty in coming up with many examples off the top of my head. There is evidence that being the tallest amongst a group of short people is advantageous, but this may be more a question of opportunity than visual trickery. We also use a vast array of visual techniques (dress, ornamentation, dancing etc). But how many of these are for the purpose of creating an illusion? I would expect that some body paint patterns would make individuals appear taller and wider. But generally when we do make illusions in art etc, those illusions are not for the purpose of making the male appear more attractive (although they may be a way of appearing clever to attract a mate).\nOne possible example (although a selected, not conscious illusion) might be narrow waists on females - making the hips appear wider. But even that may have a simpler explanation - evolving to to separate the genuinely wide hipped from cheaters who have a bit of extra fat all over.\nSome extra research on my part is going to be required on this one."
  },
  {
    "objectID": "posts/the-speed-of-cities-afterthoughts.html",
    "href": "posts/the-speed-of-cities-afterthoughts.html",
    "title": "The speed of cities - afterthoughts",
    "section": "",
    "text": "Having recently discussed cross-country variation in time preference and the pace of life, I have found it interesting reconciling the conclusions.\nRicher countries tend to have residents with lower rates of time preference and a higher pace of life. Wang, Rieger and Hens noted this relationship and showed that pace of life is strongly and positively correlated with propensity to wait, an indication of time preference. The residents of the country where everyone is scurrying around have a higher ability to delay gratification. What might be a sign of impatience (the fast walking) is actually the opposite. I have a suggestions on how to reconcile these facts, but I am not yet convinced which of them are true or more important.\nThe first is that the subjects whose walking speed was measured were downtown during office hours. If they were employed, they were likely going to or from a work task. If work is unpleasant, an impatient person who cannot delay gratification (or conversely, tries to delay pain) might walk more slowly. Conversely, a patient person who is able to delay reward for greater returns might walk faster to take advantage of the returns from work.\nThe second point is that patient people tend to have higher incomes (as shown with Mischel’s marshmallows) and their time is worth more. As a result, they might rush due to the incentive effects (and despite their patience). In the richer city, more people will be in this situation.\nAnother possibility comes down to definitions. The way economists use the ideas such as foresight, delay of gratification and rate of time preference may not coincide with the common concept of “patience”. In my case, I consider that I have the ability to delay gratification and make decisions in a relatively time consistent manner with a low discount rate. However, I could also be considered impatient as I don’t like waiting and often rush others (and myself) through tasks (I am also a very fast walker). If we drop the common understanding of patience and limit ourselves to economic concepts such as self-control and ability to delay self-gratification, we may not need to reconcile this anomaly in the first place.\nA final and related possibility is that the ability to delay gratification is actually a result of other traits, such as intelligence. A more intelligent person might have better judgement of when they should exercise patience and when not. Whether they seem patient will depend on which facet of their life we are examining."
  },
  {
    "objectID": "posts/the-speed-of-cities.html",
    "href": "posts/the-speed-of-cities.html",
    "title": "The speed of cities",
    "section": "",
    "text": "Over the weekend, I listened to a great Radiolab podcast in which Bob Levine was interviewed about the pace of walking in cities. Bob spoke about how people tend to walk faster in larger cities, with this relationship surprisingly consistent. Where does this walking pace comes from. As the host Jad asked, do we make the city, or does the city make us?\nThe early movers in this area of research were Bornstein and Bornstein, who between 1972 and 1974 went to 15 countries across Europe, North America and Asia and measured the speed of pedestrians. They took a 50 feet stretch in similar downtown areas of each city and measured the speed of single, unencumbered walkers traversing that distance.\nThe slowest walkers were from Itea, Greece (population 2,500), who took an average of 22 seconds to cover the 50 feet. In Prague, a city of over 1 million, the pedestrians covered the distance in a flying average of 8.5 seconds. Walking speed and the log of the population were strongly correlated (with a correlation coefficient of 0.91 that was significant beyond the 0.001 confidence level). Particularly surprising is the consistency of the results and the absence of any large outliers. For example, the five largest cities sampled all had higher average walking speeds than the slowest five.\nThis high level of consistency raises some obvious questions. While there were no severe outliers in the sample of 15 cities included in the paper, are there any cities that are different? If so, why? Also, where there is variation in the sample, can this be explained? Some other researchers have considered this since the Bornstein and Bornstein paper was published, and I hope to post about those in the near future.\nBornstein and Bornstein based their explanation for the consistency on the total number of people in the city as opposed to density. Borrowing from Milgram, they considered that a higher number of people in a city causes increased stimulation. A person seeking to control the sensory overload will have a higher walking speed as a protection from this excessive environmental stimulation. The variance in walking speeds between cities is the manifestation of this adaptation.\nWhile this is an interesting and possibly correct explanation, two other effects strike me as relevant. The first is the selection effect. Why do people choose to live in cities? If people who have a preference for a faster paced lifestyle wish to live in cities, and those who prefer a slower pace tend to leave, this will tend to drive the results we see. There is an element of the people making the city. This may tend to reduce any “stimulation effect” as residents who live in the city may be there as they wish to be stimulated. The easiest way to reduce stimulation would be to simply leave the city.\nThe second effect concerns the opportunity cost of time. For the pedestrians, what is the value of an earlier arrival? If incomes are higher in the city, as they tend to be in larger cities, their time has higher monetary value. If there is more or better entertainment in the city, there is more value in being at the destination than dawdling along the way."
  },
  {
    "objectID": "posts/the-success-of-the-productive.html",
    "href": "posts/the-success-of-the-productive.html",
    "title": "The success of the productive",
    "section": "",
    "text": "In another great section from the The Genetical Theory of Natural Selection, R.A. Fisher argues that the free exchange of goods and private property rights are triumphs of human organisation:\n\n[F]rom the earliest times of which we have knowledge, the hereditary proclivities, which undoubtedly form the basis of man’s fitness for social life, are found to be supplemented by an economic system, which, diverse as are the opinions which different writers have formed about it, appears to the writer to be one of the unconscious triumphs of early human organization. The basis of the economic system consists in the free interchange of goods or services between different individuals whenever such interchange appears to both parties to be advantageous. It is essential to the freedom of such agreements that the arbitrary coercion of one individual by another shall be prohibited, while, on the other hand, the coercive enforcement of obligations freely undertaken shall be supported by the public power. It is equally essential that the private possession of property, representing, as in this system it must do, the accumulation of services already performed to other members of the society, and the effective means of calling upon equivalent return services in the future, shall be rigorously protected.\n\nIn this system, success is rewarded and those who do not perform socially advantageous actions may perish.\n\nIn the theory of this system each individual is induced, by enlightened self-interest, to exert himself actively in whatever ways may be serviceable to others, and to discover by his ingenuity new ways or improved methods of making himself valuable to the commonwealth. Such individuals as succeed best in performing valuable services will receive the highest rewards, including, in an important degree, the power to direct the services of others in whatever ways seem to them most advantageous. Those, on the contrary, who fail most completely to perform socially advantageous actions have the least claim upon the wealth and amenities of the community. In theory they may perish of starvation, or may become indebted up to the amount of the entire potential services of the remainder of their lives, or of the lives of their children.\n\nFisher notes that while this economic system is not the sole basis by which we operate, it provides protection against the proliferation of the unproductive.\n\nIt need scarcely be said that this economic system has never formed the exclusive basis of social co-operation in man. It has at most been partially established in compromise with social instincts already in being, founded during the existence of less closely cooperative societies. Nevertheless, it bears a sufficient resemblance, both to the theory of rationalistic economists, and to the practice of various ancient civilizations, to indicate that we have presented, in an abstract and ideal form, a real and effective factor in human social organization. The biological importance of this factor lies in the safeguard which it appears to provide that intra-communal selection in human societies shall not favour the multiplication of unproductive or parasitic types, at the expense of those who exert themselves successfully for the common good. On the contrary, it seems to insure that those who produce the best goods or provide the most valuable services shall be continually augmented in each succeeding generation, while those who, by capacity or disposition are unable to produce goods equivalent to what they consume, shall be continually eliminated.\n\nThis selection extends to the way people consume and engage in trade.\n\nNor is this beneficial selection confined to individuals in their capacity of producers. In consumption and distribution an equally beneficial selection would seem to be in progress. The individual who, by reason of his imperfect instincts, is tempted to expend his resources in ways which are not to his biological advantage, the individual who from prejudice favours a bad market, or who is temperamentally incompetent in striking a bargain, is equally at an economic and, it would seem, at a selective, disadvantage. This selection of the consumer provides in an important respect the theoretical completion of the individualistic economic system, for it supplies a means by which the opportunities of gaining wealth by the provision of illusory benefits, shall become ever narrower, until all substantial sources of profit are confined to the provision of real public benefits. The population produced by such a system should become ingenious and energetic industrialists, shrewd and keen in the assessment of social value, and with standards of well-being perfectly attuned to their biological and reproductive interests.\n\nThe logical (thought not necessarily realised) outcome of this selection might be to evolve such that wealth accumulation is the ultimate moral pursuit.\n\nTo complete the picture, at the expense of anticipating a little a subsequent argument, our economic Utopians must be endowed with consciences which recognize the possession of wealth, at least as a means to reproduction, as the highest good, and its pursuit as the synthesis of all virtuous endeavour. To them the wealthy man would enjoy not only the rewards, but also the proofs of his own virtue, and that of his forbears; he would be in some sort a saint, to co-operate in whose virtuous proceedings would be a supreme felicity. Upon such men, no public honour could be bestowed more noble than a direct cash payment, and to purchase other honours for money would seem not so much corrupt as insane. Charity, in the sense of the uneconomic relief of poverty, would evidently be a vicious weakness, although there would be some virtue in shrewdly backing for mutual advantage the capable, but accidentally unfortunate.\n\nHowever, we have not achieved this “utopia”.\n\n[T]he instinctive feelings and prejudices of social man do not seem at all to have developed in the direction of a more strictly economic and less’ sentimental’ basis for social institutions."
  },
  {
    "objectID": "posts/the-three-faces-of-overconfidence.html",
    "href": "posts/the-three-faces-of-overconfidence.html",
    "title": "The three faces of overconfidence",
    "section": "",
    "text": "I have complained before about people being somewhat quick to label poor decisions as being due to “overconfidence”. For one, overconfidence has several distinct forms. It is a mistake to treat each as the same. Further, these forms vary in their pervasiveness.\nThe last time I made this complaint I drew on an article by Don Moore and Paul Healy, “The Trouble with Overconfidence” (pdf). A more recent article by Don Moore and Derek Schatz (pdf) provides some further colour on this point (HT: Julia Galef). It’s worth pulling out a few excerpts.\nSo what are these distinct forms? Overestimation, overplacement and overprecision. (It’s also useful to disambiguate overoptimism, which I’ll touch on at the end of this post.)\nWhen people are diagnosing overconfidence, they can conflate the three. Pointing out that 90% of people believe they are better than average drivers (overplacement) is not evidence that a CEO was overconfident in their decision to acquire a competitor (possibly overestimation)."
  },
  {
    "objectID": "posts/the-three-faces-of-overconfidence.html#overestimation",
    "href": "posts/the-three-faces-of-overconfidence.html#overestimation",
    "title": "The three faces of overconfidence",
    "section": "Overestimation",
    "text": "Overestimation\nPeople tend to overestimate their performance on hard tasks. But when easy, they tend to underestimate.\n\nIn contrast to the widespread perception that the psychological research is rife with evidence of overestimation (Sharot, 2011), the evidence is in fact thin and inconsistent. Most notably, it is easy to find reversals in which people underestimate their performance, how good the future will be, or their chances of success (Moore & Small, 2008). When a task is easy, research finds that people tend to underestimate performance (Clark & Friesen, 2009). If you ask people to estimate their chances of surviving a bout of influenza, they will radically underestimate this high probability (Slovic, Fischhoff, & Lichtenstein, 1984). If you ask smokers their chances of avoiding lung cancer, they will radically underestimate this high probability (Viscusi, 1990).\nThe powerful influence of task difficulty (or the commonness of success) on over- and underestimations of performance has long been known as the hard-easy effect (Lichtenstein & Fischhoff, 1977). People tend to overestimate their performance on hard tasks and underestimate it on easy tasks. Any attempt to explain the evidence on overestimation must contend with the powerful effect of task difficulty."
  },
  {
    "objectID": "posts/the-three-faces-of-overconfidence.html#overplacement",
    "href": "posts/the-three-faces-of-overconfidence.html#overplacement",
    "title": "The three faces of overconfidence",
    "section": "Overplacement",
    "text": "Overplacement\nIn a reverse of the pattern for overestimation, people tend to overplace on easy tasks, but underplace on harder ones.\n\nThe evidence for “better-than-average” beliefs is so voluminous that it has led a number of researchers to conclude that overplacement is nearly universal (Beer & Hughes, 2010; Chamorro- Premuzic, 2013; Dunning, 2005; Sharot, 2011; Taylor, 1989). However, closer examination of this evidence suggests it suffers from a few troubling limitations (Harris & Hahn, 2011; Moore, 2007). Most of the studies measuring better-than-average beliefs use vague response scales that make it difficult to compare beliefs with reality. The most common measure asks university students to rate themselves relative to the average student of the same sex on a 7-point scale running from “Much worse than average” to “Much better than average.” Researchers are tempted to conclude that respondents are biased if more than half claim to be above average. But this conclusion is unwarranted (Benoît & Dubra, 2011). After all, in a skewed distribution the majority will be above average. Over 99% of the population has more legs than average.\n…\nWithin the small set of studies not vulnerable to these critiques, the prevalence of overplacement shrinks. Indeed, underplacement is rife. People think they are less likely than others to win difficult competitions (Moore & Kim, 2003). When the teacher decides to make the exam harder for everyone, students expect their grades to be worse than others’ even when it is common knowledge that the exam will be graded on a forced curve (Windschitl, Kruger, & Simms, 2003). People believe they are worse jugglers than others, that they are less likely than others to win the lottery, and less likely than others to live past 100 (Kruger, 1999; Kruger & Burrus, 2004; Moore, Oesch, & Zietsma, 2007; Moore & Small, 2008). These underplacement results are striking, not only because they vitiate claims of universal overplacement, but also because they seem to contradict the hard-easy effect in overestimation, which finds that people most overestimate their performance on difficult tasks.\n\nMoore and Healy offer an explanation for the different effects of task difficulty on overestimation and overplacement - myopia. I wrote about that in the earlier post."
  },
  {
    "objectID": "posts/the-three-faces-of-overconfidence.html#overprecision",
    "href": "posts/the-three-faces-of-overconfidence.html#overprecision",
    "title": "The three faces of overconfidence",
    "section": "Overprecision",
    "text": "Overprecision\nOverprecision is pervasive but poorly understood.\n\nA better approach to the study of overprecision asks people to specify a confidence interval around their estimates, such as a confidence interval that is wide enough that there is a 90% chance the right answer is inside it and only a 10% chance the right answer is outside it (Alpert & Raiffa, 1982). Results routinely find that hit rates inside 90% confidence intervals are below 50%, implying that people set their ranges too precisely—acting as if they are inappropriately confident their beliefs are accurate (Moore, Tenney, & Haran, 2016). This effect even holds across levels of expertise (Atir, Rosenzweig, & Dunning, 2015; McKenzie, Liersch, & Yaniv, 2008). However, one legitimate critique of this approach is that ordinary people are unfamiliar with confidence intervals (Juslin, Winman, & Olsson, 2000). That is not how we express confidence in our everyday lives, so maybe unfamiliarity contributes to errors.\n…\nOverprecision is the most pervasive but least understood form of overconfidence. Unfortunately, researchers use just a few paradigms to study it, and they rely on self-reports of beliefs using questions people are rarely called on to answer in daily life.\n\n(Although not covered in Moore and Schatz’s paper, Gigerenzer also offers a critique that I’ll discuss in a forthcoming post.)"
  },
  {
    "objectID": "posts/the-three-faces-of-overconfidence.html#overoptimism",
    "href": "posts/the-three-faces-of-overconfidence.html#overoptimism",
    "title": "The three faces of overconfidence",
    "section": "Overoptimism",
    "text": "Overoptimism\nMoore and Healy don’t touch on overoptimism directly in their paper, but in an interview with Julia Galef on the Rationally Speaking podcast, Moore touches on this point:\n\nJulia: Before we conclude this disambiguation portion of the podcast I want to ask about optimism, which I am using to mean thinking that some project of yours has a greater chance of success than you’re justified in thinking it does. How does that fit into that three‐way taxonomy?\nDon: It is an excellent question, and optimism has been studied a great deal. Perhaps the most famous scholars of optimism are Charles Carver and Mike Shier who have a scale that assesses the personality trait of optimism. Their usage of the term is actually not that far from the colloquial usage of the term, where to be optimistic is just to believe that good things are going to happen. Optimism is distinctively about a forecast for the future, and whether you think good things or bad things are going to happen to you.\n…\nInterestingly, this trait of optimism seems very weakly related to actual specific measures of overconfidence. When I ask Mike Shier why his optimistic personality trait didn’t correlate with any of my measures of overconfidence he said, “Oh, I wouldn’t expect it to.”\nJulia: I would expect it to!\nDon: Yeah. My [reaction] actually was, “Well, what the heck does it mean, if it doesn’t correlate with any specific beliefs?”\nI think it’s hard to reconcile those in any sort of coherent or rational framework of beliefs. But I have since had to concede that there is a real psychological phenomenology, wherein you can have this free floating, positive expectation that doesn’t commit you to any specific delusional beliefs."
  },
  {
    "objectID": "posts/the-unrealistic-assumptions-of-biology.html",
    "href": "posts/the-unrealistic-assumptions-of-biology.html",
    "title": "The unrealistic assumptions of biology",
    "section": "",
    "text": "Biologists are usually among the first to tell me that economists rely on unrealistic assumptions about human decision making. They laugh at the idea that people are rational optimisers who care only about maximising consumption.\nSome of the points are undoubtedly correct. Humans do not care primarily about consumption. They seek mates or other objectives related to their fitness. And of course, humans do not solve complex optimisation problems with constraints in their heads.\nBut, as most economists will tell you, the assumptions of rationality and consumption maximisation are mechanisms to derive general predictions about behaviour. And the funny thing is, biologists often do the same. Biologists tend to treat their subjects as optimisers.\nThat places biologists in a similar position to economists. Biologists may be able to predict or explain behaviour, but often they have not actually explained how their subjects make decisions. If they were to attempt to predict how their subjects would behave in a changed environment – which is the type of predictive task many economists attempt to do – they would likely fail as their understanding of the decision making process is limited.\nIn Rationality for Mortals: How People Cope with Uncertainty, Gerd Gigerenzer has a great chapter considering how biologists treat decision making, and in particular, to what extent biologists consider that animals use simple decision-making tools such as heuristics. Gigerenzer provides a few examples where biologists have examined heuristics, but much of the chapter asks whether biologists are missing something with their typical approach.\nAs a start, Gigerenzer notes that biologists are seeking to make predictions rather than accurate descriptions of decision making. However, Gigerenzer questions whether this “gambit” is successful.\n\nBehavioral ecologists do believe that animals are using simple rules of thumb that achieve only an approximation of the optimal policy, but most often rules of thumb are not their interest. Nevertheless, it could be that the limitations of such rules of thumb would often constrain behavior enough to interfere with the fit with predictions. The optimality modeler’s gambit is that evolved rules of thumb can mimic optimal behavior well enough not to disrupt the fit by much, so that they can be left as a black box. It turns out that the power of natural selection is such that the gambit usually works to the level of accuracy that satisfies behavioral ecologists. Given that their models are often deliberately schematic, behavioral ecologists are usually satisfied that they understand the selective value of a behavior if they successfully predict merely the rough qualitative form of the policy or of the resultant patterns of behavior.\n\n\nYou could write the same paragraph about economists, minus the statement about natural selection. That said, if you were to give the people in an economic model objectives shaped by evolution, even that statement might hold.\nBut Gigerenzer has another issue with the optimisation approach in biology. As from most analysis of human decision making, “missing from biology is the idea that simple heuristics may be superior to more complex methods, not just a necessary evil because of the simplicity of animal nervous systems.” Gigerenzer writes:\n\nThere are a number of situations where the optimal solution to a real-world problem cannot be determined. One problem is computational intractability, such as the notorious traveling salesman problem (Lawler et al., 1985). Another problem is if there are multiple criteria to optimize and we do not know the appropriate way to convert them into a common currency (such as fitness). Thirdly, in many real-world problems it is impossible to put probabilities on the various possible outcomes or even to recognize what all those outcomes might be. Think about optimizing the choice of a partner who will bear you many children; it is uncertain what partners are available, whether each one would be faithful, how long each will live, etc. This is true about many animal decisions too, of course, and biologists do not imagine their animals even attempting such optimality calculations.\nInstead the behavioral ecologist’s solution is to find optima in deliberately simplified model environments. We note that this introduces much scope for misunderstanding, inconsistency, and loose thinking over whether “optimal policy” refers to a claim of optimality in the real world or just in a model. Calculating the optima even in the simplified model environments may still be beyond the capabilities of an animal, but the hope is that the optimal policy that emerges from the calculations may be generated instead, to a lesser level of accuracy, by a rule that is simple enough for an animal to follow. The animal might be hardwired with such a rule following its evolution through natural selection, or the animal might learn it through trial and error. There remains an interesting logical gap in the procedure: There is no guarantee that optimal solutions to simplified model environments will be good solutions to the original complex environments. The biologist might reply that often this does turn out to be the case; otherwise natural selection would not have allowed the good fit between the predictions and observations. Success with this approach undoubtedly depends on the modeler’s skill in simplifying the environment in a way that fairly represents the information available to the animal.\n\nAgain, Gigerenzer could equally be writing about economics. I think we should be thankful, however, that biologists don’t take their results and develop policy prescriptions on how to get the animals to behave in ways we believe they should.\nOne interesting question Gigerenzer asks is whether humans and animals use similar heuristics. Consideration of this question might uncover evidence of the parallel evolution of heuristics in other lineages facing similar environmental structures, or even indicate a common evolutionary history. This could form part of the evidence as to whether these human heuristics are evolved adaptations.\nBut are animals more likely to use heuristics than humans? Gigerenzer suggests the answer is not clear:\n\nIt is tempting to propose that since other animals have simpler brains than humans they are more likely to use simple heuristics. But a contrary argument is that humans are much more generalist than most animals and that animals may be able to devote more cognitive resources to tasks of particular importance. For instance, the memory capabilities of small food-storing birds seem astounding by the standards of how we expect ourselves to perform at the same task. Some better-examined biological examples suggest unexpected complexity. For instance, pigeons seem able to use a surprising diversity of methods to navigate, especially considering that they are not long-distance migrants. The greater specialism of other animals may also mean that the environments they deal with are more predictable and thus that the robustness of simple heuristics may not be such as advantage.\n\nAnother interesting question is whether animals are also predisposed to the “biases” of humans. Is it possible that “animals in their natural environments do not commit various fallacies because they do not need to generalize their rules of thumb to novel circumstances.” The equivalent for humans is mismatch theory, which proposes that a lot of modern behaviour (and likely the “biases” we exhibit) is due to a mismatch between the environment in which our decision making tools evolved and the environments we exercise them in today.\nFinally, last year I wrote about why economics is not more “evolutionary”. Part of the answer there reflects a similar pattern to the above - biologists aren’t that evolutionary either."
  },
  {
    "objectID": "posts/the-value-of-a-species.html",
    "href": "posts/the-value-of-a-species.html",
    "title": "The value of a species",
    "section": "",
    "text": "Today I listened to an old (2006) interview with E.O Wilson by Michael Novacek (thanks NYAS). Wilson had a few criticisms of economics - the heavy basis in mathematics for one - and he stated that this had come at the expense of building a microeconomic foundation based on evolutionary biology.\nHe also spent some time on the subject of valuation of diversity and ecosystem services. Much of this is a no-brainer. There is clear value to clean water, recreation etc. Where it gets more interesting is when we get to the value of a species. There might be some value in the genetic or biological information, the willingness to pay of some people to simply let it exist and some degree of moral obligation. A great example of this is gecko. In another NYAS podcast, Kellar Autumn laid out what had been learnt from the amazing properties of the hairs on gecko feet. There are applications from nanosurgery to aerospace that may come from this.\nThe flip side is, of course, that the value must have a limit. In the same way that the optimal healthcare system will see some people die, some ambulances arrive late and not every treatment is provided to everyone it could help, there is a limit to the costs that society can bear. At what point do we draw that line?"
  },
  {
    "objectID": "posts/the-wisdom-of-crowds-of-people-who-dont-believe-in-the-wisdom-of-crowds.html",
    "href": "posts/the-wisdom-of-crowds-of-people-who-dont-believe-in-the-wisdom-of-crowds.html",
    "title": "The wisdom of crowds of people who don’t believe in the wisdom of crowds",
    "section": "",
    "text": "MIT Technology reports new research on the “wisdom of the confident”:\n\nIt turns out that if a crowd offers a wide range of independent estimates, then it is more likely to be wise. But if members of the crowd are influenced in the same way, for example by each other or by some external factor, then they tend to converge on a biased estimate. In this case, the crowd is likely to be stupid.\nToday, Gabriel Madirolas and Gonzalo De Polavieja at the Cajal Institute in Madrid, Spain, say they found a way to analyze the answers from a crowd which allows them to remove this kind of bias and so settle on a wiser answer.\n… Their idea is that some people are more strongly influenced by additional information than others who are confident in their own opinion. So identifying these more strongly influenced people and separating them from the independent thinkers creates two different groups. The group of independent thinkers is then more likely to give a wise estimate. Or put another way, ignore the wisdom of the crowd in favor of the wisdom of the confident.\n\nTo test this result, they eliminated those who updated their estimates based on that of the crowd:\n\nMadirolas and De Polavieja began by studying the data from an earlier set of experiments in which groups of people were given tasks such as to estimate the length of the border between Switzerland and Italy, the correct answer being 734 kilometers.\nAfter one task, some groups were shown the combined estimates of other groups before beginning their second task. These experiments clearly showed how this information biased the answers from these groups in their second tasks. …\nThat allows them to divide the groups into independent thinkers and biased thinkers. Taking the collective opinion of the independent thinkers then gives a much more accurate estimate of the length of the border.\n\nThe funny thing about this research is that anyone who believes in the wisdom of crowds and updates their belief based on that collective wisdom is then excluded from the collective estimate. The wisdom of crowds needs someone who trusts their own opinion more than that of the crowd. It is similar to the efficient markets hypothesis relying on those who don’t believe in it - if everyone believed markets were efficient, no one would invest effort in finding and acting on information that might affect market prices. That effort is what allows prices to reflect this information.\nSo who are the confident people who form this more accurate estimate? The Dunning-Kruger effect tells us that the unskilled will be overconfident as they don’t have the cognitive skills to recognise their ineptitude. But despite this effect, the more skilled do tend to be more confident than the unskilled - just not by as much as the skill gap warrants. As a result, eliminating the less confident can still cut the least skilled."
  },
  {
    "objectID": "posts/there-is-no-quantity-quality-trade-off.html",
    "href": "posts/there-is-no-quantity-quality-trade-off.html",
    "title": "There is no quantity-quality trade-off",
    "section": "",
    "text": "Following his disappearance from Psychology Today, Satoshi Kanazawa has reappeared in big think (with not all happy with this move [Update: he’s now gone again, along with the post]). In a recent post, Kanazawa asks an interesting question - Why do people with many siblings have many children? Kanazawa writes:\n\nStudies show that fertility is substantially heritable; genes partly influence how many children one has. Children of parents who have many children also have many children; children of parents who have few children also have few children. As a result, there is a strong positive correlation between the number of siblings one has (which equals the number of children that one’s parents had minus one) and the number of children one has.\n… [T]he positive correlation between the number of siblings and the number of children makes no evolutionary sense. Evolutionary theory actually predicts a negative correlation. …\nThis is because people with many siblings have the option of investing in their younger siblings and increasing their reproductive success by doing so. Humans are just as genetically related to their full siblings as they are to their own biological children; both share half their genes. … So investing in and “raising” younger siblings is just as good genetically and evolutionarily as investing in and raising one’s own genetic children.\n\nA second reason for expecting a negative correlation is that the parents face a quality-quantity trade-off. As a parent increases the number of their children, they have fewer resources to invest in each child. If that harms their children’s reproductive success, this will reduce the number of children in the following generation.\nI would argue that the answer to this puzzle (as Kanazawa terms it) lies in what happened after the demographic transition - that is, when fertility rates declined with industrialisation. Before the transition, fertility was generally not heritable and any correlation in family size between generations was due to environmental factors and chance. But after the transition, heritability increased. This suggests that different people responded to the transition in different ways.\nOne of those differences is how they respond to the quantity-quality trade-off in the new, plentiful environment that accompanies industrialisation. Human history is one of Malthusian conditions, where available resources constrain the population. In that environment, any additional children will affect the resources available for the other children.\nIn the modern environment, there is essentially no such constraint. Increased quantity does not harm quality in a way that reduces the quantity of children in the next generation. Adoption studies generally show a low cost to more children, and those small costs are to income or education, not reproductive success. As a result, the optimal strategy in the modern environment is to have as many children as you can. If you have a genetic predisposition to do that (or even a culturally transmitted predisposition), your children are likely to too, leading to the positive correlation that Kanazawa observes.\nThis argument also applies to Kanazawa’s argument that we should expect a negative correlation through care for siblings. In the modern environment, that care for siblings does not assist the sibling in having more children. They are not resource constrained. The optimal strategy is for each sibling to have as many children as they can. Those that follow this strategy have more children in successive generations with little cost to quality.\nThis argument relates to my latest working paper. With the heritability of fertility increasing after the demographic transition, “high-fertility genotypes” can be expected to increase in number. One day a cost for those excessive children may emerge as the population starts to hit Malthusian constraints, but until then, fertility is not constrained by resources and the evolutionary dynamics point to fertility going up."
  },
  {
    "objectID": "posts/thorstein-veblens-the-theory-of-the-leisure-class.html",
    "href": "posts/thorstein-veblens-the-theory-of-the-leisure-class.html",
    "title": "Thorstein Veblen’s The Theory of the Leisure Class",
    "section": "",
    "text": "In 2011, Thorstein Veblen was ranked seventh in a poll of economists on their favourite, dead, 20th century economist. He ranked behind Keynes, Friedman, Samuelson, Hayek, Schumpeter and Galbraith. His supporters were among the least liberal (in the classical sense of the word) of the survey participants. Given his approach to consumerism, as detailed in The Theory of the Leisure Class, this is no surprise.\nThe Theory of the Leisure Class, published in 1899, was one of the earliest books to explore the economic assumption that people wish to consume. Veblen noted this was not purely a desire to consume in itself. People also care about status, reputation and honour. They care about their relative position to others, such as their relative wealth. And consumption provides a means of establishing this relative position.\nConspicuous leisure and consumption\nTo turn wealth into status and reputation, you needs to signal your wealth. Veblen explored two possible signals, conspicuous leisure and conspicuous consumption, with Veblen’s coining of the latter term his best known claim to fame. Veblen has a relatively modern take on these two concepts, recognising the need for waste. Signalling theory tells us that waste required for a signal to be reliable.\nWhen there are few goods for conspicuous consumption, as would be the case in primitive societies, conspicuous leisure is a more accessible way to signal wealth. Conspicuous leisure might involve reaching a level of manners and etiquette that could only be achieved through an excessive use of time, or becoming proficient at sports. Veblen also considers what he calls “vicarious conspicuous leisure”, whereby the head of the house employs servants (or even the housewife) in exercises that waste time.\nAs society advances, people move from conspicuous leisure to conspicuous consumption. They have an increasingly large circle of people with whom they associate and wish to signal status to. In a small village, everyone is familiar with each other and will note the habits of the servants and other householders carrying out the conspicuous leisure. In a larger city, the conspicuous waste needs to be visible, and conspicuous consumption in the nature of watches, clothing and carriages is immediately obvious. Conspicuous consumption can also be vicarious, with servants dressed up in excessive livery.\nVeblen considered that conspicuous consumption will consume all future growth in production and efficiency. He states:\n\nThe need of conspicuous waste, therefore, stands ready to absorb any increase in the community’s industrial efficiency or output of goods, after the most elementary physical wants have been provided for.\n\nVeblen also suggests that the use of additional production for conspicuous consumption acts as a Malthusian check on fertility. If signals are wasteful, then some of these resources will not be available for increasing the number of offspring. However, to be evolutionary stable, any reduction in conspicuous consumption by an individual would have them suffer a cost in the form of reputation and status, and in turn, mating opportunities.\nOne of Veblen’s interesting perspectives is that costliness masquerades under the name of beauty. Veblen states that “beauty, in the naive sense of the word, is the occasion rather than the ground of their monopolization or of their commercial value.” The marks of expensiveness, rarity and exclusivity become known as beauty.\nThis leads to imperfections in goods, which are evidence of being hand and not machine-made, becoming signs of beauty. Counterfeits lose their beauty on being identified as such. Or each year the fashion changes, which is wasteful – and people prefer the more recent fashions to the older ones.\nVeblen applies this concept to beauty in women, with tastes shifting from “women of physical presence” to a “lady”, as conspicuous consumption and leisure grew. The less suited a woman is for work, the more waste, and the more beautiful she would be perceived.\nThe evolution of the leisure class\nVeblen follows his discussion of beauty with a series of evolutionary arguments on the nature of the leisure class. The “leisure class” is an unproductive upper class, and contrasts with the “industrial class”, a subordinated but productive working class. Veblen’s line of argument is often difficult to follow, with the boundary between social and genetic selection unclear. His underlying agenda, a critique of the leisure class, also clouds his arguments.\nVeblen argues that the selection of institutions affects the selection of people within society. Institutions change fast, so although only the fittest habits of thought will normally survive, the selection of people cannot keep up. Further, changes which may be good for society as a whole may be bad for certain people. Veblen’s discussion provides a nice picture of a dynamic environment and selection pressures that vary with it.\nDespite this dynamism, society is slow to change and conservative. Veblen argues that the leisure class is able to keep society conservative through withdrawing the means of sustenance to the industrial class. As a result, the industrial class does not have the resources to invest in new ideas and habits. Even if they did gain some surplus, that would be wasted on the conspicuous consumption that the leisure class has established as the societal norm.\nOn an individual level, Veblen considers there are two basic types of people – predatory and peaceful. Predatory types are violent (in certain stages of society), selfish and dishonest, and are not diligent. Peaceful types are the opposite. Which traits are expressed will depend on the state of society. For Veblen, the spectrum of predatory to peaceful roughly coincides with the spectrum of blonde through brunette to Mediterranean ethnicities.\nVeblen suggests that society progressed from a peaceful, native state, to a barbarian state, before shifting back towards the more peaceful modern society. Peaceful traits were selected for in the native state, and predatory traits selected for in the barbarian states. Veblen states, however, that selection did not eliminate all the peaceful traits in the barbarian era, allowing peaceful traits to be present in modern society.\nAs to how these traits are distributed at his time of writing, Veblen sees the leisure class as the predatory type and the industrial class of the peaceful type. The leisure class is not able to be violent in modern society, so they use more “peaceful predatory” methods, such as fraud. The industrial class is not in need of predatory habits, with Veblen suggesting that “economic man” in the sense of the selfish person (an indirect slight on Adam Smith) is useless for modern society. It is by being diligent and honest that the industrial man thrives.\nVeblen’s shot at “economic man” is not particularly effective, and does not recognise that selfishness is required, in an evolutionary sense, for all people. The reason industrial man is diligent is because that is how he benefits. If he did not benefit, he would be selected against and disappear. That society benefits is the operation of Smith’s invisible hand.\nDespite his categorisation of types between classes, Veblen later suggests that there are no broad character differences between the leisure class and the rest. Some predatory behaviour persists in the industrial class due to the behaviour of the leisure class. He also notes that people in the leisure class, by virtue of their resources, are not subject to harsh selection pressure, so peaceful characteristics can persist. What is most determinative of the traits in the leisure class are those traits which lead to admission to the class. While these have changed over time (say, from raw violence to fraud), they are generally of a predatory nature. It is not easy to gel this position of no difference with his earlier statements, and I am not sure they can be reconciled. My one suggestion is that the differences will grow if the current institutional framework continues to exist.\nPut together, Veblen’s use of evolutionary theory is a strange mix of group selection and broad statements on inherent traits. There is little detailed consideration of the selection process that might have occurred. If nothing else, it appears that Veblen simply wanted to critique the leisure class and would use whatever tools were at his disposal. Through his evolutionary discussion, Veblen also manages to avoid addressing the basis for the desire for reputation and status.\nSport, religion and education\nThe rest of the book largely involves Veblen applying his framework to sport, religion and education.\nSports reflect the predatory skills of the leisure class and delinquents. Veblen disagreed with the common view that sports build temperament, and instead they involve chicanery, falsehood and browbeating. That is why we need umpires. For the industrial classes, Veblen felt that sport is more a diversion than a habit, although the role of sport for the industrial class seems somewhat different today.\nVeblen considered that the temperament that inclines one to sport inclines one to religion (and vice versa). Religion, and the conspicuous leisure and consumption associated with it, change the patterns of consumption in the community and lowers its vitality. As an example, Veblen referred to the religious Southern United States. He considered that their industry was more handicraft than industrial. Their range of habits, such as duels, cock-fighting and male sexual incontinence (shown by the presence of mulattoes) were evidence of barbarian traits.\nOn education, Veblen saw the alignment of education institutions with sport and religion as evidence of education’s status as a leisure class activity. Higher education has many rituals and ceremonies and encourages proper speech and spelling (conspicuous leisure), while lower schools tend to more practical. The teaching of the classics and dead languages were, in particular, conspicuous consumption.\nOne interesting sideline is Veblen’s view on how industrialisation has affected the status of women. Industrialisation allows women to revert to a more primitive type (Veblen’s primitive type being peaceful and industrial). The leisure class, however, needs to keep women in their place to engage in vicarious conspicuous leisure (they are, after all, a signal for the man). As a result, when educational institutions finally began to admit women, they were primarily enrolled in courses with a quasi-artistic quality, which help women in performing vicarious conspicuous leisure.\n[This post is a combined and edited version of three previous posts exploring the book. Those old posts are here, here and here.]"
  },
  {
    "objectID": "posts/three-podcasts.html",
    "href": "posts/three-podcasts.html",
    "title": "Three podcast episodes",
    "section": "",
    "text": "Here are three I recently enjoyed:\n\nEcontalk:Brian Nosek on the Reproducibility Project - Contains a lot of interesting context about the reproducibility crisis (of which you can get a flavour from my presentation Bad Behavioural Science: Failures, bias and fairy tales).\nEcontalk: Phil Rosenzweig on Leadership, Decisions, and Behavioral Economics - The problems with taking behavioural economics findings out of the lab and applying them to business decision making.\nRadiolab: The Rhino Hunter - I listen to Radiolab less than when it had more of a science focus, but this podcast on hunting endangered species to save them is excellent."
  },
  {
    "objectID": "posts/top-10-books-in-2010.html",
    "href": "posts/top-10-books-in-2010.html",
    "title": "My top 10 books in 2010",
    "section": "",
    "text": "As is the fashion for this time of year, here are my top ten books of 2010. As I tend to read books that are both old and new, these are the top 10 books I_ have read_ this year.\n\nThe Enlightened Economy: An Economic History of Britain 1700-1850 by Joel Mokyr - Although I don’t agree with the underlying hypothesis, it is a great analysis of the Industrial Revolution.\nThis Time Is Different: Eight Centuries of Financial Folly by Carmen Reinhart & Kenneth Rogoff - It is nice to put the recent crisis in perspective.\nToo Big to Fail: The Inside Story of How Wall Street and Washington Fought to Save the Financial System – and Themselves by Andrew Ross Sorkin - As a critic of the bailouts during the crisis, this book put a seed of doubt in my mind as to whether I would have stuck to my guns if I was the one pulling the levers. It is one thing to criticise from the ivory tower. It is another to be in the midst of the panic with the responsibility on your shoulders. Sorkin puts you in the room like no other.\nCharles Darwin - Voyaging by Janet Browne - I am only halfway through this book, but it adds some great colour to Darwin’s life. I have read a number of Darwin biographies and (so far) this is comfortably the best.\nKrakatoa: The Day the World Exploded: August 27, 1883 by Simon Winchester - I read this sitting on the beach in Bali. I could not have picked a better introduction to Indonesia (apart from possibly Wallace’s The Malay Archipelago).\nAnimal Spirits: How Human Psychology Drives the Economy, and Why It Matters for Global Capitalism by George Akerlof and Robert Shiller - As with most books that advocate the use of psychology in economics, I agree with the concept that we need an economics that incorporates real humans. As is also usually the case, I am uncomfortable with the extent that Akerlof and Shiller advocate the use of government power to constrain the “animal spirits”.  I am not sure it is so easy. However, the book is a great read (in plain English) and raises plenty of interesting ideas.\nThe Invisible Hook: The Hidden Economics of Pirates by Peter Leeson - Of all the books applying economics to new areas, this was the most fun.\nThe Bonfire of the Vanities by Tom Wolfe - A bit dated, not particularly subtle and it is hard  to like any of the characters, but I couldn’t put it down.\nBorn to Run: A Hidden Tribe, Superathletes, and the Greatest Race the World Has Never Seen by Christopher McDougall - This book had possibly the greatest practical effect on me. I have moved to bare foot (or near bare foot) running and years of shin pain have gone away. And as an aside, the story is great.\nThe Big Short: Inside the Doomsday Machine by Michael Lewis - Lewis finds the humour in the chaos better than most. The question that hangs in the air through this book is what is the social purpose behind this gambling."
  },
  {
    "objectID": "posts/trading-fish.html",
    "href": "posts/trading-fish.html",
    "title": "Trading fish",
    "section": "",
    "text": "Alex Tabarrok has posted on Marginal Revolution a piece on the expansion of “catch shares” as a fisheries management tool. Under catch shares (also called individual tradeable quotas or ITQs), each fisher owns a percentage of the quota set for the fishery. The fisher can trade the share and it provides flexibility to the fisher about when and how they choose to catch their quota.\nThe use of catch shares as a method of allocating fishing rights has many benefits. Beyond the information obtained from the prices of the quotas and the opportunity for environmental groups to buy shares, there are some important incentives that they offer. The owners of the shares know that they have an established right into the future and it is in their interest that the stock over which that right exists is maintained at a reasonable level.\nAs pointed out by Quentin Grafton, Tom Kompas and Ray Hilborn in Science, a maximised economic yield (and maximised catch share value) generally occurs when there is a larger stock size. This is mainly due to the stock effect, whereby it is easier to catch fish from a larger stock. Catching the last fish in the ocean is very expensive. If fisherman have certainty in their future catch rights through their catch share, they will be more likely to support restoration of stock size to that which delivers the maximum economic yield.\nLike John Tierney, I find the opposition to catch shares by some environmental groups perplexing from a strategic point of view. Catch shares have the potential to widen the group of allies who wish to preserve the stock. As noted by  John, research suggests that catch shares are superior to alternative allocation measures (although they are not perfect and require appropriate quota levels etc). I would suggest that the opposition is because many environmentalists do not trust markets. This  extends to doubt about the effectiveness of cap-and-trade systems for pollutants or the response to incentives provided by prices such as a carbon tax. To Oceana, ITQs are like the collateralised debt obligations at the centre of the global financial crisis. Despite the differences, evidence of one market crisis (whatever the cause) condemns all markets.\nMany environmentalists are also opposed to the “corporatisation” of  fisheries, with catch shares seen as a pathway to ownership of the fisheries by large corporate interests. While this might result, it would not be significantly different to the current state of affairs. This would also depend on the initial allocation. If small fishers were given catch shares, it would be a benefit of the scheme that they have the ability to sell their share if they wished to use that money for an alternative use.\nHaving said this, some environmentalists are more actively advocating their use (such as the Environmental Defence Fund) as it becomes clear that fisheries with allocated catch shares have better conservation outcomes. Based on the examples in Alex’s post, they are being heard."
  },
  {
    "objectID": "posts/trivers-on-romneys-sons-and-obamas-daughters.html",
    "href": "posts/trivers-on-romneys-sons-and-obamas-daughters.html",
    "title": "Trivers on Romney’s sons and Obama’s daughters",
    "section": "",
    "text": "In a National Review article a couple of months ago, Kevin Williamson questioned Obama’s status relative to Mitt Romney’s because Obama’s children were daughters, while Romney had sons.\n\nIt is a curious scientific fact (explained in evolutionary biology by the Trivers-Willard hypothesis — Willard, notice) that high-status animals tend to have more male offspring than female offspring, which holds true across many species, from red deer to mink to Homo sap. The offspring of rich families are statistically biased in favor of sons — the children of the general population are 51 percent male and 49 percent female, but the children of the Forbes billionaire list are 60 percent male. Have a gander at that Romney family picture: five sons, zero daughters. Romney has 18 grandchildren, and they exceed a 2:1 ratio of grandsons to granddaughters (13:5). When they go to church at their summer-vacation home, the Romney clan makes up a third of the congregation. He is basically a tribal chieftain.\nProfessor Obama? Two daughters. May as well give the guy a cardigan. And fallopian tubes.\nFrom an evolutionary point of view, Mitt Romney should get 100 percent of the female vote. All of it.\n\nIn response, Steve Mirsky called Robert Trivers to ask whether Williamson’s use of the Trivers-Willard hypothesis was correct. Trivers notes a couple of problems with the analysis. First, voting is not the relevant decision:\n\n“Maybe the guy should be saying that all women should try to f— [Romney]. Look, the f—er’s rich. Can you f— him and get some of the money? Or are you just voting for him? They’re two different decisions.” …\n“They [women] should all want a man with money. That’s so obvious we don’t need to talk about the sex ratio of the progeny.\n\nOf course, if you want to measure their evolutionary success, raw numbers are also a better measure:\n\n“There’s no way of looking at the sex ratios of progeny of these two couples and predicting anything about their relative superiority over time. It would be better put as an evolutionist arguing about the five-versus-two ratio [of the total number of children born to each candidate].\n\nA five to two gap would be hard to make up, regardless of the advantages sons may accrue from their high status."
  },
  {
    "objectID": "posts/trust-and-education.html",
    "href": "posts/trust-and-education.html",
    "title": "Trust and education",
    "section": "",
    "text": "Razib Khan of Gene Expression has put together a series of charts on changes in trust in the United States over the last 40 years. The trust data comes from the General Social Survey, and shows a slight decline in trust over this time.\nBesides some interesting results, such as the level of trust in the media, what struck me was the strength of the link between trust and education or vocabulary scores.\n\nThis result is consistent with earlier findings that trust correlates with IQ, as I discussed in my recent post. While that post focussed on the implications of increased trust on a country’s institutions, these results show that a range of trust levels exist within a country under these same macro-level institutions.\nOne interpretation of this is that within a country, there is assortment by IQ and education levels, which can allow micro-level institutions in which trust is rewarded to develop. What that implies, of course, is that lower IQ groups face a micro-institutional framework in which people behave in a less trustworthy way.\nOne commenter to Khan’s post suggested that high IQ people are more able to judge whether someone is trustworthy. I tend to agree with Khan’s response - that one can be trusting in an environment where trustworthiness flourishes. I would suggest that in many situations, high-IQ people are as likely to get fleeced as other people, but fortunately high-IQ people tend to live in environments where this is unlikely."
  },
  {
    "objectID": "posts/two-articles-on-genetics-and-economics.html",
    "href": "posts/two-articles-on-genetics-and-economics.html",
    "title": "Two articles on genetics and economics",
    "section": "",
    "text": "From Charles Manski in the latest Journal of Economic Perspectives (pdf):\n\nSomeone reading empirical research relating human genetics to personal outcomes must be careful to distinguish two types of work. An old literature on heritability attempts to decompose cross-sectional variation in observed outcomes into unobservable genetic and environmental components. A new literature measures specifific genes and uses them as observed covariates when predicting outcomes. I will discuss these two types of work in terms of how they may inform social policy. I will argue that research on heritability is fundamentally uninformative for policy analysis, but make a cautious argument that research using genes as covariates is potentially informative.\n\nFrom the same edition, Beauchamp and colleagues address the following question (pdf):\n\nHow, if at all, should economists use and combine molecular genetic and economic data? What challenges arise when analyzing genetically informative data?\n\nI’ll post my thoughts on these articles when I have had a chance to digest."
  },
  {
    "objectID": "posts/ultimate-population-limits.html",
    "href": "posts/ultimate-population-limits.html",
    "title": "Ultimate population limits",
    "section": "",
    "text": "Given the recent discussion on population that the release of Bryan Caplan’s Selfish Reason to Have Kids has triggered (my posts here, here and here), I have been contemplating some physical limits of what the ultimate human population could be. The current global population is around 6.8 billion and is growing at around 1.2 per cent per year. Let us suppose that the population growth rate persists (as I argued before, there is a case to argue that it might increase). So, how long does it take before the physical limits are reached? (I am sure other people have done this before, but as with most things, I always like to do these exercises myself)\nLet’s start with an outer limit. Suppose that each person can be uploaded onto a virtual machine and each person only requires one atom to exist. There are about 10^80 atoms in the universe (most of which are hydrogen). At the current growth rate, it would take less than 14,000 years for there to be more humans than there are atoms.\nHowever, there are a few constraints. The diameter of the observable universe is around 93 billion light years, so assuming we are roughly in the centre, it would take 46 to 47 billion years to reach all points if humans could travel at the speed of light. Yet, there are only 14,000 years before the population needs to access all the matter in the universe. It would be a fair assumption that most of the matter in the universe would be outside the potential travel distance (unless someone wants to argue that we will figure out how to travel faster than light or skip around the universe in some other way). Humans will not even be able to access all of the matter in our galaxy, which is around 100,000 light years in diameter.\nHow about a closer limit - the number of atoms in the solar system. I could not find a figure for this, but as the sun has more than 99 per cent of the solar system’s mass, I will use the comparative figure for the sun - around 10^57 atoms. Using our one atom per person calculation from above, there would be more people than atoms in a touch over 9,000 years. That’s still a fair bit of time, but of course, does rely on use utilising every atom in the solar system. It is perfectly plausible that humans will have travelled across the breadth of the solar system by that time. Limiting us to earth, there are around 10^50 atoms, which allows around 7,800 years. Adding Mars gets humans less than 10 extra years.\nWhat if we don’t move to this virtual utopia, still need (or want) to be in human form and are limited to earth or the solid planets in the solar system? Again allowing every piece of matter to be used in human form (no need for land to stand on, air to breathe, a biosphere etc), giving each human an average weight of 70 kilograms and with the earth’s weight of 6*10^24 kilograms, current population growth rates can continue for around 2,500 years before hitting the physical limits. Again, Mars allows less than 10 years further population growth.\nBreaking the example down further, what if humans are limited to the surface of earth or, again, the solid planets in the solar system? The earth has a surface area of 510 million square kilometres (or 510 trillion square metres), including both ocean and land area. If we assume that each human needs one square metre to provide all of their sustenance and needs, the population scenario hits the wall more quickly - in around 950 years. Mars allows around 20 extra years population growth. If we only need a square centimetre each, the population can grow for a further few hundred years.\nHaving said all the above, I think it is fair to say that within 10,000 years, which is less than the time since the first use of agriculture, humans will run up against hard physical population constraints, or population growth will be voluntarily constrained. Even allowing a virtual world where each human only needs an atom and there is no need for physical bodies or a biosphere, sooner or later population growth must stop."
  },
  {
    "objectID": "posts/unchanging-humans.html",
    "href": "posts/unchanging-humans.html",
    "title": "Unchanging humans",
    "section": "",
    "text": "One interesting thread to Don Norman’s excellent The Design of Everyday Things is the idea that while our tools and technologies are subject to constant change, humans stay the same. The fundamental psychology of humans is a relative constant.\n\nEvolutionary change to people is always taking place, but the pace of human evolutionary change is measured in thousands of years. Human cultures change somewhat more rapidly over periods measured in decades or centuries. Microcultures, such as the way by which teenagers differ from adults, can change in a generation. What this means is that although technology is continually introducing new means of doing things, people are resistant to changes in the way they do things.\n\nI feel this is generally the right perspective to think about human interaction with technology. There are certainly biological changes to humans based on their life experience. Take the larger hippocampus of London taxi drivers, increasing height through industrialisation, or the Flynn effect. But the basic building blocks are relatively constant. The humans of today and twenty years ago are close to being the same.\nEvery time I hear arguments about changing humans (or any discussion of millennials, generation X and the like), I recall the following quote from Bill Bernbach (I think first pointed out to me by Rory Sutherland):\n\nIt took millions of years for man’s instincts to develop. It will take millions more for them to even vary. It is fashionable to talk about changing man. A communicator must be concerned with unchanging man, with his obsessive drive to survive, to be admired, to succeed, to love, to take care of his own.\n\n(If I were making a similar statement, I’d use a shorter time period than “millions”, but I think Bernbach’s point still stands.)\nBut for how long will this hold? Don Norman again:\n\nFor many millennia, even though technology has undergone radical change, people have remained the same. Will this hold true in the future? What happens as we add more and more enhancements inside the human body? People with prosthetic limbs will be faster, stronger, and better runners or sports players than normal players. Implanted hearing devices and artificial lenses and corneas are already in use. Implanted memory and communication devices will mean that some people will have permanently enhanced reality, never lacking for information. Implanted computational devices could enhance thinking, problem-solving, and decision-making. People might become cyborgs: part biology, part artificial technology. In turn, machines will become more like people, with neural-like computational abilities and humanlike behavior. Moreover, new developments in biology might add to the list of artificial supplements, with genetic modification of people and biological processors and devices for machines.\n\nI suspect much of this, at least in the short term, will only relate to some humans. The masses will experience these changes with some lag.\n(See also my last post on the human-machine mix.)"
  },
  {
    "objectID": "posts/unskilled-and-unaware.html",
    "href": "posts/unskilled-and-unaware.html",
    "title": "Unskilled and unaware",
    "section": "",
    "text": "Robin Hanson has had another stab at the oft-quoted paper by Kruger and Dunning, Unskilled and Unaware of It. The first couple of sentences of the paper’s abstract gives Kruger and Dunning’s basic (and somewhat amusing) claim:\n\nPeople tend to hold overly favorable views of their abilities in many social and intellectual domains. The authors suggest that this overestimation occurs, in part, because people who are unskilled in these domains suffer a dual burden: Not only do these people reach erroneous conclusions and make unfortunate choices, but their incompetence robs them of the metacognitive ability to realize it.\n\nHanson notes that this paper is commonly used by people to suggest that those who disagree with them are confused idiots who lack the basic ability to recognise their error.\nIgnoring for a moment other possible explanations for Kruger and Dunning’s empirical results (which is the purpose of Hanson’s post), I always found the “ignore the idiot” interpretation of the paper to be missing the point. The problem is that it is not clear who the idiot is. Trying to self-assess whether you are correct is subject to the biases identified by Kruger and Dunning. If you are an idiot, you are unlikely to know this. If anything, the paper suggests that, without objective evidence, one should be more humble in assessing their ability.\nOnto the interpretation of the results, it is also an interesting question about what extent the overestimation of ability results in a “dual burden” as claimed by Kruger and Dunning. There is some evidence that self-deception can be adaptive, such as von Hippel and Trivers discuss in a recent paper. Apart from assisting in deceiving others (it’s easier to lie if you don’t know you are), self-deception in the form of overconfidence might encourage one to try tasks and persevere at them, with occasional success, where a more realistic assessment may result in a decision not to try at all. Starek and Keating’s work on self-deception and swimming also illustrates this idea (HT on the swimming: Radiolab). To the extent this is the case, an overconfident estimation of one’s ability may not be a burden but may help someone to get out there with what they have."
  },
  {
    "objectID": "posts/using-evolutionary-theory-to-shape-neighbourhoods.html",
    "href": "posts/using-evolutionary-theory-to-shape-neighbourhoods.html",
    "title": "Using evolutionary theory to shape neighbourhoods",
    "section": "",
    "text": "David Sloan Wilson has just written a book, The Neighborhood Project: Using Evolution to Improve My City, One Block at a Time, where he catalogues the use of evolutionary theory to improve life in Binghamton.\nI haven’t read it yet, but a review by Mark Oppenheimer foreshadows the content. He writes:\n\nMr. Wilson argues — more controversially than he lets on — that “human cultural diversity is like biological diversity.” … For Mr. Wilson the main thing to understand is not that culture trends toward sublimity, but that it evolves, rather than merely changing, and that its evolution is subject to rules like those that govern biological organisms.\nOur cultural forms adapt, Mr. Wilson says. They are shaped by a kind of natural selection. And once we accept that cultural evolution can be interpreted in this mechanistic way, we are better positioned to interpret the evidence we get from surveys, Christmas-light canvassing and so forth.\n…\nThe chess passage highlights Mr. Wilson’s central conceit, inspiring without being fully persuasive: that evolutionary theory is the academic’s proper lingua franca, the terminology and indeed the theoretical framework that allows everyone to talk, and to plot the improvement of humankind, across disciplines. With a reasonably scientific understanding of any habitat and the animals in it — students in Binghamton schools, Christmas revelers in assorted neighborhoods — a shrewd scientist can develop tools to help the animals survive and flourish.\n\nThe real test of Wilson’s project will be whether it works or not. However, the use of evolution as a tool to assess cultural change without using it to analyse the motivations and actions of biological humans seems to sell the potential of evolutionary theory short."
  },
  {
    "objectID": "posts/using-the-malthusian-model-to-measure-technology.html",
    "href": "posts/using-the-malthusian-model-to-measure-technology.html",
    "title": "Using the Malthusian model to measure technology",
    "section": "",
    "text": "Underlying much of Ashraf and Galor’s analysis of genetic diversity and economic development is a Malthusian model of the world. The Malthusian model, as the name suggests, originates in the work of Thomas Malthus (pictured). Malthus had the misfortune of providing an excellent description of the world across millennia, just at the point at which the model (apparently) lost much of its predictive power.\nThe Malthusian model rests on the assumption that any increase in income generates population growth. This ultimately prevents increases in technology from translating into increases in living standards. The greater resource productivity must now be  shared between more people. Of course, the reason people state that the Malthusian model no longer applies is that since 1800 many parts of the world have experienced substantial increases in per person income as population growth did not match technological progress.\nThe Malthusian model generates a couple of important predictions. First, any increase in productivity will generate population growth, not income growth. Secondly, differences in productivity between regions will be reflected in different population densities, not income differences.\nThis last point is important. It allows economists to use population density as a measure of technology and productivity in a Malthusian world. Since measuring technology is difficult but we have many measures of population density across time and societies, the Malthusian model provides a basis for conducting comparative economic analysis between countries and regions for times before 1800.\nAshraf and Galor use population density as a measure of technology for most of their analysis of genetic diversity and economic development, following a long line of economists who have done the same. But until recently, whether population density is a reasonable measure had not been properly tested.\nIn 2009, Ashraf and Galor published in the American Economic Review (ungated version here) an empirical examination of this hypothesis for the period 1 to 1500 CE (originating from Ashraf’s PhD thesis, as did the paper on genetic diversity and economic growth). The problem they faced was how to untangle population and technology when the two are so closely intertwined. Economists use the population density measure because technology is hard to measure and each flows directly into the other (more people leads to more ideas).\nTo untie the two, Ashraf and Galor use the timing of the onset of the Neolithic Revolution in different regions as a proxy for technology. The Neolithic Revolution occurred when populations moved from hunting and gathering to agricultural activities. If we accept Jared Diamond’s thesis that countries with favourable biogeographical factors gained a technological head start through the advent of agriculture that they maintain through to today, the timing of the Neolithic Revolution in different societies could be a proxy for technology and productivity.\nUsing this proxy, Ashraf and Galor found that, consistent with Malthusian theory, technology and productivity had a positive effect on population density, but no effect on per person income levels for the period 1 to 1500 CE. The result is robust to a range of controls including geographic and climactic factors, and holds when they use a more direct (but possibly less reliable) measure of technology.\nThere are two particularly interesting observations that Ashraf and Galor draw from their work. The first is that despite income stagnation, pre-Industrial times could be very dynamic. It is just that the Malthusian dynamics mask the effect of technological changes.\nSecondly, their finding can be interpreted as supporting Jared Diamond’s hypothesis (or at least, it is not inconsistent with it). Those societies that first experienced the Neolithic Revolution had the highest population densities, suggesting a persistent advantage to an early start.\nHowever, this support for the Malthusian model is not a ticket to use any population density data as a measure of technological progress. One of the more interesting points in the critique of Ashraf and Galor’s genetic diversity work published in Current Anthropology was the way some of the population density estimates used by Ashraf and Galor were developed.\n\nMcEvedy and Jones (1978:292) argue that the total population in Mexico in 1500 CE was no more than 5 million. They do so based on data from Rosenblat (1945, 1967), a source that uses problematic postconquest records. In fact, scholars contemporary with McEvedy and Jones (1978) proposed estimates in the 5–6 million range for the area corresponding only to the Aztec empire (e.g., Sanders and Price 1968). The Aztecs controlled a territory that covered no more than one quarter of contemporary Mexico and that excluded all of northwest Mexico and the Yucatan. Even while, at the time McEvedy and Jones (1978) were writing, other estimates for Mexico’s population were set at around 18–30 million (Cook and Borah 1971), McEvedy and Jones (1978: 272) discredit those estimates on the puzzling claim that they were not in line with those of other populations at “comparable levels of culture.”\n\nGiven that McEvedy and Jones are allowing the level of culture to colour their population estimates, those population estimates cannot be considered a sound basis for measuring technology. Population data shaped by the Malthusian model is not ideal to use as a measure of development. I don’t expect that changing the population density numbers substantially change Ashraf and Galor’s results (although the data is online if you want to check this), but we should use the numbers with some caution.\nMy posts on Ashraf and Galor’s paper on genetic diversity and economic growth are as follows:\n\nA summary of the paper methodology and findings\nDoes genetic diversity increase innovation?\nDoes genetic diversity increase conflict?\nIs genetic diversity a proxy for phenotypic diversity?\nIs population density a good measure of technological progress? (this post)\nWhat are the policy implications of the effects of genetic diversity on economic development?\nShould this paper have been published?\n\nEarlier debate on this paper can also be found here, here, here and here."
  },
  {
    "objectID": "posts/veblens-the-theory-of-the-leisure-class-part-ii.html",
    "href": "posts/veblens-the-theory-of-the-leisure-class-part-ii.html",
    "title": "Veblen’s The Theory of the Leisure Class, Part II",
    "section": "",
    "text": "Following last week’s post on Thorstein Veblen’s The Theory of the Leisure Class, I’ve progressed through some more of the book (to chapter 9). It hasn’t got any easier to read, but Veblen’s interesting observations on conspicuous consumption, beauty and evolution keep flowing.\nOne of his more interesting perspectives is on how costliness masquerades under the name of beauty. Veblen argues that cost determines what is considered beautiful, with the marks of expensiveness becoming known as beauty. Veblen states that “beauty, in the naive sense of the word, is the occasion rather than the ground of their monopolization or of their commercial value.” It is rarity and exclusivity that determines beauty.\nThis leads to ideas such as imperfections in goods, which are evidence of being hand and not machine-made, becoming signs of beauty. Counterfeits lose their beauty on being identified as such. Or take fashion. Each year the fashion changes, which is wasteful - and people prefer the more recent fashions to the older ones. But suppose we showed fashion from today and ten years ago to someone from 200 years ago. Would they perceive any difference in beauty?\nVeblen applies this concept to beauty in women, with tastes shifting from “women of physical presence” to a “lady” as conspicuous consumption and leisure grew. The less suited a woman is for work, the more waste and hence conspicuous consumption and the more beautiful she would be perceived.\nVeblen follows his discussion of beauty with a series of evolutionary arguments on the nature of the leisure class. His line is often difficult to follow, with the boundary between social and genetic selection unclear. His underlying agenda, a critique of the leisure class, also clouds his arguments.\nOn social selection, Veblen notes that the fittest habits of thought will survive and that the selection of institutions affects the selection of people within society. Institutions change fast, so the selection of people cannot keep up. Further, changes which may be good for society as a whole may be bad for certain people. Veblen’s discussion provides a nice picture of a dynamic environment and selection pressures that vary with it.\nHaving painted this dynamic picture, Veblen then writes of how slow society is to change and how conservative society is. Veblen argues that the leisure class is able to keep society conservative through withdrawing the means of sustenance to the industrial class. As a result, the industrial class does not have the resources to invest in new ideas and habits and even if they did gain some surplus, that would be wasted on the conspicuous consumption that the leisure class has established as the societal norm.\nOn an individual level, Veblen considers there are two basic types - predatory and peaceful. Predatory types are violent (in certain stages of society), selfish and dishonest and are not diligent. Peaceful types are the opposite. Precisely which traits are expressed will depend on the state of society - although the spectrum of predatory to peaceful roughly coincides with the spectrum of blonde through brunette to Mediterranean ethnicities.\nVeblen suggests that society progressed from a peaceful, native state, to a barbarian state, before shifting back towards the more peaceful modern society. Peaceful traits were selected for in the native state, and predatory traits selected for in the barbarian states. Veblen states, however, that selection did not eliminate all the peaceful traits in the barbarian era, allowing peaceful traits to be present in modern society.\nAs to how these traits are distributed, Veblen sees the leisure class as the predatory type and the industrial class of the peaceful type. The leisure class is not able to be violent in modern society, so they use more “peaceful predatory” methods, such as fraud. The industrial class is not in need of predatory habits, with Veblen suggesting that “economic man” in the sense of the selfish person (an indirect slight on Adam Smith) is useless for modern society. It is by being diligent and honest that the industrial man thrives.\nVeblen’s shot at “economic man” is not particularly effective. He mixes group and individual selection and does not recognise that selfishness is required, in an evolutionary sense, for all people. The reason industrial man is diligent is because that is how he benefits. If he did not benefit, he would be selected against and disappear. The fact society benefits is the operation of Smith’s invisible hand.\nDespite his categorisation of types between classes, Veblen strangely suggests that there are no broad character differences between the leisure class and the rest. He states that some predatory behaviour persists in the industrial class due to the behaviour of the leisure class. He also notes that people in the leisure class, by virtue of their resources, are not subject to harsh selection pressure, so peaceful characteristics can persist. What is most determinative of the traits in the leisure class are those traits which lead to admission to the class. While these have changed over time (say raw violence to fraud), they are generally of a predatory nature. It is not easy to gel this position of no difference with his earlier statements, and I am not sure they can be reconciled. My one suggestion is that the differences will grow if the current institutional framework continues to exist.\nAlthough Veblen has addressed some evolutionary issues, Veblen has not addressed the questions I asked in my earlier post about the basis for the desire for reputation and status. Of the evolutionary arguments he has used, they are generally focused on the welfare of the group and society and not the specific individual’s interests. I am looking forward to seeing if he takes these ideas any further in the rest of the book.\nThe link to a full review is here."
  },
  {
    "objectID": "posts/veblens-the-theory-of-the-leisure-class.html",
    "href": "posts/veblens-the-theory-of-the-leisure-class.html",
    "title": "Veblen’s The Theory of the Leisure Class",
    "section": "",
    "text": "I have started reading Thorstein Veblen’s The Theory of the Leisure Class. The book was published in 1899 and was one of the earliest books to explore the classical economic concept that people wish to consume more.\nI am finding it hard to read the book straight through (despite some classic satire), so I will break up my review into parts. This post covers the first five chapters. As I finish the book over the next week or so, I’ll post on the rest.\nOne of Veblen’s main contributions, forgotten by many neoclassical economists, was that people care about status, reputation and honour and that their economic behaviour will reflect this. As a result, people care about relative wealth.\nTo turn wealth into status and reputation, however, one needs to signal their wealth. The two signals that Veblen focuses on are conspicuous leisure and conspicuous consumption, with Veblen’s coining of the latter term being his best known claim to fame. A reading of the chapters on conspicuous consumption and conspicuous leisure suggest that Veblen has a relatively modern take on them. In particular, Veblen recognised the need for waste, which signalling theory tells us is required for the signal to be reliable.\nConspicuous leisure was something I had not thought a lot about before, but when there are few goods for conspicuous consumption, as would be the case in more primitive societies, conspicuous leisure would be a more accessible way to signal wealth. There are a number of conspicuous leisure activities that people undertake, such as reaching a level of manners and etiquette that could only be achieved through an excessive use of time, or becoming proficient at sports. Veblen also considers what he calls vicarious conspicuous leisure, whereby the head of the house employs servants (or even the housewife) in exercises that waste time.\nAs society advances, Veblen suggested that people move from conspicuous leisure to conspicuous consumption. Veblen’s primary explanation for this transition lies in the increasingly large circle of people with whom one associates and wishes to signal status to. In a small village, everyone is relatively familiar with each other and will note the habits of the servants and other householders carrying out the conspicuous leisure. In a larger city, however, the conspicuous waste needs to be visible, so conspicuous consumption in the nature of watches, clothing, carriages and the like are immediately obvious. Conspicuous consumption can also be vicarious, with servants dressed up in excessive livery.\nVeblen considered that one major result of conspicuous consumption is that it will put to use all future growths in production and efficiency. He states:\n\nThe need of conspicuous waste, therefore, stands ready to absorb any increase in the community’s industrial efficiency or output of goods, after the most elementary physical wants have been provided for.\n\nVeblen suggests that the use of additional production for conspicuous consumption acts as a Malthusian check on fertility. I think this point is very interesting. If signals are truly wasteful, then some of these resources will not be available for increasing the number of offspring. However, to be evolutionary stable, any reduction in conspicuous consumption by an individual would need to see them suffer a cost in the form of reputation and status, and in turn, mating opportunities. Veblen only mentioned this fertility check for the first time at the end of Chapter 5, so I am interested to see if he takes it any further.\nI am also interested in whether Veblen explores the basis of the desire for status and reputation. As my previous posts suggest, I consider that it has biological foundations. From flipping through the book on previous occasions, Veblen had clearly read Darwin, although I am not sure to what use Darwin’s work has been put.\nThe link to a full review is here."
  },
  {
    "objectID": "posts/videos-for-the-biological-basis-of-preferences-and-behavior-conference.html",
    "href": "posts/videos-for-the-biological-basis-of-preferences-and-behavior-conference.html",
    "title": "Videos for the Biological Basis of Preferences and Behavior Conference",
    "section": "",
    "text": "Videos of the presentations at the Biological Basis of Preferences and Behaviour conference have been put online. Many are worth watching. I hope to write more detailed posts about a few of the presentations soon, but the three presentations I got the most out of were:\n\n“Social Networks and Cooperation in Hunter-Gatherers” by Coren Apicella. Presentations such as this always remind me that I should be reading far more work by anthropologists.\n“The Genetic Architecture of Economic and Political Preferences” by David Cesarini. A good introduction to the growing field of genoeconomics.\n“Cognitive Trade-Offs in Chimpanzee Versus Human Mixed Strategy Play” by Colin Camerer. Don’t play poker against chimpanzees. I’ve posted on these chimps before.\n\nMy earlier posts on the conference include my general impressions and some comments on the presentation by Balazs Szentes."
  },
  {
    "objectID": "posts/was-it-better-for-our-paleolithic-ancestors.html",
    "href": "posts/was-it-better-for-our-paleolithic-ancestors.html",
    "title": "Was it better for our paleolithic ancestors?",
    "section": "",
    "text": "I have just started reading Geoffrey Miller’s Spent. It opens with a mildly amusing faux discussion in which a modern person seeks to convince some Cro-Magnons of the benefits of the modern way of life. The modern person is unable to do so as the discussion focuses on how the modern way of life does not increase the ability to attract and hold a mate (as opposed to, say, the rate of child mortality). In the conversation, it is noted that modern people work more, live marginally longer (compared to a Cro-Magnon that survives infancy) and have less connection to their community. Miller states:\n\nThis thought experiment has, I hope, shaken your faith that humanity has ridden a one-way escalator of ever-increasing progress and ever-greater happiness since the Aurignacian. True, modern life can be a wondrous glee-glutted Funky Town for the wealthiest .01 percent of people on the planet. However, a fairer assessment would contrast the lifeways of an average prehistoric human and the lifestyle of an average modern human.\n\nThe passage made me recall the choice facing Jemmy Button, one of the Fuegians collected by Captain FitzRoy in 1830 during the first voyage of the Beagle. After a year in England, Jemmy Button and the other two surviving Fuegians were taken on the second voyage of the Beagle to be returned home. On the Beagle for this voyage was Charles Darwin.\nDarwin wrote that during his time in England, Jemmy Button had picked up many English habits:\n\nJemmy was short, thick, and fat, but vain of his personal appearance; he used always to wear gloves, his hair was neatly cut, and he was distressed if his well-polished shoes were dirtied. He was fond of admiring himself in a looking glass\n\nWhen it came time to leave Jemmy Button in Tierra del Fuego, he was not pleased:\n\nPoor Jemmy looked rather disconsolate, and would then, I have little doubt, have been glad to have returned with us. His own brother had stolen many things from him; and as he remarked, “What fashion call that:” he abused his countrymen, “all bad men, no sabe (know) nothing” and, though I never heard him swear before, “damned fools.” Our three Fuegians, though they had been only three years with civilised men, would, I am sure, have been glad to have retained their new habits; but this was obviously impossible.\n\nSeveral months later, the Beagle returned to Jemmy Button’s area. Darwin writes:\n\nThis man was poor Jemmy, - now a thin, haggard savage, with long disordered hair, and naked, except a bit of blanket round his waist. We did not recognize him till he was close to us, for he was ashamed of himself, and turned his back to the ship. We had left him plump, fat, clean, and well-dressed; - I never saw so complete and grievous a change. As soon however as he was clothed, and the first flurry was over, things wore a good appearance. He dined with Captain Fitz Roy, and ate his dinner as tidily as formerly. He told us that he had “too much” (meaning enough) to eat, that he was not cold, that his relations were very good people, and that he did not wish to go back to England: in the evening we found out the cause of this great change in Jemmy’s feelings, in the arrival of his young and nice-looking wife.\n\nIn some ways, this hits Miller’s point - in his faux discussion, most of the discussion is about attracting mates and raising children. However, Jemmy Button’s situation is somewhat different. Jemmy Button would have been in a much worse position in England as his chance of attracting an English wife would have been near zero. There was a much stronger asymmetry in the potential of Jemmy Button’s options than if we were offered a straight decision between Cro-Magnon or modern existence."
  },
  {
    "objectID": "posts/we-need-more-complicated-mathematical-models-in-economics.html",
    "href": "posts/we-need-more-complicated-mathematical-models-in-economics.html",
    "title": "We need more complicated mathematical models in economics",
    "section": "",
    "text": "I am half way through David Colander and Roland Kupers’s book Complexity and the Art of Public Policy: Solving Society’s Problems from the Bottom Up. Overall, it’s a good book, although the authors are somewhat slow to get to the point and there are plenty of lines that perplex or annoy (Arnold Kling seemed to have a similar reaction).\nI’ll review later, but one interesting line in the book is that under a complexity approach, you may need more complicated mathematical models than used in neoclassical economics. This is because the purpose of the models under a complexity approach is different. They write:\n\nA person is walking home late one night and notices an economist searching under a lamppost for his keys. The person stops to help. After searching a while without luck he asks the economist where he lost his keys. The economist points far off into the dark abyss. The person asks, incredulously, “Then why the heck are you searching here?” To which the economist responds—“This is where the light is.”\nCritics of economists like this joke because it nicely captures economic theorists’ tendency to be, what critics consider, overly mathematical and technical in their research. Superficially, searching where the light is (letting available analytic technology guide one’s technical research) is clearly a stupid strategy; the obvious place to search is where you lost the keys.\nTelling old jokes doesn’t do much, and in this case the joke was a setup for a different punch line. That punch line is that the critic’s lesson taken from the joke is the wrong lesson if the economy is complex. For a complex system, which the social system is, a “searching where the light is” strategy makes good sense. Since the subject matter of social science is highly complex—arguably far more complex than the subject matter of most natural sciences—it is as if the social science policy keys are lost in the equivalent of almost total darkness. The problem is that you have no idea where in the darkness you lost them, so it would be pretty stupid to just go out searching in the dark. The chances of getting totally lost are almost 100 percent. In such a situation, where else but in the light can you reasonably search in a scientific way?\nWhat is stupid, however, is if the scientist thinks he or she is going to find the keys under the lamppost.\n…\nThe fact that decisions in complex systems are so uncertain and difficult to make does not mean that one should avoid dealing with them mathematically and scientifically. Quite the contrary; it allows for much more complicated mathematical models since the models are used for a different purpose. Returning to our economist joke in the first chapter, they aim not to precisely describe the real world, but to understand the topography of the landscape under the light. The mathematical models are trying to map different types of topography, which may be helpful when searching for the policy keys, but they do not represent the full search for the keys.\nThe policy answers can be found only by those searching in the dark, which involves dealing with the full complexity of the system. The fact that one is using the models primarily for guidance, rather than for prescriptions, frees one from forcing the models to have direct policy relevance, which, as we will discuss, is a major reason for the problems with existing economic models. Instead one can use higher-level mathematics that is up to the task. In technical terms, instead of using static equilibrium models that can be analytically solved, one is free to use nonlinear, dynamic models that are beyond analytic solution, but upon which computational tools can shed light. As we will discuss in later chapters, the mathematics of complex evolving systems is really hard and still developing. That is why in the past economists and other social scientists have avoided them. It’s also why their policy advice has not been especially useful when the solution required a comprehensive understanding of our complex evolving socioeconomic system.\n…\n[T]he criticism coming from complexity scientists was different from that of most heterodox economists. The usual heterodox criticism of standard economics was that it was too mathematical. This was not the criticism here. Complexity scientists were arguing that economics was not mathematical enough—not only was it not mathematical enough, it was using the wrong mathematics. They agreed that if it was to be science, it had to be “under analytical control.” But they were arguing that by using the right mathematics, highly complex systems containing high levels of agent interdependence could come under analytic or computational scrutiny. Complexity scientists argued that economists needed to start exploring nonlinear dynamic models, path-dependent models by using the mathematics and tools of complexity science.\n\nThey also give a word of caution as to where the science is at:\n\nEvery period has its excesses: the current hype about the usefulness of formal models in complexity science holds echoes of the overconfidence in models that one saw from the 1930s onward. The time when models will provide complete answers to social policy questions, if such a time ever will exist, is still far in the future. Complexity models, like all models, are very useful and necessary, but they are not sufficient."
  },
  {
    "objectID": "posts/week-links-2.html",
    "href": "posts/week-links-2.html",
    "title": "A week of links",
    "section": "",
    "text": "Links this week:\n\nKids born earlier in the year do better in sport, but this doesn’t seem to apply for academic outcomes.\nRational crack addicts.\nWhy life at 40 doesn’t seem so great.\n\nOtherwise, this wasn’t the most exciting week for links."
  },
  {
    "objectID": "posts/what-can-evolutionary-biology-offer-economics.html",
    "href": "posts/what-can-evolutionary-biology-offer-economics.html",
    "title": "What can evolutionary biology offer economics?",
    "section": "",
    "text": "This is my last post on David Sloan Wilson’s series Economics and Evolution as Different Paradigms (my earlier three posts are here, here and here). While much of Wilson’s attack on economics is against a caricature of the discipline, he ties up his series with a few recommendations that are worth noting.\nOne is the need for behavioural economics to adopt evolutionary thinking to allow it to move from being a list of anomalies and biases to a coherent framework. There is no argument from me there. Wilson has an underlying aim to this, however, which is that it may give a platform for the overthrow the classical economics assumptions of rationality.\nI am not sure that would be the result. People generally want more (for themselves), they want less as the price rises, they discount costs and benefits in the future, and so on. There are all sorts of specific situations where behavioural economics has shown that this does not apply, but if I want to know the effect of putting a price cap on petrol prices, thinking about the rational actor is a sound starting point. Behavioural economics will only see the overthrow of the rational agent when it has a coherent framework and when incorporating the framework into models produces models with higher predictive power.\nIn Wilson’s final post, he notes a few avenues by which the mission to reconcile economics and evolutionary biology will progress. One of these was a letter submitted to the National Science Foundation. Written by Wilson and John Gowdy, the letter was also signed by 64 other signatories including Elinor Ostrom, Paul Ehrlich and Edward O Wilson. It had four main points on how to integrate economics and evolutionary science.\nFirst, as discussed above, it states that evolutionary theory could help make sense of the findings of behavioural economics.\nThe second was that:\n\nEvolution can help decision makers understand the large-scale and long-run consequences of economic policies, particularly environmental and social policies\n\nAs a headline, I couldn’t agree more. But when this point is discussed in the letter, it focuses on the use of evolutionary theory to understand where people behave in their own interest at the expense of the larger group. This reflects Wilson’s earlier attacks on the invisible hand.\nThis sells the potential contribution of evolutionary theory to economics short. Evolutionary theory can help the understanding of how people act, what objectives they pursue, which people have (or had) higher fitness, what the consequences of policies will be in the long-run (regardless of whether they are benign to a group), and so on. If evolutionary theory is used solely as (another) attack on libertarian arguments, there will not be widespread (or useful) adoption.\nThe third point in the letter was that the proximate-ultimate distinction is important in economics, as it is in evolutionary theory. This point is a hangover from Wilson’s attack on Milton Friedman and I am not sure that it is important. As I wrote in my previous post, Friedman’s position is milder and the economics profession is less narrow than Wilson suggests.\nThe final point of the letter is that the non-adaptive products of evolution are best understood from an evolutionary perspective. Again, this seems to sell the opportunity short. What of the adaptive products of evolution, such as intelligence or patience?\nHaving now worked through Wilson’s posts, I am not overly optimistic that there will be much useful coming out of the Evolution Institute. There are no shortages of coherent attacks on the neo-classical, hyper-rational model of economics and if the Evolution Institute is going to simply join the queue, we will not hear much more interesting out of them. We will see if my pessimism is borne out when the Institute moves from its methodological critique and starts to deliver substantive policy proscriptions.\n*My four posts on David Sloan Wilson’s Economics and Evolution as Different Paradigms can be found here, here, here and here."
  },
  {
    "objectID": "posts/what-is-multilevel-selection.html",
    "href": "posts/what-is-multilevel-selection.html",
    "title": "What is multilevel selection?",
    "section": "",
    "text": "The arguments in the group selection debate at The Edge, as kicked off by Steven Pinker, contain some useful descriptions on what is meant by multilevel selection in a modern sense and how this varies from older formulations of group selection. Some of this is worth drawing out.\nThe old story of group selection might run as follows. There are two types of people - altruists and egoists. Altruists are willing to incur individual costs for the benefit of the group, while egoists shirk this responsibility for their own benefit. As a result, egoists have higher fitness than altruists within groups, while groups with higher proportions of altruists do better than groups with relatively more egoists. If altruistic groups have a large enough advantage over groups with more selfish individuals, and as a result grow faster and bud off new groups, it may be possible for altruists to increase in overall prevalence even though egoists have an advantage within groups.\nMaynard Smith’s haystack model was one of the many early critiques of this picture. Maynard Smith argued that the conditions required for the evolution of an altruistic trait (in his haystack model, timidity compared to the dominant aggressive trait), were so limited that they were unlikely to be satisfied. These conditions included a limited level of migration between groups and large differences in relative group fitness. Group selection theory also suffered from criticism of a lack of preciseness about how the selection at various levels should be weighted.\nSince then, multilevel selection theory has tightened a few of those issues up. First, from David Queller:\n\nModern group selection theory is as mathematically rigorous as individual selection or inclusive fitness theory. … They simply divide up fitness in slightly different ways – inclusive fitness into effects on self versus others, and multilevel selection into between-group and within-group parts – and a simple partition of fitness should not alter predictions. Inclusive fitness became popular, despite the head start enjoyed by multilevel selection thinking, because it successfully weighted the relative importance of its two fitness components, using genetic relatedness. Without a similar set of weights, group selection advantages could not be accurately judged, and their strength and importance was often overemphasized. … However, modern multilevel selection theory does have such weights, the between-group and within-group genetic variances, whose ratio happens to be relatedness of the actor to its groupmates (including itself). Once the proper weights are accounted for, the two approaches give essentially identical results.\n\nAs well as tightening up the mathematics, the definition of group was also tweaked. Rather than talking about competition between distinct populations, multilevel selection looks at competition of all levels of organisation, with groups formed within populations at various stages of the life-cycle. Pinker writes:\n\nIn most models of the new group selection, a group is defined as any subset of interacting individuals, that is, as organisms which interact with one another more intensely than they interact with organisms selected from the population at random. Two sisters who help each other, for example, or a pair of friends who trade favors, are dubbed “a group”.\n\nIt is this new grouping arrangement that is partitioned and weighted:\n\n… Once a “group” is defined as a subset of interacting individuals, the variance in the fitness of individuals can be partitioned into two statistical components: how fit the individual is with respect to his groupmates, and how fit his group is with respect to other groups. … Examples include huddling for warmth, mobbing a predator, and Tooby’s example of pooling resources to get higher expected returns in a risky investment. In such cases one can separate the benefits that accrue to the entire group (including me) and whatever benefits or costs are assumed by me but no one else in the group.\n\nThis is not to say that everyone is on board with this statement that the two approaches are mathematically equivalent. However, the mathematical equivalence and greater flexibility about what constitutes a group provide group selection advocates an alternative argument about why it is useful.\nUnfortunately, the new approach is often used as a trojan horse for the old group selection approach, particularly in popular discussions of human evolution. The responses to Pinker contain various varieties of this.\nPinker also makes some interesting points about the costs of the new approach. First, the flexibility in the definition of groups is multi-level selection’s weakness.\n\nIf the two theories really are equivalent, then any advantage of group selection (in this new sense) would have to come from the models’ being more convenient, elegant, simple, transparent, explanatory, or mathematically tractable. Yet by stretching the meaning of “group” beyond its ordinary sense, that’s just what they fail to be. …\n[A] mathematical model that submerges the psychological forces that keep different “groups” together (such as genetic relatedness or mutual sensitivity to altruism), and requires theorists to dig them out from under the equations, is hardly a perspicuous way to analyze sociality (as Coyne points out). In gene-selectionist theories, the theoretical constructs that power the models turn out to be fantastically psychologically important, including sensitivity to kinship, scrutiny of individuals, moralistic emotions elicited by benefits conferred or withheld, and the psychological differentiation of relationships into discrete models corresponding to mutualism, kinship, reciprocity, and dominance.\n\nPinker also picks up what I consider to be the major reason multilevel selection has not been as broadly accepted as its proponents hope:\n\nMathematical biologists such as Alan Grafen, Stuart West, Ashleigh Griffin, and Andy Gardner have criticized this formulation because it obfuscates the fact that individuals are still maximizing their genetic fitness: “The fundamental point is that the spread of a gene is determined by its ‘fitness relative to others in the breeding population, and not to others with which it happens to interact.’ … Natural selection selects for a gene if it causes a behavior that leads to that gene increasing in frequency in the population, not some other arbitrarily defined scale such as social partners.”\n\nThis point can be drawn out through examining some of the similarities between the cooperation identified in a multilevel selection framework and the way economists look at economic exchange. I will offer some more thoughts on this in my next post on group selection.\n*As an aside, I am deliberately avoiding the cultural group selection issue for the moment."
  },
  {
    "objectID": "posts/what-we-learn-when-we-test-everything.html",
    "href": "posts/what-we-learn-when-we-test-everything.html",
    "title": "What we learn when we test everything",
    "section": "",
    "text": "The below are my speaking notes for a presentation in the Innovative methodologies in behavioural science session at BI Connect 2024, hosted by the Behavioural Economics Team of the Australian Government (BETA) in the Department of Prime Minister and Cabinet.\nThe notes reflect many of the themes discussed in more detail in a previous post on megastudies.\nWhen BETA posts the video of the session, I will link here.\nI’m going to start with a story about a competition held by Netflix.\nThey offered $1 million to the team that could develop an algorithm that could predict film ratings with 10% better accuracy than Netflix’s own model. The competition began in October 2006, and by June 2007 over 20 000 teams had registered for the competition and 2000 teams had submitted predictions. The prize was claimed in 2009, albeit the prize algorithm was never implemented by Netflix.\nCompetitions of this nature now underpin much progress in artificial intelligence. Many date the genesis of the current AI boom to the success of the deep convolutional neural net called AlexNet in the 2012 edition of the ImageNet Large Scale Visual Recognition Challenge.\nKaggle has industrialised the running of these competitions for private and government entities. The organisation submits a problem and data, and competitors compete to develop the best algorithm. This Math Olympiad competition is running now. You can see that 1754 submissions had already been made by participants at the time I took this snapshot.\nWe have also seen informal competitions emerge, such as the measuring of generative AI against standardised benchmarks. When a new version of Claude, ChatGPT, Gemini or Llama is released, they often release measures of their performance against these benchmarks, such as in this screenshot.\nThe approach that underpins these competitions and comparisons is known as the common task framework. Researchers compete to solve the a problem using the same dataset, with each measured against the same scale.\nThere are many benefits to the common task framework. We have objective measures of performance. We can see what is the state of the art. We can compare apples with apples.\nThere are also some downsides to the common task framework that I will come to later."
  },
  {
    "objectID": "posts/what-we-learn-when-we-test-everything.html#megastudies",
    "href": "posts/what-we-learn-when-we-test-everything.html#megastudies",
    "title": "What we learn when we test everything",
    "section": "Megastudies",
    "text": "Megastudies\nIs there a behavioural science version of the common task framework?\nAccording to some behavioural scientists, the answer is yes - the megastudy. First labelled megastudy in 2021, the idea behind the megastudy is to test many interventions in a single massive experiment. Don’t test one intervention against a control. Test 50. Put these interventions in direct competition with each other.\nThis idea of testing many interventions in this way has been around since before the behavioural scientists put on their marketing hat, called it the megastudy and published it in Nature. But the megastudy has certainly increased in frequency over the last couple of years.\nThere is a fairly simple case for the megastudy. We have many studies showing the effects of idiosyncratic behavioural interventions. We have social norms. Loss framing. Scarcity. Incentives. And so on. Which is more effective in achieving the behaviour change you want? We often can’t answer this as the various interventions aren’t directly compared against each other in the academic literature. The typical academic paper compares one intervention or class of similar interventions against a control.\nA megastudy enables us to make that comparison."
  },
  {
    "objectID": "posts/what-we-learn-when-we-test-everything.html#increasing-gym-attendance",
    "href": "posts/what-we-learn-when-we-test-everything.html#increasing-gym-attendance",
    "title": "What we learn when we test everything",
    "section": "Increasing gym attendance",
    "text": "Increasing gym attendance\nTo illustrate, let me walk through the highest-profile megastudy, which as I hinted was published in Nature.\nKatherine Milkman and friends (2021) tested 54 interventions to increase the gym visits of 61,000 experimental participants.\nMembers of a national gym chain were asked if they wished to enrol in a “habit-building science-based workout program”. Those who signed up formed the subject pool and were randomly assigned to the experimental conditions, including a control under which they received no further contact.\n\nOver the following 28-days participants were subject to interventions involving varying mixes of incentives and messages. For example, those in the “Social norm (high and increasing)” treatment group received six text message reminders, with content such as:\n\nTrivia time! What percent of Americans exercised at least 3 times per week in 2016? Reply 1 for 61%, 2 for 64%, 3 for 70% or 4 for 73%.\n\nIf they respond 1, 2 or 3, they receive a message back stating:\n\nIt’s actually 73%. And this is up from 71% in 2015.\n\nThey also received emails with similar facts.\nThose in the “Social norm (low)” group received messages with a less rosy situation:\n\nTrivia time! What percent of Americans exercised at least 3 times per week in 2016? Reply 1 for 35%, 2 for 38%, 3 for 41% or 4 for 44%\n\nAs an aside, there don’t seem to be any qualms about using deception here.\nSome interventions involved incentives. For example, the “Rigidity Rewarded” intervention paid 500 Amazon points worth $1.79 each time they attended a planned gym visit, and 250 Amazon points worth $0.90 if they attended the gym at another time.\nThe headline results of all the interventions are in this figure, with the effect sizes and their 95% confidence intervals represented by the blue lines. The intervention with the largest effect size involved incentives for returning to the gym after a missed workout.\n\n\nMilkman et al. (2021a) Figure 1\n\n\n\nTwenty-four of the 53 interventions were found to have a statistically significant effect over the control of no messages, increasing visits by between 9% and 27%. That equates to 0.14 to 0.40 extra weekly gym visits over the control average of 1.48 visits per week.\nThis figure also contains predictions made by behavioural practitioners, public health academics and lay people - those orange bars on the right indicate the overestimation of effect and lack of any relationship between the predictions and the results. I’ll briefly touch on these predictions later."
  },
  {
    "objectID": "posts/what-we-learn-when-we-test-everything.html#increasing-vaccination-rates",
    "href": "posts/what-we-learn-when-we-test-everything.html#increasing-vaccination-rates",
    "title": "What we learn when we test everything",
    "section": "Increasing vaccination rates",
    "text": "Increasing vaccination rates\nAnother megastudy by most of the same authors released that same year (2021) looked at a series of messages to encourage vaccination. For example:\n\nJohn, this is a reminder that a flu vaccine has been reserved for your appt with Dr. Smith. Please ask your doctor for the shot to make sure you receive it.\n\nAgain, a bit deceptive as there was no reserved vaccine, but as you can see in this chart, that particular message of a highlighted flu dose was the most effective.\n\n\nMilkman et al. (2021) Figure 1\n\n\n\nBeyond giving us direct comparability, there are some other nice features about megastudies. There are economies to scale: while an individual megastudy is a large exercise, the cost can be lower on a per-intervention basis.\nMegastudies also have built-in publication of null findings. We get to see both the successful interventions and the duds."
  },
  {
    "objectID": "posts/what-we-learn-when-we-test-everything.html#generalising",
    "href": "posts/what-we-learn-when-we-test-everything.html#generalising",
    "title": "What we learn when we test everything",
    "section": "Generalising",
    "text": "Generalising\nBut what can you as a practitioner or behavioural scientist do with the output of a megastudy?\nIf you are that particular gym chain or vaccination provider for which the megastudy was conducted, you might scale the most successful messaging.\nBut what if you are operating in a different context? What if you are a different gym chain with different customer demographics? A yoga studio? A chess club? A university encouraging student attendance? A preventative health provider?\nAs the authors of these papers argue, the fundamental problem that the megastudy is designed to address is the lack of comparability of interventions tested in different contexts. The context of two different experiments may be sufficiently different that it is not reasonable to ask which intervention is more effective.\nBut if we cannot easily compare across experiments in different contexts, what confidence can you have that the ordering or magnitude of intervention effect sizes in the megastudy will be reflected in a different context?\nWe are in a Catch-22 situation. The bigger the comparability problem that the megastudy is seeking to solve, the less useful the megastudy results are for application in other contexts.\nUltimately, this is why good policy or business advice should typically be to run your own experiment.\nThere is also the question of translating the particular interventions into the new contexts. There are so many degrees of freedom in developing a message, from the visual design, to the precise wording, to the choice of medium, to the timing. The result is that your translated intervention may not capture what drove the success of the previous intervention. The copy may not convey the concept. The wording may be confusing. And so on.\nThis message from the first megastudy on vaccinations was the worst performing:\n\nIt’s flu season & getting a flu shot at your appt is an easy thing you can do to be healthy!\n\nWho were their copywriters?! I would argue that the poor performance of this message gives little information about the effectiveness of health messaging.\nThis implementation problem again points to the advice: test in your own domain. A megastudy isn’t going to save you from doing that."
  },
  {
    "objectID": "posts/what-we-learn-when-we-test-everything.html#power",
    "href": "posts/what-we-learn-when-we-test-everything.html#power",
    "title": "What we learn when we test everything",
    "section": "Power",
    "text": "Power\nAnd this brings me to possibly the biggest challenge with megastudies.\nOn its face, megastudies have the benefit of getting a large sample. A total of 61,293 participants for the gym megastudy sounds solid.\nBut it doesn’t take much thought to realise that across 54 interventions (including the control) there is an average of not much more than 1000 participants per intervention. And that relatively small number of participants means that we have low power - that is, a low ability to detect any effects that exist and to differentiate between interventions.\nFor instance, while the largest effect size in the gym megastudy involved a bonus for returning after a missed workout, this effect size was indistinguishable from around half the other interventions.\nThe megastudy on vaccinations has the same problem. The 19 interventions across 47 000 participants boosted vaccinations by an average of 2.1 percentage points, but the authors noted that they could not reject the null hypothesis that all 19 effects have the same true value. A megastudy where we can’t tell which message works.\nNot all megastudies have this problem, but they highlight an issue that we always grapple with as experimentalists. Increasing the number of interventions reduces power unless we can commensurately increase sample size. There’s a tradeoff, and sometimes you’re better off with fewer interventions."
  },
  {
    "objectID": "posts/what-we-learn-when-we-test-everything.html#building-theory",
    "href": "posts/what-we-learn-when-we-test-everything.html#building-theory",
    "title": "What we learn when we test everything",
    "section": "Building theory",
    "text": "Building theory\nBeyond the applied nature of these megastudies, another question worth asking is what they offer to science. The flagship megastudy paper was published in Nature after all.\nTo address this, let me first describe an experiment by Google.\nWhen you visit Google online, they really want you to click on advertising links. What colour link is most likely to induce a click?\nGoogle doesn’t mess around in answering questions such as this. In one experiment, they tested 41 shades of blue. While this experiment was ridiculed by outsiders as the “50 shades of blue” episode, it yielded an additional $200 million dollars a year in revenue.\nWhere do megastudies sit between this Google experiment - a valuable optimisation exercise with limited scientific value - and a study designed to teach us something about how the world works.\nTo date, I’d argue that megastudies are closer to the Google end of the spectrum. They’re valuable for the task being optimised, but provide limited feedback into our theoretical understanding of human behaviour.\nEach intervention tested in the megastudy is derived from empirical regularities observed in past experiments. But instead of taking on the challenge of giving this mass of empirical evidence some theoretical backbone, megastudies have become domain specific horse races. As a policy maker or business owner, you might reap the benefits. But for science, there is less gain.\nI admit I’m asking a lot. I want to see behavioural science build theoretical understanding as to what is going on. These studies are designed to test how to increase gym attendance or vaccination rates. We can’t always have everything.\nHowever, this lack of theory is not without costs. For instance, as I’ve already noted, when asked to predict the ordering of effect sizes for the gym and vaccination megastudies, the practitioners had no idea - predictions for the gym megastudy are represented by the orange bars. We don’t have a theoretical framework that can outperform common sense or guide us as to what interventions are most likely to work. You’re stuck throwing as many interventions as you can find against a wall to see which will stick. If we had better theory, we might be able to winnow down options for a higher powered study. Megastudies are in part a symptom of this failure.\nSo, where to from here. What’s the future role of megastudies?"
  },
  {
    "objectID": "posts/what-we-learn-when-we-test-everything.html#building-on-the-common-task-approach",
    "href": "posts/what-we-learn-when-we-test-everything.html#building-on-the-common-task-approach",
    "title": "What we learn when we test everything",
    "section": "Building on the common task approach",
    "text": "Building on the common task approach\nI believe there is more to the common task approach. Common task exercises have catalysed some key moments in machine learning and artificial intelligence. The iterative increases in performance have provided tangible evidence of progress.\n\n\nAI Index 2024\n\n\n\nCan we bring the megastudy even closer to the common task approach? Common task tournaments typically create an open playing field by making the dataset generally available. Anyone can enter. People can have multiple cracks.\nContrast that with the published megastudies to date from a relatively narrow set of behavioural science teams. I don’t see any evidence that behavioural science teams have skill in developing messages beyond marketers - they couldn’t predict which were more effective after all - so how could we open up and democratise who provides interventions? I saw earlier this year an open call for intervention submissions to a happiness megastudy, which is a great step, although I’m not sure it ended up on the desks of any marketing agencies. I hope they recruited some weirdos.\nMost common task frameworks also allow iterative exploration and progress. Teams can access the data outside of tournaments. People get to repeatedly return to the problem. In contrast, with the megastudies we’ve got a one-shot game.\nSo why don’t we run this experiment with a gym chain or vaccination provider every year? Solicit open entries, with a process to whittle them down to the required number of interventions. Include a range of the best interventions from previous years.\nIf we took these extra steps, however, a limitation to the common task framework will become of increasing importance to the megastudy: overfitting. If you run a contest to increase gym attendance again and again there’s a higher probability that one of the interventions will capitalise on an idiosyncratic feature of that gym and its members. Great results, poor generalisation.\nThere are also decreasing marginal gains. The winner of the Netflix prize was never implemented as other cheaper combinations of models delivered most of the same benefits. Performance against many machine learning benchmarks plateaued or maxed out, requiring the development of new benchmarks.\nBut there is something to be said for a process where we learn and build on what we have learnt, rather than simply trial (hopefully scale) and publish. And by taking the inspiration for the megastudies - the common task framework more seriously - megastudies could contribute to this.\nThese performance curves on your screen are quite amazing. In the space of a decade, many tasks went from impossible for machines to machines being vastly superior to humans, despite predictions that these advances were decades away. A similar curve in understanding the drivers of human behaviour would be an amazing thing.\nAnd that’s where I’ll wind up for today."
  },
  {
    "objectID": "posts/while-we-wait-for-the-genoeconomics-revolution.html",
    "href": "posts/while-we-wait-for-the-genoeconomics-revolution.html",
    "title": "While we wait for the genoeconomics revolution",
    "section": "",
    "text": "Following the publication of two new articles in the Annual Review of Economics and PNAS (my summary here), genoeconomics has been getting some press. From the Boston Globe:\n\nBut for all their throat-clearing and the cold water they feel compelled to throw on their work as they introduce it to the general public, they are confident that it’ll eventually be possible to match up patterns in a person’s genome with patterns of financial behavior. Perhaps parents could be alerted if their kids have genes that incline them to impulsive spending or wildly risky investments, Benjamin said. Or perhaps policy makers could use genetic information about a particular population—say, cigarette-smokers or alcoholics—in order to craft policy more effective at encouraging some behaviors and discouraging others.\n\nWhile genoeconomics might provide exciting results, some of the information that is likely to emerge is already available to be used. We know from twin studies that patterns of financial behaviour are heritable. If a parent knows they are financially reckless, this can provide a probability of that behaviour for their child. You should worry about your child smoking or being an alcoholic if you have those characteristics yourself. We can obtain further information from the behaviour of siblings or other relatives. We do not need to wait for genoeconomics to deliver results before we can make inferences about genetically influenced behaviour."
  },
  {
    "objectID": "posts/who-will-invade-economics.html",
    "href": "posts/who-will-invade-economics.html",
    "title": "Who will invade economics?",
    "section": "",
    "text": "Justin Fox has asked whether the age of economic imperialism is coming to an end and whether economics may be vulnerable to imperialism itself:\n\nLately, though, I’ve found myself talking to and reading a little of the work of sociologists and political scientists, and coming away impressed with how adept they are in quantitative methods, how knowledgeable they are about economics, and how willing they are to challenge economic orthodoxy. …\nEven anthropology, that most downtrodden of the social sciences, has been encroaching on economists’ turf. When a top executive at the world’s largest asset manager (Peter Fisher of BlackRock) lists Debt: The First 5,000 Years by anthropologist (and Occupy Wall Streeter) David Graeber as one of his top reads of 2012, you know something’s going on.\nWhat’s going on is probably not the incipient overthrow of economics. As described by Lazear, its imperialistic power has in large part been the result of its uniformity of approach over the past half century. (That, and economists have actually been right about some things.) As best I can tell, there is no such methodological consensus in sociology, political science, anthropology, or history at the moment. But the economists’ consensus is wobblier than it’s been in a while (especially in macro), there is ample motive for insurrection, and the non-economists’ stores of intellectual ammunition are growing. Economics may well have reached the stage of imperial overstretch. Interesting times lie ahead.\n\nAs you might expect me to say, evolutionary biology will be part of the move into the economists’ turf. After forty years of evolutionary biology imperialism, starting in the sociobiology days of the 1970s, evolutionary biology is vital to much of psychology and anthropology. And the potential to reshape economics remains significant. My predictions of the areas of greatest effect are as follows:\n\nEvolutionary psychology will form the bedrock on which behavioural economics sits, and will provide the basis for reconciling rational choice and behavioural economics approaches to decision making.\nAs economists become increasingly interested in the foundations of our economic preferences, such as risk or time preference, they will turn to the work already being done in the area (see my evolutionary biology and economics reading list for some examples).\nMany areas of economic imperialism, such as Becker’s work on the economics of the family and crime, will be updated from an evolutionary perspective. Economics added much to these fields, but the work was incomplete.\nBiology will be a source of the understanding of the economy as a complex system. This might include the study of the financial system as an ecosystem or broader consideration of when competition leads to positive outcomes.\nEconomic policy debates will gain more of an evolutionary flavour. The Evolution Institute is a part of that movement.\n\nIn addition, many economists underestimate how many tools and ideas used in economics are from other fields. I continue to run into economists who label John Maynard Smith as an economist, and who don’t consider how many of their statistical and mathematical tools were developed and used outside of economics. This is reflected in Sveriges Riksbank Prizes in Economic Sciences in Memory of Alfred Nobel  going to psychologists, political scientists and mathematicians. If anything, the skill of economists is recognising good tools and using them, with much economic imperialism the adoption of tools developed by others and claiming them as their own. But is an economist’s use of game theoretic tools developed by John Maynard Smith classed as economics or evolutionary biology? If an economist uses a tool from a field outside of economics in another field outside of economics, is it economics simply because the user labels themselves as an economist? (I often ask this last question about some of my work.)\nAs an aside, Fox credits the success of economic imperialism on the scientific foundations of economics, and quotes a passage from Edward Lazear’s 2002 QJE article on economic imperialism:\n\nEconomics is scientific; it follows the scientific method of stating a formal refutable theory, testing theory, and revising the theory based on the evidence. Economics succeeds where other social scientists fail because economists are willing to abstract.\n\nLazear’s description is more normative than positive, particularly in macroeconomics. What was the last macroeconomic theory that was generally rejected through the economics profession on the basis of data? (Or to make it more personal, when was the last time data changed your mind about an economic theory?) Fox hints at this problem, noting the multiple perspectives on the one-off global financial crisis, but the issue is broader. Ask a group of economists what causes business cycles. The consensus will be weak. The lack of consensus is not universal in economics, but where a divide exists, particularly on ideological lines, it is  rarely resolved through the scientific method."
  },
  {
    "objectID": "posts/why-do-rich-parents-bother.html",
    "href": "posts/why-do-rich-parents-bother.html",
    "title": "Why do rich parents bother?",
    "section": "",
    "text": "For several years, I have been relatively convinced that beyond a certain threshold, parenting does not matter. This belief came from two sources. The first was Judith Rich Harris’s book The Nurture Assumption in which she effectively argues that the children are socialised by other children, not their parents. The second was the concept that the variation attributable to genetic factors increases as you age (from memory I first read this in Matt Ridley’s The Agile Gene: How Nature Turns on Nurture - called Nature via Nurture in Australia). This implies that while environmental differences might affect the speed of development, the result is more robust to environmental factors.\nI consider the idea that parenting does not matter to be relatively positive. As Harris argues in her book, instead of fretting about how you are shaping your children, you can relax, enjoy them and not worry that your actions could ruin their futures.\nAdding to this picture, a new paper by Tucker-Drob and colleagues published in Psychological Sciences reports on a study on which 750 pairs of twins were tested at 10 months of age for mental ability. It was found that at 10 months, genes had a negligible influence on mental ability regardless of the socio-economic status of the family. However, the authors found that at the age of 2 years, genes have a measurable effect on variation in mental ability. This effect was most significant in the high socio-economic families, with genes accounting for around 50 per cent of the variation, while the effect of genes was negligible for those defined as low socio-economic status. I won’t go  further in the results here, but it is worth checking out Razib Khan’s deeper analysis of the paper.\nThis study supports the idea of larger genetic influence once a certain threshold is met and of increasing genetic influence as one ages. Razib and Jonah Lehrer noted that this study provides evidence that once the environmental variance is removed, the genetic variance remains. This comes from the diminishing marginal returns to investment in children. This makes sense, but the  question that hits me is why do rich parents bother with the effort they put into raising their children if, as Jonah suggests, rich parents don’t matter. You could write their actions off as being misguided as they chase increasingly low returns to their investment, but shouldn’t the revealed actions of these people tell us something.\nSo, here are a few suggestions to rationalise (or not) the investment, with varying degrees of plausibility. Some of these ideas are probably worth posts of their own down the track.\n\nThe old chestnut - people are irrational. We could use an evolutionary argument that we evolved in a Malthusian era in which humans had to dedicate a high proportion of their income to increasing child quality. Now that wealth is abundant, people are inclined to invest a similar proportion of their income in education despite it having severely diminishing returns. If this argument holds, we would expect that in this era of abundance, those who invest less in quality and direct those resources to quantity will grow in proportion of the population (this reflects the result of the Galor and Moav model).\nThe investment in children is a signal by the parents (conspicuous consumption), who still seek status.\nThe investment is a signal for the child. Those years in a top school may not deliver more educational benefit, but it looks good on the CV. This does assume, however, that the person looking at the CV believes that the investment matters.\nParents can influence the environmental factors that do matter. If you accept Harris’s argument that children socialise children, investment in attendance at a top private school might yield benefits (although I am not sure that a bunch of very rich class-mates is the what a child needs). However, you never know who your room mate at Yale might be.\nThe investment matters in other dimensions. Although a child may be of a certain intelligence or level of sociability no matter what you do, they will only be a pro-golfer or concert pianist if they put in their 10,000 hours of practice. I am not convinced that the rate of return from such investment is enough to underpin the huge level of parental investment that occurs. While it might have worked for Tiger Woods, how many other golf fathers are putting their kids through their paces?\n\nPersonally, I lean largely towards 2 and 3, with a dose of 1 thrown in - if status obtained through signalling does not lead to an increase in the quantity of children, the proportion of the population who do not have such a trait and invest in quantity rather than quality will grow.\nAre there any other explanations I have missed out?"
  },
  {
    "objectID": "posts/why-i-dont-trust-most-human-ai-interaction-experimental-research.html",
    "href": "posts/why-i-dont-trust-most-human-ai-interaction-experimental-research.html",
    "title": "Why I don’t trust most human-AI interaction experimental research",
    "section": "",
    "text": "Experimental psychology research before the mid-2010s was a mess. The field was littered with papers with weak to non-existent theoretical foundations. Experimental data was unavailable. (The statement “data is available on request” is somewhat of a joke.) The experiments involved small samples and barely significant p-values. File drawers were overflowing with the carcasses of experiments that didn’t quite get the result hoped for. The forking paths in the data analysis allowed multiple shots at goal.\nThose poor practices were most starkly revealed with the publication of the Open Science Collaboration’s review of the replicability of psychological science. 36% of psychology papers replicated. The stream of failed replications continues.\nToday, however, things look better, at least methodologically. Pre-registration and open experimental materials and data are becoming common, if not required. Many journals and reviewers take these elements as table stakes for publication. Registered reports are now an option. Theory is still often weak and those file drawers remain full, but at least the presence of preregistration repositories gives us a sense of what is in them. PNAS and other tabloids continue to be PNAS, but don’t hope for miracles.\nPsychology was, of course, not the only field to suffer a replication crisis and shift practices. Experimental economics has improved, despite coming through the crisis less tarnished than psychology. Many fields and the associated journals have adopted open science practices. If an experimental field hasn’t adjusted its practices in the last decade you need to question what is going on.\nWhich brings me to my experience in the human-AI and human-computer interaction literature. This literature largely concerns how we get better decisions or actions when people interact with AI, algorithms and other automated decision rules. How do we get people to use beneficial AI? How do we get “complementary performance”, where the performance of the human-AI combination is better than the human or AI by themselves? How do we help calibrate user trust in AI tools?\nI’ve been writing and thinking about the topic for close to a decade now. My first article in Behavioural Scientist (Don’t Touch The Computer) argued that often we should take the human out of the loop. In the last year or so I’ve been working on how to use AI in improving financial decision making. I’ve read 100s of experimental papers in the field looking for ideas and evidence.\nWhat I have seen is depressing. I hoped to find a field that had learned from psychology’s mistakes. Instead I found a field operating as if the replication crisis never happened. Weak theory. Negligible pre-registration. Forking paths. Underpowered experiments. The data is unavailable. (Less than 10% of papers I have read have data openly available in a repository.) Typically, there isn’t even a statement that data is available on request. The experimental materials are often well described in the paper - many human-AI interaction papers have quite novel models and software interfaces - but the detail isn’t sufficient for you to construct the models or software yourself. And this isn’t an old problem. These are papers from the last decade, some even from this year. There are exceptions and a few great papers, but I can’t see any trend in the right direction.\nI’m always looking for new ideas for practical application. And a first step in that process is replication. Unfortunately, absent open data and experimental materials, we can’t reproduce or replicate the experiment. And when you look at the weak theory and forking paths in the analysis, it’s hard to give the result sufficient credence to the idea to test it in a lab or field context.\nI’ve emailed a lot of authors asking for the experimental data and materials. In around 20% of cases, I’ve received an email in less than 24 hours with the materials, plus an offer to help if they could be useful. These are typically academics in economics, management and business schools who I suppose come from a culture where practices have changed. However, in most cases I simply haven’t received a response. Computer science and human-computer interaction researchers: it’s crickets.\nPart of me wants to be understanding. They’re busy. Is it worth their taking their time to respond to a random from Australia who has written three sentences along the lines of “I like your paper/idea and am keen to replicate in a financial services context, can you share your data and materials”?\nBut on the other hand, they’ve gone to the effort to publish the paper (typically in a conference proceeding) and they want us to take their idea seriously. (Actually, maybe that’s not what they want. They just want the publication on their record and have moved on.) Unless their materials are a shambles, it should be minimal effort to share them. (If they don’t have it in a state to share, that’s another robustness flag.) The data is also of minimal sensitivity, involving bespoke abstract tasks in the lab. Post it to a public repository and you’re done with my and every following request.\nThere isn’t much encouragement for data sharing from the major publishing forums either. Here’s the data sharing policy of some of the conferences in human-AI interaction, the venues for many of the papers I have read:\n\nCHI conference on Human Factors in Computing Systems: “Reproducibility: Where relevant, authors are strongly encouraged to provide supplementary materials to support practices around research reproducibility as much as possible. Please refer to the requirements for supplementary materials below.” They indicate how seriously they take it when the supplementary materials section is headed “Step 2. Prepare Supplementary Materials (Optional)”.\n\n\nConference on Neural Information Processing Systems (NeurIPS): “If any of the main contributions of your paper depends on an experimental result, you are strongly encouraged to submit code that produces this result. If you are using a new dataset, you are also encouraged to submit the dataset.”\n\n\nACM Conference on Intelligent User Interfaces: “Submitting supplemental material (e.g. questionnaires, demo videos of applications, data sheets) is optional but encouraged.”\n\nAfter a fair bit of digging, I did find the following ACM policy - it’s buried enough I expect most paper submitters wouldn’t even read it:\n\nSubmission and Publication of Digital Artifacts as Supplemental Material\nACM does not require, but strongly recommends, that authors of ACM published Works provide access to the artifacts used to conduct research reported on in their published Works.\n\nI can tell from the lack of data for most experimental papers that this optionality is used by authors. Welcome to 2010.\nSo, is there a coming replication crisis for human-computer interaction?\nUnfortunately, I don’t think so. When you need a bespoke machine learning algorithm or software interface, the replication simply isn’t going to happen. Although most psychology papers aren’t replicated, enough can be replicated at low cost to enable a view of the robustness of the field. I fear too many human-AI interaction papers exceed that cost threshold.\nWhat I expect instead is filtering by practitioners. Many of the readers of the human-AI interaction literature, like me, are after practical solutions. Practitioners are going to filter the good from the bad, as what doesn’t work won’t get used. The A-B testing culture in AI and design teams allows that. I see the same thing in applied behavioural science (those few external readers of the psychology literature). There are a bunch of ideas that the applied community recognises as ineffective, despite there being no “failed” replication of the original experiment. We’d simply moved on. The “sign at the top” experiment was already dead among practitioners before the failed replications and fraud emerged.\nIn the longer term, I hope some decent practices will eventually filter into the field and we can simply forget about the current body of research. There are a few computer science academics that have open sourced their experimental setups on GitHub (thank you!) and made data available. Surely someone is paying attention and giving greater weight to their work. I am. There are also some platforms built for others to use, although there doesn’t seem to be much of a culture of using them yet. (Here’s one developed by Gagan Bansal and friends, as described in this paper.)\nIn the meantime, I’m taking what I read with a grain of salt. A while ago I posted that we should approach the psychology literature with a “default of disbelief”. If you see an interesting experimental result indicating a new phenomena or result, update your beliefs in only the mildest way. Don’t trust it until you see concordant results from independent pre-registered replications. I’m reading the human-AI interaction literature in the same way."
  },
  {
    "objectID": "posts/why-prediction-is-pointless.html",
    "href": "posts/why-prediction-is-pointless.html",
    "title": "Why prediction is pointless",
    "section": "",
    "text": "One of my favourite parts of Philip Tetlock’s Expert Political Judgment is his chapter examining the reasons for “radical skepticism” about forecasting. Radical skeptics believe that Tetlock’s mission to improve forecasting of political and economic events is doomed as the world is inherently unpredictable (beyond conceding that no expertise was required to know that war would not erupt in Scandinavia in the 1990s). Before reading Expert Political Judgment, I largely fell into this radical skeptic camp (and much of me still resides in it).\nTetlock suggests skeptics have two lines of intellectual descent - ontological skeptics who argue that the properties of the world make prediction impossible, and psychological skeptics who point to the human mind as being unsuited to teasing out any predictability that might exist. Below are excerpts of Tetlock’s examinations of each (together with the occasional rejoinder by Tetlock)."
  },
  {
    "objectID": "posts/why-prediction-is-pointless.html#ontological-skeptics",
    "href": "posts/why-prediction-is-pointless.html#ontological-skeptics",
    "title": "Why prediction is pointless",
    "section": "Ontological skeptics",
    "text": "Ontological skeptics\n\nPath dependency and punctuated equilibria\n\nPath-dependency theorists argue that many historical processes should be modeled as quirky path-dependent games with the potential to yield increasing returns. They maintain that history has repeatedly demonstrated that a technology can achieve a decisive advantage over competitors even if it is not the best long-run alternative. …\nNot everyone, however, is sold on the wide applicability of increasing-returns, path-dependency views of history. Traditionalists subscribe to decreasing-returns approaches that portray both past and future as deducible from assumptions about how farsighted economic actors, working within material and political constraints, converge on unique equilibria. For example, Daniel Yergin notes how some oil industry observers in the early 1980s used a decreasing-returns framework to predict, thus far correctly, that OPEC’s greatest triumphs were behind it. They expected the sharp rises in oil prices in the late 1970s to stimulate conservation, exploration, and exploitation of other sources of energy, which would put downward pressure on oil prices. Each step from the equilibrium is harder than the last. Negative feedback stabilizes social systems because major changes in one direction are offset by counterreactions. Good judges appreciate that forecasts of prolonged radical shifts from the status quo are generally a bad bet.\n\n\n\nComplexity theorists\n\nEmbracing complexity theory, they argue that history is a succession of chaotic shocks reverberating through incomprehensibly intricate networks. To back up this claim, they point to computer simulations of physical systems that show that, when investigators link well-established nonlinear relationships into positive feedback loops, tiny variations in inputs begin to have astonishingly large effects. …\nMcCloskey illustrates the point with a textbook problem of ecology: predicting how the population of a species next year will vary as a function of this year’s population. The model is _x__t_+1 = f(xt), a one-period-back nonlinear differential equation. The simplest equation is the hump: _x__t_+1 = βxt [1 – xt], where the tuning parameter, β, determines the hump’s shape by specifying how the population of deer at t + 1 depends on the population in the preceding period. More deer mean more reproductive opportunities, but more deer also exhaust the food supply and attract wolves. The higher β is, the steeper the hump and the more precipitous the shift from growth to decline. McCloskey shows how a tiny shift in beta from 3.94 to 3.935 can alter history. The plots of populations remain almost identical for several years but, for mysterious tipping-point reasons, the hypothetical populations decisively part ways twenty-five years into the simulation.\nWe could endlessly multiply these examples of great oaks sprouting from little acorns. For radical skeptics, though, there is a deeper lesson: the impossibility of picking the influential acorns before the fact. Joel Mokyr compares searching for the seeds of the Industrial Revolution to “studying the history of Jewish dissenters between 50 A.D. and 50 B.C.\n\n\n\nGame theorists\n\nRadical skeptics can counter, however, that many games have inherently indeterminate multiple or mixed strategy equilibria. They can also note that one does not need to buy into a hyperrational model of human nature to recognize that, when the stakes are high, players will try to second-guess each other to the point where political outcomes, like financial markets, resemble random walks. Indeed, radical skeptics delight in pointing to the warehouse of evidence that now attests to the unpredictability of the stock market.\n\n\n\nProbability theorists\n\nIf a statistician were to conduct a prospective study of how well retrospectively identified causes, either singly or in combination, predict plane crashes, our measure of predictability—say, a squared multiple correlation coefficient—would reveal gross unpredictability. Radical skeptics tell us to expect the same fate for our quantitative models of wars, revolutions, elections, and currency crises. Retrodiction is enormously easier than prediction."
  },
  {
    "objectID": "posts/why-prediction-is-pointless.html#psychological-skeptics",
    "href": "posts/why-prediction-is-pointless.html#psychological-skeptics",
    "title": "Why prediction is pointless",
    "section": "Psychological skeptics",
    "text": "Psychological skeptics\n\nPreference for simplicity\n\nHowever cognitively well equipped human beings were to survive on the savannah plains of Africa, we have met our match in the modern world. Picking up useful cues from noisy data requires identifying fragile associations between subtle combinations of antecedents and consequences. This is exactly the sort of task that work on probabilistic-cue learning indicates people do poorly. Even with lots of practice, plenty of motivation, and minimal distractions, intelligent people have enormous difficulty tracking complex patterns of covariation such as “effect _y_1 rises in likelihood when _x_1 is falling, _x_2 is rising, and _x_3 takes on an intermediate set of values.”\nPsychological skeptics argue that such results bode ill for our ability to distill predictive patterns from the hurly-burly of current events. …\nWe know—from many case studies—that overfitting the most superficially applicable analogy to current problems is a common source of error.\n\n\n\nAversion to ambiguity and dissonance\n\nPeople for the most part dislike ambiguity—and we shall discover in chapter 3 that this is especially true of the hedgehogs among us. History, however, heaps ambiguity on us. It not only requires us to keep track of many things; it also offers few clues as to which things made critical differences. If we want to make causal inferences, we have to guess what would have happened in counterfactual worlds that exist—if “exist” is the right word—only in our imaginative reenactments of what-if scenarios. We know from experimental work that people find it hard to resist filling in the missing data points with ideologically scripted event sequences.\nPeople for the most part also dislike dissonance … Unfortunately, the world can be a morally messy place in which policies that one is predisposed to detest sometimes have positive effects and policies that one embraces sometimes have noxious ones. … Dominant options—that beat the alternatives on all possible dimensions—are rare.\n\n\n\nNeed for control\n\n[P]eople will generally welcome evidence that fate is not capricious, that there is an underlying order to what happens. The core function of political belief systems is not prediction; it is to promote the comforting illusion of predictability.\n\n\n\nThe unbearable lightness of our understanding of randomness\n\nOur reluctance to acknowledge unpredictability keeps us looking for predictive cues well beyond the point of diminishing returns. I witnessed a demonstration thirty years ago that pitted the predictive abilities of a classroom of Yale undergraduates against those of a single Norwegian rat. The task was predicting on which side of a T-maze food would appear, with appearances determined—unbeknownst to both the humans and the rat—by a random binomial process (60 percent left and 40 percent right). The demonstration replicated the classic studies by Edwards and by Estes: the rat went for the more frequently rewarded side (getting it right roughly 60 percent of the time), whereas the humans looked hard for patterns and wound up choosing the left or the right side in roughly the proportion they were rewarded (getting it right roughly 52 percent of the time). Human performance suffers because we are, deep down, deterministic thinkers with an aversion to probabilistic strategies that accept the inevitability of error. … This determination to ferret out order from chaos has served our species well. We are all beneficiaries of our great collective successes in the pursuit of deterministic regularities in messy phenomena: agriculture, antibiotics, and countless other inventions that make our comfortable lives possible. But there are occasions when the refusal to accept the inevitability of error—to acknowledge that some phenomena are irreducibly probabilistic—can be harmful.\nPolitical observers run the same risk when they look for patterns in random concatenations of events. They would do better by thinking less. When we know the base rates of possible outcomes—say, the incumbent wins 80 percent of the time—and not much else, we should simply predict the more common outcome."
  },
  {
    "objectID": "posts/wilson-on-economics-and-evolution.html",
    "href": "posts/wilson-on-economics-and-evolution.html",
    "title": "Wilson on economics and evolution",
    "section": "",
    "text": "As indicated in my last post, between December 2009 and October 2010, evolutionary biologist David Sloan Wilson wrote a series of posts titled Economics and Evolution as Different Paradigms. Wilson’s basic line of reasoning is that evolutionary biology should play a larger role in economics, and I naturally agree with that position.\nFor the foundation of his argument, the second post of Wilson’s series contains a reasonably typical attack on modern economics (I’ll accept this monolithic caricature for the moment). I wouldn’t call it the most convincing of attacks, particularly when it starts by providing an example that modern economics deals with quite well:\n\nConsider the following proposition: I’ll give you 1 million dollars for sure or a 50:50 chance at 2.1 million dollars. What’s your choice? If you’re like me, you’ll choose the certain 1 million. Yet, that is a violation of core economic theory that became known as the “Allais Paradox”.\n\nUnfortunately, that is not an example of the Allais Paradox, but an example of risk aversion (and a number of the comments on Wilson’s post pointed this out). The Allais Paradox is, however, nicely set out in the Wikipedia article to which Wilson links. This mistake tends to make much of Wilson’s critique that economics cannot deal with variance of utility fall flat, as for the example given, a simple utility model with risk aversion does this well.\nMoving past that hiccup, Wilson suggests that we can find the reason that the Allais Paradox has not been incorporated into modern economic theory in Milton Friedman’s essay The Methodology of Positive Economics (the first essay in his book Essays in Positive Economics). Friedman suggested, among other things, that it is not problematic if the assumptions underpinning the model are unrealistic if the predictive power of the model is good. We should test models by their predictive power. Wilson states that this approach is a recipe for confirmation bias (although, what isn’t?) and that the lax criteria for assessing models allows economists to ignore the Allais Paradox.\nWhenever someone uses this essay by Friedman to attack economics, I feel that the reader generally misunderstands what Friedman means when he suggested that the assumptions do not have to be descriptively realistic. Friedman notes that assumptions, by their very nature, never are descriptively realistic, but that they need to be sufficiently good for the purpose at hand. This is a rather mild claim, as any model (and its assumptions) must be more simple than reality or there will be no value to the model.\nRegardless, Wilson suggested that the Allais Paradox presented a risk to economic theory. He states:\n\nIt violated the principle of maximization of returns, it could not easily be incorporated into the body of formal analytical theory, and it was a move toward realism that would definitely have consequences for economic predictions.\n\nI only partially agree with that assessment. If a paradox like the Allais Paradox is particularly important for a set of decisions that an economic model is seeking to predict, then Friedman’s test of predictability is useful in determining whether it should be incorporated in the model. If it is not important, then it won’t improve the model’s predictive power.\nThat reflects the general issue I have with the Allais Paradox, and the many other of the biases that behavioural economics has unearthed. Are there tangible examples of how they could be incorporated into an economic model and out-predict a neoclassical model that ignores these biases (they may exist - I am asking this question more of ignorance than confidence that they don’t)? Part of the reason for this is that much of behavioural economics is a catalogue of biases and it lacks an evolutionary framework (and as I noted in my last post, the Evolution Institute seeks to address this).\nThis is not to say that we should ignore the raft of biases. But instead of pointing out each individual bias and demanding to know why it isn’t incorporated in the model, we should ask whether there is a more systematic problem that the bias is symptomatic of. Instead of trying to add bias by bias, we could be asking more fundamental questions, such as what the agent(s) actual objective is.\nI have more sympathy with Wilson’s argument that testing the predictive power of a model can be difficult where the evidence is in a complex system and difficult to collect and document. In this case, Friedman’s test of predictive power may not be a particularly strong filter. Despite this, it is an underused filter for many economic theories. As I have discussed before, economics has no shortage of theories that are still kicking around despite a lack of any empirical support. Use of that filter might get rid some of highly complex, mathematically beautiful but predictively useless models that should have been discarded long ago.\n*My four posts on David Sloan Wilson’s Economics and Evolution as Different Paradigms can be found here, here, here and here."
  },
  {
    "objectID": "posts/world-economic-history-in-two-diagrams.html",
    "href": "posts/world-economic-history-in-two-diagrams.html",
    "title": "World economic history in two diagrams",
    "section": "",
    "text": "Gregory Clark opens A Farewell to Alms with a strong claim:\n\nThe basic outline of world economic history is surprisingly simple. Indeed it can be summarized in one diagram: figure 1.1.\n\n\nI like Clark’s claim, but I’m now convinced that we need a second. From Michael Kremer’s Population Growth and Technological Change: One Million B.C. to 1990:\n\nFigure I plots the growth rate of population against its level from prehistoric times to the present.\n\n\nEven though there was negligible per person income growth through the Malthusian era, technological change was accelerating. As more people leads to more ideas (as there are more people to come up with them), a larger population leads to faster technological progress. Technological progress in turn allows for further population growth. The resulting pattern is faster than exponential growth in technology and population - a dynamic that does not show up in Clark’s chart.\nIf I were to stretch it to a third diagram I would want something that captures the dynamism of the Malthusian era - population bottlenecks, different rates of growth across different populations and the like - but I’m not sure what that chart would look like yet."
  },
  {
    "objectID": "posts/would-julian-simon-worry.html",
    "href": "posts/would-julian-simon-worry.html",
    "title": "Would Julian Simon worry?",
    "section": "",
    "text": "This month’s Cato Unbound is on The Politics of Family Size. The lead essay is by Bryan Caplan, who is on a mission to get people to have more kids.\nCaplan frames his piece around Julian Simon’s argument that people are the ultimate resource and that increased population is a good thing. He suggests that decreasing fertility should be worrying for those who believe Simon’s argument and he makes a case for libertarian approaches to increasing population (I should note that as Caplan seems to be an optimist, this worry is probably not a case of foreseeing impending doom, but a suggestion that things could be even better).\nI’ll comment on Caplan’s general argument in another post, but, the first thing that struck me about Caplan’s piece was that I cannot recall another example of Julian Simon being referenced in support of an argument suggesting that there is a problem (that is, beyond arguments that government intervention is a problem). Simon is a favourite source of those who suggest we don’t need to worry about environmental degradation or human living conditions and, as Simon has shown, human living conditions over the last 200 years have invariably trended up.\nDespite the manner in which Simon is usually quoted, Simon’s position was not that we never need to worry. As the following quote in Scarcity or Abundance: A Debate of the Environment suggests, worry is one of the mechanisms that solves problems:\n\nLet me correct a misapprehension. I have never said that we don’t need to worry about anything. We need to worry about everything, in the same sense that you had to worry whether you’d get here on time, whether there’ll be enough food in your kitchen for next week, and so on. The world needs the best effort of all of us. I’m saying that the result of all this worry - and of your constructive work, of your throwing your life into trying to do good things for the world and for other people - is that on balance you will create more than you will use in your lifetime, and you will leave the world a little better than before, on average. So, while we all need to worry, we can forecast that the result of all the worries will be that we will wind up better off than we are now.\nI don’t preach complacency. And certainly in my own life I don’t think you’ll find complacency. We have to struggle like the dickens. But we’ll win, we’ll overcome.\n\nSimon’s statement describes a situation similar to the operation of the efficient market hypothesis, which in the semi-strong form suggests that fundamental analysis is of no use as current market prices reflect all publicly available information. If everyone believed that was true, no-one would spend effort trying to earn excess returns, which is what causes prices to reflect the publicly available information. The hypothesis is more likely to be true if some people do not believe it. Similarly, Simon’s argument suggests that one of the reasons we do not need to worry is because we worry.\nOn that count, Caplan’s concern might sit comfortably with Simon’s view, and Simon might have worried about declining population growth. But as for what action Simon would recommend, I am not so sure.\nUpdate: I found a second Simon quote that I was looking for when I initially wrote this post - from The Ultimate Resource 2:\n\nOf course progress does not come about automatically. And my message certainly is not one of complacency, though anyone who predicts reduced scarcity of resources has always drawn that label. In this I agree with the doomsayers - that our world needs the best efforts of all humanity to improve our lot. I part company with them in that they expect us to come to a bad end despite the efforts we make, whereas I expect a continuation of successful efforts. And I believe that their message is self-fulfilling, because if you expect your efforts to fail because of inexorable natural limits, then you are likely to feel resigned, and therefore to literally resign. But if you recognize the possibility - in fact the probability - of success, you can tap large reservoirs of energy and enthusiasm."
  },
  {
    "objectID": "posts/zimbardos-the-lucifer-effect.html",
    "href": "posts/zimbardos-the-lucifer-effect.html",
    "title": "Zimbardo’s The Lucifer Effect",
    "section": "",
    "text": "The situation is more important than a person’s disposition. This message permeates through Philip Zimbardo’s The Lucifer Effect: Understanding How Good People Turn Evil, and while I disagree with some of the implications that he draws from this message, Zimbardo’s case is compelling.\nZimbardo builds the book on the Stanford Prison Experiment, one of the most cited psychological experiments. Zimbardo and his colleagues selected a group of “psychologically normal” young men and randomly assigned them roles as guards and prisoners in a role-play that they would conduct in the basement at Stanford University. Almost as soon as the experiment started, the guards started to abuse the prisoners and create more intricate methods of psychological torture. Ultimately, Zimbardo terminated the two-week experiment on day six due to fears for the mental health of the participants. The experiment now stands as the prime example of how good people can go bad due to the situation they are in.\nThe first half of The Lucifer Effect gives a blow-by-blow account of the experiment, with a chapter allocated to each day from the Sunday through to the experiment’s termination on the Friday morning. The detail of the experiment is enthralling, and, for me, there were some interesting new factoids.\nThe first was that the abuse, while clearly terrifying to the prisoners, never breached the guidelines set down by Zimbardo and his colleagues. There was no physical violence and the participants in the experiment acted as though the prohibition was a clear constraint. It would be interesting to conduct the experiment with less upstanding citizens to have seen if that constraint would have held.\nThe second was how Zimbardo and the others who ran the experiment played roles within the experiment, and how they were also transformed by these roles. Zimbardo was the prison superintendent and the others acted as wardens. It did not take long for them to fall into these roles, nudging guards to be tougher, implying to prisoners that they had a limited right to leave the role-play and manipulating outsiders to create the impression that the prisoners were well treated. Others who became involved in the experiment such as a prison chaplain, pro bono lawyer and the prisoners’ parents all quickly fell under the spell of the experiment and acted as though the experiment was real. The power of the situation extended beyond the experimental subjects.\nThe Stanford Prison Experiment, and Zimbardo and others’ subsequent experimental work, makes clear that situational factors are important. But in placing the emphasis on the situational assessment, Zimbardo is somewhat blind to the role that people’s disposition plays in forming the situation faced by others. For example, when a couple of guards were clearly reluctant to push the prisoners, it was a nudge from the experimenters (in their role as wardens and superintendent) that caused them to be more aggressive. The more passive guards were also motivated by the actions of the most aggressive on their shift.\nAnother result of people’s disposition forming part of the situational environment for others is the potential for feedback loops. People with bad dispositions create a situation in which good people might do bad things, creating a permanently negative situation for all people. How can this feedback loop be broken?\nThe second half of the book seeks to place the Stanford Prison Experiment in a broader context. A chapter on power, conformity and obedience (including a description of Stanley Milgram’s almost as famous electrocution experiment) examines how people are unwilling to oppose power or breach norms. The next chapter on dehumanisation and deindividuation discusses how depersonalising people can lead to people treating each other as less than human.\nThe book then moves to an analysis of the events in Abu Ghraib prison in Iraq. Zimbardo acted as an expert witness for one of the Abu Ghraib accused, Chip Frederick. Frederick was one of the seven low ranking people who faced the full brunt of prosecution for the abuse. As the military framed it, they were the “bad apples” in the barrel.\nZimbardo provides an extended discussion of the environment in which the accused existed and the strong effects of this situation (which make the Stanford prison seem a bit soft). He then details the abuses. Zimbardo’s argument, based on the events in the Stanford Prison Experiment, was that the situation drove the conduct, not Chip Frederick’s disposition. And Zimbardo’s argument is strong as he paints a compelling picture of how the situation faced by the people working in the Abu Ghraib would have affected them. If the prison had been under better control with clear, strong instruction from above, the abuses are unlikely to have happened.\nThis leads to where Zimbardo’s and my views on the right punishment for his client part company. Zimbardo argues that as the situation was the larger driver of Chip Frederick’s conduct, the military tribunal should not hold him to be as culpable and his punishment should reflect this.\nZimbardo’s position reflects the first of two ways to treat people who, while not of a “bad disposition”, do bad things due to situational forces. The other way is to recognise that due to the power of situational forces, we need to strengthen the incentives for people to fight against them – and that comes in the form of harsher penalties. The framework for punishing someone forms part of the situational forces. This is a similar argument to that crafted by Steven Pinker in The Blank Slate, who notes that it is equally credible to argue for harsher prison sentences if people are genetically predisposed to crime as it is to argue that they should be held less culpable.\nThe other question that arises through this, and in Zimbardo’s discussion of heroism later in the book, is why some people fall under the spell of the situational forces and others don’t. Are they distinguished by disposition? Would harsher penalties mean that even more people would refrain from abusing the prisoners? These questions are not answered.\nZimbardo then presents the case for those higher up to the command ladder to be held culpable for the Abu Ghraib abuses. It is obvious that many higher ranking officers, civilian contractors and the CIA either explicitly or implicitly ordered the abuse, making the failure to fully prosecute many of them disgraceful. But Zimbardo takes his case to the top - to Dick Cheney and George W Bush. I am sympathetic with his argument. Their implicit approval of torture through their treatment of the Geneva Conventions and practice of renditioning, among other things, were significant factors in the situational forces faced by those lower down the chain. An interesting question is whether it was situational forces that drove Cheney and Bush’s conduct? It’s not easy to fit in with your conservative base and Republican buddies if you are soft on the enemy. Do we simply need the strong spectre of punishment or consequences at all levels?\nZimbardo closes the book with a plea for heroism. He suggests a range of ways in which people can prepare themselves to act heroically, such as humanising others and questioning authority at the right times.\nWhen it comes to why people do act heroically, strangely (to me), Zimbardo reaches a conclusion that, like those who commit evil, heroes are normal people. This may be true, but the question then becomes why they resist the situational forces and avoid committing evil acts, and then take action at personal risk to themselves? Perhaps this should be the topic of Zimbardo’s next book."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Jason Collins blog",
    "section": "",
    "text": "The study that just won’t die: disfluency edition\n\n\n\n\n\nNov 29, 2025\n\n\n\n\n\n\n\nMore options, more action: contradicting a classic finding\n\n\n\n\n\nNov 18, 2025\n\n\n\n\n\n\n\nPulling apart a classic nudge story: the loft insulation trial\n\n\n\n\n\nOct 31, 2025\n\n\n\n\n\n\n\nIs there an AI workslop problem?\n\n\n\n\n\nOct 29, 2025\n\n\n\n\n\n\n\nIs following AI advice “anchoring bias”?\n\n\n\n\n\nOct 24, 2025\n\n\n\n\n\n\n\nA week of links\n\n\n\n\n\nSep 5, 2025\n\n\n\n\n\n\n\nWhy I don’t trust most human-AI interaction experimental research\n\n\n\n\n\nJun 12, 2025\n\n\n\n\n\n\n\nEconomists’ Genetic Blindspot: The Data We’re Not Collecting\n\n\n\n\n\nMay 28, 2025\n\n\n\n\n\n\n\nA critical behavioural economics and behavioural science reading list\n\n\n\n\n\nFeb 19, 2025\n\n\n\n\n\n\n\nBooks I read in 2024\n\n\n\n\n\nJan 17, 2025\n\n\n\n\n\n\n\nHuman-AI collaboration: is it better when the human is asleep at the wheel?\n\n\n\n\n\nDec 5, 2024\n\n\n\n\n\n\n\nWhat we learn when we test everything\n\n\n\n\n\nNov 13, 2024\n\n\n\n\n\n\n\nThe human benchmark is typically unimpressive\n\n\n\n\n\nOct 28, 2024\n\n\n\n\n\n\n\nA comment on the manifesto for behavioural science\n\n\n\n\n\nOct 25, 2024\n\n\n\n\n\n\n\nSubject notes on behavioural economics\n\n\n\n\n\nAug 8, 2024\n\n\n\n\n\n\n\nThe illusion of evidence-based nudges\n\n\n\n\n\nAug 1, 2024\n\n\n\n\n\n\n\nHumans 1, Chimps 0: Correcting the Record\n\n\n\n\n\nJul 25, 2024\n\n\n\n\n\n\n\nUsing generative AI as an academic - July 2024 edition\n\n\n\n\n\nJul 15, 2024\n\n\n\n\n\n\n\nThe psychological and genes’ eye view of ergodicity economics\n\n\n\n\n\nJul 8, 2024\n\n\n\n\n\n\n\nBryan Caplan’s The Case Against Education: A Review\n\n\n\n\n\nApr 19, 2024\n\n\n\n\n\n\n\nThe preregistration halo\n\n\n\n\n\nJan 26, 2024\n\n\n\n\n\n\n\nA bunch of links\n\n\n\n\n\nJan 19, 2024\n\n\n\n\n\n\n\nBooks I read in 2023\n\n\n\n\n\nJan 18, 2024\n\n\n\n\n\n\n\nBehavioral science policy recommendations early in the pandemic were LARGELY CORRECT, if you ignore those that were not\n\n\n\n\n\nDec 22, 2023\n\n\n\n\n\n\n\nDo students learn less from experts?\n\n\n\n\n\nNov 2, 2023\n\n\n\n\n\n\n\nJohn List’s The Voltage Effect: A review\n\n\n\n\n\nJul 5, 2023\n\n\n\n\n\n\n\nUsing large language models as an academic\n\n\n\n\n\nApr 28, 2023\n\n\n\n\n\n\n\nBooks I read in 2022\n\n\n\n\n\nJan 10, 2023\n\n\n\n\n\n\n\nWhy I don’t believe that signs with fatality numbers cause more crashes\n\n\n\n\n\nAug 19, 2022\n\n\n\n\n\n\n\nPlease not another bias: Take two\n\n\n\n\n\nJul 28, 2022\n\n\n\n\n\n\n\nPlease not another bias: correcting the record\n\n\n\n\n\nJul 11, 2022\n\n\n\n\n\n\n\nRevised course notes on Consumer Financial Decision Making\n\n\n\n\n\nJul 4, 2022\n\n\n\n\n\n\n\nMegastudy scepticism\n\n\n\n\n\nMay 27, 2022\n\n\n\n\n\n\n\nExplaining base rate neglect\n\n\n\n\n\nApr 12, 2022\n\n\n\n\n\n\n\nA bunch of links\n\n\n\n\n\nMar 17, 2022\n\n\n\n\n\n\n\nBankers are more honest than the rest of us\n\n\n\n\n\nMar 10, 2022\n\n\n\n\n\n\n\nMy podcast appearances\n\n\n\n\n\nMar 3, 2022\n\n\n\n\n\n\n\nThe outsider to the narrow-minded profession\n\n\n\n\n\nFeb 24, 2022\n\n\n\n\n\n\n\nReplicating scarcity\n\n\n\n\n\nFeb 17, 2022\n\n\n\n\n\n\n\nHow big is the effect of a nudge?\n\n\n\n\n\nFeb 11, 2022\n\n\n\n\n\n\n\nThe academic experiment\n\n\n\n\n\nFeb 4, 2022\n\n\n\n\n\n\n\nThe 1/N portfolio versus the optimal strategy: Does a simple heuristic outperform?\n\n\n\n\n\nJan 21, 2022\n\n\n\n\n\n\n\nA default of disbelief\n\n\n\n\n\nJan 13, 2022\n\n\n\n\n\n\n\nBest books I read in 2021\n\n\n\n\n\nJan 6, 2022\n\n\n\n\n\n\n\nCourse notes on Applied Consumer Financial Decision Making\n\n\n\n\n\nSep 5, 2021\n\n\n\n\n\n\n\nBest books I read in 2020\n\n\n\n\n\nJan 6, 2021\n\n\n\n\n\n\n\nAren’t we smart, fellow behavioural scientists\n\n\n\n\n\nJun 12, 2020\n\n\n\n\n\n\n\nThe limits of behavioural science: coronavirus edition\n\n\n\n\n\nApr 7, 2020\n\n\n\n\n\n\n\nRisk and loss aversion in ergodicity economics\n\n\n\n\n\nFeb 18, 2020\n\n\n\n\n\n\n\nBest books I read in 2019\n\n\n\n\n\nJan 28, 2020\n\n\n\n\n\n\n\nErgodicity economics: a primer\n\n\n\n\n\nJan 22, 2020\n\n\n\n\n\n\n\nThe case against loss aversion\n\n\n\n\n\nDec 5, 2019\n\n\n\n\n\n\n\nThe next decade of behavioural science: a call for intellectual diversity\n\n\n\n\n\nNov 14, 2019\n\n\n\n\n\n\n\nWhat can we infer about someone who rejects a 50:50 bet to win $110 or lose $100? The Rabin paradox explored\n\n\n\n\n\nNov 6, 2019\n\n\n\n\n\n\n\nMy latest article at Behavioral Scientist: Principles for the Application of Human Intelligence\n\n\n\n\n\nOct 9, 2019\n\n\n\n\n\n\n\nKahneman and Tversky’s “debatable” loss aversion assumption\n\n\n\n\n\nSep 10, 2019\n\n\n\n\n\n\n\nDavid Leiser and Yhonatan Shemesh’s How We Misunderstand Economics and Why it Matters: The Psychology of Bias, Distortion and Conspiracy\n\n\n\n\n\nAug 5, 2019\n\n\n\n\n\n\n\nNick Chater’s The Mind is Flat: The Illusion of Mental Depth and the Improvised Mind\n\n\n\n\n\nMay 13, 2019\n\n\n\n\n\n\n\nThree algorithmic views of human judgment, and the need to consider more than algorithms\n\n\n\n\n\nApr 30, 2019\n\n\n\n\n\n\n\nGigerenzer versus Kahneman and Tversky: The 1996 face-off\n\n\n\n\n\nApr 1, 2019\n\n\n\n\n\n\n\nBarry Schwartz’s The Paradox of Choice: Why More Is Less\n\n\n\n\n\nFeb 25, 2019\n\n\n\n\n\n\n\nGary Klein’s Sources of Power: How People Make Decisions\n\n\n\n\n\nJan 17, 2019\n\n\n\n\n\n\n\nA review of 2018 and some thoughts on 2019\n\n\n\n\n\nJan 14, 2019\n\n\n\n\n\n\n\nCarol Dweck’s Mindset: Changing the Way You Think to Fulfil Your Potential\n\n\n\n\n\nJan 10, 2019\n\n\n\n\n\n\n\nBooks I read in 2018\n\n\n\n\n\nJan 3, 2019\n\n\n\n\n\n\n\nGary Klein on confirmation bias in heuristics and biases research, and explaining everything\n\n\n\n\n\nDec 27, 2018\n\n\n\n\n\n\n\nIn contrast to less-is-more claims, ignoring information is rarely, if ever optimal\n\n\n\n\n\nDec 20, 2018\n\n\n\n\n\n\n\nMy latest in Behavioral Scientist: Simple heuristics that make algorithms smart\n\n\n\n\n\nDec 14, 2018\n\n\n\n\n\n\n\nA problem in the world or a problem in the model\n\n\n\n\n\nDec 7, 2018\n\n\n\n\n\n\n\nThe Rhetoric of Irrationality\n\n\n\n\n\nNov 30, 2018\n\n\n\n\n\n\n\nGenoeconomics and designer babies: The rise of the polygenic score\n\n\n\n\n\nNov 23, 2018\n\n\n\n\n\n\n\nHow happy is a paraplegic a year after losing the use of their legs?\n\n\n\n\n\nNov 16, 2018\n\n\n\n\n\n\n\nHow likely is “likely”?\n\n\n\n\n\nNov 9, 2018\n\n\n\n\n\n\n\nAvoiding trite lists of biases and pictures of human brains on PowerPoint slides\n\n\n\n\n\nNov 1, 2018\n\n\n\n\n\n\n\nChris Voss’s Never Split the Difference: Negotiating as if your life depended on it\n\n\n\n\n\nOct 25, 2018\n\n\n\n\n\n\n\nMe on Rationally Speaking, plus some additional thoughts\n\n\n\n\n\nOct 18, 2018\n\n\n\n\n\n\n\nAn evolutionary projection of global fertility and population: My new paper (with Lionel Page) in Evolution & Human Behavior\n\n\n\n\n\nOct 11, 2018\n\n\n\n\n\n\n\nThe Paradox of Trust\n\n\n\n\n\nOct 4, 2018\n\n\n\n\n\n\n\nNudging and the problem of context dependent preferences\n\n\n\n\n\nSep 28, 2018\n\n\n\n\n\n\n\nRobert Sugden’s The Community of Advantage: A Behavioural Economist’s Defence of the Market\n\n\n\n\n\nSep 26, 2018\n\n\n\n\n\n\n\nDo nudges diminish autonomy?\n\n\n\n\n\nSep 19, 2018\n\n\n\n\n\n\n\nA New Useless Class?\n\n\n\n\n\nSep 12, 2018\n\n\n\n\n\n\n\nHas the behavioural economics pendulum swung too far?\n\n\n\n\n\nSep 5, 2018\n\n\n\n\n\n\n\nThe three faces of overconfidence\n\n\n\n\n\nAug 29, 2018\n\n\n\n\n\n\n\nConcern about the “tyranny of choice”? Or condescension towards others’ preferences?\n\n\n\n\n\nAug 24, 2018\n\n\n\n\n\n\n\nGerd Gigerenzer’s Gut Feelings: Short Cuts to Better Decision Making\n\n\n\n\n\nAug 22, 2018\n\n\n\n\n\n\n\nGerd Gigerenzer’s Rationality for Mortals: How People Cope with Uncertainty\n\n\n\n\n\nAug 15, 2018\n\n\n\n\n\n\n\nThe difference between knowing the name of something and knowing something\n\n\n\n\n\nAug 8, 2018\n\n\n\n\n\n\n\nMichael Mauboussin’s Think Twice: Harnessing the Power of Counterintuition\n\n\n\n\n\nAug 1, 2018\n\n\n\n\n\n\n\nRobert Sapolsky’s Why Zebra’s Don’t Get Ulcers\n\n\n\n\n\nJul 25, 2018\n\n\n\n\n\n\n\nTom Griffiths on Gigerenzer versus Kahneman and Tversky. Plus a neat explanation on why the availability heuristic can be optimal\n\n\n\n\n\nJul 18, 2018\n\n\n\n\n\n\n\nOpposing biases\n\n\n\n\n\nJul 11, 2018\n\n\n\n\n\n\n\nHypotheticals versus the real world: The trolley problem\n\n\n\n\n\nJul 4, 2018\n\n\n\n\n\n\n\nExplaining the hot-hand fallacy fallacy\n\n\n\n\n\nJun 28, 2018\n\n\n\n\n\n\n\nWealth and genes\n\n\n\n\n\nJun 21, 2018\n\n\n\n\n\n\n\nIs the marshmallow test just a measure of affluence?\n\n\n\n\n\nJun 13, 2018\n\n\n\n\n\n\n\nDoes a moral reminder decrease cheating?\n\n\n\n\n\nJun 7, 2018\n\n\n\n\n\n\n\nThe marshmallow test held up OK\n\n\n\n\n\nMay 31, 2018\n\n\n\n\n\n\n\nTeacher expectations and self-fulfilling prophesies\n\n\n\n\n\nMay 24, 2018\n\n\n\n\n\n\n\nNoise\n\n\n\n\n\nMay 9, 2018\n\n\n\n\n\n\n\nBehavioural economics: underrated or overrated?\n\n\n\n\n\nMay 2, 2018\n\n\n\n\n\n\n\nMy blogroll\n\n\n\n\n\nApr 27, 2018\n\n\n\n\n\n\n\nThorstein Veblen’s The Theory of the Leisure Class\n\n\n\n\n\nApr 25, 2018\n\n\n\n\n\n\n\nHow I focus (and live)\n\n\n\n\n\nApr 19, 2018\n\n\n\n\n\n\n\nMichael Mauboussin’s More Than You Know: Finding Financial Wisdom in Unconventional Places\n\n\n\n\n\nApr 12, 2018\n\n\n\n\n\n\n\nSusan Cain’s Quiet: The Power of Introverts in a World That Can’t Stop Talking\n\n\n\n\n\nApr 5, 2018\n\n\n\n\n\n\n\nCass Sunstein and Reid Hastie’s Wiser: Getting Beyond Groupthink to Make Groups Smarter\n\n\n\n\n\nMar 28, 2018\n\n\n\n\n\n\n\nSome podcast recommendations\n\n\n\n\n\nMar 21, 2018\n\n\n\n\n\n\n\nPeople should use their judgment … except they’re often lousy at it\n\n\n\n\n\nMar 14, 2018\n\n\n\n\n\n\n\nMike Walsh interviews me on algorithm aversion\n\n\n\n\n\nMar 7, 2018\n\n\n\n\n\n\n\nPhilip Tetlock on messing with the algorithm\n\n\n\n\n\nFeb 28, 2018\n\n\n\n\n\n\n\nMichael Lewis’s The Undoing Project: A Friendship That Changed The World\n\n\n\n\n\nFeb 21, 2018\n\n\n\n\n\n\n\nAngela Duckworth’s Grit: The Power of Passion and Perseverance\n\n\n\n\n\nFeb 14, 2018\n\n\n\n\n\n\n\nDealing with algorithm aversion\n\n\n\n\n\nFeb 7, 2018\n\n\n\n\n\n\n\nDan Ariely’s Payoff: The Hidden Logic That Shapes Our Motivations\n\n\n\n\n\nJan 31, 2018\n\n\n\n\n\n\n\nAI in medicine: Outperforming humans since the 1970s\n\n\n\n\n\nJan 24, 2018\n\n\n\n\n\n\n\nIs there a “backfire effect”?\n\n\n\n\n\nJan 17, 2018\n\n\n\n\n\n\n\nBenartzi (and Lehrer’s) The Smarter Screen: Surprising Ways to Influence and Improve Online Behaviour\n\n\n\n\n\nJan 10, 2018\n\n\n\n\n\n\n\nBest books I read in 2017\n\n\n\n\n\nJan 3, 2018\n\n\n\n\n\n\n\nPaul Ormerod on Thaler’s Misbehaving\n\n\n\n\n\nNov 20, 2017\n\n\n\n\n\n\n\nUnchanging humans\n\n\n\n\n\nNov 16, 2017\n\n\n\n\n\n\n\nGetting the right human-machine mix\n\n\n\n\n\nNov 13, 2017\n\n\n\n\n\n\n\nCoursera’s Data Science Specialisation: A Review\n\n\n\n\n\nNov 8, 2017\n\n\n\n\n\n\n\nCharles Perrow’s Normal Accidents: Living with High-Risk Technologies\n\n\n\n\n\nNov 2, 2017\n\n\n\n\n\n\n\nThe benefit of doing nothing\n\n\n\n\n\nOct 9, 2017\n\n\n\n\n\n\n\nAdam Alter’s Irresistible: Why We Can’t Stop Checking, Scrolling, Clicking and Watching\n\n\n\n\n\nOct 5, 2017\n\n\n\n\n\n\n\nRats in a casino\n\n\n\n\n\nOct 3, 2017\n\n\n\n\n\n\n\nGreg Ip’s Foolproof: Why Safety Can Be Dangerous and How Danger Makes Us Safe\n\n\n\n\n\nSep 28, 2017\n\n\n\n\n\n\n\nDoes presuming you can take a person’s organs save lives?\n\n\n\n\n\nAug 30, 2017\n\n\n\n\n\n\n\nCathy O’Neil’s Weapons of Math Destruction: How Big Data Increases Inequality and Threatens Democracy\n\n\n\n\n\nAug 22, 2017\n\n\n\n\n\n\n\nIs it irrational?\n\n\n\n\n\nAug 16, 2017\n\n\n\n\n\n\n\nGarry Kasparov’s Deep Thinking: Where Machine Intelligence Ends and Human Creativity Begins\n\n\n\n\n\nJul 19, 2017\n\n\n\n\n\n\n\nHumans vs algorithms\n\n\n\n\n\nJul 14, 2017\n\n\n\n\n\n\n\nThe “effect is too large” heuristic\n\n\n\n\n\nJul 6, 2017\n\n\n\n\n\n\n\nBehavioral Scientist is live\n\n\n\n\n\nJun 23, 2017\n\n\n\n\n\n\n\nGerd Gigerenzer, Peter Todd and the ABC Research Group’s Simple Heuristics That Make Us Smart\n\n\n\n\n\nMar 13, 2017\n\n\n\n\n\n\n\nPedro Domingos’s The Master Algorithm: How the Quest for the Ultimate Learning Machine Will Remake Our World\n\n\n\n\n\nFeb 27, 2017\n\n\n\n\n\n\n\nCoursera’s Executive Data Science Specialisation: A Review\n\n\n\n\n\nJan 23, 2017\n\n\n\n\n\n\n\nBrian Christian and Tom Griffiths’s Algorithms to Live By: The Computer Science of Human Decisions\n\n\n\n\n\nJan 20, 2017\n\n\n\n\n\n\n\nBest books I read in 2016\n\n\n\n\n\nJan 17, 2017\n\n\n\n\n\n\n\nNewport’s So Good They Can’t Ignore You: Why Skills Trump Passion in the Quest for Work You Love\n\n\n\n\n\nNov 30, 2016\n\n\n\n\n\n\n\nRosenzweig’s Left Brain, Right Stuff: How Leaders Make Winning Decisions\n\n\n\n\n\nNov 25, 2016\n\n\n\n\n\n\n\nThe illusion of the illusion of control\n\n\n\n\n\nNov 21, 2016\n\n\n\n\n\n\n\nOverconfident about overconfidence\n\n\n\n\n\nNov 18, 2016\n\n\n\n\n\n\n\nHenrich’s The Secret of Our Success: How Culture Is Driving Human Evolution, Domesticating Our Species, and Making Us Smarter\n\n\n\n\n\nNov 14, 2016\n\n\n\n\n\n\n\nJones’s Hive Mind: How Your Nation’s IQ Matters So Much More Than Your Own\n\n\n\n\n\nNov 4, 2016\n\n\n\n\n\n\n\nMandelbrot (and Hudson’s) The (mis)Behaviour of Markets: A Fractal View of Risk, Ruin, and Reward\n\n\n\n\n\nOct 12, 2016\n\n\n\n\n\n\n\nWhy prediction is pointless\n\n\n\n\n\nSep 27, 2016\n\n\n\n\n\n\n\nRosenzweig’s The Halo Effect … and the Eight Other Business Delusions That Deceive Managers\n\n\n\n\n\nSep 21, 2016\n\n\n\n\n\n\n\nTetlock and Gardner’s Superforecasting: The Art and Science of Prediction\n\n\n\n\n\nSep 12, 2016\n\n\n\n\n\n\n\nTetlock’s Expert Political Judgment: How Good Is It? How Can We Know?\n\n\n\n\n\nAug 25, 2016\n\n\n\n\n\n\n\nBias in the World Bank\n\n\n\n\n\nJul 25, 2016\n\n\n\n\n\n\n\nKaufmann’s Shall the Religious Inherit the Earth?: Demography and Politics in the Twenty-First Century\n\n\n\n\n\nJul 22, 2016\n\n\n\n\n\n\n\nThree podcast episodes\n\n\n\n\n\nJul 20, 2016\n\n\n\n\n\n\n\nLast’s What to Expect When No One’s Expecting: America’s Coming Demographic Disaster\n\n\n\n\n\nJul 18, 2016\n\n\n\n\n\n\n\nBaumeister and Tierney’s Willpower: Rediscovering the Greatest Human Strength\n\n\n\n\n\nJul 15, 2016\n\n\n\n\n\n\n\nThe Behavioural Economics Guide 2016 (with an intro by Gerd Gigerenzer)\n\n\n\n\n\nJul 4, 2016\n\n\n\n\n\n\n\nRe-reading Kahneman’s Thinking, Fast and Slow\n\n\n\n\n\nJun 29, 2016\n\n\n\n\n\n\n\nLevine’s Is Behavioural Economics Doomed?\n\n\n\n\n\nJun 23, 2016\n\n\n\n\n\n\n\nReplicating anchoring effects\n\n\n\n\n\nMay 27, 2016\n\n\n\n\n\n\n\nSaint-Paul’s The Tyranny of Utility: Behavioral Social Science and the Rise of Paternalism\n\n\n\n\n\nMay 26, 2016\n\n\n\n\n\n\n\nBad Behavioural Science: Failures, bias and fairy tales\n\n\n\n\n\nMay 11, 2016\n\n\n\n\n\n\n\nEvolutionary Biology in Economics: A Review\n\n\n\n\n\nMay 10, 2016\n\n\n\n\n\n\n\nAriely’s The Honest Truth About Dishonesty\n\n\n\n\n\nApr 22, 2016\n\n\n\n\n\n\n\nThe Macrogenoeconomics of Comparative Development\n\n\n\n\n\nApr 20, 2016\n\n\n\n\n\n\n\nFailure to replicate: ego depletion edition\n\n\n\n\n\nApr 15, 2016\n\n\n\n\n\n\n\nNotes on a few books\n\n\n\n\n\nApr 13, 2016\n\n\n\n\n\n\n\nMasel’s Bypass Wall Street: A Biologist’s Guide to the Rat Race\n\n\n\n\n\nApr 6, 2016\n\n\n\n\n\n\n\nGottschall’s The Storytelling Animal\n\n\n\n\n\nApr 1, 2016\n\n\n\n\n\n\n\nMy first biology publication\n\n\n\n\n\nMar 31, 2016\n\n\n\n\n\n\n\nGigerenzer on system one and system two\n\n\n\n\n\nMar 17, 2016\n\n\n\n\n\n\n\nKay’s Other People’s Money\n\n\n\n\n\nMar 8, 2016\n\n\n\n\n\n\n\nThiel’s Zero to One\n\n\n\n\n\nFeb 15, 2016\n\n\n\n\n\n\n\nKenrick and Griskevicius’s The Rational Animal\n\n\n\n\n\nFeb 8, 2016\n\n\n\n\n\n\n\nBest books I read in 2015\n\n\n\n\n\nJan 18, 2016\n\n\n\n\n\n\n\nPhD thesis passed\n\n\n\n\n\nNov 16, 2015\n\n\n\n\n\n\n\nEvonomics is live!\n\n\n\n\n\nOct 12, 2015\n\n\n\n\n\n\n\nEconomics and Biology of Contests Conference 2016\n\n\n\n\n\nOct 6, 2015\n\n\n\n\n\n\n\nAnother #MSiX reading list\n\n\n\n\n\nJul 31, 2015\n\n\n\n\n\n\n\nPlease, not another bias! An evolutionary take on behavioural economics\n\n\n\n\n\nJul 30, 2015\n\n\n\n\n\n\n\nA week of links\n\n\n\n\n\nJul 3, 2015\n\n\n\n\n\n\n\nSam Bowles on the death of ‘Homo Economicus’\n\n\n\n\n\nJul 2, 2015\n\n\n\n\n\n\n\nA grumpy take on behavioural economics\n\n\n\n\n\nJun 30, 2015\n\n\n\n\n\n\n\nA week of links\n\n\n\n\n\nJun 26, 2015\n\n\n\n\n\n\n\nWe have no idea\n\n\n\n\n\nJun 24, 2015\n\n\n\n\n\n\n\nPlease experiment on us\n\n\n\n\n\nJun 23, 2015\n\n\n\n\n\n\n\nThe Evolutionary Foundations of Economics\n\n\n\n\n\nJun 22, 2015\n\n\n\n\n\n\n\nA week of links\n\n\n\n\n\nJun 19, 2015\n\n\n\n\n\n\n\nThe human factor in accidents\n\n\n\n\n\nJun 17, 2015\n\n\n\n\n\n\n\nMarketing Science Ideas Xchange (MSiX) 2015\n\n\n\n\n\nJun 15, 2015\n\n\n\n\n\n\n\nA week of links\n\n\n\n\n\nJun 12, 2015\n\n\n\n\n\n\n\nThe winner effect in humans\n\n\n\n\n\nJun 11, 2015\n\n\n\n\n\n\n\nFamily friendly backfires\n\n\n\n\n\nJun 9, 2015\n\n\n\n\n\n\n\nA week of links\n\n\n\n\n\nJun 5, 2015\n\n\n\n\n\n\n\nMerton on retirement incomes\n\n\n\n\n\nJun 4, 2015\n\n\n\n\n\n\n\nMeasurement error in 23andme\n\n\n\n\n\nJun 2, 2015\n\n\n\n\n\n\n\nRation information and avoid news\n\n\n\n\n\nJun 1, 2015\n\n\n\n\n\n\n\nA week of links\n\n\n\n\n\nMay 29, 2015\n\n\n\n\n\n\n\nFifty years of twin studies\n\n\n\n\n\nMay 28, 2015\n\n\n\n\n\n\n\nConspicuous consumption and economic growth\n\n\n\n\n\nMay 26, 2015\n\n\n\n\n\n\n\nA week of links\n\n\n\n\n\nMay 17, 2015\n\n\n\n\n\n\n\nA week of links\n\n\n\n\n\nMay 1, 2015\n\n\n\n\n\n\n\nBad nudges - organ donation edition\n\n\n\n\n\nApr 27, 2015\n\n\n\n\n\n\n\nA week of links\n\n\n\n\n\nApr 24, 2015\n\n\n\n\n\n\n\nReturns to self control - unemployment edition\n\n\n\n\n\nApr 15, 2015\n\n\n\n\n\n\n\nUncertainty and understanding behaviour\n\n\n\n\n\nApr 13, 2015\n\n\n\n\n\n\n\nA week of links\n\n\n\n\n\nApr 10, 2015\n\n\n\n\n\n\n\nPredicting replication\n\n\n\n\n\nApr 10, 2015\n\n\n\n\n\n\n\nA week of links\n\n\n\n\n\nApr 3, 2015\n\n\n\n\n\n\n\nThe law of law’s leverage\n\n\n\n\n\nApr 2, 2015\n\n\n\n\n\n\n\nThe gender reading gap and love of learning\n\n\n\n\n\nMar 30, 2015\n\n\n\n\n\n\n\nA week of links\n\n\n\n\n\nMar 27, 2015\n\n\n\n\n\n\n\nAn evolutionary perspective on behavioural economics\n\n\n\n\n\nMar 25, 2015\n\n\n\n\n\n\n\nThe Gell-Mann amnesia effect\n\n\n\n\n\nMar 23, 2015\n\n\n\n\n\n\n\nA week of links\n\n\n\n\n\nMar 20, 2015\n\n\n\n\n\n\n\nA week of links\n\n\n\n\n\nMar 13, 2015\n\n\n\n\n\n\n\nThe patience of economists\n\n\n\n\n\nMar 12, 2015\n\n\n\n\n\n\n\nThe other gender gap\n\n\n\n\n\nMar 10, 2015\n\n\n\n\n\n\n\nA week of links\n\n\n\n\n\nMar 6, 2015\n\n\n\n\n\n\n\nOvercoming implicit bias\n\n\n\n\n\nMar 4, 2015\n\n\n\n\n\n\n\nIntroducing Evonomics\n\n\n\n\n\nMar 2, 2015\n\n\n\n\n\n\n\nA week of links\n\n\n\n\n\nFeb 27, 2015\n\n\n\n\n\n\n\nAccepting heritability\n\n\n\n\n\nFeb 25, 2015\n\n\n\n\n\n\n\nWisdom from Tolstoy\n\n\n\n\n\nFeb 23, 2015\n\n\n\n\n\n\n\nA week of links\n\n\n\n\n\nFeb 20, 2015\n\n\n\n\n\n\n\nA week of links\n\n\n\n\n\nFeb 13, 2015\n\n\n\n\n\n\n\nCharts that don’t seem quite right - organ donation edition\n\n\n\n\n\nFeb 11, 2015\n\n\n\n\n\n\n\nThe death of defaults?\n\n\n\n\n\nFeb 9, 2015\n\n\n\n\n\n\n\nA week of links\n\n\n\n\n\nFeb 6, 2015\n\n\n\n\n\n\n\nObesity is not a public health problem\n\n\n\n\n\nFeb 5, 2015\n\n\n\n\n\n\n\nDurant’s The Paleo Manifesto\n\n\n\n\n\nFeb 4, 2015\n\n\n\n\n\n\n\nNudging for freedom\n\n\n\n\n\nFeb 2, 2015\n\n\n\n\n\n\n\nA week of links\n\n\n\n\n\nJan 30, 2015\n\n\n\n\n\n\n\nManzi’s Uncontrolled\n\n\n\n\n\nJan 28, 2015\n\n\n\n\n\n\n\nManzi on the abortion-crime hypothesis\n\n\n\n\n\nJan 26, 2015\n\n\n\n\n\n\n\nA week of links\n\n\n\n\n\nJan 23, 2015\n\n\n\n\n\n\n\nGrade inflation and the Dunning-Kruger effect\n\n\n\n\n\nJan 21, 2015\n\n\n\n\n\n\n\nThe benefits of cognitive limits\n\n\n\n\n\nJan 19, 2015\n\n\n\n\n\n\n\nA week of links\n\n\n\n\n\nJan 16, 2015\n\n\n\n\n\n\n\nThat chart doesn’t match your headline - fertility edition\n\n\n\n\n\nJan 14, 2015\n\n\n\n\n\n\n\nBad statistics - cancer edition\n\n\n\n\n\nJan 12, 2015\n\n\n\n\n\n\n\nA week of links\n\n\n\n\n\nJan 9, 2015\n\n\n\n\n\n\n\nThe blogs I read\n\n\n\n\n\nJan 8, 2015\n\n\n\n\n\n\n\nSelf evident but unexplored - how genetic effects vary over time\n\n\n\n\n\nJan 5, 2015\n\n\n\n\n\n\n\nA week of links\n\n\n\n\n\nJan 2, 2015\n\n\n\n\n\n\n\nBest books I read in 2014\n\n\n\n\n\nDec 30, 2014\n\n\n\n\n\n\n\nComplexity and the Art of Public Policy\n\n\n\n\n\nDec 29, 2014\n\n\n\n\n\n\n\nA week of links\n\n\n\n\n\nDec 27, 2014\n\n\n\n\n\n\n\nA week of links\n\n\n\n\n\nDec 19, 2014\n\n\n\n\n\n\n\nComplexity versus chaos\n\n\n\n\n\nDec 18, 2014\n\n\n\n\n\n\n\nMore praise of mathematics\n\n\n\n\n\nDec 16, 2014\n\n\n\n\n\n\n\nA week of links\n\n\n\n\n\nDec 12, 2014\n\n\n\n\n\n\n\nMy year\n\n\n\n\n\nDec 11, 2014\n\n\n\n\n\n\n\nWe need more complicated mathematical models in economics\n\n\n\n\n\nDec 8, 2014\n\n\n\n\n\n\n\nA week of links\n\n\n\n\n\nDec 6, 2014\n\n\n\n\n\n\n\nThe unrealistic assumptions of biology\n\n\n\n\n\nDec 4, 2014\n\n\n\n\n\n\n\nThe power of heuristics\n\n\n\n\n\nDec 2, 2014\n\n\n\n\n\n\n\nA week of links\n\n\n\n\n\nNov 28, 2014\n\n\n\n\n\n\n\nFour perspectives on human decision making\n\n\n\n\n\nNov 25, 2014\n\n\n\n\n\n\n\nA week of links\n\n\n\n\n\nNov 22, 2014\n\n\n\n\n\n\n\nGenetics and education policy\n\n\n\n\n\nNov 19, 2014\n\n\n\n\n\n\n\nThe beauty of self interest\n\n\n\n\n\nNov 18, 2014\n\n\n\n\n\n\n\nE.O. Wilson’s The Social Conquest of Earth\n\n\n\n\n\nNov 17, 2014\n\n\n\n\n\n\n\nA week of links\n\n\n\n\n\nNov 14, 2014\n\n\n\n\n\n\n\nIgnorance feels so much like expertise\n\n\n\n\n\nNov 12, 2014\n\n\n\n\n\n\n\nA week of links\n\n\n\n\n\nNov 7, 2014\n\n\n\n\n\n\n\nGenome Wide Association Studies and socioeconomic outcomes\n\n\n\n\n\nNov 4, 2014\n\n\n\n\n\n\n\nA week of links\n\n\n\n\n\nOct 31, 2014\n\n\n\n\n\n\n\nImproving behavioural economics\n\n\n\n\n\nOct 29, 2014\n\n\n\n\n\n\n\nAn updated economics and evolutionary biology reading list and a collection of book reviews\n\n\n\n\n\nOct 27, 2014\n\n\n\n\n\n\n\nA week of links\n\n\n\n\n\nOct 25, 2014\n\n\n\n\n\n\n\nFinding taxis on rainy days\n\n\n\n\n\nOct 23, 2014\n\n\n\n\n\n\n\nA week of links\n\n\n\n\n\nOct 19, 2014\n\n\n\n\n\n\n\nThe invisible hand of Jupiter\n\n\n\n\n\nOct 14, 2014\n\n\n\n\n\n\n\nLazy analysis - inequality edition\n\n\n\n\n\nOct 11, 2014\n\n\n\n\n\n\n\nA week of links\n\n\n\n\n\nOct 10, 2014\n\n\n\n\n\n\n\nA week of links\n\n\n\n\n\nOct 4, 2014\n\n\n\n\n\n\n\nTamed by an influx of women\n\n\n\n\n\nOct 2, 2014\n\n\n\n\n\n\n\nThe genetic basis of social mobility\n\n\n\n\n\nSep 30, 2014\n\n\n\n\n\n\n\nA week of links\n\n\n\n\n\nSep 27, 2014\n\n\n\n\n\n\n\nKahneman’s optimistic view of the mind\n\n\n\n\n\nSep 24, 2014\n\n\n\n\n\n\n\nA week of links\n\n\n\n\n\nSep 20, 2014\n\n\n\n\n\n\n\nScarcity of time, money, friends and bandwidth\n\n\n\n\n\nSep 18, 2014\n\n\n\n\n\n\n\nA week of links\n\n\n\n\n\nSep 12, 2014\n\n\n\n\n\n\n\nGerd Gigerenzer’s Risk Savvy: How to Make Good Decisions\n\n\n\n\n\nSep 10, 2014\n\n\n\n\n\n\n\nA week of links\n\n\n\n\n\nSep 5, 2014\n\n\n\n\n\n\n\nThe biology of boom and bust\n\n\n\n\n\nSep 4, 2014\n\n\n\n\n\n\n\nA week of links\n\n\n\n\n\nAug 31, 2014\n\n\n\n\n\n\n\nTwin studies stand up to the critique, again\n\n\n\n\n\nAug 28, 2014\n\n\n\n\n\n\n\nA week of links\n\n\n\n\n\nAug 22, 2014\n\n\n\n\n\n\n\nA week of links\n\n\n\n\n\nAug 17, 2014\n\n\n\n\n\n\n\nShaping the brain and humans as complex systems\n\n\n\n\n\nAug 12, 2014\n\n\n\n\n\n\n\nA week of links\n\n\n\n\n\nAug 8, 2014\n\n\n\n\n\n\n\nNot the jam study again\n\n\n\n\n\nAug 6, 2014\n\n\n\n\n\n\n\nA week of links\n\n\n\n\n\nAug 1, 2014\n\n\n\n\n\n\n\nAn MSiX reading list\n\n\n\n\n\nJul 31, 2014\n\n\n\n\n\n\n\nGigerenzer versus nudge\n\n\n\n\n\nJul 29, 2014\n\n\n\n\n\n\n\nA week of links\n\n\n\n\n\nJul 25, 2014\n\n\n\n\n\n\n\nOur visual system predicts the future\n\n\n\n\n\nJul 24, 2014\n\n\n\n\n\n\n\nA week of links\n\n\n\n\n\nJul 18, 2014\n\n\n\n\n\n\n\nThe wisdom of crowds of people who don’t believe in the wisdom of crowds\n\n\n\n\n\nJul 17, 2014\n\n\n\n\n\n\n\nThe behaviour genetics to eugenics to Nazi manoeuvre\n\n\n\n\n\nJul 16, 2014\n\n\n\n\n\n\n\nMSiX: Marketing Science Ideas Xchange\n\n\n\n\n\nJul 11, 2014\n\n\n\n\n\n\n\nA week of links\n\n\n\n\n\nJul 7, 2014\n\n\n\n\n\n\n\nGenes and socioeconomic aggregates\n\n\n\n\n\nJun 24, 2014\n\n\n\n\n\n\n\nThe benefit of uncertainty\n\n\n\n\n\nJun 15, 2014\n\n\n\n\n\n\n\nA week of links\n\n\n\n\n\nJun 9, 2014\n\n\n\n\n\n\n\nA week of links\n\n\n\n\n\nMay 19, 2014\n\n\n\n\n\n\n\nDoubling down\n\n\n\n\n\nMay 13, 2014\n\n\n\n\n\n\n\nA week of links\n\n\n\n\n\nMay 11, 2014\n\n\n\n\n\n\n\nBecker on evolution and economics\n\n\n\n\n\nMay 6, 2014\n\n\n\n\n\n\n\nA week of links\n\n\n\n\n\nMay 3, 2014\n\n\n\n\n\n\n\nA week of links\n\n\n\n\n\nApr 18, 2014\n\n\n\n\n\n\n\nHumbling wingnuts\n\n\n\n\n\nApr 11, 2014\n\n\n\n\n\n\n\nThe magic of commerce\n\n\n\n\n\nApr 9, 2014\n\n\n\n\n\n\n\nIgnore the sunk costs\n\n\n\n\n\nApr 7, 2014\n\n\n\n\n\n\n\nA week of links\n\n\n\n\n\nApr 6, 2014\n\n\n\n\n\n\n\nA week of links\n\n\n\n\n\nMar 12, 2014\n\n\n\n\n\n\n\nA week of links\n\n\n\n\n\nFeb 11, 2014\n\n\n\n\n\n\n\nCooperation and Conflict in the Family Conference wrap\n\n\n\n\n\nFeb 9, 2014\n\n\n\n\n\n\n\nA week of links\n\n\n\n\n\nFeb 2, 2014\n\n\n\n\n\n\n\nA week of links\n\n\n\n\n\nJan 27, 2014\n\n\n\n\n\n\n\nA week of links\n\n\n\n\n\nJan 19, 2014\n\n\n\n\n\n\n\nThe interplay of genetic and cultural evolution\n\n\n\n\n\nJan 16, 2014\n\n\n\n\n\n\n\nDoing cultural evolution right\n\n\n\n\n\nJan 14, 2014\n\n\n\n\n\n\n\nA week of links\n\n\n\n\n\nJan 11, 2014\n\n\n\n\n\n\n\nThe origin of the phrase “sneaky f**cker”\n\n\n\n\n\nJan 9, 2014\n\n\n\n\n\n\n\nA week of links\n\n\n\n\n\nJan 5, 2014\n\n\n\n\n\n\n\nBest books I read in 2013\n\n\n\n\n\nDec 24, 2013\n\n\n\n\n\n\n\nA week of links\n\n\n\n\n\nDec 21, 2013\n\n\n\n\n\n\n\nThe benefits of math skills to forager-farmers\n\n\n\n\n\nDec 19, 2013\n\n\n\n\n\n\n\nA week of links\n\n\n\n\n\nDec 16, 2013\n\n\n\n\n\n\n\nThe theoretical ambition of behavioural science\n\n\n\n\n\nDec 12, 2013\n\n\n\n\n\n\n\nA week of links\n\n\n\n\n\nDec 9, 2013\n\n\n\n\n\n\n\nNeoclassical theory won because it backed the right horse\n\n\n\n\n\nDec 3, 2013\n\n\n\n\n\n\n\nNatural selection and saving\n\n\n\n\n\nNov 30, 2013\n\n\n\n\n\n\n\nSexual selection on the American frontier\n\n\n\n\n\nNov 28, 2013\n\n\n\n\n\n\n\nConspicuous consumption as a handicap\n\n\n\n\n\nNov 25, 2013\n\n\n\n\n\n\n\nAn evolutionary explanation of consumption\n\n\n\n\n\nNov 22, 2013\n\n\n\n\n\n\n\nDoes mathematical training increase our risk tolerance?\n\n\n\n\n\nNov 21, 2013\n\n\n\n\n\n\n\nA week of links\n\n\n\n\n\nNov 16, 2013\n\n\n\n\n\n\n\nWhy isn’t economics evolutionary?\n\n\n\n\n\nNov 14, 2013\n\n\n\n\n\n\n\nNelson and Winter’s An Evolutionary Theory of Economic Change\n\n\n\n\n\nNov 12, 2013\n\n\n\n\n\n\n\nA week of links\n\n\n\n\n\nNov 9, 2013\n\n\n\n\n\n\n\nIs intelligence at the root of cooperation?\n\n\n\n\n\nNov 7, 2013\n\n\n\n\n\n\n\nThaler and Sunstein’s Nudge\n\n\n\n\n\nNov 5, 2013\n\n\n\n\n\n\n\nA week of links\n\n\n\n\n\nNov 1, 2013\n\n\n\n\n\n\n\n“Behavioural economics” versus “behavioural science”\n\n\n\n\n\nOct 31, 2013\n\n\n\n\n\n\n\nSix signs you’re reading good criticism of economics\n\n\n\n\n\nOct 29, 2013\n\n\n\n\n\n\n\nA week of links\n\n\n\n\n\nOct 26, 2013\n\n\n\n\n\n\n\nWarfare and the transition to agriculture\n\n\n\n\n\nOct 25, 2013\n\n\n\n\n\n\n\nLife expectancy and the dawn of agriculture\n\n\n\n\n\nOct 22, 2013\n\n\n\n\n\n\n\nA week of links\n\n\n\n\n\nOct 19, 2013\n\n\n\n\n\n\n\nIn praise of Malcolm Gladwell\n\n\n\n\n\nOct 15, 2013\n\n\n\n\n\n\n\nA week of links\n\n\n\n\n\nOct 12, 2013\n\n\n\n\n\n\n\nA week of links\n\n\n\n\n\nOct 4, 2013\n\n\n\n\n\n\n\nDefending economics from the anthropologists\n\n\n\n\n\nSep 30, 2013\n\n\n\n\n\n\n\nA week of links\n\n\n\n\n\nSep 27, 2013\n\n\n\n\n\n\n\nSilver’s The Signal and the Noise\n\n\n\n\n\nSep 25, 2013\n\n\n\n\n\n\n\nDan Ariely’s The Upside of Irrationality\n\n\n\n\n\nSep 23, 2013\n\n\n\n\n\n\n\nA week of links\n\n\n\n\n\nSep 20, 2013\n\n\n\n\n\n\n\nWhat is evolutionary economics?\n\n\n\n\n\nSep 18, 2013\n\n\n\n\n\n\n\nA week of links\n\n\n\n\n\nSep 13, 2013\n\n\n\n\n\n\n\nDesign principles for the efficacy of groups\n\n\n\n\n\nSep 11, 2013\n\n\n\n\n\n\n\nMonkeys respond to the Malthusian limit\n\n\n\n\n\nSep 9, 2013\n\n\n\n\n\n\n\nA week of links\n\n\n\n\n\nSep 6, 2013\n\n\n\n\n\n\n\nGalor’s Unified Growth Theory\n\n\n\n\n\nSep 4, 2013\n\n\n\n\n\n\n\nAriely’s Predictably Irrational\n\n\n\n\n\nSep 2, 2013\n\n\n\n\n\n\n\nA week of links\n\n\n\n\n\nAug 30, 2013\n\n\n\n\n\n\n\nEconomic cosmology - Equilibrium\n\n\n\n\n\nAug 29, 2013\n\n\n\n\n\n\n\nEconomic cosmology - The invisible hand\n\n\n\n\n\nAug 26, 2013\n\n\n\n\n\n\n\nA week of links\n\n\n\n\n\nAug 23, 2013\n\n\n\n\n\n\n\nEconomic cosmology - The rational egotistical individual\n\n\n\n\n\nAug 20, 2013\n\n\n\n\n\n\n\nA week of links\n\n\n\n\n\nAug 16, 2013\n\n\n\n\n\n\n\nFour reasons why evolutionary theory might not add value to economics\n\n\n\n\n\nAug 12, 2013\n\n\n\n\n\n\n\nA week of links\n\n\n\n\n\nAug 9, 2013\n\n\n\n\n\n\n\nThe love principle\n\n\n\n\n\nAug 7, 2013\n\n\n\n\n\n\n\nAn Economic Theory of Greed, Love, Groups, and Networks\n\n\n\n\n\nAug 5, 2013\n\n\n\n\n\n\n\nA week of links\n\n\n\n\n\nAug 2, 2013\n\n\n\n\n\n\n\nThe intergenerational transmission of economic development\n\n\n\n\n\nJul 31, 2013\n\n\n\n\n\n\n\nThe deep roots of economic development\n\n\n\n\n\nJul 29, 2013\n\n\n\n\n\n\n\nA week of links\n\n\n\n\n\nJul 26, 2013\n\n\n\n\n\n\n\nSocial Darwinism is back\n\n\n\n\n\nJul 24, 2013\n\n\n\n\n\n\n\nA week of links\n\n\n\n\n\nJul 19, 2013\n\n\n\n\n\n\n\nDarwin’s Conjecture - Generalising Darwinism\n\n\n\n\n\nJul 18, 2013\n\n\n\n\n\n\n\nGenetic diversity, economic development and policy\n\n\n\n\n\nJul 15, 2013\n\n\n\n\n\n\n\nA week of links\n\n\n\n\n\nJul 12, 2013\n\n\n\n\n\n\n\nObservations on happiness, biases and preferences\n\n\n\n\n\nJul 11, 2013\n\n\n\n\n\n\n\nGrandparents affect social mobility\n\n\n\n\n\nJul 9, 2013\n\n\n\n\n\n\n\nA week of links\n\n\n\n\n\nJul 5, 2013\n\n\n\n\n\n\n\nEconomic growth and evolution: Parental preference for quality and quantity of offspring\n\n\n\n\n\nJul 4, 2013\n\n\n\n\n\n\n\nPopulation, technological progress and the evolution of innovative potential\n\n\n\n\n\nJul 2, 2013\n\n\n\n\n\n\n\nA week of links\n\n\n\n\n\nJun 28, 2013\n\n\n\n\n\n\n\nAccelerating adaptive evolution in humans\n\n\n\n\n\nJun 26, 2013\n\n\n\n\n\n\n\nMore people means more ideas AND mutations\n\n\n\n\n\nJun 24, 2013\n\n\n\n\n\n\n\nA week of links\n\n\n\n\n\nJun 21, 2013\n\n\n\n\n\n\n\nWhen your neighbour wins the lottery\n\n\n\n\n\nJun 19, 2013\n\n\n\n\n\n\n\nHeight through the millennia\n\n\n\n\n\nJun 17, 2013\n\n\n\n\n\n\n\nA week of links\n\n\n\n\n\nJun 14, 2013\n\n\n\n\n\n\n\nWorld economic history in two diagrams\n\n\n\n\n\nJun 12, 2013\n\n\n\n\n\n\n\nGenetics and the increase in obesity\n\n\n\n\n\nJun 10, 2013\n\n\n\n\n\n\n\nA week of links\n\n\n\n\n\nJun 7, 2013\n\n\n\n\n\n\n\nPaleo-hypotheses\n\n\n\n\n\nJun 5, 2013\n\n\n\n\n\n\n\nZuk’s Paleofantasy\n\n\n\n\n\nJun 3, 2013\n\n\n\n\n\n\n\nA week of links\n\n\n\n\n\nMay 31, 2013\n\n\n\n\n\n\n\nModelling versus theory\n\n\n\n\n\nMay 28, 2013\n\n\n\n\n\n\n\nA week of links\n\n\n\n\n\nMay 24, 2013\n\n\n\n\n\n\n\nA science of intentional change\n\n\n\n\n\nMay 22, 2013\n\n\n\n\n\n\n\nSexual selection and entrepreneurship\n\n\n\n\n\nMay 20, 2013\n\n\n\n\n\n\n\nA week of links\n\n\n\n\n\nMay 17, 2013\n\n\n\n\n\n\n\nA week of links\n\n\n\n\n\nMay 10, 2013\n\n\n\n\n\n\n\nCluelessness\n\n\n\n\n\nMay 8, 2013\n\n\n\n\n\n\n\nImpatience and aggregate risk\n\n\n\n\n\nMay 6, 2013\n\n\n\n\n\n\n\nA week of links\n\n\n\n\n\nMay 3, 2013\n\n\n\n\n\n\n\nSelection during pregnancy\n\n\n\n\n\nMay 2, 2013\n\n\n\n\n\n\n\nHwang and Horowitt’s The Rainforest\n\n\n\n\n\nApr 30, 2013\n\n\n\n\n\n\n\nA week of links\n\n\n\n\n\nApr 26, 2013\n\n\n\n\n\n\n\nAltruists and the knowledge problem\n\n\n\n\n\nApr 25, 2013\n\n\n\n\n\n\n\nDeep Rationality: The Evolutionary Economics of Decision Making\n\n\n\n\n\nApr 22, 2013\n\n\n\n\n\n\n\nA week of links\n\n\n\n\n\nApr 19, 2013\n\n\n\n\n\n\n\nEvolution of time preference by natural selection\n\n\n\n\n\nApr 18, 2013\n\n\n\n\n\n\n\nA unified behavioural theory of economic activity\n\n\n\n\n\nApr 15, 2013\n\n\n\n\n\n\n\nA week of links\n\n\n\n\n\nApr 13, 2013\n\n\n\n\n\n\n\nEvolutionary psychology, fertility and economic ambition\n\n\n\n\n\nApr 10, 2013\n\n\n\n\n\n\n\nThe evolution of happiness\n\n\n\n\n\nApr 8, 2013\n\n\n\n\n\n\n\nA week of links\n\n\n\n\n\nApr 6, 2013\n\n\n\n\n\n\n\nEconomics from a biological viewpoint\n\n\n\n\n\nApr 1, 2013\n\n\n\n\n\n\n\nA week of links\n\n\n\n\n\nMar 30, 2013\n\n\n\n\n\n\n\nUsing the Malthusian model to measure technology\n\n\n\n\n\nMar 28, 2013\n\n\n\n\n\n\n\nThe success of the productive\n\n\n\n\n\nMar 26, 2013\n\n\n\n\n\n\n\nA week of links\n\n\n\n\n\nMar 23, 2013\n\n\n\n\n\n\n\nCooperation and Conflict in the Family Conference\n\n\n\n\n\nMar 20, 2013\n\n\n\n\n\n\n\nBusiness adaptation\n\n\n\n\n\nMar 18, 2013\n\n\n\n\n\n\n\nA week of links\n\n\n\n\n\nMar 15, 2013\n\n\n\n\n\n\n\nGenetic distance and income differences - evidence from China\n\n\n\n\n\nMar 14, 2013\n\n\n\n\n\n\n\nA week of links\n\n\n\n\n\nMar 9, 2013\n\n\n\n\n\n\n\nGenetic diversity, phenotypic diversity and the founder effect\n\n\n\n\n\nMar 7, 2013\n\n\n\n\n\n\n\nVictorian naturalists\n\n\n\n\n\nMar 4, 2013\n\n\n\n\n\n\n\nA week of links\n\n\n\n\n\nMar 2, 2013\n\n\n\n\n\n\n\nPublishing on genetic diversity and economic growth\n\n\n\n\n\nFeb 28, 2013\n\n\n\n\n\n\n\nFlynn’s Are We Getting Smarter?\n\n\n\n\n\nFeb 26, 2013\n\n\n\n\n\n\n\nA week of links\n\n\n\n\n\nFeb 23, 2013\n\n\n\n\n\n\n\nDoes genetic diversity increase conflict?\n\n\n\n\n\nFeb 22, 2013\n\n\n\n\n\n\n\nFisher on the evolution of time preference\n\n\n\n\n\nFeb 19, 2013\n\n\n\n\n\n\n\nA week of links\n\n\n\n\n\nFeb 16, 2013\n\n\n\n\n\n\n\nSocial mobility across the generations\n\n\n\n\n\nFeb 15, 2013\n\n\n\n\n\n\n\nDoes genetic diversity increase innovation?\n\n\n\n\n\nFeb 12, 2013\n\n\n\n\n\n\n\nA week of links\n\n\n\n\n\nFeb 9, 2013\n\n\n\n\n\n\n\nThe ‘Out of Africa’ Hypothesis, Human Genetic Diversity, and Comparative Economic Development\n\n\n\n\n\nFeb 8, 2013\n\n\n\n\n\n\n\nA model of the quantity-quality trade-off\n\n\n\n\n\nFeb 6, 2013\n\n\n\n\n\n\n\nThere is no quantity-quality trade-off\n\n\n\n\n\nFeb 4, 2013\n\n\n\n\n\n\n\nA week of links\n\n\n\n\n\nFeb 1, 2013\n\n\n\n\n\n\n\nFertility is going to go up\n\n\n\n\n\nJan 31, 2013\n\n\n\n\n\n\n\nSpontaneous order\n\n\n\n\n\nJan 29, 2013\n\n\n\n\n\n\n\nA week of links\n\n\n\n\n\nJan 25, 2013\n\n\n\n\n\n\n\nUpdating Maddison\n\n\n\n\n\nJan 23, 2013\n\n\n\n\n\n\n\nJames Crow on the quality of people\n\n\n\n\n\nJan 21, 2013\n\n\n\n\n\n\n\nThe benefits of Chinese eugenics\n\n\n\n\n\nJan 18, 2013\n\n\n\n\n\n\n\nEvolution, the Human Sciences and Liberty meeting\n\n\n\n\n\nJan 17, 2013\n\n\n\n\n\n\n\nIs poverty in our genes? From the comments\n\n\n\n\n\nJan 15, 2013\n\n\n\n\n\n\n\nO-ring and foolproof sectors\n\n\n\n\n\nJan 14, 2013\n\n\n\n\n\n\n\nKremer’s O-ring theory of economic development\n\n\n\n\n\nJan 11, 2013\n\n\n\n\n\n\n\nConsensus in economics and biology\n\n\n\n\n\nJan 9, 2013\n\n\n\n\n\n\n\nWho will invade economics?\n\n\n\n\n\nJan 6, 2013\n\n\n\n\n\n\n\nIs poverty in our genes?\n\n\n\n\n\nJan 3, 2013\n\n\n\n\n\n\n\nThe best books I read in 2012\n\n\n\n\n\nDec 28, 2012\n\n\n\n\n\n\n\nThe rationale of the family\n\n\n\n\n\nDec 22, 2012\n\n\n\n\n\n\n\nExploring genes\n\n\n\n\n\nDec 19, 2012\n\n\n\n\n\n\n\nThe bright tax\n\n\n\n\n\nDec 17, 2012\n\n\n\n\n\n\n\nKrugman on Gould and Maynard Smith\n\n\n\n\n\nDec 14, 2012\n\n\n\n\n\n\n\nFailure to respond as a measure of conscientiousness and IQ\n\n\n\n\n\nDec 12, 2012\n\n\n\n\n\n\n\nReligion, personality and fertility\n\n\n\n\n\nDec 10, 2012\n\n\n\n\n\n\n\nA flood of new genetic variation\n\n\n\n\n\nDec 7, 2012\n\n\n\n\n\n\n\nPositive eugenics\n\n\n\n\n\nDec 4, 2012\n\n\n\n\n\n\n\nBoyd and Richerson’s group selection\n\n\n\n\n\nNov 26, 2012\n\n\n\n\n\n\n\nGenoeconomics at the AEA Annual Meeting\n\n\n\n\n\nNov 22, 2012\n\n\n\n\n\n\n\nSexual selection and inequality\n\n\n\n\n\nNov 19, 2012\n\n\n\n\n\n\n\nThe decline in intelligence?\n\n\n\n\n\nNov 16, 2012\n\n\n\n\n\n\n\nCritique of conspicuous consumption and economic growth\n\n\n\n\n\nNov 13, 2012\n\n\n\n\n\n\n\nGenes, economics and happiness\n\n\n\n\n\nNov 7, 2012\n\n\n\n\n\n\n\nBoyd and Richerson’s The Origin and Evolution of Cultures\n\n\n\n\n\nNov 5, 2012\n\n\n\n\n\n\n\nUsing neuroeconomics in economics\n\n\n\n\n\nOct 31, 2012\n\n\n\n\n\n\n\nDeriving the demand for children\n\n\n\n\n\nOct 29, 2012\n\n\n\n\n\n\n\nGenetics without genes\n\n\n\n\n\nOct 25, 2012\n\n\n\n\n\n\n\nLong-term social mobility is low\n\n\n\n\n\nOct 23, 2012\n\n\n\n\n\n\n\nTrivers on Romney’s sons and Obama’s daughters\n\n\n\n\n\nOct 19, 2012\n\n\n\n\n\n\n\nNobel prizes and marriage markets\n\n\n\n\n\nOct 16, 2012\n\n\n\n\n\n\n\nGenetic diversity and economic development: Ashraf and Galor respond\n\n\n\n\n\nOct 13, 2012\n\n\n\n\n\n\n\nHarvard academics on genetic diversity and economic development\n\n\n\n\n\nOct 10, 2012\n\n\n\n\n\n\n\nThe benefits of competition\n\n\n\n\n\nOct 8, 2012\n\n\n\n\n\n\n\nAyn Rand and altruism\n\n\n\n\n\nOct 4, 2012\n\n\n\n\n\n\n\nCooperation is intuitive\n\n\n\n\n\nOct 3, 2012\n\n\n\n\n\n\n\nInequality and declining fertility\n\n\n\n\n\nOct 1, 2012\n\n\n\n\n\n\n\nHaidt’s group selection\n\n\n\n\n\nSep 28, 2012\n\n\n\n\n\n\n\nHaidt’s The Righteous Mind\n\n\n\n\n\nSep 26, 2012\n\n\n\n\n\n\n\nSocioeconomic status versus fitness\n\n\n\n\n\nSep 24, 2012\n\n\n\n\n\n\n\nVideos for the Biological Basis of Preferences and Behavior Conference\n\n\n\n\n\nSep 21, 2012\n\n\n\n\n\n\n\nKelly’s What Technology Wants\n\n\n\n\n\nSep 19, 2012\n\n\n\n\n\n\n\nAgriculture and population growth\n\n\n\n\n\nSep 17, 2012\n\n\n\n\n\n\n\nHenrich on markets, trust and monogamy\n\n\n\n\n\nSep 14, 2012\n\n\n\n\n\n\n\nGenetic diversity and economic development\n\n\n\n\n\nSep 12, 2012\n\n\n\n\n\n\n\nAge-dependent evolution\n\n\n\n\n\nSep 10, 2012\n\n\n\n\n\n\n\nGenoeconomics and the ENCODE project\n\n\n\n\n\nSep 7, 2012\n\n\n\n\n\n\n\nEconomy-IQ feedback\n\n\n\n\n\nSep 3, 2012\n\n\n\n\n\n\n\nDoes equality increase conspicuous consumption?\n\n\n\n\n\nAug 30, 2012\n\n\n\n\n\n\n\nHunter-gatherer workouts\n\n\n\n\n\nAug 28, 2012\n\n\n\n\n\n\n\nKeeping economists honest\n\n\n\n\n\nAug 27, 2012\n\n\n\n\n\n\n\nNot quite paleo\n\n\n\n\n\nAug 24, 2012\n\n\n\n\n\n\n\nThe intelligent inheriting the earth\n\n\n\n\n\nAug 22, 2012\n\n\n\n\n\n\n\nRecent selection for height\n\n\n\n\n\nAug 20, 2012\n\n\n\n\n\n\n\nOngoing selection against violent behaviour\n\n\n\n\n\nAug 18, 2012\n\n\n\n\n\n\n\nModels without data\n\n\n\n\n\nAug 16, 2012\n\n\n\n\n\n\n\nThe Stigler diet\n\n\n\n\n\nAug 14, 2012\n\n\n\n\n\n\n\nSexual selection, conspicuous consumption and economic growth\n\n\n\n\n\nAug 10, 2012\n\n\n\n\n\n\n\nCliodynamics and complexity\n\n\n\n\n\nAug 6, 2012\n\n\n\n\n\n\n\nThe evolution of cornets\n\n\n\n\n\nJul 31, 2012\n\n\n\n\n\n\n\nOrmerod’s Why Most Things Fail\n\n\n\n\n\nJul 28, 2012\n\n\n\n\n\n\n\nSelective sweeps in humans\n\n\n\n\n\nJul 24, 2012\n\n\n\n\n\n\n\nEugenics versus economics\n\n\n\n\n\nJul 22, 2012\n\n\n\n\n\n\n\nGroups, kin and self interest\n\n\n\n\n\nJul 19, 2012\n\n\n\n\n\n\n\nSimon’s Models of My Life\n\n\n\n\n\nJul 16, 2012\n\n\n\n\n\n\n\nWhat is multilevel selection?\n\n\n\n\n\nJul 14, 2012\n\n\n\n\n\n\n\nLabelling cultural group selection\n\n\n\n\n\nJul 12, 2012\n\n\n\n\n\n\n\nThe Origins of Savings Behaviour\n\n\n\n\n\nJul 10, 2012\n\n\n\n\n\n\n\nCharity as conspicuous consumption\n\n\n\n\n\nJul 8, 2012\n\n\n\n\n\n\n\nConspicuous consumption and poverty traps\n\n\n\n\n\nJul 7, 2012\n\n\n\n\n\n\n\nIs biology easier than physics?\n\n\n\n\n\nJul 5, 2012\n\n\n\n\n\n\n\nThe lipstick effect\n\n\n\n\n\nJul 1, 2012\n\n\n\n\n\n\n\nInequality persistence circa 5000 BCE\n\n\n\n\n\nJun 29, 2012\n\n\n\n\n\n\n\nThe deep roots of development\n\n\n\n\n\nJun 25, 2012\n\n\n\n\n\n\n\nPinker takes on group selection\n\n\n\n\n\nJun 22, 2012\n\n\n\n\n\n\n\nPopulation, connectivity and innovation\n\n\n\n\n\nJun 21, 2012\n\n\n\n\n\n\n\nEuropeans and economic growth\n\n\n\n\n\nJun 19, 2012\n\n\n\n\n\n\n\nEvolutionary science as the new “classics”\n\n\n\n\n\nJun 17, 2012\n\n\n\n\n\n\n\nEvolutionary policy making\n\n\n\n\n\nJun 16, 2012\n\n\n\n\n\n\n\nSome perspectives on Elinor Ostrom\n\n\n\n\n\nJun 13, 2012\n\n\n\n\n\n\n\nRobert Frank’s Passions Within Reason\n\n\n\n\n\nJun 11, 2012\n\n\n\n\n\n\n\nEconomists on autopilot\n\n\n\n\n\nJun 8, 2012\n\n\n\n\n\n\n\nEconomists are different?\n\n\n\n\n\nJun 6, 2012\n\n\n\n\n\n\n\nHayek, planning and eugenics\n\n\n\n\n\nJun 4, 2012\n\n\n\n\n\n\n\nA critique of behavioural economics from 1988\n\n\n\n\n\nJun 3, 2012\n\n\n\n\n\n\n\nShrinking brains and intelligence\n\n\n\n\n\nJun 1, 2012\n\n\n\n\n\n\n\nThe consequences of shrinking brains\n\n\n\n\n\nMay 30, 2012\n\n\n\n\n\n\n\nEugenics and regression to the mean\n\n\n\n\n\nMay 28, 2012\n\n\n\n\n\n\n\nConflict and social evolution\n\n\n\n\n\nMay 26, 2012\n\n\n\n\n\n\n\nMarkets and morals\n\n\n\n\n\nMay 24, 2012\n\n\n\n\n\n\n\nSeabright’s The War of the Sexes\n\n\n\n\n\nMay 22, 2012\n\n\n\n\n\n\n\nInstitutions are endogenous\n\n\n\n\n\nMay 20, 2012\n\n\n\n\n\n\n\nEntanglement\n\n\n\n\n\nMay 19, 2012\n\n\n\n\n\n\n\nCould this critique apply to economics?\n\n\n\n\n\nMay 18, 2012\n\n\n\n\n\n\n\nChimps 1, Humans 0\n\n\n\n\n\nMay 16, 2012\n\n\n\n\n\n\n\nWhile we wait for the genoeconomics revolution\n\n\n\n\n\nMay 15, 2012\n\n\n\n\n\n\n\nMaladaptive ideas\n\n\n\n\n\nMay 14, 2012\n\n\n\n\n\n\n\nThe genetic architecture of economic and political preferences\n\n\n\n\n\nMay 11, 2012\n\n\n\n\n\n\n\nGame theory and the peacock’s tail\n\n\n\n\n\nMay 9, 2012\n\n\n\n\n\n\n\nRubin’s Darwinian Politics\n\n\n\n\n\nMay 8, 2012\n\n\n\n\n\n\n\nThe Biological Basis of Preferences and Behaviour conference\n\n\n\n\n\nMay 7, 2012\n\n\n\n\n\n\n\nIQ as a necessary but not sufficient condition for genius\n\n\n\n\n\nMay 4, 2012\n\n\n\n\n\n\n\nGandolfi, Gandolfi and Barash’s Economics as an Evolutionary Science\n\n\n\n\n\nMay 3, 2012\n\n\n\n\n\n\n\nConsilience conference afterthoughts\n\n\n\n\n\nMay 1, 2012\n\n\n\n\n\n\n\nGroup selection and the social sciences\n\n\n\n\n\nApr 27, 2012\n\n\n\n\n\n\n\nThe recent evolution of musical talent\n\n\n\n\n\nApr 22, 2012\n\n\n\n\n\n\n\nWhy do we work less?\n\n\n\n\n\nApr 20, 2012\n\n\n\n\n\n\n\nSelfish herding\n\n\n\n\n\nApr 18, 2012\n\n\n\n\n\n\n\nMatt Ridley’s The Rational Optimist\n\n\n\n\n\nApr 16, 2012\n\n\n\n\n\n\n\nBeinhocker’s The Origin of Wealth\n\n\n\n\n\nApr 11, 2012\n\n\n\n\n\n\n\nThe three stages of evolutionary economics\n\n\n\n\n\nApr 9, 2012\n\n\n\n\n\n\n\nSaad’s The Evolutionary Bases of Consumption\n\n\n\n\n\nApr 5, 2012\n\n\n\n\n\n\n\nThe return of group selection\n\n\n\n\n\nApr 3, 2012\n\n\n\n\n\n\n\nEconomics and evolutionary biology reading list\n\n\n\n\n\nApr 1, 2012\n\n\n\n\n\n\n\nAn economics and evolutionary biology reading list\n\n\n\n\n\nApr 1, 2012\n\n\n\n\n\n\n\nTeaching evolution in economics\n\n\n\n\n\nMar 28, 2012\n\n\n\n\n\n\n\nBowles and Gintis’s A Cooperative Species\n\n\n\n\n\nMar 26, 2012\n\n\n\n\n\n\n\nEducation, income and children\n\n\n\n\n\nMar 21, 2012\n\n\n\n\n\n\n\nThe political implications of group selection\n\n\n\n\n\nMar 19, 2012\n\n\n\n\n\n\n\nSubsidise the rich for the good of our species\n\n\n\n\n\nMar 17, 2012\n\n\n\n\n\n\n\nAre children normal goods?\n\n\n\n\n\nMar 14, 2012\n\n\n\n\n\n\n\nNew books on the evolution of cooperation\n\n\n\n\n\nMar 12, 2012\n\n\n\n\n\n\n\nMale income and reproductive success\n\n\n\n\n\nMar 9, 2012\n\n\n\n\n\n\n\nThe eugenics of contraception\n\n\n\n\n\nMar 7, 2012\n\n\n\n\n\n\n\nWhy do married men earn more?\n\n\n\n\n\nFeb 29, 2012\n\n\n\n\n\n\n\nKahneman on the price of freedom\n\n\n\n\n\nFeb 28, 2012\n\n\n\n\n\n\n\nForesight by H. G. Wells\n\n\n\n\n\nFeb 24, 2012\n\n\n\n\n\n\n\nParental income and SAT scores\n\n\n\n\n\nFeb 22, 2012\n\n\n\n\n\n\n\nEvolutionary strategies\n\n\n\n\n\nFeb 21, 2012\n\n\n\n\n\n\n\nHarford’s Adapt: Why Success Always Starts with Failure\n\n\n\n\n\nFeb 17, 2012\n\n\n\n\n\n\n\nQuantifying children\n\n\n\n\n\nFeb 15, 2012\n\n\n\n\n\n\n\nPopulation genetics and economic growth\n\n\n\n\n\nFeb 13, 2012\n\n\n\n\n\n\n\nRisk aversion is not irrational\n\n\n\n\n\nFeb 9, 2012\n\n\n\n\n\n\n\nTrivers on biology in economics\n\n\n\n\n\nFeb 7, 2012\n\n\n\n\n\n\n\nTrivers’s The Folly of Fools\n\n\n\n\n\nFeb 3, 2012\n\n\n\n\n\n\n\nStrength by outbreeding\n\n\n\n\n\nFeb 2, 2012\n\n\n\n\n\n\n\nPayment for winning the genetic lottery\n\n\n\n\n\nJan 31, 2012\n\n\n\n\n\n\n\nAbsolute improvement\n\n\n\n\n\nJan 29, 2012\n\n\n\n\n\n\n\nFrank’s Luxury Fever\n\n\n\n\n\nJan 28, 2012\n\n\n\n\n\n\n\nExcess males\n\n\n\n\n\nJan 24, 2012\n\n\n\n\n\n\n\nConsilience Conference\n\n\n\n\n\nJan 21, 2012\n\n\n\n\n\n\n\nEconomic mobility and reproductive success\n\n\n\n\n\nJan 20, 2012\n\n\n\n\n\n\n\nThe mating reservation wage\n\n\n\n\n\nJan 19, 2012\n\n\n\n\n\n\n\nKahneman’s Thinking, Fast and Slow\n\n\n\n\n\nJan 18, 2012\n\n\n\n\n\n\n\nCrime, abortion and genes\n\n\n\n\n\nJan 16, 2012\n\n\n\n\n\n\n\nEvolution and education policy\n\n\n\n\n\nJan 13, 2012\n\n\n\n\n\n\n\nDysgenics and war\n\n\n\n\n\nJan 11, 2012\n\n\n\n\n\n\n\nStatus, signalling and the handicap principle\n\n\n\n\n\nJan 9, 2012\n\n\n\n\n\n\n\nIntelligence and assortive mating\n\n\n\n\n\nJan 6, 2012\n\n\n\n\n\n\n\nGaron’s Beyond Our Means\n\n\n\n\n\nJan 4, 2012\n\n\n\n\n\n\n\nA Nobel Prize for biology\n\n\n\n\n\nJan 2, 2012\n\n\n\n\n\n\n\nNot so irrational\n\n\n\n\n\nDec 30, 2011\n\n\n\n\n\n\n\nBest books I read in 2011\n\n\n\n\n\nDec 27, 2011\n\n\n\n\n\n\n\nIQ externalities\n\n\n\n\n\nDec 22, 2011\n\n\n\n\n\n\n\nZimbardo’s The Lucifer Effect\n\n\n\n\n\nDec 19, 2011\n\n\n\n\n\n\n\nThe perfection of man\n\n\n\n\n\nDec 18, 2011\n\n\n\n\n\n\n\nGenoeconomics: molecular genetics and economics\n\n\n\n\n\nDec 16, 2011\n\n\n\n\n\n\n\nThe use of heritability in policy development\n\n\n\n\n\nDec 13, 2011\n\n\n\n\n\n\n\nA passion for equality?\n\n\n\n\n\nDec 12, 2011\n\n\n\n\n\n\n\nHuman evolution goes on\n\n\n\n\n\nDec 8, 2011\n\n\n\n\n\n\n\nDubreuil’s Human Evolution and the Origins of Hierarchies\n\n\n\n\n\nDec 6, 2011\n\n\n\n\n\n\n\nAn evolutionary Occupy\n\n\n\n\n\nDec 3, 2011\n\n\n\n\n\n\n\nFukuyama’s The Origins of Political Order\n\n\n\n\n\nNov 19, 2011\n\n\n\n\n\n\n\nTwo articles on genetics and economics\n\n\n\n\n\nNov 18, 2011\n\n\n\n\n\n\n\nIntelligence changes\n\n\n\n\n\nNov 15, 2011\n\n\n\n\n\n\n\nMonkey inequality\n\n\n\n\n\nNov 12, 2011\n\n\n\n\n\n\n\nMalthus and the feast\n\n\n\n\n\nNov 11, 2011\n\n\n\n\n\n\n\nThe IQ taboo\n\n\n\n\n\nNov 6, 2011\n\n\n\n\n\n\n\nIs loss aversion a bias?\n\n\n\n\n\nNov 5, 2011\n\n\n\n\n\n\n\nTake the evolutionary economics pill\n\n\n\n\n\nNov 2, 2011\n\n\n\n\n\n\n\nVariation in reproductive success\n\n\n\n\n\nOct 22, 2011\n\n\n\n\n\n\n\nTwo perspectives on sex differences\n\n\n\n\n\nOct 21, 2011\n\n\n\n\n\n\n\nWhitfield on the Darwin Economy\n\n\n\n\n\nOct 17, 2011\n\n\n\n\n\n\n\nIs Darwin or Smith the father of economics?\n\n\n\n\n\nOct 10, 2011\n\n\n\n\n\n\n\nRobert Frank’s The Darwin Economy\n\n\n\n\n\nOct 8, 2011\n\n\n\n\n\n\n\nHunting, gathering and comparative advantage\n\n\n\n\n\nOct 6, 2011\n\n\n\n\n\n\n\nBeauty as a fitness indicator\n\n\n\n\n\nOct 1, 2011\n\n\n\n\n\n\n\nIs it human nature to riot?\n\n\n\n\n\nSep 30, 2011\n\n\n\n\n\n\n\nMarkets and family values\n\n\n\n\n\nSep 29, 2011\n\n\n\n\n\n\n\nDisease and liberalisation\n\n\n\n\n\nSep 28, 2011\n\n\n\n\n\n\n\nHuman nature and property rights\n\n\n\n\n\nSep 27, 2011\n\n\n\n\n\n\n\nPinker on violence\n\n\n\n\n\nSep 25, 2011\n\n\n\n\n\n\n\nBrooks on hunter-gatherers and egalitarianism\n\n\n\n\n\nSep 24, 2011\n\n\n\n\n\n\n\nHamermesh’s Beauty Pays\n\n\n\n\n\nSep 20, 2011\n\n\n\n\n\n\n\nHuman nature and libertarianism\n\n\n\n\n\nSep 17, 2011\n\n\n\n\n\n\n\nWhat economics misses\n\n\n\n\n\nSep 14, 2011\n\n\n\n\n\n\n\nHappiness is not the objective\n\n\n\n\n\nSep 12, 2011\n\n\n\n\n\n\n\nMale incentives\n\n\n\n\n\nSep 10, 2011\n\n\n\n\n\n\n\nUsing evolutionary theory to shape neighbourhoods\n\n\n\n\n\nSep 8, 2011\n\n\n\n\n\n\n\nThe genetic and social lottery\n\n\n\n\n\nSep 7, 2011\n\n\n\n\n\n\n\nDo economists satisfice?\n\n\n\n\n\nSep 1, 2011\n\n\n\n\n\n\n\nThe end of women\n\n\n\n\n\nAug 29, 2011\n\n\n\n\n\n\n\nElite envy\n\n\n\n\n\nAug 28, 2011\n\n\n\n\n\n\n\nSports team ownership as conspicuous consumption\n\n\n\n\n\nAug 27, 2011\n\n\n\n\n\n\n\nUnderestimating heritability\n\n\n\n\n\nAug 25, 2011\n\n\n\n\n\n\n\nDoes epigenetics matter?\n\n\n\n\n\nAug 23, 2011\n\n\n\n\n\n\n\nEconomics is a branch of ecology\n\n\n\n\n\nAug 20, 2011\n\n\n\n\n\n\n\nEconomists 1, Biologists 0\n\n\n\n\n\nAug 18, 2011\n\n\n\n\n\n\n\nEnvy has its benefits\n\n\n\n\n\nAug 17, 2011\n\n\n\n\n\n\n\nLow social mobility equals success\n\n\n\n\n\nAug 16, 2011\n\n\n\n\n\n\n\nMore people, more ideas - in the long run\n\n\n\n\n\nAug 15, 2011\n\n\n\n\n\n\n\nThe gender gap\n\n\n\n\n\nAug 12, 2011\n\n\n\n\n\n\n\nKeynes and the solved economic problem\n\n\n\n\n\nAug 9, 2011\n\n\n\n\n\n\n\nHealth trade-offs\n\n\n\n\n\nAug 4, 2011\n\n\n\n\n\n\n\nDarwin on female preferences\n\n\n\n\n\nAug 3, 2011\n\n\n\n\n\n\n\nThe costs of polygamy\n\n\n\n\n\nAug 1, 2011\n\n\n\n\n\n\n\nFree sterilisation\n\n\n\n\n\nJul 31, 2011\n\n\n\n\n\n\n\nReturn to equilibrium\n\n\n\n\n\nJul 30, 2011\n\n\n\n\n\n\n\nOnly economists are rational\n\n\n\n\n\nJul 25, 2011\n\n\n\n\n\n\n\nIs everyone the same?\n\n\n\n\n\nJul 24, 2011\n\n\n\n\n\n\n\nBrooks on evolution and obesity\n\n\n\n\n\nJul 19, 2011\n\n\n\n\n\n\n\nFukuyama’s biological approach\n\n\n\n\n\nJul 18, 2011\n\n\n\n\n\n\n\nThe growth of atheism\n\n\n\n\n\nJul 16, 2011\n\n\n\n\n\n\n\nClark on the remnants of rural idiocy\n\n\n\n\n\nJul 11, 2011\n\n\n\n\n\n\n\nJones on IQ and productivity\n\n\n\n\n\nJul 10, 2011\n\n\n\n\n\n\n\nDarwin and Marx\n\n\n\n\n\nJul 6, 2011\n\n\n\n\n\n\n\nWrong predictions\n\n\n\n\n\nJul 5, 2011\n\n\n\n\n\n\n\nGalbraith on evolution and the invisible hand\n\n\n\n\n\nJul 1, 2011\n\n\n\n\n\n\n\nWilson and Pinker on evolutionary psychology\n\n\n\n\n\nJun 27, 2011\n\n\n\n\n\n\n\nDefending Stephen Jay Gould\n\n\n\n\n\nJun 26, 2011\n\n\n\n\n\n\n\nGalton trivia\n\n\n\n\n\nJun 24, 2011\n\n\n\n\n\n\n\nGenetic thresholds\n\n\n\n\n\nJun 23, 2011\n\n\n\n\n\n\n\nCrime and biology\n\n\n\n\n\nJun 22, 2011\n\n\n\n\n\n\n\nFerguson on Malthus again\n\n\n\n\n\nJun 21, 2011\n\n\n\n\n\n\n\nFerguson’s Civilization: The West and the Rest\n\n\n\n\n\nJun 20, 2011\n\n\n\n\n\n\n\nHeritability, political views and personality\n\n\n\n\n\nJun 17, 2011\n\n\n\n\n\n\n\nDiversity and consumerism\n\n\n\n\n\nJun 17, 2011\n\n\n\n\n\n\n\nThe evolution of conscientiousness\n\n\n\n\n\nJun 16, 2011\n\n\n\n\n\n\n\nMiller’s Spent: Sex, Evolution, and Consumer Behavior\n\n\n\n\n\nJun 15, 2011\n\n\n\n\n\n\n\nFerguson on Malthus\n\n\n\n\n\nJun 14, 2011\n\n\n\n\n\n\n\nEvolution and obesity\n\n\n\n\n\nJun 10, 2011\n\n\n\n\n\n\n\nMaslow’s hierarchy\n\n\n\n\n\nJun 9, 2011\n\n\n\n\n\n\n\nBrooks’s Sex, Genes & Rock ‘n’ Roll\n\n\n\n\n\nJun 8, 2011\n\n\n\n\n\n\n\nModelling populations\n\n\n\n\n\nJun 7, 2011\n\n\n\n\n\n\n\nNatural selection and the collapse of economic growth\n\n\n\n\n\nJun 6, 2011\n\n\n\n\n\n\n\nNatural selection and economic growth\n\n\n\n\n\nJun 3, 2011\n\n\n\n\n\n\n\nWas it better for our paleolithic ancestors?\n\n\n\n\n\nMay 31, 2011\n\n\n\n\n\n\n\nCoyle on happiness\n\n\n\n\n\nMay 30, 2011\n\n\n\n\n\n\n\nDangerous ideas\n\n\n\n\n\nMay 27, 2011\n\n\n\n\n\n\n\nThe benefit to being right\n\n\n\n\n\nMay 25, 2011\n\n\n\n\n\n\n\nVeblen’s The Theory of the Leisure Class, Part III\n\n\n\n\n\nMay 24, 2011\n\n\n\n\n\n\n\nHungry judges\n\n\n\n\n\nMay 23, 2011\n\n\n\n\n\n\n\nThe Simon-Ehrlich bet\n\n\n\n\n\nMay 20, 2011\n\n\n\n\n\n\n\nEvolutionary psychology and the left\n\n\n\n\n\nMay 19, 2011\n\n\n\n\n\n\n\nUltimate population limits\n\n\n\n\n\nMay 18, 2011\n\n\n\n\n\n\n\nHappiness adjusts\n\n\n\n\n\nMay 17, 2011\n\n\n\n\n\n\n\nVeblen’s The Theory of the Leisure Class, Part II\n\n\n\n\n\nMay 16, 2011\n\n\n\n\n\n\n\nHeritability of religion and fertility\n\n\n\n\n\nMay 13, 2011\n\n\n\n\n\n\n\nPopulation and the tragedy of the commons\n\n\n\n\n\nMay 12, 2011\n\n\n\n\n\n\n\nVeblen’s The Theory of the Leisure Class\n\n\n\n\n\nMay 11, 2011\n\n\n\n\n\n\n\nTrust and education\n\n\n\n\n\nMay 10, 2011\n\n\n\n\n\n\n\nBryan Caplan’s Selfish Reasons to Have More Kids\n\n\n\n\n\nMay 9, 2011\n\n\n\n\n\n\n\nLibertarians and fertility\n\n\n\n\n\nMay 6, 2011\n\n\n\n\n\n\n\nWould Julian Simon worry?\n\n\n\n\n\nMay 5, 2011\n\n\n\n\n\n\n\nFogel and supersized humans\n\n\n\n\n\nMay 3, 2011\n\n\n\n\n\n\n\nRotten kids and altruism\n\n\n\n\n\nApr 29, 2011\n\n\n\n\n\n\n\nMorris’s Why the West Rules For Now - Part II\n\n\n\n\n\nApr 28, 2011\n\n\n\n\n\n\n\nMorris’s Why the West Rules For Now\n\n\n\n\n\nApr 27, 2011\n\n\n\n\n\n\n\nGenetically testing similarity\n\n\n\n\n\nApr 23, 2011\n\n\n\n\n\n\n\nIn the company of a stranger\n\n\n\n\n\nApr 22, 2011\n\n\n\n\n\n\n\nConsumption and fitness\n\n\n\n\n\nApr 19, 2011\n\n\n\n\n\n\n\nSocial Decision Making: Bridging Economics and Biology\n\n\n\n\n\nApr 19, 2011\n\n\n\n\n\n\n\nJones on IQ and immigration\n\n\n\n\n\nApr 14, 2011\n\n\n\n\n\n\n\nWhat can evolutionary biology offer economics?\n\n\n\n\n\nApr 13, 2011\n\n\n\n\n\n\n\nLehrer on measurement\n\n\n\n\n\nApr 12, 2011\n\n\n\n\n\n\n\nEvolution and the invisible hand\n\n\n\n\n\nApr 11, 2011\n\n\n\n\n\n\n\nWilson on economics and evolution\n\n\n\n\n\nApr 8, 2011\n\n\n\n\n\n\n\nThe Evolution Institute\n\n\n\n\n\nApr 6, 2011\n\n\n\n\n\n\n\nMeasurement nihilism\n\n\n\n\n\nApr 4, 2011\n\n\n\n\n\n\n\nThe heritability debate, again\n\n\n\n\n\nApr 1, 2011\n\n\n\n\n\n\n\nMicromotives and macrobehavior\n\n\n\n\n\nMar 30, 2011\n\n\n\n\n\n\n\nIncome and IQ\n\n\n\n\n\nMar 28, 2011\n\n\n\n\n\n\n\nEvolution and irrationality\n\n\n\n\n\nMar 25, 2011\n\n\n\n\n\n\n\nEconomists and biology\n\n\n\n\n\nMar 22, 2011\n\n\n\n\n\n\n\nGladwell’s Outliers\n\n\n\n\n\nMar 21, 2011\n\n\n\n\n\n\n\nDiamond on biological differences\n\n\n\n\n\nMar 13, 2011\n\n\n\n\n\n\n\nUnskilled and unaware\n\n\n\n\n\nMar 8, 2011\n\n\n\n\n\n\n\nCrisis in human genetics?\n\n\n\n\n\nMar 4, 2011\n\n\n\n\n\n\n\nGenetic distance and economic development\n\n\n\n\n\nMar 2, 2011\n\n\n\n\n\n\n\nTrade and natural selection\n\n\n\n\n\nFeb 23, 2011\n\n\n\n\n\n\n\nJanet Browne’s Charles Darwin: Voyaging\n\n\n\n\n\nFeb 19, 2011\n\n\n\n\n\n\n\nKling on patterns of sustainable specialisation and trade\n\n\n\n\n\nFeb 15, 2011\n\n\n\n\n\n\n\nCrime and selection of aggressive males\n\n\n\n\n\nFeb 12, 2011\n\n\n\n\n\n\n\nBanking as an ecosystem\n\n\n\n\n\nFeb 8, 2011\n\n\n\n\n\n\n\nEvolutionary economics and group selection\n\n\n\n\n\nFeb 7, 2011\n\n\n\n\n\n\n\nWhat is the objective?\n\n\n\n\n\nJan 29, 2011\n\n\n\n\n\n\n\nWhy do rich parents bother?\n\n\n\n\n\nJan 26, 2011\n\n\n\n\n\n\n\nIs aid really so complex?\n\n\n\n\n\nJan 21, 2011\n\n\n\n\n\n\n\nDeLong on the pace of evolution\n\n\n\n\n\nJan 19, 2011\n\n\n\n\n\n\n\nThe speed of cities - afterthoughts\n\n\n\n\n\nJan 14, 2011\n\n\n\n\n\n\n\nThe speed of cities, part II\n\n\n\n\n\nJan 12, 2011\n\n\n\n\n\n\n\nThe speed of cities\n\n\n\n\n\nJan 10, 2011\n\n\n\n\n\n\n\nClark on violence\n\n\n\n\n\nJan 6, 2011\n\n\n\n\n\n\n\nMore on violence\n\n\n\n\n\nJan 5, 2011\n\n\n\n\n\n\n\nSelection for aggression\n\n\n\n\n\nJan 5, 2011\n\n\n\n\n\n\n\nMy top 10 books in 2010\n\n\n\n\n\nDec 31, 2010\n\n\n\n\n\n\n\nTrading fish\n\n\n\n\n\nDec 28, 2010\n\n\n\n\n\n\n\nA wasteful Christmas\n\n\n\n\n\nDec 23, 2010\n\n\n\n\n\n\n\nFitness spreading\n\n\n\n\n\nDec 12, 2010\n\n\n\n\n\n\n\nThe evolution of technology\n\n\n\n\n\nDec 10, 2010\n\n\n\n\n\n\n\nPost-crisis economics\n\n\n\n\n\nDec 5, 2010\n\n\n\n\n\n\n\nCoal and the industrial revolution\n\n\n\n\n\nDec 2, 2010\n\n\n\n\n\n\n\nBetter school performance leads to more children\n\n\n\n\n\nNov 6, 2010\n\n\n\n\n\n\n\nProcrastination\n\n\n\n\n\nOct 20, 2010\n\n\n\n\n\n\n\nThe value of a species\n\n\n\n\n\nOct 6, 2010\n\n\n\n\n\n\n\nThere is but one social science\n\n\n\n\n\nSep 26, 2010\n\n\n\n\n\n\n\nThe short wingman - do humans use visual illusions to attract a mate?\n\n\n\n\n\nSep 23, 2010\n\n\n\n\n\n\n\nEconomics versus history - is this the right debate?\n\n\n\n\n\nSep 14, 2010\n\n\n\n\n\n\n\nEducation in the developing world\n\n\n\n\n\nSep 12, 2010\n\n\n\n\n\n\n\nSports stars born early in the year\n\n\n\n\n\nSep 12, 2010\n\n\n\n\n\n\n\nShould we tax education?\n\n\n\n\n\nSep 11, 2010\n\n\n\n\n\n\n\nPatience and IQ\n\n\n\n\n\nAug 28, 2010\n\n\n\n\n\n\n\nThe predictive power of marshmallows\n\n\n\n\n\nAug 22, 2010\n\n\n\n\n\n\n\nIt’s a risky business attracting a mate\n\n\n\n\n\nAug 9, 2010\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/a-comment-on-the-manifesto-for-behavioural-science.html",
    "href": "posts/a-comment-on-the-manifesto-for-behavioural-science.html",
    "title": "A comment on the manifesto for behavioural science",
    "section": "",
    "text": "I wrote this post based on notes for a proposed “lunch and learn” session. Illness got in the way, so rather than let those notes sit on the shelf, I’ve cleaned them up to share here. Many points were intended to be (provocative) conversation prompts rather than statements, so a few parts are light on evidence or end with a question.\nIn the first half of 2023, A manifesto for applying behavioural science was published in Nature Human Behaviour. Written by the head of the North American arm of the Behavioural Insights Team, Michael Hallsworth, the manifesto “looks at the challenges facing the field and sets out ten proposals to address them.”\nThe Behavioural Insights Team also published a longer version of the manifesto, plus a summary document. Hallsworth gave me the opportunity to comment on a draft of the long-form manifesto, so I have a small insight into its development.\nRather than address the document as a whole - I must admit I am sceptical of manifestos for direction from up high - I am going to discuss three of Hallsworth’s proposals:"
  },
  {
    "objectID": "posts/a-comment-on-the-manifesto-for-behavioural-science.html#see-the-system",
    "href": "posts/a-comment-on-the-manifesto-for-behavioural-science.html#see-the-system",
    "title": "A comment on the manifesto for behavioural science",
    "section": "See the system",
    "text": "See the system\nThe summary table in the Nature Human Behaviour article describes the “See the system” proposal as follows:\n\nUse aspects of complexity thinking to improve behavioural science so that it can exploit leverage points, model the collective implications of heuristics, alter specific features of systems to create wider changes, and understand the longer-term impact on a system of a collection of policies with varying goals.\n\nPutting this section into my words, much of the public policy territory in which we work is a complex adaptive system. That is, there is a dynamic network of many interacting agents, each with their own strategies. They are constantly acting and reacting and adapting to the environment they find themselves in. Changes are not linear. Small changes can cascade into large consequences. Major efforts can produce little change. “Emergent” behaviour can arise from these interactions, with the system as a whole producing something more than the sum of its parts. If we examine public policy issues with this lens, we might be able to identify leverage points, model the collective implications of people’s decision making strategies, make system-level changes, and understand the impact of multiple policies.\nHallsworth illustrates this proposal with the UK tax on sugared drinks. This tax was implemented in tiers, with drinks with higher sugar content hit with higher taxes. While this might be seen as a way of changing consumer behaviour via higher prices for the sugary drinks, this approach primarily affected sugar consumption in soft drinks by altering the incentives presented to manufacturers. They could reduce the price of their product by reducing the sugar content. Price signals via a tax reduced the sugar content of the drinks on the shelf.\nBefore digging into the proposal and example, I want to describe two related concepts, chaos and complexity. Both involve the study of non-linear dynamics.\nChaos concerns the study of dynamic systems that are sensitive to the initial starting conditions.\nA famous example is Edward Lorenz’s replication of results of his weather simulations. He took the numbers from a previous weather simulation, re-entered them and started the simulation. He found that these new simulations diverged wildly from the previous runs. When he examined the results, he realised he was taking numbers from the prior simulations to six decimal places. Beyond six decimal places, there was variation. These very slight changes in initial conditions led to large divergences over time. This idea is typically referred to as the butterfly effect; a butterfly flapping its wings in the Amazon causing a tornado in Florida.\nChaos suggests there is an inherent limit in our ability to predict weather and other chaotic systems. Seemingly insignificant short-term variations from the model will cascade into larger differences.\nChaos can emerge in very simple systems. Lorenz’s weather simulation was based on just 12 equations. Robert May (1976) famously showed that by adjusting a single parameter ‘a’ in a deterministic equation, he was able to generate anything from a stable equilibrium to multiple equilibria to what looks like random noise.\nComplexity also relates to non-linear systems, in which order can emerge at a higher level that’s not easily seen as the sum of the lower parts. Complexity theory is the study of these systems. Complex adaptive systems are a type of complex system with agents who learn and adapt in the changing environment, adding an additional complication to the study of the dynamical system.\nWhat does embracing this complexity mean for applied behavioural science?\nI see two possibilities.\nThe first, suggested in the manifesto, is that we can identify leverage points to trigger a large change. Imagine you could control the parameter ‘a’ in May’s equations. The sugar tax is one example, manipulating the price of sugar. Similarly, if trying to regulate carbon emissions, instead of identifying all the different sources and setting specific regulations around them, you could simply set a carbon price.\nThe second possibility from taking complexity (and chaos) seriously is that some features of the policy environment will be inherently unpredictable.\nTo discuss this, let’s dig into that example of the sugar tax more deeply. A tax on sugar content of soft drinks led to organisations to reformulate their products (sidestepping the question of whether this is complexity or just second order effects). However, obesity in the UK has continued to go up. The pandemic didn’t help, but it was going up before that. The best we have are desperate data mining studies that, after enough slices and dices, claim a benefit for year 6 girls (but fatter year 6 boys) (Rogers et al., 2023).\nSo, despite the second order effect of reformulation of soft drinks, for the measure of interest, there was no measurable reduction on obesity. (To be fair, it’s questionable whether you’d expect it to be detectable.) And what of other measures? Over 300 million pounds a year out of people’s pockets during a supposed cost of living crisis. What of the distribution of that cost? I expect a relatively regressive distribution. And drinks that taste worse.\nAs another example, one of the Behavioural Insights Team’s early projects involved offering loft cleaning services in conjunction with insulation installation, removing a hassle that was standing in the way of energy efficiency improvements. (After reading the original report on this work, this topic is deserving of another post.) Yet, in line with previous research, gas use rebounds to original levels within two years of loft insulation installation (Peñasco and Anadón, 2023). Again, this might just be thought about as second-order effects rather than complexity, but here we have seemingly simple interventions failing to achieve the desired objective.\nTaking complexity seriously and seeing the system also requires thinking about ourselves. What is the process by which our work results in change? Even this is not simple. Stefan Della Vigna and Elizabeth Linos analysed the many pre-registered trials conducted by the Behavioural Insights Team in North America (full credit for transparency). In a paper published in Econometrica (DellaVigna and Linos, 2022) they demonstrated that although the effects of ‘nudges’ in the field were smaller than those in academic publications, they were real and would in many cases pass a cost-benefit test. A second paper in the Journal of Political Economy (DellaVigna et al., 2024), however, is somewhat depressing. There was no link between the evidence from the trial and implementation. We have what seems a simple lever - identify those interventions that work - and yet we don’t see this down the line in the outcomes that matter.\nAnd here’s one further dimension of complexity that should be considered. We’re working on a lot of objectives. Look at the bodies of work focused on gender equity, retirement savings, education, and the like. Achieving these objectives have second order effects. How do they affect each other? Does encouraging female participation in STEM increase the gender wage gap? Does encouraging paternity leave affect productivity or family income? Does encouraging university attendance among low-socioeconomic status increase inequality (when they drop out or end up in a lower paying job than a trade)? Some of these might be easy to measure. But some of the interactions are likely beyond us.\nSo, the question I have is, should we take a message of greater humility from “seeing the system”? (Humility is one of the principles in the manifesto - but this comes from a different foundation.)"
  },
  {
    "objectID": "posts/a-comment-on-the-manifesto-for-behavioural-science.html#no-view-from-nowhere",
    "href": "posts/a-comment-on-the-manifesto-for-behavioural-science.html#no-view-from-nowhere",
    "title": "A comment on the manifesto for behavioural science",
    "section": "No view from nowhere",
    "text": "No view from nowhere\nAs an outsider reading the manifesto, the question I kept asking was “what is the objective?”. (I asked that in my comments on the draft manifesto too!)\nIf the recipe in the manifesto is followed, what would be the measure of success? The manifesto contains some general statements about applied behavioural science fulfilling its potential. I have no idea what fulfilled potential looks like. The ten proposals are largely quiet about what the manifesto is trying to achieve, outside of the explicit “Data Science for Equity”. I”ll come to that proposal later.\nSimilarly, what are we trying to achieve as behavioural science practitioners? This is a big question, but it’s fair to ask it of a “manifesto”. What are the consequences for the subjects of our work - citizens, customers, employees - from our interventions as applied behavioural practitioners, and how might the manifesto shape these outcomes if the manifesto’s proposals are implemented?\nThe point of the manifesto where the question of objective stood out most to me was in the “No view from nowhere” proposal. The summary of this proposal states:\n\nCultivate self-scrutiny, find new ways for the subjects of research to judge researchers, and take actions to increase diversity among behavioural scientists and their teams, such as building professional networks between the Global North and Global South.\n\nAgain, let me give my summary of that principle from the broader text. We don’t come to work with a blank slate. We bring assumptions. We bring values. Gender, race and sexuality influence our viewpoints. And so on. Because of this, we cannot view a situation from nowhere. These assumptions and values are always there. This problem is relevant as behavioural science practitioners are homogeneous. Few teams come from the “Global South”. Our research subjects have traditionally represented only a small fraction of the global population. As a result, we should give more scrutiny to our starting points and build diversity among data scientists and their teams. The call to increase diversity notes that we need to increase diversity “of several kinds”. Directly mentioned is increasing collaboration with the Global South and increasing ethnic and racial diversity.\nEvery time I see a call for diversity, I wonder what kind of diversity. And there is one specific type that isn’t questioned in the manifesto.\nSuppose I was to ask a set of behavioural science practitioners the following questions (excuse their Australian flavour - I’m sure you can come up with versions for whatever country you wish):\n\nWho voted “No” in the referendum to amend the constitution to give a “Voice” to Indigenous Australians? (At one stage, the presentation was scheduled for the weekend before this referendum. And despite 60% of the Australian voting population rejecting the proposal, I am still to meet even one person in my professional world or town where I live who has openly stated that they voted no.)\nRelatedly, should we have an indigenous land acknowledgement at the start of every meeting?\nWho voted Liberal at the last Federal election? (The Australian conservative party.)\nWho believes there are only two sexes?\nShould there be a sugar tax in Australia?\n\nI am not asking for a show of hands, but I suspect a skew in a specific direction. And that is largely reflected in the types of projects that applied behavioural practitioners work on. Equity. Diversity. Climate. The sins of the lower classes.\nWe work on sugar taxes in soft drinks when a large flat white has more calories than a can of Coke. (Products with more than 75% milk are excluded from the UK sugar tax.) I can’t find a serious mention of the effect of the sugar tax on taste. We work on net zero rather than energy abundance. We work on how can we get more men to take paternity leave to close the gender wage gap rather than ‘What is the optimal level of paternity leave and how can we support it’? (Given the most credible evidence on the gender gap points to a motherhood penalty from time out of the workforce (Goldin, 2014), are we trying to convert the motherhood penalty into a family penalty?) There’s hardly any questioning of whether this is even a policy issue where income is shared within a household.\nThe omissions also stand out. We rarely work on projects to boost productivity or economic growth. For all the talk of “libertarian paternalism”, I’m not aware of a single example of a behavioural team working hard to remove regulation and replace it with good behavioural design? Where is the push to move to a voluntary superannuation system in Australia, where we let people have their superannuation if they want it, with a behaviourally designed system to support their savings? Applied behavioural science is simply paternalism.\nHere’s a final example. Many Australian women fail to meet their fertility intentions (for example, see Wilkins et al. (2021) and a summary of the report in the Conversation). This is not just an Australian phenomenon (e.g. Guzzo and Hayford (2023)). There is also considerable evidence that people don’t understand how early in life fertility starts to decline (e.g. Hammarberg et al. (2013)). Is there a project to help people to overcome this shortcoming? Contrast this to the volume of work on retirement savings.\nPutting these examples together, there is a lack of intellectual diversity in the behavioural science world, and this is reflected in the work that we do. Further, there seems to be little interest in rectifying this. We’re only calling for the types of diversity that don’t challenge our world view."
  },
  {
    "objectID": "posts/a-comment-on-the-manifesto-for-behavioural-science.html#data-science-for-equity",
    "href": "posts/a-comment-on-the-manifesto-for-behavioural-science.html#data-science-for-equity",
    "title": "A comment on the manifesto for behavioural science",
    "section": "Data Science for Equity",
    "text": "Data Science for Equity\nThe third proposal I will briefly discuss, “Data Science for Equity”, is a view from somewhere very specific. The summary in the Nature Human Behaviour paper reads:\n\nUse data science to identify the ways in which an intervention or situation appears to increase inequalities and introduce features to reduce them. For example, groups that are particularly likely to miss a filing requirement could be offered pre-emptive help.\n\nI take the choice of the word “equity” as deliberate and coming with a meaning different to equality. Equity is about outcomes, not equality of opportunity. On my previous point about diversity, I’m not sure a more intellectually diverse team would have chosen that word. Think of all the things behavioural science could be achieving: productivity, happiness, financial wellbeing, economic growth, sustainability. Why single out equity?\nThat this proposal is “data science” and not “behavioural science” for equity is also interesting. It seems somewhat narrow to highlight one particular tool.\nBut the question I want to flag concerns trade-offs. When you choose a specific objective, what are you are willing to trade-off to achieve it? Absent trade-offs, the objective has no teeth. How does it guide choice of problem (including what problems you won’t work on)? What does it imply for choice of intervention and measurement of success?\nThe example offered in the above summary doesn’t provide much guidance. Helping people who might miss a filing requirement is innocuous. Assuming a small cost of a reminder or prompt, there’s no real trade-off here.\nSo let’s pull out a cartoon example. You discover that abolishing advanced math classes reduces the gap between indigenous and non-indigenous students, largely by reducing the performance of the top cohort. Is this the equity you want to achieve?\nMaybe not, but it’s not clear from the proposal. And if it’s not, where would we draw the line? Do we want to prioritise problems that lift the bottom but harm no-one else? (Is it now data science for Pareto improvement?)\nAnd this brings me back to my previous question about objectives. Should our goal be equity? Or should it be to bring out the best in us in our own way?\nI don’t expect Hallsworth to have the complete answer in his manifesto, but I’m not sure I get any useful guidance from this objective of equity."
  },
  {
    "objectID": "posts/an-evolutionary-projection-of-global-fertility-and-population-my-new-paper-with-lionel-page-in-evolution-human-behavior.html",
    "href": "posts/an-evolutionary-projection-of-global-fertility-and-population-my-new-paper-with-lionel-page-in-evolution-human-behavior.html",
    "title": "An evolutionary projection of global fertility and population: My new paper (with Lionel Page) in Evolution & Human Behavior",
    "section": "",
    "text": "Forecasting fertility is a mug’s game. Here is a picture of fertility forecasts by the US Census Bureau through the baby boom and subsequent fertility drop (from Lee and Tuljapurkar’s Population Forecasting for Fiscal Planning: Issues and Innovations). The dark line is actual, the dotted line the various forecasts.\n\n\nUS Census forecasts\n\n\n\nI am not sure that the science of fertility forecasting in developed countries has made substantial progress since any of those forecasts were made. But that doesn’t stop a lot of people from trying.\nOne of the most high profile forecasts of fertility and population comes from the United Nations, which publishes global population forecasts through to 2100. Individual country forecasts are currently developed using a Bayesian methodology, which are then aggregated to form a global picture. The development of this methodology led to a heavily cited 2014 paper titled “World population stabilization unlikely this century” (pdf) and the conclusion that there was only a 30% probability that global population growth would cease this century.\nThese projections contain an important fertility assumption. For countries that have undergone the demographic transition to low fertility, the assumption is that their fertility rate will oscillate around a long-term mean. While there has been some debate around whether this long-term mean would be the replacement rate or lower, the (almost theory-free) assumption of oscillation around a long-term level dominates the forecasts.\nThere is at least one theoretical basis for doubting this assumption. In a 2013 working paper (co-authored with Oliver Richards), we argued that as fertility was heritable, this would tend to increase fertility and population growth. Those with a preference for higher fertility would have more children, with their children in turn having a preference for more children. This high-fertility type would eventually come to dominate the population, leading to markedly higher population that forecast.\nAs I noted when the working paper was released, we were hardly the first to propose this idea. Fisher noted the power of higher fertility groups in The Genetical Theory of Natural Selection. I had seen Razib Khan, Robin Hanson and John Hawks mention the idea. Murphy and Wang examined the concept in a microsimulation. Many papers on the heritability of fertility hint at it. Rowthorn’s paper on fertility and religiosity also points in this direction. We simply added a touch of quantitative modelling to explore the speed of the change, and have now been followed by others with different approaches (such as this).\nShortly after I posted about the working paper, I received an email from Lionel Page suggesting that we should turn this idea into more detailed simulation of world population. Five years after Lionel’s email, that simulation has just been released in a paper published in Evolution & Human Behavior. Here is the abstract:\n\nThe forecasting of the future growth of world population is of critical importance to anticipate and address a wide range of global challenges. The United Nations produces forecasts of fertility and world population every two years. As part of these forecasts, they model fertility levels in post-demographic transition countries as tending toward a long-term mean, leading to forecasts of flat or declining population in these countries. We substitute this assumption of constant long-term fertility with a dynamic model, theoretically founded in evolutionary biology, with heritable fertility. Rather than stabilizing around a long-term level for post-demographic transition countries, fertility tends to increase as children from larger families represent a larger share of the population and partly share their parents’ trait of having more offspring. Our results suggest that world population will grow larger in the future than currently anticipated.\n\nOur methodology is almost identical to the United Nations methodology, except we substitute the equation by which fertility converges to a long-term mean with the breeder’s equation, which captures the response to selection of a trait.\nAnd here are a few charts showing the simulation results: grey is the base United Nations simulation, black is the evolutionary simulation, and the dashed lines are the 90% confidence interval.\nFirst, European total fertility rate (TFR) and population, which shifts from terminal decline to growth:\n\n\n\n\n\n\n\n\n\n\nNext, North America, which increases its rate of growth:\n\n\n\n\n\n\n\n\n\n\nNext, Asia:\n\n\n\n\n\n\n\n\n\n\nAnd finally, the global result:\n\n\n\n\n\n\n\n\n\n\nThe punchline is that the probability of global population stabilisation this century becomes less than 5%. Europe and North America are most affected within this century. Asia is less affected, but still shifts from a scenario of decline to one of growth, and due to its size has the largest effect on the global projections.\nHaving opened by saying that fertility forecasting is a mug’s game, should the same be said about these forecasts? The answer to that question is largely yes. Cultural and technological change, environmental shocks and the like will almost certainly lead to a different outcome to the one the United Nations or we have forecast. We effectively argue this in the section of the paper on cultural evolution (which was added following some helpful reviewer comments).\nBut to get lost in the specific numbers is to lose sight of the exercise. We are arguing that an important assumption underpinning the United Nations exercise should be reconsidered. We’ve given a rough idea of how far that assumption could shift the fertility and population outcomes, and they are of a magnitude that would see some parts of the world looking quite different by the end of the century. If we assume constant fertility despite this evolutionary dynamic, we risk a material downward bias in projecting future fertility and population.\n\nAs an aside, the freely available methodology and R packages that underpin the United Nations forecasts greatly facilitated our efforts. We spent a lot of time considering how to implement the simulations, but on discovering the openness of the United Nations approach, we found a great place to implement our tweaked approach. In that spirit, you can access our modified packages and the data used to generate them here at OSF.\nIf you can’t access the paper through the paywall and would like me to email you a copy, let me know."
  },
  {
    "objectID": "posts/bryan-caplans-the-case-against-education-a-review.html",
    "href": "posts/bryan-caplans-the-case-against-education-a-review.html",
    "title": "Bryan Caplan’s The Case Against Education: A Review",
    "section": "",
    "text": "My first job out of university was as a lawyer. Later, when I switched to a non-legal role, I enrolled in a Master of Laws. I selected some subjects relevant to my new job and that might be useful if I wanted to return to a legal firm. Among other subjects, I studied constitutional theory, international trade law, human rights law and energy law.\nHow much do I know about those topics today? I can’t remember what we covered in constitutional theory, except for a recollection that we kicked off with some classic British philosophers. I know nothing about human rights law beyond the existence of a few international frameworks. I recall some of international trade law: the game-theoretic basis of the analysis and some basic principles, although I doubt I could write more than a page about what I learned. For energy law, I recall nothing.\nDid I waste the time and money I spent on that Masters?\nIn his book The Case Against Education, Bryan Caplan argues that much (but not all) of the income premium for education is due to signalling. A Masters degree signals the intelligence and conscientiousness required to complete it. Even if the knowledge within is useless (and, in my case, largely forgotten), the fact I finished my studies signals I should be a productive worker.\nThat is how it played out. The signal from that Masters landed me a role in the Australian Treasury. They would not otherwise have looked at me with my underwhelming undergraduate record. When in Treasury, I worked on competition policy, an economic area with a legal tint. However, I used none of my legal knowledge. I also started post-graduate study in economics. I used none of that in Treasury either.\nAs occurred in my case, graduates get paid. In the United States in 2011, the average earnings of a high school graduate was $41,000. For a college graduate, it was $70,000. For someone with a Masters degree, it was $90,000.\nThe premium in Australia is less but still solid. My back on the envelope calculations using numbers from this report put the university graduate premium at about 40%.\nLet’s take this large premium to education as given. Why do graduates get paid more?\nCaplan compares three competing explanations, here spelled out in their pure form:\nCaplan argues that the share of signalling exceeds 50% and is “[p]robably more like 80%” of the education premium. However, he also makes the case that even if the share is closer to 30%, the education system still wastes a lot of money from a social perspective.\nBelow, I break down Caplan’s analysis of the human capital and signalling models. I’ll weave the ability model into my discussion of those two models, as both sides of the human capital-signalling debate concede that you need to control for ability bias. The presence of ability bias reduces the estimated return to education from human capital accumulation. Conversely, ability bias is required for the signalling model. If there it no ability bias, any difference in ability between people with different levels of education must come from the skills they learned during those studies. With no ability bias, there is no unobserved ability to signal."
  },
  {
    "objectID": "posts/bryan-caplans-the-case-against-education-a-review.html#the-human-capital-model",
    "href": "posts/bryan-caplans-the-case-against-education-a-review.html#the-human-capital-model",
    "title": "Bryan Caplan’s The Case Against Education: A Review",
    "section": "1. The human capital model",
    "text": "1. The human capital model\nMost economists subscribe to the human capital model. Education builds marketable skills. Those skills boost national productivity and income.\nCaplan presents a series of stylised facts against the human capital model. These relate to student behaviour, the fading of education over time, and the lack of transfer of knowledge to the workplace.\n\n1.1. Student behaviour\nStudents act as though they don’t care about building human capital. The best education in the world is free: no one would stop you if you rocked up to Princeton or Harvard and sat in class. But people don’t. Similarly, you can access free subject materials and lectures online from Harvard, MIT and many other top universities. Few ever use this.\nRelatedly, most students try to consume as little education as they can. They want a good grade, but most will not do anything that does not lead to that grade (and many won’t even do that). They cheer when a class is cancelled.\nAmong my undergraduate students, that is what I see. I would cheer if more than a couple appeared mildly interested in the content. Most don’t even show up. In the undergraduate subject I taught last semester, 19 of 65 students attended the first (online) class. They didn’t have to wear their pants (none turned their cameras on). That is before they have any idea about my teaching style. I suspect many who turned up did so because some lecturers have attendance or participation requirements. As soon as they learned that I didn’t, they were gone.\nAttendance does look different in the post-graduate subjects I teach. Many in that course want a change of job. And they don’t just want any job - they want a particular vocation where knowledge is typically tested through the application process and regularly on the job. But even that has limits. There aren’t many takers when I offer opportunities beyond the subjects themselves. Attendance is also stronger for the post-graduate students, but barely above 50%. It is perplexing under the human capital model as to why they would fork out over $50k for a degree in which they don’t engage.\nAnother student behaviour identified by Caplan is that students often seek the easiest classes. Easy markers are rewarded with student enrolment. While that reputation guides student choices inside the university, there is less visibility and harm to the signal externally. I suspect that is a driver of the decline in economics enrolment in Australia.\nThen there is cheating. Under the human capital model, cheating doesn’t pay. If you don’t get the skills, you get found out. So why would students cheat?\nOne response to these behavioural observations is that these students are short-sighted (preferring to avoid short-term costs) or have incorrect beliefs about how their skills will be rewarded in the marketplace. Students make plenty of poor decisions. Just look at their choice of degrees. Perhaps their misjudgment of whether they should cheat or take an easy subject occurs because they are disciples of the signalling explanation for education. That said, I suspect these students are not making a grievous error about the role of signalling.\n\n\n1.2. From the classroom to the workplace\nWhen students are tested in an exam, they clearly have learned something.\nBut the important question for the human capital model is not how well they do in the exam. It is how they use that knowledge when they get out of the exam room and into the workplace.\nAnd this is where the education enterprise becomes depressing.\nFirst, knowledge fades. There is a well-known phenomenon called summer fade-out, where kids regress over the summer, forgetting material they had learned previously. (Since I first wrote that sentence, I have read that summer fade-out may be more of a problem of never learning in the first place (Workman et al., 2023).) Similarly, I regularly teach students who appear to have no recollection of material taught in previous subjects despite passing the exam. They forgot.\nThere is substantial evidence that people don’t retain what they learned in school. Caplan refers to one study of algebra and geometry, where it was found that half the content was gone after five years (better than I expected), and nothing was left after 25. Similarly, tests of foreign language skills and civics show pitiful results for the time spent on these subjects at school.\nThe one buffer to forgetting is continual use: probably why most of us can continue to write, but recall nothing from our geometry lessons. This aligns with the idea of intermittent practice; repeated exposure over time is required to make a concept stick.\nIn discussing forgetting, Caplan draws an interesting contrast between failing and forgetting. Failing is penalised. Forgetting isn’t. As per my opening story about the lack of retention from the Master of Laws, there seems to be little penalty because I can’t recall the finer points of constitutional theory. A fail on my transcript would have been more consequential.\nEven if we were to remember what we were taught, there is also the question of whether we would use what we are taught. Caplan uses the example of his field of economics, stating “I assure you that my profession makes near-zero effort to train our undergrads for the job market.” I share his scepticism about what is taught in economics. There are some useful skills: the stats and microeconomic frameworks among them. But when I worked in economic policy consulting and Treasury, I hardly used any of my economic education. Less than one per cent of economics students would pull out a macroeconomic or trade model in their later work. For me, that has changed now I’m working in academia. Learning the content of an economics degree is a great thing to learn if you need to teach the content of an economics degree.\nMore specifically, Caplan uses economics as a counterpoint to the wheat-chaff defence of the human capital model. This defence runs along the lines of: even if there is a lot of irrelevant content (chaff) in education, there is enough wheat to make it worthwhile. (As Caplan notes, this is no ringing endorsement.) Caplan’s response, however, is to observe that chaff pays. An economics degree pays almost as much as engineering, despite the chaff. Philosophy and religion degrees pay substantially, and the pay goes up for religion and philosophy graduates if they are mismatched (not working in a relevant field).\nOne common defence of education is that it teaches you how to think, even if specific knowledge is lost. You might not recall any geometry, but it helped build mental muscle that you can turn to the job at hand.\nBut this is where we come to the problem of transfer. Or, more particularly, the lack of evidence of transfer.\nI like to think of transfer in a localised and general sense. The localised sense relates to what happens if I teach a concept and then ask for that concept to be used in a new context. The broader sense is that “mental muscle” sense, where my education equips me for the unrelated mental tasks in the broader world.\nIn both senses, the evidence is weak. When students are taught to answer a question and then immediately asked to answer a second question that can be solved with the same approach, they typically can’t answer the second. The degree of transfer goes down further with decreasing similarity, with time delay, with distractions (e.g. a problem between the two questions), if the second question is outside the classroom or if there are different teachers for the first and second questions. When you consider that the leap between school and the real world involves all of these hurdles, it’s hard to expect the direct application of learned concepts elsewhere.\nBut what of the broader sense of transfer, the building of critical thinking skills? Caplan discusses one interesting study where students in the first and fourth year of high school, college and graduate school were asked questions such as “Does violence on television significantly increase the likelihood of violence in real life?”. Their answers were then assessed for critical reasoning. Fourth-year high school students did better than those in the first year. However, there was no difference between first and fourth-year college students, and a minor difference between first and fourth-year graduate students. Education beyond high school barely shifts thinking skills.\nAnother study examined university students assessing the claim that students should “eat more nutritiously because the majority of students needing psychological counselling had poor dietary habits”. Despite many having six years of science in high school and college, plus advanced calculus, less than 1 per cent of students gave what was assessed as a “good scientific response”. Despite their years of education, college students are poor at applying their scientific and mathematical knowledge to outside problems.\nAcross the other studies Caplan examines, there is little promise outside of some examples with transfer of mathematical and statistical knowledge to other problems, although the tasks were generally softballs (e.g. asking a basic statistical sports question straight after a statistics course). There is some evidence of specific transfer if the testing is narrow enough relative to the study. Students that use a lot of statistics do get better at statistics. Humanities majors slightly improve at verbal reasoning. It’s a fairly uninspiring outcome for the effort invested.\n\n\n1.3. Does education make you smarter?\nIf education under-delivers on both knowledge and transferable skills, what of a more general claim that it makes us smarter? Maybe the human capital built is raw processing power.\nMore years of education seems to boost IQ scores. However, Caplan is not convinced this gain is meaningful. School is effectively practice for IQ tests, directly teaching facts that are tested. In one study of Swedish men, time in school boosted synonym and technical comprehension subtests without raising spatial or logical subtest scores. Then there is fadeout. Caplan argues that most interventions that have been found to boost IQ see the gains disappear a few years later.\nI’m not convinced by Caplan’s summary of the literature here. Here’s part of the abstract of a meta-analysis by Stuart Ritchie and Elliot Tucker-Drob (2018):\n\nAcross 142 effect sizes from 42 data sets involving over 600,000 participants, we found consistent evidence for beneficial effects of education on cognitive abilities of approximately 1 to 5 IQ points for an additional year of education. Moderator analyses indicated that the effects persisted across the life span and were present on all broad categories of cognitive ability studied. Education appears to be the most consistent, robust, and durable method yet to be identified for raising intelligence.\n\nThis meta-analysis was published just before Caplan’s book, so might not have appeared in time, but it contests Caplan’s claims of fadeout and a narrow boost on those items that can be taught. The studies that made up the meta-analysis were available to Caplan, which raises question of how Caplan picked the studies. Why does he prefer some and not others?\nI searched for whether Caplan later addressed Ritchie and Tucker-Drobs’s paper and found two times. The first was a response by Caplan to Noah Smith. Smith tweeted Ritchie and Tucker-Drob’s finding with the comment “Bryan Caplan and the other education skeptics are going to need to revise their beliefs a bit, eh?”. Caplan states that the paper did lead him to update his view, “slightly”. Why only slightly? Because he had already accepted Stephen Ceci’s (1991) findings of a 1 to 3 point gain per year of education. Caplan did quote Ceci in the book, although he did not directly reference that 1 to 3 point gain. Caplan’s main use of Ceci’s article in the book was to support of the idea that school teaches to the IQ test. Caplan repeats Ceci’s point about teaching to the test in his response to Smith.\nCaplan was also asked about the paper by Robert Wiblin on the 80,000 hours podcast. Caplan again notes that its only a slight update on his previous position, before repeating his points on school effectively being practice for IQ tests and fade-out. Between these two responses, it seems he has taken on the higher IQ estimate, but not Ritchie and Tucker Drob’s findings that the increase is across all components of IQ and persists across the lifespan.\nI have to admit that I am sceptical about the upper bounds of Ritchie and Tucker-Drobs’s estimates. Even if the higher estimates were robust, I doubt they would hold through the entire educational experience and at the margins where most interventions are now targeted, such as increasing post-secondary education. If these effects persisted year by year, I would expect to see larger differences in IQ across cohorts with different levels of education than we do. Further, if education boosts IQ so markedly, why doesn’t it appear in other areas? For example, the lack of gain in critical thinking between first- and fourth-year college mentioned above does not align with a substantive IQ gain.\nPart of this may be due to diminishing gains once we get beyond high school into graduate and post-graduate study. Ritchie and Tucker-Drobs’s evidence is largely drawn from high-school data, particularly those studies that indicate larger gains. Ritchie and Tucker-Drobs examined 28 studies. Of these, 7 involved controls for prior intelligence, 11 involved policy changes and 10 used school-age cutoff data.\nThe school-age cutoff data leads to estimates of around 5 IQ points, but by the nature of the device - variation in when people are born in the year relative to the age that they can leave school - means that these effects are at the high-school level.\nThe policy change studies, which generated a mean estimate around 2 IQ points, only had one study involving additional years of education beyond high school (Kamhöfer et al. (2019), which was an unpublished working paper at the time of Ritchie and Tucker-Drob’s article).\nThe studies control for prior intelligence by measuring intelligence at two points in time generated a mean gain of 2 points per year of education on composite tests and 1 point on tests of fluid intelligence. These studies cover all years of education from the time of the first test, so they typically contain early high school years mixed in with any university education. However, there were three studies where the first intelligence tests were conducted at an age where their education would primarily be university education - the subjects were 18, 18 and 20 at the time of the first IQ test. These found gains of around 1 point per year. (I find the simpler study design of two IQ tests more compelling.)\nCaplan’s final rejoinder to the idea that school increases intelligence is that the link between IQ and income is weak. Each IQ point leads to around a one percent increase in income. Even if each year of school raised IQ by, say, 3 IQ points, most of the education premium would remain unexplained.\nCaplan’s source for this claim of limited income gain from IQ is drawn from Jones and Schneider (2010). Jones and Schneider have a larger point, however. Despite the small individual gain from higher IQ, the gain to society is massive. An earlier paper by Jones and Schneider (2006) estimated a 6.1% increase in living standards from an increase of IQ by one point. Under this measure, the IQ gain from education seems to be a fantastic deal for society. (Caplan does cite Jones’s book “The Hive Mind: How Your Nations IQ Matters So Much More Than Your Own” about the large gains to a nation through higher IQ, but references it in support of a different point.)\n\n\n1.4. Human capital and ability bias\nUnder the human capital model, employers pay for skills. Those skills are a combination of ability and education. Therefore, any measure of the effect of education on income needs to account for the differences in ability.\nHow might you control for ability, ensuring your estimate of the effect of education is not tainted by “ability bias”?\nOne way would be to get measures of ability, such as IQ, and measure the effect of education holding ability constant. Controlling for IQ shrinks the size of the education premium. High-quality IQ tests reduce it by 20 to 30 per cent. Measures that control for math, reading, vocabulary and the kitchen sink can get the education premium down by around 50 per cent.\nBut there is a problem with this approach. If education increases ability, we have reverse causation and should attribute more of the premium to education. Caplan argues, however, that this isn’t a concern. Studies that measure IQ before school completion and studies that measure IQ after school completion find similar levels of cognitive ability bias. If education was materially affecting ability, we would expect more ability bias in the studies with measurements after school completion.\nAnother approach to account for ability bias would be to find quasi-experimental situations from which causation might be inferred. For example, if a pair of identical twins receive different education, and you assume equal ability, is there a difference in outcome? If compulsory attendance laws are changed, what happens to the income of students who are now forced to stay in school?\nThis body of research results in what Caplan calls the “Card Consensus”. The Card Consensus comes from two reviews of the literature by David Card (1999, 2001), in which Card argues that ability bias is in the order of 10 per cent if it exists at all. It is the educational investment itself that drives the impressive returns to education. Caplan states that most elite labour economists now embrace this perspective.\nHow does Card reconcile the studies he reviews with those Caplan cites? Card’s approach is somewhat dismissive. As Caplan notes in a footnote, Card’s 1999 article states:\n\nOne strand of literature that I do not consider are studies of the return to schooling that attempt to control for ability using observed test scores.\n\nI suspect Card’s dismissiveness matches that of much of the economics profession, who have a strong preference for the types of natural experiments that Card relies on. Assuming the assumptions underlying those causal methodologies hold, they should be less vulnerable to confounding.\nCaplan’s response is to question the robustness of the studies on which Card’s review is built. For example, Caplan cites research noting that the twin who receives more education is typically smarter; twins are not completely identical. However, accounting for differences in twin ability only shaves about 15% of the education premium (Sandewall et al., 2014). Similarly, studies capitalising on time of birth or changes in the ability of students to leave school are vulnerable to the patterns in birth by time of year (Bound et al., 1995) and a breach of the assumption that there are common trends across states (Stephens Jr. and Yang, 2014). Caplan also footnotes a comment by Stephens Jr. and Yang (2014) noting “schooling law changes outside of the United States finds either small or zero returns”.\nGiven the shakiness of the quasi-experimental research, Caplan points to what he feels is the stronger body of evidence. However, Caplan doesn’t deal with the major critique of his preferred data: Caplan’s data on ability bias is observational. We have correlation, not evidence of causation. Caplan’s argument is logical but lacks an instrument to pull causation out. That is what Card’s body of research attempts to do.\nHowever, I lean toward Caplan’s preferred evidence. Economists are right that to isolate causation, you need an appropriate methodology. But those methodologies have assumptions that, on reflection (and as noted above), often don’t hold. Then you throw in a good dose of publication bias - for example, this recent working paper by (Clark and Nielsen, 2024) suggests substantial publication bias in the literature on the returns to education - and we’re likely seeing inflated estimates of the return. I have to admit that it would take stronger evidence than that presented by Card and friends to dislodge me from my view that ability bias is real and that the returns to education measured in those experiments are untainted.\nCaplan does have one final thread to his critique of the Card Consensus, which is to consider what a lack of ability bias means: if you measure the ability of those deciding to further their education and those who don’t, you won’t see any difference in ability. Of the educated and less educated people you know, were they (on average) of equal ability when they made their education decisions? It doesn’t match experience.1\nCaplan quotes a paragraph from Joshua Angrist and Jörn-Steffen Pischke’s book Master Metrics: The Path from Cause to Effect, which reads:\n\nSome people cut their schooling short so as to pursue more immediately lucrative activities. Sir Mick Jagger abandoned his pursuit of a degree at the London School of Economics in 1963 to play with an outfit known as the Rolling Stones. . . . No less impressive, Swedish épée fencer Johan Harmenberg left MIT after 2 years of study in 1979, winning a gold medal in the 1980 Moscow Olympics, instead of earning an MIT diploma. Harmenberg went on to become a biotech executive and successful researcher. These examples illustrate how people with high ability—musical, athletic, entrepreneurial, or otherwise—may be economically successful without the benefit of an education. This suggests that . . . ability bias, can be negative as easily as positive.\n\nAngrist and Pischke scrape together a few outliers. Do you believe the claim of ability bias being as likely to be negative as positive? I don’t."
  },
  {
    "objectID": "posts/bryan-caplans-the-case-against-education-a-review.html#the-signalling-model",
    "href": "posts/bryan-caplans-the-case-against-education-a-review.html#the-signalling-model",
    "title": "Bryan Caplan’s The Case Against Education: A Review",
    "section": "2. The signalling model",
    "text": "2. The signalling model\nHaving laid out the case against the human capital model, what is the case for the signalling model?\nMuch of the case comes from how signalling explains many of the mysteries identified in the previous section. Useless subjects? They demonstrate ability. Forgotten what you learnt? Again, the fact you could learn the master once do it indicates ability. Why hasn’t online education hasn’t taken off? Because people want credentials, not skills. Online education doesn’t signal the conformity and persistence required for a college degree.\nI find those arguments compelling at face value, although Caplan provides several others in support. These include the skeepskin effect, malemployment and the gap between the personal and national education premium.\n\n2.1 The sheepskin effect\nThe payoff for each year of education is not equal. The twelfth grade pays more than grades 9, 10 and 11 combined. The final year of college pays more than twice as much as the first three year combined. Graduation signals conformity and that you takes norms seriously. As a result, you get a special bonus called the “sheepskin effect” (diplomas used to be printed on sheepskin) for finishing. Students know this. Students tend not to quit after year 11 or their second last year of college. When close to the finish line, they crawl over it.\nAbility bias rears its head here as a potential explanation for the sheepskin effect. What if graduates have better prospects than dropouts? Caplan argues that, correcting for ability bias, the sheepskin effect remains. Controlling for ability bias reduce the payoff for both years of education and the graduation diploma, leaving the relative premium for that sheepskin largely unchanged.\n\n\n2.2. Malemployment\nMany workers are overqualified. Depending on the measurement technique, between 10 and 35% of workers have more education than required for their job. And “malemployment” is increasing. The level of education required to get a job has risen faster than the amount needed to do it.\nMalemployment could occur under both the human capital model and signalling model. In the human capital model, malemployment occurs when people fail to acquire skills in school. In contrast, the signalling model points to an arms race in qualifications, where you need more education than your competitor. Being a high-school graduate when everyone else dropped out signals ability and perseverance. Graduating from high school when almost everyone completes a bachelor’s degree suggests you are a poor student.\nHow do we distinguish between the two? Caplan suggests the answer lies in whether the labour market rewards education an employee does not use. This would only occur under the signalling model. And the evidence seems to point in this direction. Bartenders with degrees earn more. College graduates out-earn high school graduates, regardless of their occupation and even for jobs that don’t require school.\n\n\n2.3 The personal education premium versus the national\nUnder the human capital model, education is good news for both the individual and the nation. Education increases both the worker’s income and the nation’s productivity.\nThe signalling model also suggests education is good for the individual, making you appear more productive. But as their productivity doesn’t go up, we see nothing at the national level. Productivity stays the same.\nWhat does the data show? Caplan compares estimates of the effect of a year of education on individual income with estimates of a year of education on national income. The comparison is stark: a year of personal education increases income by around 8 to 12%, while it increases national income by only 1 to 3%. Crudely, that puts the human capital-signalling mix at around 20:80.\nThat signalling arms races are socially inefficient was the centrepiece of Robert Frank’s book The Darwin Economy. If the order remains the same but everyone invests more in the signal, we’ve burnt resources for no gain. Caplan digs into this in much more detail in a chapter on the social returns to education I note below."
  },
  {
    "objectID": "posts/bryan-caplans-the-case-against-education-a-review.html#who-does-education-pay-for",
    "href": "posts/bryan-caplans-the-case-against-education-a-review.html#who-does-education-pay-for",
    "title": "Bryan Caplan’s The Case Against Education: A Review",
    "section": "3. Who does education pay for?",
    "text": "3. Who does education pay for?\n\n3.1 The individual return to education\nIn 1973 Michael Spence published one of the papers that led to his 2001 Sveriges Riksbank Prize in Economic Sciences in Memory of Alfred Nobel. The paper, Job Market Signalling (1973), provided a simple model of how employers could use education to identify high-productivity workers. In effect, he developed the formal signalling model.\nThe key element of the model is that the cost of obtaining education differs between low- and high-productivity workers. For example, a low-intelligence worker might find the lectures hard. A low-conscientiousness worker might not have the persistence to study and stick through the full degree.\nIn biology, this idea become known as the handicap principle (Zahavi, 1975). To be an honest signal, it needs to impose a “handicap” beyond what someone who does not have the trait can bear.\nThe consequence of this differential cost and ability to incur the handicap is that education may not be a good deal for everyone. Education must impose a cost that some can’t bear. Some students will benefit from more education, but some don’t.\nCaplan spends most of the chapter on the selfish return to education calculating the benefits to education for what he calls excellent, good, fair and poor students. An excellent student is the typical Masters holder. The good student is the typical college graduate who does not continue to further education. A fair student graduates high school but does not attempt college. And the poor student is the typical high-school dropout.\nThrough a range of calculations, Caplan ultimately concludes the high-school graduation is a great deal for all students. College, however, is only a good deal for good and excellent students. Why doesn’t college pay for poor or fair students? Largely, they don’t complete. They lose years of wage earning, incur student debt and don’t even get the sheepskin to show for it. Finally, Masters degrees are OK for excellent students and a waste for others.\nThere are plenty of factors that swing these broad calculations. Foer example, studying a subject with a high return (e.g. engineering, economics) makes it a better deal. Lower tuition is helpful (going to a more expensive college doesn’t pay).\n\n\n3.2. The social return to education\nAs hinted in the comparison of individual versus national returns to education, the social return to education appears weak. Caplan calculates that it is OK for high school, pathetically low for undergraduate study and negative for Masters. This return also varies across students; boosting the education of poor students is even worse. Even under low estimates of the share of signalling in the education premium, it is hard to resurrect the case for Masters degrees.\nCaplan looks hard for the social benefits, looking beyond the economic measures considered in analyses of national income. He looks at workforce participation and tax payments. A material gain could come from a reduction in crime, with even minor crime reductions having large social benefit. These broader benefit don’t change the case.\nWe could return here to the question of a national IQ boost noted above. Caplan does not revisit the potential boost to IQ in this chapter, but to the extent the boost in IQ is real, we should see it in the various factors he analyses, such as higher national income and lower crime."
  },
  {
    "objectID": "posts/bryan-caplans-the-case-against-education-a-review.html#education-good-for-the-soul",
    "href": "posts/bryan-caplans-the-case-against-education-a-review.html#education-good-for-the-soul",
    "title": "Bryan Caplan’s The Case Against Education: A Review",
    "section": "4. Education good for the soul",
    "text": "4. Education good for the soul\nWhen I question whether some students should be in university, one of the most common responses I hear is that education is good for the soul. It broadens horizons. Caplan has heard those same claims. For example, through school we are exposed to high culture and ideas. We read books and are exposed to political questions we won’t likely come across elsewhere. This exposure has the benefit of demonstrating the richness of the world for people who otherwise exhibit little curiosity.\nThere are two problems with this. The first is that it doesn’t work. We’re mandating culture for those who aren’t interested. During high school (a regional Western Australian school), I read Shakespeare, Pinter, Beckett, Camus and Bronte. My modal estimate of how many of my cohort of 150 students would have read any of those since graduating is three (one being me). If I’m incorrect, I won’t be off by more than a couple of people. I would guess most wouldn’t even recall that they read Camus, Pinter and Beckett. Contrast that with pop culture, barely featured in education.\nOn ideas, there is little evidence that education changes political views (despite educators leaning left). Caplan also looks at religion and voting and finds little evidence that education changes us.\nThe second problem is that the “broader horizon” is actually quite narrow. The music, art and poetry comes from ossified lists. Attempts to “modernise” just bring in a new kind of narrowness.\nCaplan clarifies that he is not cynical not about education, but about students (the Philistines) and the teachers (the majority uninspiring and not even excited themselves about what they are teaching). The alternative is to genuinely broaden their horizons. How about exposing people to plausible careers? Expose math students to insurance. Expose boys to nursing. Expose them to things that might actually feature in their future."
  },
  {
    "objectID": "posts/bryan-caplans-the-case-against-education-a-review.html#caplans-policy-recommendations",
    "href": "posts/bryan-caplans-the-case-against-education-a-review.html#caplans-policy-recommendations",
    "title": "Bryan Caplan’s The Case Against Education: A Review",
    "section": "5. Caplan’s policy recommendations",
    "text": "5. Caplan’s policy recommendations\nCaplan’s first-best policy is the separation of education from government. He is a libertarian.\nMoving to the feasible, his second-best policy is a combination of vocational education and “less”.\n\nVocational education\nHow do people get good at their jobs? By doing their jobs! This argument could be applied more broadly than vocational education. Ask any white collar worker where they learnt their skills. I learnt how to be a lawyer in a law firm.\nWhy is vocational education so effective? Because you’re learning the skills you will use, and rather than learning by listening, you are learning by doing.\nA critique of vocational education is that it is narrow and targeted to one career. But, traditional education is narrow and targeted toward to no career. As Caplan notes, education often teaches little about the world in which we live.\nSo how does vocational education stack up financially? Caplan crunches some numbers to show how vocational students stack up against comparable students who didn’t study a trade. The outcome: more pay, more likely to graduate, less unemployment and even less crime. Employment outcomes after age 50 aren’t quite as good, but the 30-years before that more than makes up for it.\nFor an Australian context, here’s some analysis by Andrew Norton: the top quartile of people with a Certifcate IV (a post-graduate trade degree) earn more than the median Bachelor degree graduate over their lifetime. It’s hard to know to what extent we’re getting insight into a counterfactual about choosing vocational versus university education, but I suspect there are many Bachelors students who would have been better off in a trade.\n\nCaplan’s final argument on vocational education is to let kids work earlier. As soon as my kids are old enough, I’m sending them down to the local McDonald’s or some other employer to start earning some money. There are skills such as turning up (something most students today don’t learn) on time (something students still struggle with on assignment submissions) well worth learning.\n\n\nLess\nWhen people get into a signalling competition, they waste resources. They invest more in the signal, yet the ranking and outcome don’t change.\nAs a result, in The Darwin Economy, Robert Frank argues for constraints to signalling contests. If people spend too much on conspicuous consumption and positional goods, apply a progressive consumption tax. Only 20% of people can be in the top 20%.\nCaplan applies a related argument, arguing for education austerity (the opposite of the near-universal call for more educational funding). Stop subsidising the arms race with taxpayer money. Only 20% can be among the 20% most educated.\nI spent some time thinking out what “less” could look like in my (Australian) context. First, provide zero support for post-graduate study. If universities want to use post-graduate courses as a cash cow, let them go for it, but we could remove tax deductions for domestic students. I would also be tempted to reshape the current student loan scheme for these post-graduate courses to require either faster repayment or a market-based interest rate.\nAcross my post-graduate study, I likely paid in the order of $100,000AUD (about $65,000USD) and got around $35,000 back in tax deductions. I then paid the remainder back through inflation-indexed loans. Studying increased my near-term income and smoothed it over time, as the tax deductions were immediate and the loan repayments were in the future. Absent the tax deductions, I wouldn’t have completed the Masters of Law, but I probably would have still done the economics (assuming my path hadn’t diverged). My study probably hasn’t boosted wages, given my choices, nor government tax receipts, but it has undoubtedly given me more mobility and put me closer to where I want to be.\nLess would also involve killing off targets for getting more students to university (the opposite of current trends), support more vocational training and have more of that vocational training happening at high school level. Absent continued growth in the number of international students, the university sector would shrink. That’s not a bad thing. If we simply up everyone’s signal without a commensurate increase in productivity, we’re not helping those we push into tertiary education.\nAs I’ve implied above, I suspect the balance between the human capital and signalling models changes as we move through the levels of education. I believe Caplan’s arguments are more robust in the university sector and he doesn’t refer to much evidence for years of high school education below those where you can drop out. Students learn some reading, writing and math (although to somewhat disappointing levels), so I’d be reluctant to start slashing too much there. Still, there seems scope to get rid of some of the fluff and let kids play around.\nBeyond that, I could be convinced to do more. But what I’ve proposed is already beyond the bounds of current feasibility. We could tweak toward less, see what happens, then tweak some more."
  },
  {
    "objectID": "posts/bryan-caplans-the-case-against-education-a-review.html#footnotes",
    "href": "posts/bryan-caplans-the-case-against-education-a-review.html#footnotes",
    "title": "Bryan Caplan’s The Case Against Education: A Review",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI have heard Card described as someone smart enough to construct a strong case for whatever position he wants to hold. His brief to the Supreme Court arguing that affirmative action involves no discrimination is a case in point.↩︎"
  },
  {
    "objectID": "posts/course-notes-on-consumer_financial_decision_making.html",
    "href": "posts/course-notes-on-consumer_financial_decision_making.html",
    "title": "Course notes on Applied Consumer Financial Decision Making",
    "section": "",
    "text": "These notes are now out-of-date. See the updated notes here.\n–\nBelow are the notes I developed in preparing content for a unit in Applied Consumer Financial Decision Making as part of UTS’s Graduate Certificate in Behavioural Economics. (It was titled Behavioural Approach to Investment and Insurance Decisions).\nThe course was for post-graduates with no assumed prior knowledge of economics or behavioural economics. This unit taken taken after introductory economics and behavioural economics units.\nThis content was placed into an online learning system, with built-in interactivity (hence the presence of some questions below), plus supplemental content such as videos.\nThere’s a lot that can be improved - I will update or add to these notes from time to time - but I thought I’d put them here if anyone thought they could be of use. It has an Australian flavour as that as where I am.\nI drew on many sources when developing the notes (as per the reference list), but one reference that helped greatly and that is reflected in the structure of some of these notes is Beshears et al (2018) “Behavioral Household Finance” in Bernheim et al (eds), Handbook of Behavioral Economics - Foundations and Applications 1\nAny comments or suggestion to improve are welcome.\n–"
  },
  {
    "objectID": "posts/course-notes-on-consumer_financial_decision_making.html#what-is-money",
    "href": "posts/course-notes-on-consumer_financial_decision_making.html#what-is-money",
    "title": "Course notes on Applied Consumer Financial Decision Making",
    "section": "1.1 What is money?",
    "text": "1.1 What is money?\nMoney comprises bank notes, bank deposits or anything else that can be used to buy goods and services, and that is accepted as payment because others can use it for the same purpose.\nMoney can be thought of as having three functions.\n\nMoney is a medium of exchange: We exchange money for goods and services, avoiding the limitations of barter.\nMoney is a measure of value, or unit of account: The value of things tend to be measured in specific currencies, enabling us to compare them against each other.\nMoney is a store of value: Money can be saved, retrieved and later exchanged. If performing this function well, it will retain its purchasing power into the future.\n\nCurrencies are the most typical way that these functions are performed in modern times. As a result, we often think of money and currency as interchangeable terms. But money could also be a precious metal or any other easily exchangeable item that can perform the these three functions. The video below illustrates this.\nThe three functions enable us to use money to:\n\nSave and borrow to rearrange our consumption over time\nInvest money in expectation of some benefit in the future\nInsure ourselves against risk\n\nOver the next three pages we will examine each of these activities."
  },
  {
    "objectID": "posts/course-notes-on-consumer_financial_decision_making.html#consumption-saving-and-borrowing",
    "href": "posts/course-notes-on-consumer_financial_decision_making.html#consumption-saving-and-borrowing",
    "title": "Course notes on Applied Consumer Financial Decision Making",
    "section": "1.2 Consumption, saving and borrowing",
    "text": "1.2 Consumption, saving and borrowing\n\n1.2.1 Consumption\nConsumption is a term in economics that refers to expenditure on consumer goods.\nIn economic models, consumption is often the primary driver of utility. People choose consumption at each point in time through their life to maximise a utility function that depends on both current consumption and future consumption.\nThe starting point for economists thinking about consumption over time is that people and households save and borrow to smooth consumption over the lifecycle. This is most famously captured in Milton Friedman’s (1957) permanent income hypothesis, which in a simplified form states that it is only changes in “permanent income” - the combination of current and all future income - that leads to changes in consumption. Permanent income might also be thought of as someone’s long-term income.\nSuppose Julia has a two period life. Utility depends on consumption today and in the future, so we might write her utility function as follows:\nU=u(C_0)+\\beta u(C_1)\nwhere U is utility, and C_0 and C_1 are consumption in the first and second periods respectively. \\beta is a discount factor (typically less than but close to one) reflecting how much Julia weights consumption in the future relative to today. The utility function u(C_t) is concave, meaning that there is diminishing marginal utility for each additional increment of consumption. This means Julia would prefer to spread her consumption across the two periods, but with a tendency for slightly more consumption today.\n\n\n1.2.2 Saving\nSaving is deferred consumption. We can save by putting money in a bank deposit account, a savings account, or stashing cash under our mattress. In economics, savings is often defined as income minus consumption.\nSuppose Julia receives $100 in salary today, but does not expect any income in the second period. She could save some of this $100, possibly receiving interest payments on her savings. This will then allow her to smooth her consumption across the two periods, giving her higher total utility.\nSavings can also take the form of the purchase of a financial asset such as shares. This is known as investing, and is discussed on the next page.\n\n\n1.2.3 Borrowing\nBorrowing is consumption brought forward. We borrow through avenues as diverse as personal loans, mortgages, credit cards, buy-now pay-later, overdrafts, and payday loans.\nSuppose Julia has no money today but will receive $100 in salary in the next period. She can borrow at a 10% interest rate between the two periods. This means that Julia could, if she wished, borrow to consume $91 today, and then pay the $91 plus $9 interest when she receives her salary in the next period. However, due to Julia’s utility function and her desire to smooth consumption, she would likely borrow around half her income, with the precise amount depending on her particular discount rate, the interest rate and the form of the utility function.\nClick here to open an external resource. Look at Figures 10.2 and 10.3a"
  },
  {
    "objectID": "posts/course-notes-on-consumer_financial_decision_making.html#investing",
    "href": "posts/course-notes-on-consumer_financial_decision_making.html#investing",
    "title": "Course notes on Applied Consumer Financial Decision Making",
    "section": "1.3 Investing",
    "text": "1.3 Investing\nInvesting is the allocation of money in the expectation of a return. People invest because investing can help to smooth consumption, or it can increase their consumption relative to other options such as saving.\nInvesting in this sense should be disambiguated from the economic concept of investment, which relates to the purchase of durable goods that are not consumed immediately.\n\n1.3.1 Risk-return trade-off\nInvesting typically involves risk. When a person invests, they often do not know the exact return they will receive. They might know the mean and variance of the returns, and in economic analysis this is often assumed to be the case. But they might not even know those.\nAs for most economic actors, investors are typically assumed to be risk averse. As a result, they require compensation in the form of higher returns for taking on risk. The greater the risk, the greater the return required.\nBefore investing in an asset, the investor will want to know the expected future return and future variance in returns. This is, of course, not observable. As a result, it is typical to estimate them from historical data. This can give the investor data points, such as the mean return and sample variance or standard deviation, which enables them to assess the risk-return trade-off (at least in an idealised world).\n\n\n1.3.2 Portfolios and diversification\nPeople often hold portfolios comprising many assets. In this case, the risk of the portfolio is not a simple average of the portfolio assets. The act of placing assets in a portfolio has the effect of eliminating some of the variability. This holding of multiple assets to reduce variability is known as diversification.\nBurton Malkiel (2020) provides an example of how this works in A Random Walk Down Wall Street, which I have summarised as follows:\n\nAn investor in an island economy has two options: a resort and an umbrella manufacturer. In sunny weather the resort earns a 50% return while the umbrella manufacturer loses 25%. In wet weather, the umbrella manufacturer delivers a 50% return, while the resort loses 25%. There is a 50:50 chance that each season will be sunny or rainy.\nAn investor in the resort would make 50% half the time and lose 25% half the time, giving an average return of 12.5%. Similarly, the umbrella manufacturer will deliver an average return of 12.5%, but with considerable volatility between the 50% gains and 25% losses. However, if the investor puts half their money in the resort and half in the umbrella manufacturer, they will earn 12.5% every season with no volatility. They have effectively eliminated risk while maintaining the same return.\n\nThis is an extreme example of the benefits of diversification, with the fortunes of the two business negatively correlated. However, to the extent there is any lack of parallelism in the fortunes of investment options, diversification can reduce risk.\nThis concept underlies modern portfolio theory, which tells investors how to achieve optimal diversification by determining the portfolio that can provide the desired return with the least risk possible. As Harry Markowitz, the founder of modern portfolio theory, is claimed to have said (although I cannot find a source), diversification is “the only free lunch in finance”."
  },
  {
    "objectID": "posts/course-notes-on-consumer_financial_decision_making.html#insuring-against-loss",
    "href": "posts/course-notes-on-consumer_financial_decision_making.html#insuring-against-loss",
    "title": "Course notes on Applied Consumer Financial Decision Making",
    "section": "1.4 Insuring against loss",
    "text": "1.4 Insuring against loss\nInsurance is a method by which an individual or entity can protect themselves against financial loss.\nThere are many different types of insurance: home and contents, health, life, auto, credit, and income protection, among others. There are other ways of insuring, such as private risk pooling and annuities. We will touch on these later in the unit.\nIn the classical economic model, people use insurance to smooth consumption across different states of the world, maximising their expected utility of consumption. If they were to suffer a major loss when they are not insured, this could result in a sharp change in consumption. As you would have noticed, smoothing is a common theme across consumer savings, borrowing, investing and insurance behaviour.\nInsurance is typically provided as a financial product by an insurer. The insured person or entity buys an insurance policy from the insurer. The insured pay a premium that entitles them to a promise from insurer to be compensated in the event of a loss that is covered by the insurance policy. The insurer collects premiums from the policy holders to cover the losses of those who experience a loss.\nInsurance benefits both the insured and the insurer. As the insured are risk averse, they are willing to pay a premium that exceeds their expected loss (the size of the loss multiplied by its probability). Insurers pool risks by insuring many people or entities. If the loss by each individual is statistically independent of the others, by the law of large numbers the average loss experienced by the insurer will be close or equal to the expected loss. The amount that the insured are willing to pay to avoid the risk thus becomes the insurer’s return on their investment.\nInsurance is only feasible in the presence of risk or uncertainty. If the insured knew they definitely would not incur the loss, they would not purchase insurance. If insurer knew the insured would definitely incur the loss, they would not insure them.\n\n1.4.1 Adverse selection\nA problem emerges when the insured and insurer have different information.\nSuppose there is a population comprising two types of person, high risk and low risk. These two types are found in equal proportions across the population. The high risk people have a 30% chance of experiencing a loss each year, while the low risk have a 10% probability of a loss. In either case, if they experience a loss event, the loss will be $100. Since there are equal numbers of each type, the expected loss of a random person in the population is $20.\nWhat if an insurer offered to insure anyone who wants insurance for $20? If no-one knew which type was which, this insurance would be attractive to both low and high-risk types and the insurer’s expected losses would equal the premiums it collects.\nBut what if the people in the population know which type they are, but the insurer doesn’t? Unless they are extremely risk averse, a $20 insurance premium is unattractive to the low risk types, who have an expected loss of only $10. They don’t buy insurance. Only the high-risk types get insured, getting a great bargain of a $20 premium to insure against their expected loss of $30. The insurer would then suffer a loss, unless it boosted premiums to $30.\nThis phenomena where only the high-risk types buy coverage, called adverse selection, was highlighted in a classic paper by Michael Rothschild and Joseph Stiglitz (1976). The problem can be pervasive. How does an insurer set life premiums for smokers and non-smokers if it can’t differentiate the two? Or good and bad drivers?\n\n\n1.4.2 Moral hazard\nWhereas adverse selection involves an information asymmetry about type, moral hazard emerges when the asymmetry involves information about the insured’s intention to take on risk. (Sometimes the distinction is described as hidden information in the first case, and hidden action in the second).\nMoral hazard is the idea that when someone is insured, they may take on greater risks because they know that they will not pay the costs. The insurer will. If their behaviour is not observable or contractable, there are constraints as to what the insurer can do about this.\nMoral hazard might be seen in risky driving, not wearing a seatbelt, taking less care on a black diamond ski run, or failing to prepare properly for the bushfire season."
  },
  {
    "objectID": "posts/course-notes-on-consumer_financial_decision_making.html#financial-literacy-capability-and-wellbeing",
    "href": "posts/course-notes-on-consumer_financial_decision_making.html#financial-literacy-capability-and-wellbeing",
    "title": "Course notes on Applied Consumer Financial Decision Making",
    "section": "1.5 Financial literacy, capability and wellbeing",
    "text": "1.5 Financial literacy, capability and wellbeing\nIn the first half of this chapter we examined the classical economic foundations to some financial activities. There the objective of the consumer is taken to be the maximisation of their expected utility.\nExpected utility maximisation is typically not the most practicable framework for assessing whether someone is achieving their financial objectives. The dominant approach in assessing financial outcomes in applied settings is financial wellbeing. How can we improve our customers’ or the population’s financial wellbeing?\nIn attempting to improve financial wellbeing, a number of intermediate objectives are often targeted, such as financial literacy and financial capability. I will define each of these below, together with financial wellbeing. These definitions are often debated - you don’t need to get into the semantics - but you should be able to distinguish between them.\n\n1.5.1 Financial literacy\nFinancial literacy is the ability to make informed judgements and to take effective decisions regarding the use and management of money. It is concerned with people’s skills and knowledge of financial information.\nThe following questions are classic questions to test financial literacy.\n\nSuppose you put $100 into a no-fee savings account with a guaranteed interest rate of 2% per year. You don’t make any further payments into this account and you don’t withdraw any money. How much would be in the account at the end of the first year, once the interest payment is made?\nImagine now that the interest rate on your savings account was 1% per year and inflation was 2% per year. After one year, would you be able to buy more than today, exactly the same as today, or less than today with the money in this account?” [More, Same, Less]\nBuying shares in a single company usually provides a safer return than buying shares in a number of different companies. [True, False]\nAn investment with a high return is likely to be high risk. [True, False]\nSuppose that by the year 2020 your income has doubled, but the prices of all of the things you buy have also doubled. In 2020, will you be able to buy more than today, exactly the same as today, or less than today with your income? [More, Same, Less]\n\nThe first three questions are classic financial literacy questions that are used in many surveys globally. Those three questions plus the latter two are currently asked as part of the Household, Income and Labour Dynamics in Australia (HILDA) Survey (Wilkins and Lass (2018)), a household-based panel study conducted annually in Australia. The questions cover: 1. numeracy, via the ability to do simple calculation involving compounding of interest rates 2. understanding of inflation 3. knowledge of diversification 4. understanding of the risk-return trade-off 5. the money illusion.\nThere is a high correlation between financial literacy and financial outcomes. But this does not mean that measures intended to target financial literacy are valuable. One study by Fernandes et al (2014) found that interventions to improve financial literacy explain 0.1% of the variance in behaviours studied, although some recent arguments are more positive (e.g. Kaiser et al (2020)). We will discuss this in further detail in a later chapter.\n\n\n1.5.2 Financial capability\nFinancial capability is the combination of knowledge, skills, attitudes and behaviours necessary to make sound financial decisions, based on personal circumstances, to improve financial wellbeing.\nFinancial capability includes financial literacy, but extends to capture attitudes and behaviours.\nMany organisations with a historic remit to improve financial literacy have broadened their scope to financial capability (including ASIC). This was in part a recognition that literacy was an overly narrow approach to improving financial wellbeing.\n\n\n1.5.3 Financial wellbeing\nMuir et al define financial wellbeing as when a person is able to meet expenses and has some money left over, is in control of their finances and feels financially secure, now and in the future. Financial wellbeing is an outcome metric. It is what interventions relating to the other concepts are trying to achieve.\nFinancial wellbeing is also essentially a subjective measure (once basic needs are met) as it is typically measured by survey, but objective outcomes are major determinants of subjective wellbeing. It is generally wise to consider both.\nThe Consumer Financial Protection Bureau (2017) defines financial wellbeing as:\n\nFinancial well-being is a state of being wherein a person can fully meet current and ongoing financial obligations, can feel secure in their financial future, and is able to make choices that allow them to enjoy life.\n\nAs an example of the types of questions in a financial wellbeing survey, the following are drawn from the CFPB Financial Wellbeing Scale (2017):\n\nThis statement describes me (completely, very well, somewhat, very little, not at all): 1. I could handle a major unexpected expense 2. I am securing my financial future 3. Because of my money situation, I feel like I will never have the things I want in life 4. I can enjoy life because of the way I’m managing my money 5. I am just getting by financially 6. I am concerned that the money I have or will save won’t last"
  },
  {
    "objectID": "posts/course-notes-on-consumer_financial_decision_making.html#why-are-financial-decisions-so-hard",
    "href": "posts/course-notes-on-consumer_financial_decision_making.html#why-are-financial-decisions-so-hard",
    "title": "Course notes on Applied Consumer Financial Decision Making",
    "section": "1.6 Why are financial decisions so hard?",
    "text": "1.6 Why are financial decisions so hard?\nAs this discussion of financial wellbeing and other concepts shows, a person’s behaviour is a major influence on their financial wellbeing.\nThere is substantial evidence that people’s behaviours and decisions harm their financial wellbeing. One reason for this is that we make decisions in ways that can lead to errors or suboptimal outcomes. That is the major focus of the next chapter.\nBut before we discuss how we make decisions, we should also note that financial decision making is particularly hard. Some of the reasons for this follow (drawn from Erta et al (2013)).\nFinancial decisions involve trade-offs between the present and the future. This delay requires us to solve conflicts between present and future selves, and to determine how to trade-off consumption today with the investment returns that could increase consumption in the future.\nFinancial decisions involve risk and uncertainty. We often do now know what will happen in the future, nor even the spectrum of possible outcomes from which that future could be drawn.\nFinancial products are inherently complex. Financial products in the marketplace have many more features that the basic elements required to save, borrow, invest or insure. Their precise form has often emerged over decades of competition in imperfect markets between financial institutions. For example, savings accounts may not just pay interest, but may also have conditions to achieve that interest, tiered interest rates based on your balance, interest rate caps, and honeymoon interest rates on opening an account. A bank is more likely to offer nine credit cards than one.\nFinancial decisions can involve emotions, such as fear of loss, or regret. We know that financial decisions can have major effects on our life outcomes, so we fear making the wrong one. The wellbeing of of family and loved ones can hinge on these decisions.\nMany financial decisions provide little chance to learn. Many of our most important decisions are one-off decisions with outcomes only known or experienced after a long delay, such as a decision about how to invest for retirement, or whether to purchase a house. If we make a poor decision, we often do not know until it is too late (assuming we ever realise)."
  },
  {
    "objectID": "posts/course-notes-on-consumer_financial_decision_making.html#financial-literacy-1",
    "href": "posts/course-notes-on-consumer_financial_decision_making.html#financial-literacy-1",
    "title": "Course notes on Applied Consumer Financial Decision Making",
    "section": "2.1 Financial literacy",
    "text": "2.1 Financial literacy\nMany people have low financial literacy.\nIn the 2018 HILDA survey, 42.5% of participants got all five of the financial literacy questions you answered in the first chapter correct (Wilkins and Lass (2018)). The proportion of correct responses for each question was:\n\nNumeracy: 85.5%\nInflation: 69.8%\nDiversification: 74.9%\nRisk-return: 83.5%\nMoney illusion: 77.0%\n\nRemember that all except the numeracy question were multiple choice.\nThe Australian Financial Attitudes and Behaviours Tracker, a periodic survey run by ASIC, consistently finds that only one-third of respondents have heard of and understand the risk-return trade-off. Only 40% have heard of and understand the concept of diversification (Australian Securities and Investments Commission and EY Sweeney (2018)[https://financialcapability.gov.au/files/afab-tracker_wave-6-key-findings.pdf]).\nYou can see how misunderstanding of these concepts could affect borrowing, savings, investment and insurance decisions. To determine the benefits of savings or costs of borrowing, you need base numeracy and need to understand inflation. Diversification is a core principle to achieving investment returns at lower risk.\nThere is considerable research demonstrating a correlation between financial literacy and financial wellbeing, as well as other financial outcomes (for example, see Lusardi and Mitchell (2014)). Financial literacy is correlated with day-to-day financial management skills, financial market participation and investment, the holding of precautionary savings, planning for retirement, cheaper mortgages, more regular refinancing of debt, and lower transaction costs, among other things.\nHowever, the evidence of a causal relationship between the financial literacy and financial outcomes is debated. We will discuss this debate in a later chapter."
  },
  {
    "objectID": "posts/course-notes-on-consumer_financial_decision_making.html#time-preference",
    "href": "posts/course-notes-on-consumer_financial_decision_making.html#time-preference",
    "title": "Course notes on Applied Consumer Financial Decision Making",
    "section": "2.2. Time preference",
    "text": "2.2. Time preference\nSaving, borrowing, investment and insurance decisions all involve intertemporal choice. Decisions are made at one point in time, with effects at another. The timing of the costs and benefits do not align.\nPeople discount future costs and benefits. They prefer to receive benefits earlier, rather than later, and prefer to incur costs later rather than earlier.\nWe have already encountered discounting, where Julia had the utility function:\nU=u(C_0)+\\beta u(C_1)\n0 \\leq \\beta \\leq 0\nwhere U is utility, and C_0 and C_1 are consumption in the first and second periods respectively. \\beta is a discount factor reflecting how much Julia weights consumption in the future relative to today.\n\n2.2.1 Exponential discounting\nExponential discounting occurs where future costs and benefits are discounted at a consistent rate through time. The following equation is an example of exponential discounting.\nU=\\sum_{t=0}^{t=T}\\delta^t u(C_t)\n0 &lt; \\delta \\leq 1\nThe degree of discounting in this equation evolves over time as 1, \\delta, \\delta^2, \\delta^3, \\delta^4 and so on. This results in a smooth decline in present value over time. Decisions made with exponential discounting are consistent over time.\n\n\n2.2.2 Present bias\nPresent bias occurs when we place additional weight on costs and benefits at the present time. One simple model of present bias is the quasi-hyperbolic discounting model. It is a discrete time version of hyperbolic discounting.\nU=u(C_0)+\\sum_{t=1}^{t=T}\\beta\\delta^t u(C_t)\nThe degree of discounting in this equation evolves over time as 1, \\beta\\delta, \\beta\\delta^2, \\beta\\delta^3, \\beta\\delta^4 and so on. This progression results in a larger discount for the first period of delay (\\beta\\delta) than the degree of discount for each subsequent period of delay (\\delta). There is a relative weighting toward the present.\nPresent bias of this nature can result in time inconsistency, with decisions at one point reversed at another if the decision maker is given the opportunity."
  },
  {
    "objectID": "posts/course-notes-on-consumer_financial_decision_making.html#prospect-theory",
    "href": "posts/course-notes-on-consumer_financial_decision_making.html#prospect-theory",
    "title": "Course notes on Applied Consumer Financial Decision Making",
    "section": "2.3 Prospect theory",
    "text": "2.3 Prospect theory\nProspect theory (Kahneman and Tversky (1979)) is a descriptive theory of the decisions people make when faced with a gamble. It is not a theory of how people make decisions, nor a theory of how they should make decisions.\nYou have covered prospect theory and its components in earlier units. Here is a recap.\n\n2.3.1 Reference dependence\n\n\nYou have not checked your share portfolio in a while. You expect it is worth around $40,000. Today when you check, it is worth $30,000. Do you feel rich or poor?\nYou have not checked your share portfolio in a while. You expect it is worth around $20,000. Today when you check, it is worth $30,000. Do you feel rich or poor?\n\n\nPeople assess choices based on their reference point - where they currently are - as opposed to an overarching assessment of their position. Potential outcomes are coded as losses and gains relative to that reference point.\nReference points can be thought of a state to which you have become adapted.\n\n\n2.3.2 Loss aversion\n\nYou are offered a gamble on the toss of a coin. If you flip a heads, you lose $100. If you flip a tails, you win $150. Do you accept the gamble?\n\nLoss aversion is the concept that losses loom larger than gains. People feel a loss more strongly about a loss than they do an equivalent gain, so are often willing to reject gambles with a materially positive expected value.\nRejection of bets of this nature cannot easily be explained by risk aversion. As shown by Rabin (2000), rejection of bets over moderate stakes requires absurd rates of risk aversion. For instance, if a person who acts consistent with expected utility theory always turns down a 50:50 bet to win $110 or lose $100 whatever their initial level of wealth, they will also turn down a 50:50 bet to win $1 billion, lose $1,000.\n\n\n2.3.3 Reflection effect\nKahneman and Tversky (1984) reported the following experiment:\n\nImagine that the U.S. is preparing for the outbreak of an unusual Asian disease, which is expected to kill 600 people. Two alternative programs to combat the disease have been proposed. Assume that the exact scientific estimates of the consequences of the pro-grams are as follows:\nIf Program A is adopted, 200 people will be saved.\nIf Program B is adopted, there is a one-third probability that 600 people will be saved and a two-thirds probability that no people will be saved.\nWhich of the two programs would you favor?\n\nIn the initial experiment, 72% of participants chose option A.\nBut what if the experimental participants were presented with the following options?\n\nIf Program C is adopted, 400 people will die.\nIf Program D is adopted, there is a one-third probability that nobody will die and a two-thirds probability that 600 people will die.\n\nOnly 22% chose option C, despite it being equivalent to option A.\nThis phenomena is the reflection effect. When people make a risky choice related to gains, they are risk averse. They prefer a certain option with lower expected utility than the expected utility of the risky choice. When making a choice in the loss domain, they become risk seeking.\nThis phenomena might also be thought of as diminishing sensitivity to gains or losses in either direction. This contrasts with expected utility theory where the pain of losses increases as they grow in size.\nThe combination of loss aversion and the reflection effect results in the famous value function as in the following diagram:\n\n\n\n2.3.4 Probability weighting\n\n\nYou are granted entry into a prize draw that that gives you a 5% probability of gaining $10,000. How do you feel?\nYou are granted an additional entry into a prize draw for $10,000 that increases your probability of winning from 5% to 10%. How do you feel?\nYou are granted an additional entry into a prize draw for $10,000 that increases your probability of winning from 50% to 55%. How do you feel?\nYou are granted an additional entry into a prize draw for $10,000 that increases your probability of winning from 95% to 100%. How do you feel?\n\n\nFrom the perspective of expected utility theory, each of these four scenarios results in the same expected gain of $500. But they often feel markedly different.\nPeople overweight small probabilities, giving them disproportionately more weight than they deserve. They also underweight large probabilities that fall short of certainty, giving them less weight than is justified. This results in the shifts in scenarios 1 and 4 above generally being received more gratefully than that in scenarios 2 and 3.\nKahneman (2011) calls the large psychological value of the change from 0 to 5% (or some other small probability) the possibility effect. Very unlikely but possibles outcomes are given more weight than similar increases in probability for events that are already possible. He calls the large psychological value of the change to 100% the certainty effect. We will pay a lot more for certainty than near certainty.\nThis pattern of probability weighting can be seen in the following diagram, where the probability p on the x axis is mapped to a new probability weight on the y axis.\n\nFigure 1, Tversky and Kahneman (1992)\n\n\n2.3.5 Fourfold pattern of risk attitudes\nProspect theory results in a four-fold pattern of risk attitudes, as shown in this table. For moderate to high probability gambles, the reflection effect dominates and people are risk averse in the domain of gains and risk seeking in the domain of losses.\nBut for low probability gambles, the probability weighting shifts the decision calculus. The possibility of a gain is overweighted, making the gamble attractive and inducing risk seeking behaviour. A similar effect occurs for a low probability of loss, with the overweighted probability making the potential loss less attractive, inducing risk averse behaviour.\n\n\n\n\n\n\n\n\n\nGains\nLosses\n\n\n\n\nHigh probability (certainty effect)\nRisk aversion\nRisk seeking\n\n\nLow probability (possibility effect\nRick seeking\nRisk aversion\n\n\n\n\n\n2.3.6 Optional reading\nRabin and Thaler (2001) “Anomalies: Risk Aversion”, Journal of Economic Perspectives, 15(1), 219-232, https://doi.org/10.1257/jep.15.1.219 (For a readable presentation of Rabin’s calibration theorem)"
  },
  {
    "objectID": "posts/course-notes-on-consumer_financial_decision_making.html#mental-accounting",
    "href": "posts/course-notes-on-consumer_financial_decision_making.html#mental-accounting",
    "title": "Course notes on Applied Consumer Financial Decision Making",
    "section": "2.4 Mental accounting",
    "text": "2.4 Mental accounting\n\nImagine we just spent $100 for a ticket to the hottest new Broadway show. It’s a musical combining potty-mouthed Muppets, sassy superheroes, Founding Fathers and high school hijinks. When we arrive at the theatre on opening day, we look in our wallet and discover to our horror that we’ve lost the ticket. Luckily, we have another $100 note in our wallet. Would we buy another ticket? When people are asked this question, the vast majority say no. After all, they’ve spent the money on the ticket, the ticket is lost, and that’s just too bad. Now, if we ask people to imagine that they went ahead and bought a replacement ticket, how much would they say that night of theatre cost them? Most people say the experience cost them $200 – the combined cost of the first and the second ticket.\nNow imagine things went differently on the day of the show. We didn’t buy a ticket in advance, but we’re still just as excited about the production. When we arrive at the theatre, we open our wallet and realize we lost one of the two crisp $100 notes we had in there. Oh, no! We are now $100 poorer. Luckily, we still have another $100 note. Oh, yes! So, would we buy the ticket or just go home? In this case, the clear majority of people say they would buy the ticket. After all, what does losing a $100 note have to do with not going to the theatre? And, if like most people, we were to go ahead and get the ticket, how much would we feel we’d paid for it? In this case, the most common answer we get is $100.\nAriely and Kreisler, Dollars and Sense: Money Mismaps and How to Avoid Them\n\nDoes this behaviour make economic sense?\nThe behaviour of those who will not buy a replacement ticket in the first instance, but will in the second, involves mental accounting. Mental accounting was named by Richard Thaler (1985), who described several different ways that we form mental accounts. These include putting labels on different pots of money, and creating mental accounts that are linked to a topic or temporary occasion.\nIn the case of our potential Broadway Show attendees, $100 has already been spent in the entertainment account. They are not willing to increase their expenditure to $200. In the second, nothing has yet been spent on the entertainment account. The loss of the $100 note does not change that, so they are willing to increase their expenditure in that account to $100.\nMental accounting provides a hook for the application of prospect theory. Gains and losses are assessed within mental accounts. The reference point is shaped by the mental account, not their entire financial position."
  },
  {
    "objectID": "posts/course-notes-on-consumer_financial_decision_making.html#attention-and-memory",
    "href": "posts/course-notes-on-consumer_financial_decision_making.html#attention-and-memory",
    "title": "Course notes on Applied Consumer Financial Decision Making",
    "section": "2.5 Attention and memory",
    "text": "2.5 Attention and memory\n\n[I]n an information-rich world, the wealth of information means a dearth of something else: a scarcity of whatever it is that information consumes. What information consumes is rather obvious: it consumes the attention of its recipients. Hence a wealth of information creates a poverty of attention and a need to allocate that attention efficiently among the overabundance of information sources that might consume it.\nSimon (1971) Designing Organizations for an Information-Rich World\n\nMost economic analysis contains the implicit assumption that we make decisions using all information that is freely available to us. But think about the last purchase you made. What did you pay attention to? The price? Its quality? Any other features of the product? How this purchase could inform future choices? Your future income? The interest rate or potential gain from investing the money?\nWe have limited attention. This is likely to reflect the cognitive costs of applying greater attention, cognitive constraints on our ability to process the information, or in some cases a strategy for making better decisions (more on that below).\nAs our attention is limited, the task is often to attract it. Simon noted that many designers build systems as through the problem is information scarcity, rather than attention scarcity. Instead, we need systems that excel at filtering information and providing the most important information at the right time.\nRelated to our limited attention, we also have limited memory.\nShort-term memory is that capacity for holding information in mind in a readily available state. If someone gave you a phone number that you were to immediately dial, this would involve short-term memory. Short-term memory is constrained. It is often measured through memory span tests, such as asking someone to recall a sequence of digits they have just heard. By that measure, short-term memory can typically hold around 4$$1 digits or “chunks”.\nRelated to (and often considered part of) short-term memory is working memory. Working memory involves the manipulation of stored information. Like short-term memory, it is constrained.\nLong-term memory involves the indefinite storage of knowledge. Our long-term memory is incomplete, is highly selective, and fades with time. Further, it changes over time, and can be changed through the act of recall.\nConstrained long-term memory and recall is a foundation of the availability heuristic. People tend to weight their judgements toward more recent terms or concepts that are readily available in memory. In determining the probability or frequency of an event, the more available events will be assessed as more probable. For example, when asked about the relative frequency of words starting with the letter K compared to those with K as the third letter, people assume relatively more of the former as words starting with K are easier to recall.\nOur lack of attention and memory are a factor behind the success of techniques such as reminders to change people’s decisions or behaviour. Simple strategies such as text message have been found to improve outcomes such as increasing attendance at appointments, reducing missed credit card payments and reducing re-offending.\n\n2.5.1 Less is more\nWhile limited attention and memory is typically thought of as a constraint and source of error, in some instances it might support better decision making. Often, “less is more”, in that there is a beneficial degree of ignorance, or benefits to excluding information from consideration. For example, incomplete memory might lead to better learning of language (Elman (1993)).\nSimilarly, most machine learning techniques try to reduce the scope of the variables to which the algorithm pays attention to avoid overfitting. Overfitting is an over-sensitivity to the observed data in developing a model. The inclusion of every detail helps the model match the observed data, but prevents generalisation to new situations. Complex strategies can explain too much in hindsight. In an uncertain world where only part of the information is useful for the future, a simple rule that focuses on only the best or a limited subset of information has a good chance of hitting that useful information and less chance of incorporating irrelevant information.\nThe most common explanation for less-is-more effects is the bias-variance trade-off."
  },
  {
    "objectID": "posts/course-notes-on-consumer_financial_decision_making.html#scarcity",
    "href": "posts/course-notes-on-consumer_financial_decision_making.html#scarcity",
    "title": "Course notes on Applied Consumer Financial Decision Making",
    "section": "2.6 Scarcity",
    "text": "2.6 Scarcity\n\n2.6.1 What is scarcity?\nMani et al (2013) report an experiment in which people in a New Jersey mall were presented with hypothetical scenarios such as the following:\n\nImagine that your car has some trouble, which requires a $300 service. Your auto insurance will cover half the cost. You need to decide whether to go ahead and get the car fixed, or take a chance and hope that it lasts for a while longer. How would you go about making such a decision? Financially, would it be an easy or a difficult decision for you to make?\n\nAfter the scenario, they were given series of Raven’s Matrices problems, a test of fluid intelligence. When the results were analysed by whether the experimental subjects were rich or poor, there was no difference in performance on the Raven’s Matrices problems.\nHowever, when the scenario was tweaked such that the car trouble “requires an expensive $3,000 service”, a gap between the rich and poor emerges. The rich subjects did just as well on the Raven’s Matrices after being told they would require an expensive service as for the $300 service. But the poor scored lower, an effect equivalent to as decline of between 13 and 14 IQ points. This is larger than the effect that would be expected from missing a full night’s sleep.\nMani et al found a similar effect in a field study involving sugarcane farmers in India. The farmers were given cognitive tests before harvest, when they face considerable financial pressure, and post-harvest. Those farmers showed diminished cognitive performance before harvest compared to after harvest.\nThis effect has been branded scarcity (For example, see Mullainathan and Shafir (2013)). People have limited cognitive capacity. The poor must manage sporadic income and expenses that they may not be able to meet. Even when they are not making a financial decision, these issues may preoccupy their minds. These preoccupations consume cognitive resources, leaving less “bandwidth” available for decision making.\n\n\n2.6.2 The consequences of scarcity\nShah et al (2012) examined the consequences of scarcity across a set of lab experiments. When participants were “poor”, in that they were given a lower endowment of shots in a computer game, they tended to use the shots well and score more points per shot than the “rich”. However, when given the opportunity to “borrow” shots from later rounds, they tended to overborrow and degrade their overall performance. Similar effects were found when they could borrow time in a trivia game. The poor overborrowed.\nShah et al argued that scarcity elicits greater engagement, which can be a good thing, as evidenced by the better usage of shots by poor participants in the computer game. However, focus on some problems leads to neglect of others, such as neglect of the future costs of borrowing.\n\n\n2.6.3 Robustness and replication\nThe concept of scarcity has been subject to considerable debate. Wicherts and Scholtern (2013) argued that the Mani et al (2013) results were only achieved because income was bifurcated into “rich” and “poor” rather than treated as a continuous variable. Mani et al (2013a) resurrected their effect by pooling three experiments, although this does raise questions about the robustness.\nCamerer et al (2018) reported a replication of experiment 1 in Shah et al, and found no effect. This led [S]hah et al (2019)](https://doi.org/10.1016/j.joep.2018.12.001) to conduct a replication across all of the experiments in their original paper, confirming the failure to replicate the first experiment, but finding most of the others did replicate.\nFinally, Carvalho, Meier and Wang (2016) examined cognitive function, risk preferences and time preferences in low-income households before and after payday. They found an effect on time preference when considering monetary rewards, but no effect on cognitive function, risk taking or the quality of decision making."
  },
  {
    "objectID": "posts/course-notes-on-consumer_financial_decision_making.html#consumption-and-saving",
    "href": "posts/course-notes-on-consumer_financial_decision_making.html#consumption-and-saving",
    "title": "Course notes on Applied Consumer Financial Decision Making",
    "section": "3.1 Consumption and saving",
    "text": "3.1 Consumption and saving\n\nLast New Year’s day, after a long evening of rooting the right team to victory in the Orange Bowl, I was lucky enough to win $300 in a college football betting pool. I then turned to the important matter of splurging the proceeds wisely. Would a case of champagne be better than dinner and a play in New York? At this point my son Greg came in and congratulated me. He said, “Gee Dad, you should be pretty happy. With that win you can increase your lifetime consumption by $20 a year!” Greg, it seems, had studied the life-cycle theory of savings.\nRichard Thaler (1990)\n\n\n3.1.1 Observed low levels of smoothing\nDespite the economic theory suggesting that people will smooth their incomes over their lifecycle, the observed level of smoothing is low. Consumption responds strongly to both unexpected and predictable changes in income. The marginal propensity to consume within year is typically 50-75%. That is, if someone has an unexpected windfall one year, they will tend to consume 50% to 75% of that windfall within that year, rather than saving it.\nSuppose a patient person anticipates a regular fixed income of $10 a month during their one year of life. In this world there is no inflation or interest paid on borrowing or savings. Further imagine that they received a surprise windfall in August. Someone who perfectly smoothed consumption would spread that surprise over the remaining months of their life.\n\nWhat we tend to see instead is this - a large spike in consumption at the time of the surprise, with only some of the windfall smoothed over coming months.\n\nThis lack of smoothing is observed in relation to many major life events. For example, one US study found that when households reach end of unemployment benefits, which in the US have a predictable end date, consumption falls by 13% at that end. Consumption also tends to fall with income at retirement.\nThe effect of windfalls on our consumption path depends heavily on whether they relate to liquid assets or not. (A liquid asset is one that can readily be converted to cash. Cash is, of course, highly liquid. A house is illiquid as it takes considerable time and effort to convert.) We blow windfall gains of cash, but not windfalls of less liquid assets. For instance, if there is a large increase in share value, we tend not to spend it. But if a company takeover delivers a cash payment, we will tend to spend it rather than smooth consumption of that payment over our lifetime.\n\n\n3.1.2 Lifetime savings\nIn conjunction with this lack of consumption smoothing, households do not tend to accumulate substantial liquid assets over their lifetime. However, they do accumulate substantial illiquid assets.\nIn Australia, two of the most prominent illiquid assets are housing and superannuation account balances. Liquid assets comprise only around 15% of total household wealth (with less than 2% of that liquid wealth is held by the least wealthy half of households) (Adams et al (2020))."
  },
  {
    "objectID": "posts/course-notes-on-consumer_financial_decision_making.html#rational-explanations",
    "href": "posts/course-notes-on-consumer_financial_decision_making.html#rational-explanations",
    "title": "Course notes on Applied Consumer Financial Decision Making",
    "section": "3.2 Rational explanations",
    "text": "3.2 Rational explanations\nThere are a number of rational explanations for the lack of observed consumption smoothing. Below are three.\n\n3.2.2 Liquidity constraints\nThe first relates to liquidity, in that people are unable to sell claims to their future labour income or borrow substantial sums in expectation of its receipt. They cannot simply access the net present value of their lifetime earnings and consume smoothly through time. They have access to less liquidity than the net present value of future earnings.\nConsider the increase in lifetime income you could obtain by completing this course. Could you now go to a bank, tell them about this great course you are completing and how it will affect your future income, and then borrow on the basis of that expectation?\nAs they cannot access future income growth, people increase their consumption as their income grows.\nThis explanation, however, does not adequately explain the size of the co-movement between income and consumption unless they are highly impatient. But that level of impatience would not accord how much we do accumulate assets. People often accumulate substantial illiquid assets over their life. The explanation does not cover the full range of behaviour that we see.\n\n\n3.2.3 Dependants\nThe cost of child rearing often peaks at the same time as earnings (think private school fees). Further, there is little evidence of changes in household consumption as children leave the house, implying that per person consumption increases. This suggests the alignment of child rearing and the peak of earnings may just be coincidence.\n\n\n3.2.4 Durables\nPeople purchase many “durables” during their lives. These are lumpy purchases that do not quickly wear out and provide utility over time. They are not “consumed” in one use.\nCars and houses are durables. Household goods such as furniture are also durables.\nEconomists often use expenditure as a proxy for consumption. Durables can make expenditure lumpy (not smoothed) even though the durable good’s consumption occurs over time (is smooth).\nThere is evidence to support this argument at the micro-level, in that payments such as rent often occur in alignment to pay cycles. On, say, a fortnightly or monthly basis, expenditure does not appear smoothed, whereas consumption is.\nHowever, when we examine the empirical data, durable purchases also do a poor job of explaining the lack of consumption smoothing over a person’s full lifetime."
  },
  {
    "objectID": "posts/course-notes-on-consumer_financial_decision_making.html#psychological-explanations",
    "href": "posts/course-notes-on-consumer_financial_decision_making.html#psychological-explanations",
    "title": "Course notes on Applied Consumer Financial Decision Making",
    "section": "3.3 Psychological explanations",
    "text": "3.3 Psychological explanations\nThere are also many explanations for the lack of consumption smoothing based on consumer psychology. Below are three.\n\n3.3.1 Present bias\nA prominent explanation of the lack of consumption smoothing is present bias.\nRecall from page 2.2 that present bias (in a quasi-hyperbolic model) involves an immediate discount for any delay at all (\\beta) on top of the regular exponential discount function.\nThe immediate discount of \\beta generates a distinction between the treatment of liquid and illiquid assets. An illiquid asset is impossible or costly to access for immediate consumption, so any consumption of the illiquid asset is always subject to a delay and a minimum discount of \\beta, making is less attractive. Liquid assets such as cash are on hand for consumption now.\nSomeone with high present bias (i.e. \\beta substantially below one) will have trouble holding any liquid assets, but could accumulate substantial illiquid assets if they do not have a high rate of exponential discounting (i.e. \\delta is close to one).\nA related concept is myopia, whereby people consider their income over a limited horizon. This was a feature of Friedman’s (1957a) original model of consumption smoothing. If people only think about their income for, say, the next three years, you will see some smoothing. But that smoothing will be limited compared to changes in income and consumption over the complete lifecycle.\n\n\n3.3.2 Mental accounting\nMental accounting provides a potential explanation for the differences in consumption based on where income came from and what bucket it is currently in. For instance, windfall gains may be in a different mental account to a pay rise, and consumed differently.\nOne mental account may be for wealth saved for retirement. US data suggests that the medium-term (6-month) marginal propensity to consume out of retirement accounts is effectively zero. The medium marginal propensity to consume out of a transaction account is close to one.\nMental accounts can also be defined around categories of expenditure. For example, money in form of shopping coupons increases shopping more than would be predicted by consumption smoothing. The value of the coupon is not spread over all expenditures.\nMental accounting has some similarity to the liquidity explanation, but in the case of mental accounting it is a self-imposed category or rule. Liquidity constraints are external or natural features of the asset.\n\n\n3.3.3 Reference point models\nUnder prospect theory, utility is measured from a reference point. This reference point might be expectations for current consumption, which means that changes in consumption relative to expectations could generate (or cause the loss of) utility (Kőszegi and Rabin (2006)).\nSuppose today I get to consume five pieces of chocolate. If I had previously expected to consume four pieces, this could generate extra utility. However, if I had previously expected to consume six, this would be painful. In fact, due to loss aversion, it would be more painful than the equivalent pleasant surprise.\nThis concept can lead to over-consumption, under-savings and high levels of co-movement between income and consumption (if you calibrate the model with certain parameters). For instance, a windfall in income today could be used to markedly increase consumption above expectations, giving a person utility both from the consumption itself and the pleasant surprise. Any consumption shifted into the future would not generate a pleasant surprise on the day of consumption, as by that time it would be expected.\nA person’s reference point may also be the consumption of others. Bertrand and Morse (2016) argued that consumption among rich households had induced those at lower income to consume a larger share of their income."
  },
  {
    "objectID": "posts/course-notes-on-consumer_financial_decision_making.html#a-case-study-earnd",
    "href": "posts/course-notes-on-consumer_financial_decision_making.html#a-case-study-earnd",
    "title": "Course notes on Applied Consumer Financial Decision Making",
    "section": "3.4 A case study: Earnd",
    "text": "3.4 A case study: Earnd\nMost Australian employees are paid in arrears. They accrue pay while they work, which is paid on a regular cyclical basis after its accrual.\nFor example, suppose you are paid fortnightly, with your next pay on Thursday August 27. On that day, you are paid for your labour since the last payday on 13 August; that is, you are paid for your labour from August 13 to August 26.\nSydney-based fintech Earnd has developed an app that integrates into employer payroll systems and enables employees to see what they have earned at any point during their pay period. They would be able to see at the end of August 14 that they had accrued two days pay.\nEarnd enables employee access to a proportion of that accrued income in advance of payday. This enables them to smooth their consumption and meet any unexpected expenses, a major source of financial stress.\nEarnd believes that the value proposition to employers is employee retention. Employers incur a cost by making pay available early (as they normally can accrue interest or reduce borrowing costs by keeping it until payday), but that is outweighed by reduced recruitment and training costs.\n\n3.4.1 Discussion\nConsider Earnd from the perspective of what we have discussed in this unit to date. How might employees use Earnd? How might Earnd affect savings and consumption? Why?"
  },
  {
    "objectID": "posts/course-notes-on-consumer_financial_decision_making.html#credit-cards",
    "href": "posts/course-notes-on-consumer_financial_decision_making.html#credit-cards",
    "title": "Course notes on Applied Consumer Financial Decision Making",
    "section": "4.1 Credit cards",
    "text": "4.1 Credit cards\nCredit cards present three puzzles that a traditional economic framework has difficulty resolving.\n\nPeople borrow far more on credit cards that you would expect from exponentially discounters\nPeople fail to choose credit cards with the lowest borrowing costs\nPeople hold both high-cost credit card debt and liquid assets that earn low rates of return.\n\n\n4.1.1 Excessive borrowing\nAlthough it is possible to justify borrowing at any interest rate if income is sufficiently lumpy, the levels of observed credit card debt are hard to justify. In particular, the impatience required to justify the high levels of credit card debt does not reconcile with the patience required to justify the savings in illiquid assets such as housing and retirement accounts.\nPresent bias provides one possible explanation. As noted in the discussion of savings, illiquid savings are hard to access immediately, and so their potential consumption is substantially discounted by someone with high present bias. This enables savings of illiquid assets. However, consumption using a credit card suffers no such discount. It can occur immediately. Meier and Sprenger (2010) found that more present biased individuals were more likely to have credit card debt and had higher levels of debt.\n\n\n4.1.2 Poor card choices\nThe explanation of present bias is, however, incomplete, as demonstrated by another puzzle. People don’t choose the credit card with the lowest borrowing costs.\nAt least a part of this relates to customers being attracted by teaser rates, which they pay more attention to than the long-term rates they will end up paying.\nCustomers also exhibit poor understanding of exponential growth and how a credit card debt can compound over time. (Recall the compounding question that formed part of the financial literacy test.) Poor understanding of compounding can lead to an underestimation of the cost of high interest rates.\n\n\n4.1.3 Co-holding debt and savings\nPeople often hold both high-cost credit card debt and liquid assets that provide low rates of return. In one UK survey, 12% of households in the sample held an average of $$3800 in revolving credit on which they incurred interest charges, while at the same time holding liquid assets that they could use to clear all of this debt (Gathergood and Weber (2014)).\nOne rational explanation for co-holding is that that some expenses must be paid by cash or direct debit, not credit card. This requirement means that funds must be available in these forms.\nAn alternative explanation is that co-holding is a self-control strategy. By reducing the amount of unused credit capacity, it may reduce future spending. (Note the use of mental accounts here.)\nOne shortfall with these explanations is that people who hold multiple cards do not minimise costs when using the cards they have. They pay little attention to relative interest rates when choosing which card to use. They don’t repay the card with highest interest rate first."
  },
  {
    "objectID": "posts/course-notes-on-consumer_financial_decision_making.html#payday-loans",
    "href": "posts/course-notes-on-consumer_financial_decision_making.html#payday-loans",
    "title": "Course notes on Applied Consumer Financial Decision Making",
    "section": "4.2 Payday loans",
    "text": "4.2 Payday loans\nRelative to credit cards, payday loans charge a higher rate of interest, with short-term charges implying huge annual costs.\nPayday lending has been subject to much regulatory and legislative action in Australia in recent years. Since 2012, payday loan interest and fees have been legislatively capped. The caps are:\n\nEstablishment fee of 20% of the amount borrowed\nMaximum monthly fee of 4% of the amount borrowed\nDefault fees up to a maximum of double the amount you borrowed\nCan also pass on government fees and charge missed payment fees and enforcement expenses\n\nEven though capped, this structure can lead to very high interest rates, particularly when considered on an annual basis. Consider a one month loan with the establishment and monthly fee. That is effectively 24% interest for one month!\n\n4.2.1 Harm to consumers\nThere is an active academic debate on whether payday loans are helpful or harmful.\nOn the evidence of harm, people tend to use payday loans even though less expensive options are available. Bertrand and Morse (2011) showed that better disclosure marginally reduces take-up, suggesting payday loan use is at least partly due to misunderstanding the terms or consequences of the loan. (We will tackle disclosure in more detail in chapters 8 and 9.) Further, the debt burden created by payday loans can lead to a debt spiral that harms the ability to cover basic financial needs.\nAn important consideration, however, is the counterfactual of whether the harm would occur in the absence of the payday loans. Bhutta et al (2016) found evidence that, when payday lending is banned, people shift to other forms of high-interest credit rather than shifting back to traditional credit instruments. This may suggest that constraints to payday lending are addressing the symptom rather than the cause.\n\n\n4.2.2 Who uses payday lenders?\nPayday loan use is linked to low self control and low financial literacy.\nGathergood (2012) examined payday loan use in a survey sample where self control was measured by self-reported agreement with statements such as “I am impulsive and tend to buy things even when I can’t really afford them.” He found a that those with low self-control were more likely to use payday loans, although there were various mechanisms by which this occurred. Low self-control people had more income shocks. They were more likely to have other sources of credit withdrawn. They had more unforeseen durable expenses. All of these could trigger a need for high-cost short-term credit.\nYou can think about the low self-control in terms of present bias. Payday lending attractive is presence of high \\small \\beta; that is, a large discount for any delay. However, the variety of mechanisms by which payday loans are required suggests we require a richer story than high present bias.\nAs for credit cards, financial literacy may also play a role. Payday lender users score poorly on tests of financial literacy. Lusardi and Scheresberg (2013) found that those with high financial literacy (measured by answering each of the numeracy, inflation and diversification questions) were around 5 percentage points less likely to use a payday lender (20% compared to 25% across the full sample)."
  },
  {
    "objectID": "posts/course-notes-on-consumer_financial_decision_making.html#mortgages",
    "href": "posts/course-notes-on-consumer_financial_decision_making.html#mortgages",
    "title": "Course notes on Applied Consumer Financial Decision Making",
    "section": "4.3 Mortgages",
    "text": "4.3 Mortgages\nThe major source of household credit in Australia is the mortgage. Mortgages comprise over 90% of household credit.\nBelow we examine two features of the Australian mortgage market: the difficulty in comparing loans, and the “loyalty tax” paid by those who stay with their home loan lender.\n\n4.3.1 Comparing loans\nAustralian banks tend to advertise a headline variable interest rate for their mortgage products. Yet almost 90% of customers of the big four banks receive a discount from that rate. This can include advertised discounts that they receive when obtaining the loan, and discretionary discounts that are given during the application process or after disbursement of the loan. The Australian Competition and Consumer Commission (2020) found that, as at 31 October 2019, the average discount on the headline variable rate for standard owner-occupier loans was between 1.23% and 1.31% for each of the four major banks.\nThe advertisement of rates that are not the rate paid means that interest rate comparison is weakly informative when shopping for a loan. And people tend not to do much shopping around. For instance, Australian Securities and Investments Commission (2019) research found that 38% of mortgage customers visited only one mortgage provider (be that a lender or broker, but typically their existing financial institution), with another 26% visiting only two (typically their existing financial provider plus on other).\nResearch in the United States has highlighted the costs of failing to search for the best rate. Gurun et al (2016) found the difference between the 5th and 95th percentile adjustable rate mortgage interest rate within a geographic region was 3.1 percentage points, and that was after accounting for borrower and loan characteristics.\n\n\n4.3.2 Punishing loyalty\nThe Australian Competition and Consumer Commission (2020) found that existing borrowers pay around 0.26% interest more for their loan than new customers (as at 30 September 2019). If the existing loan is more than five years old, they are paying 0.40% more than what big four bank new customers are paying. As an estimate of the associated costs, those customers of more than 5 years had loans averaging $200,000. If they refinanced, they could save around $850 in the first year. Given these customers typically have lower loan balances and the lender knows the reliability of their repayment history, this difference in rate is hard to justify on basis of pricing for risk.\n\n\n4.3.3 Explaining these phenomena\nBoth rational and psychological arguments can be constructed for the failure of customers to shop around.\nOn the rational, search takes time and has a cost. The benefits of any improvement in interest rates need to outweigh those costs.\nHowever, the scale of the differences in interest rates makes it hard to justify the failure to search without assuming an unreasonably high cost of search or value of the borrowers time. In particular, most long-term borrowers could likely receive some further discount by sending an email or making a phone call requesting a discount (possibly accompanied by a threat to leave). A minimal cost action can achieve large long-term gain, but is not taken.\nPresent bias provides one explanation as the costs of search are today, whereas the benefits are distant. The benefits of the search receive unduly low weight to a hyperbolic discounter. This is still somewhat an incomplete explanation, as some of the steps to gain lower rates are of such low cost it requires unrealistic levels of present bias.\nAnother explanation relates to attention and knowledge. A customer with a long-term mortgage may not have given any attention to their current rate relative to the rates they could achieve in the market. The opacity of advertised rates would further cloud their comparison even if they were to focus attention. They do not take the steps to seek a reduced rate because they do not realise it is an option, not because they have calculated the costs and benefits of their action."
  },
  {
    "objectID": "posts/course-notes-on-consumer_financial_decision_making.html#abstract-payments-and-rewards",
    "href": "posts/course-notes-on-consumer_financial_decision_making.html#abstract-payments-and-rewards",
    "title": "Course notes on Applied Consumer Financial Decision Making",
    "section": "5.1 Abstract payments and rewards",
    "text": "5.1 Abstract payments and rewards\n\n5.1.1 Abstract payments\nPrelec and Simester (2001) ran an experiment in which they sought bids from college students for tickets to see the Boston Celtics and Boston Red Sox. Some students were told that they had to pay in cash. Others were told they had to pay by credit card. In both cases, payment was to be made the next day. Those who bid by credit card bid around twice as much as those who were asked to pay by cash.\nKnutson and colleagues (2007) suggested phenomena such as this may be because excessive prices trigger a pain-like response. The abstract nature of a non-cash method of paying (together with the delay that may occur with credit) might “anaesthetize” consumers against the pain of paying.\n\n\n5.1.2 Rewards\nMany of our financial transactions don’t just involve an exchange of money for a good or service. Often our choice of transaction method can involve other costs, such as fees, or benefits, such as rewards points.\nRewards points increase the proportion of transactions that occur via the reward-attracting purchase method. However, we are often poor at assessing the value of rewards.\nA reward point in itself is essentially valueless. The reward point only has value in that it can be exchanged for something else of value. As a result, when someone is considering whether they want to use a particular payment method that accrues rewards, they should ask what is the cost of the method relative to other options, and what is the value of the goods or services they could obtain through the reward points. The particular “number” of the reward points is irrelevant.\nDespite this, people do not just try to maximise the value of what they can receive by earning reward points. They also seek to maximise the reward points themselves. Hsee and colleagues (2003) call this “medium maximisation”.\nMedium maximisation implies that people can be induced to take a more costly action through an offer of more of the medium, even if that additional medium can be used to obtain the same ultimate good or service. For instance, double rewards points for each purchase will attract more purchases through that method even when the value of those reward points, in terms of the goods and services they can be exchanged for, is halved."
  },
  {
    "objectID": "posts/course-notes-on-consumer_financial_decision_making.html#low-rates-of-stock-market-participation",
    "href": "posts/course-notes-on-consumer_financial_decision_making.html#low-rates-of-stock-market-participation",
    "title": "Course notes on Applied Consumer Financial Decision Making",
    "section": "6.1 Low rates of stock market participation",
    "text": "6.1 Low rates of stock market participation\nAustralia has one of the highest rates of share market participation in the world (Deloitte Access Economics (2017)). In 2017, 37% of Australian adults held on-exchange investments. This level is a drop from over 50% in the early to mid-2000s, but above the 10% participation in many countries in Europe. Part of the reason for the high levels of Australian ownership is the large public offerings associated with the sale of public assets, including the Commonwealth Bank in 1991, Qantas in 1993 and Telstra in 1997.\n\nDespite the relatively high level of Australian ownership, there is still a question why barely more than a third of Australians own shares. If the stock market is not correlated with other income sources, there is a benefit of diversification by owning some shares.\nOne rational explanation for low participation are costs such as acquiring information or opening a trading account. Even though these costs are small, the small financial wealth of most households means that their level of participation would also be small. However, these costs cannot be the only explanation, as many at the top of the wealth distribution also do not participate.\nThere are many behavioural explanations for non-participation. One is disappointment aversion, which is the tendency to make choices in a way that reduces the potential for future expected disappointment (Ang et al (2005)). (Disappointment is closely tied to and often an alternative name for “regret” in the behavioural literature.) Disappointment occurs when an outcome falls short of the person’s reference point, such as the expected utility of the lottery or the certainty equivalent. Someone who is disappointment averse will be less attracted to a gamble that an expected utility maximiser due to that potential disappointment. Stock market participation, obviously, provides an opportunity for disappointment.\nLoss aversion could provide another potential behavioural explanation, but it is inadequate to explain the rejection of small, favourable stock market investments. As a result, loss aversion is often combined with narrow framing. Narrow framing occurs where people evaluate gambles in isolation. They do not place the gamble in the context of other gambles they are taking. But combined with a narrow frame, whereby they don’t consider their full basket of risks, the potential for loss becomes clear.\nThere is also a link between stock market participation and knowledge. Those with more schooling, higher IQ or higher financial literacy are more likely to participate in the stock market.\nFinally, an assessment that more people should participate in the stock market has an implicit assumption that people will participate optimally, such as by buying a diversified portfolio. However, there is a risk that if they did invest in the stock market, they would exhibit many of the problems identified on this page, including a lack of diversification and overtrading. In practice, it might be better if some households stayed out. That is the topic of the next tab."
  },
  {
    "objectID": "posts/course-notes-on-consumer_financial_decision_making.html#under-diversification",
    "href": "posts/course-notes-on-consumer_financial_decision_making.html#under-diversification",
    "title": "Course notes on Applied Consumer Financial Decision Making",
    "section": "6.2 Under-diversification",
    "text": "6.2 Under-diversification\nThose who invest in shares often hold an undiversified portfolio. The median US household holds only two stocks directly. We also disproportionately hold stocks from our own country, with only 8% of Australians directly holding foreign shares.\nThere is some evidence that those with undiversified portfolios earn outsize returns, due to factors such an information advantage. Even if this was the case, however, that outsize return comes with increased risk, and it is not clear that the higher return compensates for the greater risk.\nAs you will recall, one of the questions in the basic financial literacy questionnaire concerns diversification. Lower financial literacy is linked to lower diversification.\nEven when we do diversify, we often use crude metrics such as the 1/n rule, in which investments are simply spread across options in equal proportions. This means that if more shares are offered in a menu of options, we will end up with a great proportion of shares in our portfolio.\nBut is this irrational? The following story from Bower (2011) about a pioneer of modern portfolio theory raises some questions.\n\nHarry Markowitz won a 1990 Nobel Prize in economics for efficiently passing the buck — make that bucks. He was honored for developing a mathematical formula that helps investors maximize profit and minimize loss in their portfolios. After an exhaustive analysis of financial information, Markowitz’s procedure allocates a per- son’s stash of cash to an array of assets, with more money going to better bets.\nMany banks rely on this or similar investment approaches, warning customers to avoid picking investments intuitively. Yet Markowitz, now at the University of California, San Diego, followed a hunch in 1952 when he split paycheck contributions to his retirement account equally between stocks and bonds.\nEconomists call this simple approach “1 over N,” distributing money evenly among the number of available investment options, the Ns. The 1/N strategy is also called “naïve diversification,” a presumably second-rate alternative to crunching the numbers and calculating gain and loss probabilities for each potential investment. Nonetheless, many people with stock-and-bond retirement accounts opt for an even split.\nAs a young economist, Markowitz just wanted to avoid future regrets about fouling up his nest egg. “I thought, ‘You know, if the stock market goes way up and I’m not in it, I’ll feel stupid. And if it goes way down and I’m in it, I’ll feel stupid,’” he recalls. “So I went 50–50.”\n\nMarkowitz did not following the optimal behaviour as defined by himself. Was Markowitz erring?\nGerd Gigerenzer argues that optimisation is not always the best solution. Where a problem is computationally intractable or the optimisation solution lacks robustness due to estimation errors, heuristics may outperform. Gigerenzer notes work showing that 500 years of data would have been required for Markowitz’s optimisation rule to outperform his practice of 1/N. Markowitz was using a simple heuristic for an important decision, but rightfully so as it was superior for the environment in which he is making the decision."
  },
  {
    "objectID": "posts/course-notes-on-consumer_financial_decision_making.html#poor-trading-performance",
    "href": "posts/course-notes-on-consumer_financial_decision_making.html#poor-trading-performance",
    "title": "Course notes on Applied Consumer Financial Decision Making",
    "section": "6.3 Poor trading performance",
    "text": "6.3 Poor trading performance\n\nD\nDAY-TRADER, n. See IDIOT\nI\nIDIOT, n. See DAY_TRADER\nJason Zweig (2015) The Devil’s Financial Dictionary\n\nOn average, the more people trade, the worse they perform (O’Dean (1999)). This is driven largely, but not solely, by transactions costs (Barber and O’Dean (2000)).\n\nThose who believe they are better than others (overplace) trade more. There is mixed evidence in support of a link between overprecision and over-trading (Barber and O’Dean (2001)). There is also a difference by gender. Men trade more than women, and suffer a larger trading penalty as a result."
  },
  {
    "objectID": "posts/course-notes-on-consumer_financial_decision_making.html#poor-investment-options",
    "href": "posts/course-notes-on-consumer_financial_decision_making.html#poor-investment-options",
    "title": "Course notes on Applied Consumer Financial Decision Making",
    "section": "6.4 Poor investment options",
    "text": "6.4 Poor investment options\nOne cheap, easy option to achieve stock market diversification is a low-cost managed fund. However, people hold individual stocks more often than you would expect given the diversification benefits of a fund. Then when they choose funds, they often choose actively managed funds,which typically underperform passively managed funds. And of those they choose, they pay high fees.\nOne rational explanation involves the broker or adviser. Funds tend to flow to funds with higher commissions, suggesting the broker is acting in their own interest. Conflicted remuneration has been severely curtailed in Australia in recent years, and low-cost index funds have become increasingly available. This change may influence the proportion of funds in high-fee funds in the future.\nThere is some evidence that financial illiteracy is a cause of poor fund choice. High-IQ investors choose cheaper funds. But even when clear fee information is given to an ostensibly bright group (Harvard and Wharton students and staff) with which to choose between four index funds, many fail to minimise fees (Choi et al (2009)). This failure suggests a lack of financial sophistication even among that group."
  },
  {
    "objectID": "posts/course-notes-on-consumer_financial_decision_making.html#life-insurance-and-annuities",
    "href": "posts/course-notes-on-consumer_financial_decision_making.html#life-insurance-and-annuities",
    "title": "Course notes on Applied Consumer Financial Decision Making",
    "section": "7.1 Life insurance and annuities",
    "text": "7.1 Life insurance and annuities\nAt the household level, the standard economic model predicts a household will purchase insurance to protect against the death of household members, particularly those that are the highest earning. This is not, however, the pattern that is observed. Households often insure spouses when they would suffer no decline in living standard were their spouse to die. They also often fail to insure when they would suffer a substantial decline.\nA similar puzzle exists around life annuities. A life annuity is a product that a consumer purchases through payment of a lump sum in return for a stream of income that lasts until they or the beneficiaries in the household dies. Life annuities protect against the risk of living too long and running out of assets. The fact that only 1% of US households over the age of 65 hold life annuities is often called the “annuity puzzle”.\n\n7.1.1 Rational explanations\nThere are some rational explanations for this puzzle. Life annuities are often priced poorly and offer low yields relative to alternative investments. Public pensions already provide protection against longevity risk. There are also arguments that many people have bequest motives, which life annuities cannot satisfy as they only have value while the annuity holder is alive. Finally, annuities are a poor option if there are other uninsurable risks in the future, such as medical costs, which will require access to lump sums rather than an income stream.\nAll of these are likely factors, although the evidence that people are responsive to prices is weak.\n\n\n7.1.2 Psychological explanations\nThe link between financial literacy and insurance through annuities is complex and debated. The decision to annuitise is complex, although the decisions required through alternative options such as maintaining assets and determining drawdown requirements each period are possibly more difficult. The result is that in different contexts low financial literacy has been linked to both lower and higher rates of annuitision.\nThe choice of annuities is sensitive to the frame. When consumers were told about the potential returns from purchasing an annuity (an investment frame), they were far less likely to annuitise than if they were told about the potential future consumption from the annuity.\nLoss aversion can also make annuities unattractive, as the possibility of an early death might be seen as a potential loss (Brown et al (2016)). The future income stream is “lost” in the event of death."
  },
  {
    "objectID": "posts/course-notes-on-consumer_financial_decision_making.html#under-insurance",
    "href": "posts/course-notes-on-consumer_financial_decision_making.html#under-insurance",
    "title": "Course notes on Applied Consumer Financial Decision Making",
    "section": "7.2 Under-insurance",
    "text": "7.2 Under-insurance\nHouseholds often fail to insure against catastrophic risks to their property, and when they do, they often under-insure against the full extent of the catastrophe. For example, Quantum Market Research (2014) found that 81% of homeowners and renters do not have insurance that enables them to resume the same standard of living in the event of a crisis.\nWhile some of this failure to fully insure is rational, due to the small maximum possible loss, the main explanation for this under-insurance is simply that households underestimate the probability of a large loss. They also do little to understand the extent of the risk."
  },
  {
    "objectID": "posts/course-notes-on-consumer_financial_decision_making.html#low-excess",
    "href": "posts/course-notes-on-consumer_financial_decision_making.html#low-excess",
    "title": "Course notes on Applied Consumer Financial Decision Making",
    "section": "7.3 Low excess",
    "text": "7.3 Low excess\nOnce households do insure they often over-insure against small losses. They do this by choosing low levels of deductibles, also called “excess”. Excess is the amount the policy holder must contribute in the event of a claim. Excess is designed to reduced moral hazard through sharing risk, and administration costs by reducing the number of claims.\nThe increased premium required to be paid for a low excess means that those who choose it must be very risk averse; in fact, an implausible level of risk aversion under standard economic models.\nLoss aversion is one possible alternative explanation, as the potential for loss, even if small, is strongly felt. The difficulty with this explanation, however, is that the premium itself should also be felt as a loss. Prospect theory also provides another challenge to explaining this phenomena in that people tend to be risk seeking in the domain of losses, making the certain loss of the insurance premium unattractive when they have a chance of going uninsured but not suffering the negative event.\nAnother element of prospect theory, however, can increase the attractiveness of insurance. This is probability weighting, which can lead to small probability events being given greater weight. This exaggeration of the probability could be sufficient to overcome the risk seeking behaviour in the loss domain.\nIf you remember the four-fold pattern of risk attitudes generated by prospect theory, insurance is a combination of low probability and potentially large loss. In that schema, a person will be on net risk averse and seek to insure."
  },
  {
    "objectID": "posts/course-notes-on-consumer_financial_decision_making.html#junk-insurance",
    "href": "posts/course-notes-on-consumer_financial_decision_making.html#junk-insurance",
    "title": "Course notes on Applied Consumer Financial Decision Making",
    "section": "7.4 Junk insurance",
    "text": "7.4 Junk insurance\nPeople regularly buy insurance with limited value.\nThe prototypical junk insurance in the Australian market is consumer credit insurance. Consumer credit insurance is sold to consumers to cover them in the event that they cannot meet the minimum payments of a loan due to unemployment, injury or illness, or to pay the balance in the event that they die.\nAustralian Securities and Investments Commission (2019a) found that for consumer credit insurance, only around 19 cents in the dollar was paid out. For insurance associated with credit cards, that payout rate was only 11 cents in the dollar.\nMost major consumer credit providers have ceased selling many, if not all, of the forms of consumer credit insurance since ASIC’s report. But this still leaves open the question of why consumers were purchasing this insurance in the first place.\nA major issue was understanding the products. Many people were ineligible to ever claim as they were not meeting work requirements such as a working a minimum number of days or having permanency, or having a pre-existing condition excluded by the policy. They simply did not know (and were not told) this.\nAnother factor is the attention of the customers. They are primarily engaged in obtaining a credit card or loan at the time of purchasing the insurance. The add-on insurance is an immaterial part of the overall purchase, so receives little attention or scrutiny. There is also little opportunity for the consumer to shop around or compare prices."
  },
  {
    "objectID": "posts/course-notes-on-consumer_financial_decision_making.html#evidence-of-adverse-selection-and-moral-hazard",
    "href": "posts/course-notes-on-consumer_financial_decision_making.html#evidence-of-adverse-selection-and-moral-hazard",
    "title": "Course notes on Applied Consumer Financial Decision Making",
    "section": "7.5 Evidence of adverse selection and moral hazard",
    "text": "7.5 Evidence of adverse selection and moral hazard\nAdverse selection emerges where there is an information asymmetry between the insurer and potential customer about what type of customer is seeking insurance. Only high-risk customers buy coverage, whereas low-risk customers find the pricing unattractive.\nThe evidence for adverse selection actually occurring is ambiguous. In support of the concept, some studies have found that drivers who choose a lower excess tend to be higher risk drivers (Cohen (2005)).\nOther evidence provides little support. For example, people with lower life expectancy are not more likely to purchase life insurance than those likely to live longer (Cawley and Philipson (1999)).\nThere is even some evidence of an opposing trend, whereby low-risk customers are more likely to seek coverage. “Advantageous risk selection” occurs where risk averse people attach a high value to insurance due to their risk aversion, but are also lower risks due to this risk aversion. There is also evidence that higher risks are less capable of making insurance decisions involving comparison of costs and benefits than those who are lower risk, affecting their insurance purchase decisions (Fang et al (2008)[https://doi.org/10.1086/587623]).\nEvidence for moral hazard is more robust, although not always consistent. It is also difficult to disentangle moral hazard from adverse selection. Moral hazard has been found in health, medical and automobile insurance markets (Kunreuther et al (2013))."
  },
  {
    "objectID": "posts/course-notes-on-consumer_financial_decision_making.html#product-design",
    "href": "posts/course-notes-on-consumer_financial_decision_making.html#product-design",
    "title": "Course notes on Applied Consumer Financial Decision Making",
    "section": "8.1 Product design",
    "text": "8.1 Product design\nProduct design is the first lever available to improve financial wellbeing. Good design can make distribution easier, and engagement after sale easier.\nSimilarly, behavioural interventions during distribution and after-sale engagement can be used to reduce the impact of poor design, but this often has lower efficacy than addressing the problem at source.\nMany of the problems we have explored in this unit come through poor product design. The simple act of removing the problematic design features can solve the problem. The following are some examples:\n\nThe ability to give discretionary discounts to mortgagees reduces transparency and can lead to less-sophisticated customers paying higher interest. Common rates across customers advertised up-front would reduce that harm.\nSavers and borrowers are more attentive to initial rates (e.g. honeymoon rates) than to rates they will ultimately pay. A design with a lower initial rate will tend to lead consumers to underestimate the future costs of borrowing and to overborrow. Fixed flat interest rates can reduce that problem.\nBalance transfer discounts on credit cards can lead to some customers paying substantial interest when their discount period ends, having either miscalculated the likelihood of clearing their debt in advance, or through simple lapse and failing to take an action such as moving to another card. Replacing balance transfer discounts with a lower flat interest rate through time can reduce that problem.\n\nThese remedies are not, however, without cost. There are typically trade-offs between customers (and obviously for the firm itself). For instance, the removal of discretionary mortgage discounts may result in some borrowers paying more, and may even result in less credit availability for high-risk borrowers.\nSimilarly, balance transfer policies help credit card holders on net. The amount of credit card debt in Australia accruing interest has not increased in the last 15 years despite a more than 50% increase in credit card debt. The distribution of those payments, however, has changed markedly.\n\n8.1.1. Save More Tomorrow\nThe classic example of successful design of a financial product is the Save More Tomorrow Program (Thaler and Benartzi (2004)). Under Save More Tomorrow, customers are asked to commit in advance to allocating a fraction of their future salary increases toward their retirement savings accounts.\nSave More Tomorrow is designed to reduce loss aversion as a factor in deciding contribution amounts. A commitment of a proportion of pay rises means that the contribution can increase over time, but pay never decreases. The program capitalises on their propensity to stick with the status quo, as people are unlikely to unwind their future commitments despite being able to opt out at any time. That ability opt out also reduces regret/disappointment aversion.\nThe first tests of the Save More Tomorrow program resulted in 78 per cent of those offered the plan joining, 80% of those remaining in the plan through the fourth pay rise, and average savings rates increasing from 3.5% to 13.6% over 40 months. (Note the savings rate is higher than the default rate in Australia. Could the default in Australia create a low anchor for some people?)\n\n\n8.1.2 Example: Designing the bonus saver account\nMany banks offer a ‘bonus saver’ account. The major feature of these accounts is the ability to earn “bonus interest” each month if the customer satisfies certain criteria, such as making a deposit each month, withdrawing no savings, growing the balance, or making a minimum number of card transactions.\nWhen these accounts are offered, many account holders do not receive bonus interest each month. They fail to make the required deposits, withdraw funds despite the effective penalty, and do not make the requisite number of transactions. Many customers accumulate no substantive savings or regularly withdraw their accumulated balances.\nA range of customer characteristics could be causing those failures, including: - Lack of attention or mental lapses, leading them to forget to deposit or to make a withdrawal without considering the consequences - Present bias, whereby withdrawn money has far higher value today that the savings or potential interest - Regret/disappointment aversion, whereby customers do not deposit (or constrain deposits) as they fear they may regret that later if they have to withdraw\nOne simple intervention to improve the customer decision would be to remove the requirement for deposit and/or withdrawal. The customer would no longer have an opportunity for failure, and the ‘bonus’ interest would be paid by default.\nThere are some questions we might wish to ask before taking this step: 1. Do the criteria to receive the bonus interest incentivise the accumulation of savings? Would removal of the criteria have unintended consequences for some customers? 2. Does this feature enable better pricing? What would be the negative effect on the interest received for those who normally met the criteria? 3. What are the distributional consequences of the pricing arrangement? What are the characteristics of those who fail to receive bonus interest and those who might receive lower interest if pricing was changed?\nObviously, the bank would likely have other questions. Does the ability to advertise a higher interest rate assist marketing? Is the product viable without the attraction of the bonus rate?"
  },
  {
    "objectID": "posts/course-notes-on-consumer_financial_decision_making.html#product-distribution",
    "href": "posts/course-notes-on-consumer_financial_decision_making.html#product-distribution",
    "title": "Course notes on Applied Consumer Financial Decision Making",
    "section": "8.2 Product distribution",
    "text": "8.2 Product distribution\nThe process of product distribution often results in the wrong customers being in the wrong products.\nThe below provides two illustrations of how distribution design could affect consumer decision making.\n\n8.2.1 Add-on insurance\nCustomer outcomes can often be improved simply by ceasing poor distribution practices. Add-on insurance provides an illustration of this.\nAdd-on insurance is an insurance product sold to accompany another financial transaction, such as purchase of a motor vehicle or a credit product. The add-on insurance then covers events associated with that other transaction, such as a motor vehicle accident. Consumer credit insurance, which we covered previously, is typically sold as add-on insurance.\nAdd-on insurance is not usually offered up-front, but rather only at the completion of the purchase of the associated product. As a result, the customer has typically not prepared for the purchase of insurance, such as researching the market or shopping around. This means customers are often buying the first and only insurance product they see. They are more likely to accept very expensive offers and unlikely to choose the best deal available (as the add-on offer is rarely the best deal). The relativity of the add-on price to the primary product (e.g. a car) also reduces the attention given to the add-on price. Further, some consumers are confused about whether they are required to purchase the insurance as a condition of buying the product. (See Iscenko et al (2014) for a summary.)\nThe Financial Services Royal Commission recommended that add-on insurance providers be required to change distribution practices by adopting a deferred sales model. Add-on insurance should not be offered until a set time after the sale of the product to which it relates. A deferred model will make the price of the add-on more salient, increase the likelihood of price comparison or shopping around, and enable decision making at a time where they may be experiencing decreased cognitive load. It also removes any confusion about the insurance being a condition of the purchase.\nThe Australian Government released exposure draft legislation for this model in early 2020, but it has not yet been implemented.\n\n\n8.2.2 Default retirement savings\nDefaults are a powerful distribution feature.\nAutomatic enrolment of employees into retirement savings plans has been one of the most successful behavioural interventions to shift employee behaviour. Madrian and Shea (2001) examined automatic enrolment in 401(k) plans. They found that participation was materially increased. Among those with 3 to 15 months tenure, the increase due to default enrolment was an increase from around 37% to 86% participation.\nIn the case of retirement savings in Australia, we have a mandatory default requirement to contribute a portion of our salary. Obviously, it is successful in achieving participation in retirement savings.\nThe Australian superannuation system also has voluntary defaults in the form of the particular superannuation provider (an employer selected default) and the investment options within that provider. The stickiness of those defaults and the high fees (poor outcomes) led to the “MySuper” legislation with default low-fee low-frill accounts. Around two out of three members stick with their fund’s default option, so defaulting into the best designed option could deliver substantial benefits.\nDefaults are also set for death, total permanent disability and income insurance within superannuation. As for default funds, these are sticky. The question then becomes what default is better for customers. Recent government reforms mean that account holders are not defaulted into insurance if they are under 25 or have a balance of less than $6000."
  },
  {
    "objectID": "posts/course-notes-on-consumer_financial_decision_making.html#product-distribution---advice",
    "href": "posts/course-notes-on-consumer_financial_decision_making.html#product-distribution---advice",
    "title": "Course notes on Applied Consumer Financial Decision Making",
    "section": "8.3 Product distribution - advice",
    "text": "8.3 Product distribution - advice\nOne major method of financial product distribution is the advice channel. People see an adviser who can recommend products and provide ongoing advice as what steps they should take to meet their financial goals.\nGiven the psychological factors affecting financial decision making, advice could be seen as a way to enlist an expert less subject to those weaknesses. However, there is considerable evidence that advisors do a poor job for their customers. For instance, Australian Securities and Investments Commission (2012) assessment of advice obtained through a shadow shopping exercise rated 3% of the advice as good quality. In contrast, 86% of participants in the study felt they had received good advice.This matches evidence from around the world.\n\n8.3.1 Adviser incentives and competence\nThe rational explanations for this poor advice primarily rest on the incentive structure experienced by the advisors. For example, Mullainathan and colleagues (2012) showed that advisers tend to either support the mistaken beliefs of their clients or argue against their correct beliefs whenever it was in the adviser’s interests to do so. There is also empirical evidence that consumers are largely naive about this conflict.\nAn alternative explanation for poor advice is that advisors lack competence. Linnainmaa and colleagues (2018) found that advisers tend to invest in accordance with the advice given to their clients. They trade too frequently, chase past returns, and invest in expensive actively managed funds. The result is that their returns are similar to their clients’ net returns.\n\n\n8.3.2 Accepting advice\nMany adviser clients follow poor advice or fail to follow good advice. Often base this on trust, but trust can be formed on factors such as credentials and first impressions (including confirming the client’s own views) (Agnew et al (2018)).\nPart of the advice task is to create advice that is compatible with the client’s psychology. Advice that is sub-optimal but that is followed may be superior to optimal advice that the client does not accept. Determining a framework to manage anxiety and emotional comfort over time can be as important as the initial advice.\nAs an example, consider the equity premium puzzle and Bernartzi and Thaler’s (1995) explanation.\nSuppose an investor has a choice between risky stocks, with an expected annual return of 7% and standard deviation of 20%, and a sure return of 1%. The attractiveness of stocks to a loss averse investor will depend on both the time horizon of the investor and the frequency with which they evaluate the returns. If they monitor their portfolio frequently, they will often observe losses from stocks, which they feel with greater force than gains.\nSuppose that one loss averse investor examines their portfolio every day. Since on a daily basis stocks go down almost as often as they go up, this investor will experience a lot of pain, making the stocks unattractive. Another loss averse investor only checks in on their portfolio once a decade. At that horizon, stocks have only a small probability of losing money, so will be much more attractive to someone who is loss averse.\nIt is a combination of loss aversion and a short evaluation period that will drive an investor to require a large premium for holding the risky option. Benartzi and Thaler call this myopic loss aversion.\nAn adviser that can set up a framework where their client only periodically checks their portfolio may be more likely to have that client adhere to their ongoing advice.\n\n\nOptional reading\nSah (2018) “Conflicts of Interest and Disclosure”, Research Paper prepared for the Royal Commission into Misconduct in the Banking,Superannuation and Financial Services Industry, https://financialservices.royalcommission.gov.au/publications/Documents/research-paper-conflicts-interest-disclosure.pdf"
  },
  {
    "objectID": "posts/course-notes-on-consumer_financial_decision_making.html#product-distribution---information",
    "href": "posts/course-notes-on-consumer_financial_decision_making.html#product-distribution---information",
    "title": "Course notes on Applied Consumer Financial Decision Making",
    "section": "8.4 Product distribution - information",
    "text": "8.4 Product distribution - information\nThe way in which information is provided through the distribution and sales process can markedly change customer outcomes.\n\n8.4.1 Marketing and advertising\nThe most salient way in which this occurs is marketing and advertising. As one example, research by Hastings and colleagues (2017) into the privatised Mexican social security system found that marketing raises demand for financial products, and lowers the price elasticity of demand (low elasticity means price goes up, but demand doesn’t go down). This enabled the system to have fees so high that the average fee would reduce a 100 peso deposit to 95 pesos five years later even when the investments earned a 5% annual return.\nSimilarly, research into the US mortgage market found that lenders sell more expensive mortgages in regions where they advertise more (Gurun et al (2016)).\nPart of the way in which these negative effects occur is that firms make appealing attributes salient, and shroud fee and quality problems. Customers don’t seem to infer that what is hidden is bad news. For instance, UK research by Armstrong and Vickers (2012) found that a small cohort of bank customers often suffer repeated “unexpected” overdraft charges. These fees were easily found after a few clicks on the bank’s website, but were effectively hidden relative to other features of the accounts.\nThis leads to an obvious intervention of ensuring advertising covers more than just the positive attributes of a product, possibly enabling a better decision. Are the long-term interest rates advertised with the same salience as the honeymoon rate? However, if a product is poorly designed (e.g. excessive fees), there is no way to fix this through marketing.\n\n\n8.4.2 Disclosure in payday lending\nBertrand and Morse (2011) trialled information disclosure in a field experiment in payday lending stores in the United States. They included one of the following three designs on the loan documentation envelopes.\nDollar information\nOne treatment provided the customer with dollar information on accumulated fees over time, compared with same amount on credit card.\n\nCustomers who received this treatment were 5.9 percentage points less likely to borrow in the pay cycle following the intervention (an 11% decline relative to the control group). They also reduced the amount borrowed if they did return by 23%. The success of this intervention suggests an initial lack of understanding of the power of compounding (financial literacy).\nComparison of annual percentage rates\nAlthough regulation already required lenders to provide an annualised percentage rate to customers, this treatment involved comparison of the annualised percentage rate of the payday loan with other financial instruments that the customers are likely familiar with.\n\nCustomers who were provided with the annual percentage rate comparisons reduced the amount borrowed for future loans by 16%. (There was no effect on likelihood of taking out a loan.)\nTypical repayment profile\nThe third treatment provided information on the typical repayment profile for a customer.\n\nCustomers provided with the typical repayment profile reduced future sums borrowed by 12%. (There was no effect on likelihood of taking out a loan.) This change might be because the information reduced the overoptimism of the borrower as to their future actions and financial situation .\n\n\n8.4.3 Credit card transparency\nA large nationwide retail bank showed the trade-offs associated with credit cards at the beginning of the sale process Buell and Choi (2019). Monthly spending by those who saw the trade-offs was 10% higher, with 20% lower cancellation rates and 10% fewer late payments.\n\nIt is not clear exactly how the transparency measure worked, but may be due to both selection effects (people got cards more suited to their needs) or an education effect (they used the card better).\n\n\n8.4.4 Distributing the bonus saver account\nYou weren’t able to convince the product owner or pricing team that they should remove the criteria on the bonus saver account. What measures could you introduce during the sales process to increase the proportion of customers who appropriately select the product?"
  },
  {
    "objectID": "posts/course-notes-on-consumer_financial_decision_making.html#post-sales",
    "href": "posts/course-notes-on-consumer_financial_decision_making.html#post-sales",
    "title": "Course notes on Applied Consumer Financial Decision Making",
    "section": "8.5 Post-sales",
    "text": "8.5 Post-sales\nWhile post-sales interactions with customers are often an inferior way of addressing poor product design and distribution practices, the long-term nature of many financial products, the ongoing decisions required of customers, and customers’ often changing financial positions necessitates that those interactions are well designed.\n\n8.5.1 Text messages and credit card payments\nThe classic post-sale interaction is a behaviourally designed prompt to trigger an action at a critical time.\nAs one example, BETA (2019) partnered with Treasury and Westpac to see if reminders could encourage consumers to pay credit card debt earlier. They sent emails and text messages with various content such as a basic message (“Hello Name, Payment on your Westpac credit card is due next week.”), a loss frame (“To avoid paying more interest, think about lowering or even clearing your full debt”) or social norm (“Many people choose to pay the full debt on time.”)\nSMS reminders resulted in an increase in payments of $134 the following month (a 28 per cent increase). There was no difference between the different types of messages, including the basic message. This suggests the effect of the response was not due to the amelioration of any “bias”, but rather by gaining attention.\n\n\n8.5.2 A text message backfire (at least for the bank)\nA Turkish bank sent text messages highlighting a discount on overdraft fees (Alan et al (2016)). The messages reduced overdraft usage, which suggested that people were not aware that there was a price on overdrafts. The discount was an increase from their reference point.\nConversely, text messages simply mentioning to customers that they had an overdraft available increased usage.\n\n\n8.5.3 Lemonade\nFor insurance products, a critical interaction occurs when a consumer makes a claim. Often these claims are fraudulent. For instance, claims for damaged or lost iPhones typically surge just before release of the next model.\nLemonade has introduced several features to their claims process to increase honesty. Policy holders sign a digital pledge of honesty at the beginning of the claim process, rather than the usual certification after entering the claim.\nThis approach is based on experiments by Dan Ariely and colleagues on honesty. In one, students were induced to reduce cheating by citing honour codes before completing a test (Mazar et al. (2008)). In another, drivers gave more accurate mileage information when seeking insurance by signing at the beginning of the form (Shu et al (2012)).\nUnfortunately (for this approach at least), these experiments have not replicated in large-scale multi-lab replications or additional fieldwork (see Verschuere et al. (2018) and Kristal et al. (2020)), and the field work data concerning driver mileage was fraudulent (Anonymous (2021)). It is not clear that this measure has any effect.\n\n\n8.5.5 Required reading\nKristal et al. (2020a) “When We’re Wrong, It’s Our Responsibility as Scientists to Say So”, Scientific American, https://blogs.scientificamerican.com/observations/when-were-wrong-its-our-responsibility-as-scientists-to-say-so/"
  },
  {
    "objectID": "posts/course-notes-on-consumer_financial_decision_making.html#education-and-financial-literacy",
    "href": "posts/course-notes-on-consumer_financial_decision_making.html#education-and-financial-literacy",
    "title": "Course notes on Applied Consumer Financial Decision Making",
    "section": "9.1 Education and financial literacy",
    "text": "9.1 Education and financial literacy\nThrough this unit we have seen substantial evidence that those with higher financial literacy have higher financial wellbeing and better outcomes for many financial decisions.\nHowever, that link in itself does not mean that: - there is a causal relationship between financial literacy and wellbeing - interventions to increase financial literacy will do so - any increase in financial literacy will translate into higher financial wellbeing.\nThese points are the subject of a substantial debate.\n\n9.1.1 The case against financial education\nThe case against financial education is laid out by Fernandes and colleagues (2014) who examined 201 previous studies and found that financial education explained only 0.1% of variation in the financial behaviours studied. Further, the minor effect of the financial education decayed with time, having negligible effects 20 months after the intervention.\nFernandes and colleagues also found that when they controlled for other psychological traits of consumers (such as propensity to plan, willingness to take financial risks, and numeracy), the effect of financial literacy diminish dramatically. As a result it may be other traits that are driving the observed effects\nFinally, they noted that few studies explicitly seek a causal effect, rather than just correlational. Financial literacy effects are far smaller when manipulated rather than measured.\n\n\n9.1.2 The case for financial education\nA more recent paper by Kaiser and colleagues (2020) pushes back at this interpretation of the evidence.\nTheir paper had the benefit of more recent studies, with the number of randomised controlled trials that they could draw on having grown from 13 to 76 since Fernandes and colleagues’ meta-analysis. Adding that new work increased the effect three to five times (depending on methodology) from that found by Fernandes.\nKaiser and colleagues were also critical of the use of “variance explained” measure to describe the effect of financial literacy, arguing that it can hide materials effects. Using alternative measures more common in the meta-analysis literature, they find that financial literacy interventions have an average 0.1 standard deviation effect on financial behaviours and 0.2 standard deviation effect on financial knowledge. This is similar to many math and reading interventions (although it should be noted there is a similar debate playing out across the broader education field).\nUltimately, Kaiser and colleagues argue that it is actually a comparison of the economic costs and benefits that are required. This was seldom done in the papers that they examined, but they did argue that they could be meaningful.\nThey also suggest that there are not enough long-term studies investigating decay to make any definitive statements about whether it occurs. If we were to draw evidence from the broader education literature, however, we would expect to see large decay.\n\n\n9.1.3 Just-in-time financial education\nOne area where there is substantial evidence in favour of financial education involves interventions that are provided “just-in-time”. These are interventions designed to affect a decision or behaviour at an important moment. By their nature, they are less subject to decay and do not require the consumer to retrieve and apply financial knowledge from much earlier education.\nWe have covered one such example with Bertrand and Morse’s (2011) interventions to decrease payday lending use. By providing simple information at a critical moment, they were able to shift behaviours in the short-term, with potential longer-term benefits."
  },
  {
    "objectID": "posts/course-notes-on-consumer_financial_decision_making.html#mandated-disclosure",
    "href": "posts/course-notes-on-consumer_financial_decision_making.html#mandated-disclosure",
    "title": "Course notes on Applied Consumer Financial Decision Making",
    "section": "9.2 Mandated disclosure",
    "text": "9.2 Mandated disclosure\nWe have seen in this unit that the information provided to customers, whether in the form of marketing, more formal information provision, or advice, can affect customer decision making and financial wellbeing. This opens the door, at least theoretically, for mandated disclosure to improve outcomes.\nHowever, the evidence for the positive effect of mandated disclosure is limited. Disclosure is often found to be inadequate to overcome fundamental problems with products, and can sometimes backfire.\nOne of the main barriers to disclosure is that complexity is hard to explain simply. If a product is complex, no amount of disclosure can change that. People can hold only a small number of chunks in their mind, and even if the language is plain, they will have that fundamental constraint.\nThe other barrier is that disclosure is typically implemented by self-interested firms. The idealised implementation wanted by a regulator and actual implementation are often markedly different. For example, find the required warning label on the homepage for Nimble.\nAchieving the desired effect with disclosure is also difficult. Below is one example.\n\n9.2.1 Superannuation disclosure\nAustralian superannuation funds are required to provide short product disclosure statements to customers. They are designed to be shorter than historic product disclosure statements and enable people to compare superannuation products across areas such as risk, returns and asset allocation.\nBateman and colleagues (2015) examined the effect of the standard information in these documents on consumer choice (using Unisuper templates as a foundation). They found that one third of consumers were not affected by the information provided and the risk information was irrelevant to three quarters.\nThe most influential element of the dashboard was the asset allocation pie chart. Customers preferred options where the assets were allocated evenly across the categories. This finding opens the potential for manipulation of choice by changing the asset categories such that allocations appear more even.\n\n\n\n9.2.2 Required reading\nPage (2019) “Disclosure for real humans”, Behavioural Public Policy, https://doi.org/10.1017/bpp.2019.23"
  },
  {
    "objectID": "posts/course-notes-on-consumer_financial_decision_making.html#disclosure-of-conflicts-of-interest",
    "href": "posts/course-notes-on-consumer_financial_decision_making.html#disclosure-of-conflicts-of-interest",
    "title": "Course notes on Applied Consumer Financial Decision Making",
    "section": "9.3 Disclosure of conflicts of interest",
    "text": "9.3 Disclosure of conflicts of interest\nEarlier in this unit we saw evidence that financial advisers often act in their own interest when giving advice to customers. Customers also appear to be naive as to these effects.\nOne possible intervention to overcome this conflict is to require disclosure of conflicts. However, there is experimental evidence that this could backfire.\nCain and colleagues (2005) found in a lab experiment that when conflicts of interest are disclosed, advisers give even more biased advice. This may be due to moral licensing, or a strategic response as the adviser believes they need to be more extreme in their recommendation in anticipation that their advice will be discounted. Consumers also fail to discount the advice due to the conflict as much as they should, even though disclosed. The net result is that disclosure could worsen outcomes.\nSah and colleagues (2012) show in another lab experiment that although disclosure can decrease trust in advice, it can create a perverse incentive for the customer to follow the advice. Failure to follow the advice would signal that they don’t trust the adviser, creating social pressure to give in to the adviser’s interest. Given the nature of this particular effect, disclosure of the conflict by an external party or an opportunity to make the decision to follow advice in private reduced the extent of this unintended consequence.\nLowestein and colleagues (2011) provide a review of some of this evidence."
  },
  {
    "objectID": "posts/course-notes-on-consumer_financial_decision_making.html#regulation",
    "href": "posts/course-notes-on-consumer_financial_decision_making.html#regulation",
    "title": "Course notes on Applied Consumer Financial Decision Making",
    "section": "9.4 Regulation",
    "text": "9.4 Regulation\nOne direct means of improving financial decision making is through regulatory obligations placed upon financial service providers.\nMany of the interventions that we identified in the first half of this chapter could be (or are) imposed by regulation. This includes the MySuper defaults, the forthcoming deferred sales requirement for add-on insurance, and the many forms of mandatory disclosure.\nOther examples of regulatory interventions include:\n\nCooling off periods: This prevents people making purchases made when they were in an emotional state (or enables them to unwind them), and provides an opportunity for reflection and price comparison.\nRestricting the use of certain defaults: As defaults are sticky, legislating the better default may improve outcomes. For example, financial advisers in Australia are now required to periodically renew the client agreement for ongoing fees, a change from the previous default arrangement of ongoing fees.\nPrice caps: If competition in a market is for naive, price-insensitive customers, price caps can limit the damage. As noted earlier in this course, payday lending fees in Australia are capped. Price caps can, however, reduce supply if poorly targeted."
  },
  {
    "objectID": "posts/course-notes-on-consumer_financial_decision_making.html#commitment-and-self-control",
    "href": "posts/course-notes-on-consumer_financial_decision_making.html#commitment-and-self-control",
    "title": "Course notes on Applied Consumer Financial Decision Making",
    "section": "9.5 Commitment and self-control",
    "text": "9.5 Commitment and self-control\nMost of this and the previous chapter has examined product design features or external interventions to improve financial wellbeing.\nConsumers, however, can and do use their own behavioural interventions to overcome their own limitations.\nOne of the primary ways consumers can do this is through committing themselves to a future course of action. If they have a degree of sophistication and know that they may fall victim to present bias in the future, they can act now to remove that possibility. (You learnt about commitment devices in Behavioural Game Theory)\nA simple way people can do this is by placing their savings into a less liquid investment, such as a term deposit. In one experiment, Beshears and colleagues (2015) showed that when people can invest in two accounts, one liquid and the other with liquidity constraints such as withdrawal penalties, the experimental participants put nearly half of their money in the illiquid account even though it paid the same interest rate. This contrasts with the standard economic prediction that all money should go to the liquid account, which enables all actions plus more than can be done from the illiquid account. Even when the interest rate on the illiquid account was lower, it still attracted around a quarter of the money.\nEven more interesting, the harsher the constraints in the illiquid account, the more money it attracted. Those account with a higher withdrawal penalty (20% compared to 10% attracted more money), and those that barred withdrawals attracted even more.\nThis result suggests a strong demand among sophisticated, present-based agents for products that will enable them to control their future behaviours.\n\n9.5.1 Mental accounts\nMental accounts can often act as a form of commitment despite the absence of any physical barriers.\nOne example of mental accounts that we have already come across this semester is the co-holding of savings and debt to create constraints against even worse outcomes (Gathergood and Weber (2014)). People often hold both high-cost credit card debt and savings that provide low rates of return.\nCo-holding the two can be a self-control strategy. A failure to divert the savings to pay off the credit card decreases the amount of unused credit capacity, which may reduce future spending.\nAnother example of the use of mental accounts working as a control was an intervention designed to help micro-entrepreneurs in the Dominican Republic make financial decisions ([Drexler et al (2014)(https://doi.org/10.1257/app.6.2.1)]). They were placed in one of two programs. The first was standard accounting training. The second was a rule-of-thumb training that taught basic financial heuristics. The major heuristic was for them to physically store their their household and business money in separate drawers.\nThe rule-of-thumb training improved their financial practices and revenues. Among those with lower skills or poorer initial practices, the rule of thumb training had better results than the accounting training. (While being seen as a self control mechanism that can be implemented by someone, this could also be seen as a tick for a non-traditional financial literacy intervention.)"
  },
  {
    "objectID": "posts/ergodicity-economics-a-primer.html",
    "href": "posts/ergodicity-economics-a-primer.html",
    "title": "Ergodicity economics: a primer",
    "section": "",
    "text": "In my previous posts on loss aversion (here, here and here), I foreshadowed a post on how “ergodicity economics” might shed some light on whether we need loss aversion to explain people’s choices under uncertainty. This was to be that post, but the background material that I drafted is long enough to be a stand-alone piece. I’ll turn to the application of ergodicity economics to loss aversion in a future post.\nThe below is largely drawn from presentations and papers by Ole Peters and friends, with my own evolutionary take at the end. For a deeper dive, see the lecture notes by Peters and Alexander Adamou, or a recent Perspective by Peters in Nature Physics."
  },
  {
    "objectID": "posts/ergodicity-economics-a-primer.html#the-choice",
    "href": "posts/ergodicity-economics-a-primer.html#the-choice",
    "title": "Ergodicity economics: a primer",
    "section": "1. The choice",
    "text": "1. The choice\nSuppose you have $100 and are offered a gamble involving a series of coin flips. For each flip, heads will increase your wealth by 50%. Tails will decrease it by 40%. Flip 100 times."
  },
  {
    "objectID": "posts/ergodicity-economics-a-primer.html#the-expected-payoff",
    "href": "posts/ergodicity-economics-a-primer.html#the-expected-payoff",
    "title": "Ergodicity economics: a primer",
    "section": "2. The expected payoff",
    "text": "2. The expected payoff\nWhat will happen? For that first flip, you have a 50% chance of a $50 gain, and a 50% chance of a $40 loss. Your expected gain (each outcome weighted by its probability, 0.5*50 + 0.5*-40) is $5 or 5% of your wealth. The absolute size of the stake for future flips will depend on past flips, but for every flip you have the same expected gain of 5% of your wealth.\nShould you take the bet?\nI simulated 10,000 people who each started with $100 and flipped the coin 100 times each.\n\n\nCode\n# Load the required packages\n\nlibrary(ggplot2)\nlibrary(scales) #use the percent scale later\n\n# Create a function for running of the bets.\n\nbet &lt;- function(p, n, t, start=100, gain, loss, ergodic=FALSE, absorbing=FALSE){\n\n  #p is probability of a gain\n  #n is how many people in the simulation\n  #t is the number of coin flips simulated for each person\n  #start is the number of dollars each person starts with\n  #if ergodic=FALSE, gain and loss are the multipliers\n  #if ergodic=TRUE, gain and loss are the dollar amounts\n  #if absorbing=TRUE, zero wealth ends the series of flips for that person\n\n  params &lt;- as.data.frame(c(p, n, t, start, gain, loss, ergodic, absorbing))\n  rownames(params) &lt;- c(\"p\", \"n\", \"t\", \"start\", \"gain\", \"loss\", \"ergodic\", \"absorbing\")\n  colnames(params) &lt;- \"value\"\n\n  sim &lt;- matrix(data = NA, nrow = t, ncol = n)\n\n  if(ergodic==FALSE){\n    for (j in 1:n) {\n      x &lt;- start\n      for (i in 1:t) {\n      outcome &lt;- rbinom(n=1, size=1, prob=p)\n      ifelse(outcome==0, x &lt;- x*loss, x &lt;- x*gain)\n      sim[i,j] &lt;- x\n      }\n    }\n  }\n\n if(ergodic==TRUE){\n    for (j in 1:n) {\n      x &lt;- start \n      for (i in 1:t) {\n      outcome &lt;- rbinom(n=1, size=1, prob=p)\n      ifelse(outcome==0, x &lt;- x-loss, x &lt;- x+gain)\n      sim[i,j] &lt;- x\n      if(absorbing==TRUE){\n        if(x&lt;0){\n          sim[i:t,j] &lt;- 0\n            break\n        }\n        }\n      }\n    }\n  }\n\n  sim &lt;- rbind(rep(start,n), sim) #placing the starting sum in the first row\n  sim &lt;- cbind(seq(0,t), sim) #number each period\n  sim &lt;- data.frame(sim)\n  colnames(sim) &lt;- c(\"period\", paste0(\"p\", 1:n))\n  sim &lt;- list(params=params, sim=sim)\n  sim\n}\n\n# Simulate 10,000 people who accept a series of 1000 50:50 bets to win \\$50 or lose \\$40 from a starting wealth of \\$100.\n\nset.seed(20191215)\nnonErgodic &lt;- bet(p=0.5, n=10000, t=1000, gain=1.5, loss=0.6, ergodic=FALSE)\n\n# Create a function to generate summary statistics.\n\nsummaryStats &lt;- function(sim, t=100){\n\n  meanW &lt;- mean(as.matrix(sim$sim[(t+1),2:(sim$params[2,]+1)])) # mean wealth\n  medianW &lt;- median(as.matrix(sim$sim[(t+1),2:(sim$params[2,]+1)])) # median wealth\n  num99 &lt;- sum(sim$sim[(t+1),2:(sim$params[2,]+1)]&lt;(sim$params[4,]/100)) #number who lost more than 99% of their wealth\n  per99 &lt;- num99/sim$params[2,]*100 #percentage who lost more than 99% of their wealth\n  numGain &lt;- sum(sim$sim[(t+1),2:(sim$params[2,]+1)]&gt;sim$params[4,]) #number who gain\n  perGain &lt;- numGain/sim$params[2,]*100 #percentage who gain\n  num100 &lt;- sum(sim$sim[(t+1),2:(sim$params[2,]+1)]&gt;(sim$params[4,]*100)) #number who increase their wealth more than 100-fold\n  winner &lt;- max(sim$sim[(t+1),2:(sim$params[2,]+1)]) #wealth of wealthiest person\n  winnerShare &lt;- winner / sum(sim$sim[(t+1),2:(sim$params[2,]+1)]) #wealth share of wealthiest person\n\n  stats &lt;- data.frame(meanW = meanW, medianW = medianW, num99 = num99, per99 = per99, numGain = numGain, perGain = perGain, num100 = num100, winner = winner, winnerShare = winnerShare)\n}\n\nnonErgodicStats &lt;- summaryStats(nonErgodic, 100)\n\n# Create a function for plotting the average wealth of the population over a set number of periods.\n\naveragePlot &lt;- function(sim, t=100){\n\n  basePlot &lt;- ggplot(sim$sim[c(1:(t+1)),], aes(x=period)) +\n    labs(y = \"Average Wealth ($)\")\n\n  averagePlot &lt;- basePlot +\n    geom_line(aes(y = rowMeans(sim$sim[c(1:(t+1)),2:(sim$params[2,]+1)])), color = 1, linewidth=1)\n\n  averagePlot\n}\n\naveragePlot(nonErgodic, 100)\n\n\n\n\n\nFigure 1: Average wealth of population\n\n\n\n\n\n\n\n\nThis line in Figure 1 represents the mean wealth of the 10,000 people. It looks good, increasing roughly in accordance with the expected gain, despite some volatility, and finishing at a mean wealth of $16697.\nYet people regularly decline gambles of this nature. Are they making a mistake?\nOne explanation for declining this gamble is risk aversion. A risk-averse person will value the expected outcome of a gamble lower than the same sum with certainty.\nRisk aversion can be represented through the concept of utility, where each level of wealth gives subjective value (utility) for the gambler. If people maximise utility instead of the value of a gamble, a person would possibly reject the bet.\nFor example, one common utility function to represent a risk-averse individual is the logarithm of their wealth. If we apply the log utility function to the gamble above, the gambler will reject the offer of the coin flip. [The maths here is simply that the expected utility of the gamble is 0.5\\times \\ln(150) + 0.5\\times \\ln(60)=4.55, which is less than the utility of the sure $100, ln(100)=4.61.]"
  },
  {
    "objectID": "posts/ergodicity-economics-a-primer.html#the-time-average-growth-rate",
    "href": "posts/ergodicity-economics-a-primer.html#the-time-average-growth-rate",
    "title": "Ergodicity economics: a primer",
    "section": "3. The time average growth rate",
    "text": "3. The time average growth rate\nFor a different perspective, below is the plot for the first 20 of these 10,000 people. Interestingly, only two people do better than break even (represented by the black line at $100). The richest person has less than $1,000 at period 100.\n\n\nCode\n# Create a function for plotting the path of individuals in the population over a set number of flips.\n\nindividualPlot &lt;- function(sim, t, people){\n\n  basePlot &lt;- ggplot(sim$sim[c(1:(t+1)),], aes(x=period)) +\n    labs(y = \"Wealth ($)\")\n\n  for (i in 1:people) {\n    basePlot &lt;- basePlot +\n      geom_line(aes(y = !!sim$sim[c(1:(t+1)),(i+1)]), color = 2)\n  }\n\nbasePlot\n\n}\n\n# Plot of the path of the first 20 people over 100 periods (Figure 2).\n\nnonErgodicIndiv &lt;- individualPlot(nonErgodic, 100, 20)\nnonErgodicIndiv\n\n\n\n\n\nFigure 2: Path of first 20 people\n\n\n\n\n\n\n\n\nWhat is happening here? The first plot shows that the average wealth across all 10,000 people is increasing. When we look at the first 20 individuals, their wealth generally declines. Even those who make money make less than the gain in aggregate wealth would suggest.\nTo show this more starkly, here is a plot of the first 20 people on a log scale, together with the average wealth for the full population. They are all below average in final wealth.\n\n\nCode\n# Plot both the average outcome and first twenty people on the same plot.\n\njointPlot &lt;- function(sim, t, subset) {\n  individualPlot(sim, t, subset) +\n    geom_line(aes(y = rowMeans(sim$sim[c(1:(t+1)),2:(sim$params[2,]+1)])), color = 1, linewidth=1)+\n    scale_y_log10()\n}\n\nnonErgodicPlot &lt;- jointPlot(sim=nonErgodic, t=100, subset=20)\nnonErgodicPlot\n\n\n\n\n\nFigure 3: Plot of first 20 people against average wealth (log scale)\n\n\n\n\n\n\n\n\nIf we examine the full population of 10,000, we see an interesting pattern. The mean wealth is $16697, but the median wealth after 100 periods is $0.52, a loss of over 99% of the initial wealth. 54% of the population ends up with less than one dollar. 86% finishes with less than the initial wealth of $100. Yet 171 people end up with more than $10,000. The wealthiest person finishes with $117,183,281, which is 70% of the total wealth of the population.\nFor most people, the series of bets is a disaster. It looks good only on average, propped up by the extremely good luck and massive wealth of a few people. The expected payoff does not match the experience of most people.\n\n3.1 Four possible outcomes\nOne way to think about what is happening is to consider the four possible outcomes over the first two periods.\nThe first person gets two heads. They finish with $225. The second and third person get a head and a tail (in different orders) and finish with $90. The fourth person ends up with $36.\nThe average across the four is $110.25, reflecting the compound 5% growth. That’s our positive picture. But three of the four lost money. As the number of flips increases, the proportion who lose money increases, with a rarer but more extraordinarily rich cohort propping up the average.\n\n\n3.2 Almost surely\nOver the very long term, an individual will tend to get around half heads and half tails. As the number of flips goes to infinite, the proportion of heads or tails “almost surely” converges to 0.5.\nThis means that each person will tend to get a 50% increase half the time (or 1.5 times the initial wealth), and a 40% decrease half the time (60% of the initial wealth). A bit of maths and the time average growth in wealth for an individual is (1.5\\times 0.6)^{0.5} \\sim 0.95, or approximately a 5% decline in wealth each period. Every individual’s wealth will tend to decay at that rate.\nTo get an intuition for this, a long run of equal numbers of heads and tails is equivalent to flipping a head and a tail every two periods. Suppose that is exactly what you did - flipped a head and then flipped a tail. Your wealth would increase to $150 in the first round ($100\\times 1.5), and then decline to $90 in the second ($150\\times 0.6). You get the same result if you change the order. Effectively, you are losing 10% (or getting only 1.5\\times 0.6=0.9) of your money every two periods.\nA system where the time average converges to the ensemble average (our population mean) is known as an ergodic system. The system of gambles above is non-ergodic as the time average and the ensemble average diverge. And given we cannot individually experience the ensemble average, we should not be misled by it. The focus on ensemble averages, as is typically done in economics, can be misleading if the system is non-ergodic.\n\n\n3.3 The longer term\nHow can we reconcile this expectation of loss when looking at the time average growth with the continued growth of the wealth of some people after 100 periods? It does not seem that everyone is “almost surely” on the path to ruin.\nBut they are. If we plot the simulation for, say, 1,000 periods rather than 100, there are few winners. Here’s a plot of the average wealth of the population for 1000 periods (the first 100 being as previously shown), plus a log plot of that same growth (Figures 4 and 5).\n\n\nCode\nnonErgodicStats1000 &lt;- summaryStats(nonErgodic, 1000)\n\n# Plot the average wealth of the non-ergodic simulation over 1000 periods\n\naveragePlot(nonErgodic, 1000)\n\n\n\n\n\nFigure 4: Plot of average wealth over 1000 periods\n\n\n\n\n\n\n\n\n\n\nCode\n# Plot the average wealth of the non-ergodic simulation over 1000 periods using a log plot\n\naveragePlot(nonErgodic, 1000)+\n    scale_y_log10()\n\n\n\n\n\nFigure 5: Plot of average wealth over 1000 periods (log plot)\n\n\n\n\n\n\n\n\nWe can see that despite a large peak in wealth around period 400, wealth ultimately plummets. Average wealth at period 1000 is $24, below the starting average of $100, with a median wealth of 1x10-21 (rounding to the nearest cent, that is zero). The wealthiest person has $242 thousand dollars, with that being 98.5% of the total wealth. If we followed that wealthy person for another 1000 generations, I would expect them to be wiped out too. [I tested that. At 2000 periods the wealthiest person had $4x10-7.] Despite the positive expected value, the wealth of the entire population is wiped out."
  },
  {
    "objectID": "posts/ergodicity-economics-a-primer.html#losing-wealth-on-a-positive-value-bet",
    "href": "posts/ergodicity-economics-a-primer.html#losing-wealth-on-a-positive-value-bet",
    "title": "Ergodicity economics: a primer",
    "section": "4. Losing wealth on a positive value bet",
    "text": "4. Losing wealth on a positive value bet\nThe first 100 periods of bets forces us to hold a counterintuitive idea in our minds. While the population as an aggregate experiences outcomes reflecting the positive expected value of the bet, the typical person does not. The increase in wealth across the aggregate population is only due to the extreme wealth of a few lucky people.\nHowever, the picture over 1000 periods appears even more confusing. The positive expected value of the bet is nowhere to be seen. How could this be the case?\nThe answer to this lies in the distribution of bets. After 100 periods, one person had 70% of the wealth. We no longer have 10,000 equally weighted independent bets as we did in the first round. Instead, the path of the wealth of the population is largely subject to the outcome of the bets by this wealthy individual. As we have already shown, the wealth path for an individual almost surely leads to a compound 5% loss of wealth. That individual’s wealth is on borrowed time. The only way for someone to maintain their wealth would be to bet a smaller portion of their wealth, or to diversify their wealth across multiple bets."
  },
  {
    "objectID": "posts/ergodicity-economics-a-primer.html#the-kelly-criterion",
    "href": "posts/ergodicity-economics-a-primer.html#the-kelly-criterion",
    "title": "Ergodicity economics: a primer",
    "section": "5. The Kelly criterion",
    "text": "5. The Kelly criterion\nOn the first of these options, the portion of a person’s wealth they should enter as stakes for a positive expected value bet such as this is given by the Kelly Criterion. The Kelly criterion gives the bet size that would maximise the geometric growth rate in wealth.\nThe Kelly criterion formula for a simple bet is as follows:\nf=\\frac{bp-q}{b}=\\frac{p(b+1)-1}{b}\nwhere\n\nf is the fraction of the current bankroll to wager\nb is the net odds received on the wager (i.e. you receive $b back on top of the $1 wagered for the bet)\np is the probability of winning\nq is the probability of losing (1-p)\n\nFor the bet above, we have p=0.5 and b=0.5/0.4=1.25. As offered, we are effectively required to bet f=0.4, or 40% of our wealth, for that chance to win a 50% increase.\nHowever, if we apply the above formula given p and b, a person should bet 10%, of their wealth each round to maximise the geometric growth rate.\n\\frac{(0.5*(1.25+1)-1)}{1.25}=0.1\nThe Kelly criterion is effectively maximising the expected log utility of the bet through setting the size of the bet. The Kelly criterion will result in someone wanting to take a share of any bet with positive expected value.\nThe Kelly bet “almost surely”” leads to higher wealth than any other strategy in the long run.\nIf we simulate the above scenarios, but risking only 10% of wealth each round rather than 40% (i.e. heads wealth will increase by 12.5%, tails it will decrease by 10%), what happens? The expected value of the Kelly bet is 0.5\\times 0.125+0.5\\times -0.1=0.0125 or 1.25% per round. This next figure shows the ensemble average, showing a steady increase.\n\n\nCode\n# Calculate the optimum Kelly bet size.\n\np &lt;- 0.5\nq &lt;- 1-p\nb &lt;- (1.5-1)/(1-0.6)\nf &lt;- (b*p-q)/b\n\n# Run a simulation using the optimum bet size.\n\nset.seed(20191215)\nkelly &lt;- bet(p=0.5, n=10000, t=1000, gain=1+f*b, loss=1-f, ergodic=FALSE)\n\nkellyStats &lt;- summaryStats(kelly, 1000)\n\n# Plot ensemble average of Kelly bets\n\naveragePlotKelly &lt;- averagePlot(kelly, 1000)\naveragePlotKelly\n\n\n\n\n\nFigure 6: Average wealth of population applying Kelly criterion (1000 periods)\n\n\n\n\n\n\n\n\nIf we look at the individuals in this population, we can also see that their paths more closely resemble that of the population average. Most still under-perform the mean (the system is still non-ergodic - the time average growth rate is (1.125\\times 0.9)^{0.5}=1.006 or 0.6%), and there is a large wealth disparity with the wealthiest person having 36% of the total wealth after 1000 periods (after 100, they have 0.5% of the wealth). Still, most people are better off, with 70% and 95% of the population experiencing a gain after 100 and 1000 periods respectively. The median wealth is almost $50,000 after 1000 periods.\n\n\nCode\n# Plot of the path of the first 20 people over 1000 periods\n\nlogPlotKelly &lt;- jointPlot(kelly, 1000, 20)\nlogPlotKelly\n\n\n\n\n\nFigure 7: Plot of first 20 people applying Kelly criterion against average wealth (log scale, 1000 periods)\n\n\n\n\n\n\n\n\nUnfortunately, given the take-it or leave-it choice we opened with involving 40% of our wealth, we can’t use the Kelly Criterion to optimise the bet size and should refuse the bet.\nUpdate clarifying some comments on this post:\nAn alternative more general formula for the Kelly criterion that can be used for investment decisions is:\nf=\\frac{p}{a}-\\frac{q}{b}\nwhere\n\nf is the fraction of the current bankroll to invest\nb is the value by which your investment increases (i.e. you receive $b back on top of each $1 you invested)\na is the value by which your investment decreases if you lose (the first formula above assumes a=1)\np is the probability of winning\nq is the probability of losing (1-p)\n\nApplying this formula to the original bet at the beginning of this post, a=0.4 and b=0.5, by which f=0.5/0.4-0.5/0.5=0.25 or 25%. Therefore, you should put up 25% of your wealth, of which you could potentially lose 40% or win 50%.\nThis new formulation of the Kelly criterion gives the same recommendation as the former but refers to different baselines. In the first case, the optimal bet is 10% of your wealth, which provides for a potential win of 12.5%. In the second case, you invest 25% of your wealth to possibly get a 50% return (12.5% of your wealth) or lose 40% of your investment (40% of 25% which is 10%). Despite the same effective recommendation, in one case you talk of f being 10%, and in the second 25%."
  },
  {
    "objectID": "posts/ergodicity-economics-a-primer.html#evolving-preferences",
    "href": "posts/ergodicity-economics-a-primer.html#evolving-preferences",
    "title": "Ergodicity economics: a primer",
    "section": "6. Evolving preferences",
    "text": "6. Evolving preferences\nSuppose two types of agent lived in this non-ergodic world and their fitness was dependent on the outcome of the 50:50 bet for a 50% gain or 40% loss. One type always accepted the bet, the other always rejected it. Which would come to dominate the population?\nAn intuitive reaction to the above examples might be that while the accepting type might have a short-term gain, in the long run they are almost surely going to drive themselves extinct. There are a couple of scenarios where that would be the case.\nOne is where the children of a particular type were all bound to the same coin flip as their siblings for subsequent bets. Suppose one individual had over 1 million children after 100 periods, comprising around 70% of the population (which is what they would have if we borrowed the above simulations for our evolutionary scenario, with one coin flip per generation). If all had to bet on exactly the same coin flip in period 101 and beyond, they are doomed.\nIf, however, each child faces their own coin flip (experiencing, say, idiosyncratic risks), that crash never comes. Instead, the risk of those flips is diversified and the growth of the population more closely resembles the ensemble average, even over the very long term.\nBelow is a chart of population for a simulation of 100 generations of the accepting population, starting with a population of 10,000. For this simulation, I have assumed that at the end of each period, the accepting types will have a number of children equal to the proportional increase in their wealth. For example, if they flip heads, they will have 1.5 children, For tails, they will have 0.6 children. They then die. (The simulation works out largely the same if I make the number of children probabilistic in accord with those numbers.) Each child takes their own flip.\n\n\nCode\nset.seed(20191215)\nevolutionBet &lt;- function(p, n, t, gain, loss){\n\n  #p is probability of a gain\n  #region  is how many people in the simulation\n  #t is the number of generations simulated\n\n  params &lt;- as.data.frame(c(p, n, t, gain, loss))\n  rownames(params) &lt;- c(\"p\", \"n\", \"t\", \"gain\", \"loss\")\n  colnames(params) &lt;- \"value\"\n\n  sim &lt;- matrix(data = NA, nrow = t, ncol = 1)\n\n  sim &lt;- rbind(n, sim) #placing the starting population in the first row\n\n  for (i in 1:t) {\n    for (j in 1:round(n)) {\n      outcome &lt;- rbinom(n=1, size=1, prob=p)\n      ifelse(outcome==0, x &lt;- loss, x &lt;- gain)\n      n &lt;- n + (x-1)\n    }\n    n &lt;- round(n)\n    sim[i+1] &lt;- n #\"+1\" as have starting population in first row\n  }\n\n  sim &lt;- cbind(seq(0,t), sim) #number each period\n  sim &lt;- data.frame(sim, row.names=NULL)\n  colnames(sim) &lt;- c(\"period\", \"n\")\n  sim &lt;- list(params=params, sim=sim)\n  sim\n}\n\nevolution &lt;- evolutionBet(p=0.5, n=10000, t=100, gain=1.5, loss=0.6) #more than 100 periods can take a very long time, simulation slows markedly as population grows\n\n# Plot the population growth for the evolutionary scenario (Figure 8).\n\nbasePlotEvo &lt;- ggplot(evolution$sim[c(1:101),], aes(x=period))\n\nexpectationPlotEvo &lt;- basePlotEvo +\n  geom_line(aes(y=n), color = 1, linewidth=1) +\n  labs(y = \"Population\")\n\nexpectationPlotEvo\n\n\n\n\n\nFigure 8: Population of accepting types\n\n\n\n\n\n\n\n\nThis has an expected population growth rate of 5%.\nThis evolutionary scenario differs from the Kelly criterion in that the accepting types are effectively able to take many independent shares of the bet for a tiny fraction of their inclusive fitness.\nIn a Nature Physics paper summarising some of his work, Peters writes:\n\n[I]n maximizing the expectation value - an ensemble average over all possible outcomes of the gamble - expected utility theory implicitly assumes that individuals can interact with copies of themselves, effectively in parallel universes (the other members of the ensemble). An expectation value of a non-ergodic observable physically corresponds to pooling and sharing among many entities. That may reflect what happens in a specially designed large collective, but it doesn’t reflect the situation of an individual decision-maker.\n\nFor a replicating entity that can diversify future bets across many offspring, they can do just this.\nThere are a lot of wrinkles that could be thrown into this simulation. How many bets does someone have to make before they reproduce and effectively diversify their future? The more bets, the higher the chance of a poor end. There is also the question of whether bets by children would be truly independent (Imagine a highly related tribe)."
  },
  {
    "objectID": "posts/ergodicity-economics-a-primer.html#risk-and-loss-aversion-in-ergodicity-economics",
    "href": "posts/ergodicity-economics-a-primer.html#risk-and-loss-aversion-in-ergodicity-economics",
    "title": "Ergodicity economics: a primer",
    "section": "7. Risk and loss aversion in ergodicity economics",
    "text": "7. Risk and loss aversion in ergodicity economics\nIn my next post on this topic I ask whether, given the above, we need risk and loss aversion to explain our choices."
  },
  {
    "objectID": "posts/how-i-focus-and-live.html",
    "href": "posts/how-i-focus-and-live.html",
    "title": "How I focus (and live)",
    "section": "",
    "text": "This post is a record of some strategies that I use to focus and be mildly productive. It also records a few other features of my lifestyle.\nWhy develop these strategies? On top of delivering in my day job, I have always tried to invest heavily in my human capital, and that takes a degree of focus.\nThe need to adopt many of the below also reflects how easily distracted I am. I have horrible habits when I get in front of a device. The advent of the web has been a mixed blessing for me.\nMy approaches can shift markedly over time, so it will be interesting to see which of the below are still reflected in my behaviour in a couple of years (and which continue to be supported by the evidence as effective).\nIf there is a common theme to the below, it is that creating the right environment, not reliance on willpower, is the path to success.\nPeriods of focus: Most of my productive output occurs in two places. One is on the train, with an hour commute at the beginning and end of each day that I travel to work. The only activities I do on the train are reading (books or articles) and writing. Internet is turned off. This is now an ingrained habit. The train is largely empty for most of the journey, with half through a national park, so it’s a pleasant way to work.\nThe rest of my output occurs in productive blocks (pomodoros) during the day. At the beginning of each day I schedule a set of half-hour blocks in my diary around my other commitments. In these blocks, I will turn off or close everything I don’t need for the task. I am typically less successful at putting up barriers to human (as opposed to digital) interruptions, except for occasionally closing my office door.\nIdeally I will have several blocks in a row (in the morning), with a couple of minutes to stretch in between. I aim for at least 20 half-hour sessions each week. I average maybe 30. I block out the occasional morning in my diary to make sure each week is not completely filled with meetings (with eight direct reports and working in a bureaucracy, that is a real risk).\nI also read whenever I can, and that fills a lot of the other space in my life. I read around 100 books per year (about 70-80 non-fiction).\nPhone: My iPhone is used for four main purposes: as a phone; as a train timetable; as a listening device (podcasts, audiobooks and music); and for my meditation apps (more on meditation below). It also has a few utilities such as Uber that I rarely use. I don’t use my phone for social media, as a diary, or for email. Most of the day it stays in my pocket or on my desk. All notifications, except calls and text messages, are turned off. I rarely have any reason to look at it.\nEven when I do look at my phone, the view is sparse. These are the two screens I see.\n\n\n\n\n\n\n\n\n\n\nOne thing you can’t see in these screenshots (for some strange technical reason) is that my phone is in grey scale. There is little colour to get me excited (although I am colour blind….). Except when I make a phone call, message someone, or (loosely) lock the phone with Forest, I use search to find the app. They are hidden in the Dump folder. When I go to my phone, there is little to divert me from my original intention.\niPad: I have an iPad, and it is similarly constrained. All notifications are turned off. It has email, but the account is turned off in settings, with account changes restricted. It takes me about a minute to disable restrictions to turn email on, which slows me down enough to make sure I am checking it for a reason. More on email below.\nI also use the iPad for reading and writing (including these posts) on the train. When reading, I use my Kindle in preference to my iPad when I can, as the Kindle has far fewer rabbit holes.\nInternet: I subscribe to Freedom which cuts off internet for certain apps and certain times. Among other things, I use it to block the internet from 8pm through to 7am (I don’t want to be checking email or browsing when I first get up), and on Sundays (generally a screen free day). I also use Freedom to shut off internet or certain apps at ad hoc times when I want to focus.\nI try not to randomly browse at other times. I have little interest in news (see below), so that reduces the probability of messing around. I have previously used RescueTime to track my time online, but don’t currently as I can’t install it on my work computer, phone or iPad. The tracking had a subtle but limited effect on my behaviour on my home computer when I tried it.\nEmail: Currently my biggest failure, particularly when I am in the office. I aim to batch my email to a few times per day, but I check and am distracted by new emails more often than I would like. Partly that is because part of my workflow occurs through email, so it is hard not to look.\nSocial media: I have a Facebook account, but zero friends, so it provides little distraction. (I also like that when I run into people who I haven’t seen for a while, I don’t already know what they have been up to.) I only have the account because this blog has a Facebook page. I try to limit my visits to Twitter and LinkedIn to once a week (normally successful with Twitter, less so with LinkedIn as direct messages sometimes draw me in). Freedom helps constrain this.\nPaper diary: My paper diary is an attempt to keep myself away from distracting devices. I also find it faster than the electronic alternative. I have an electronic calendar for work, but it is replicated in the paper diary.\nNews: I consume little news. I don’t have a television, don’t purchase newspapers and don’t visit internet news sites unless I follow a link based on a recommendation. I rarely miss anything important. If something big happens, someone will normally tell me.\nI used to apply a filter to political news of “if this was happening in Canada, would I care?” That eliminated most political news, but I have found that after a few years, I have become so disconnected from Australian politics that most of it flows around me. I don’t recognise most politicians, and I feel unconnected to any of the personalities. Voting is compulsory in Australia, so to avoid being fined or voting for people I know nothing about, I get my name ticked off the electoral roll at a polling place, take the voting slip, but don’t bother filling it out. (And I have almost no idea what Trump is up to.)\nI am in a similar place for sports news. Now that I have been disconnected for a while, I have no interest. Any names I overhear mean nothing to me. I couldn’t tell you who won any of the tennis grand slams last year or who the World Series champion is. I don’t think I could recognise a current Australian cricketer on sight.\nBlogs: In substitute to going to any news sources, I subscribe to around 25 blogs using a feed reader (Feedly). I scan them around once a day. They provide more reading material than I can get through (through the posts themselves or links), so I have a backlog of reading material in Instapaper (I used to use Pocket, but dumped it when the ads appeared).\nSleep and rest: The evidence on the effect of lack of sleep is strong. I need eight hours a night and generally get it (children permitting). I don’t use screens (except for the Kindle) after 8pm at night. I also subscribe to the broader need for rest and the declining productivity that comes from overwork.\nMeditation: Meditation is new for me (around four months), and I am still in the experimental phase. I meditate for around 15 to 20 minutes every day. I find it puts me on the right track at the start of the day (which is when I meditate, children permitting). It also acts as a daily reminder of what I am trying to do.\nThe evidence of increased concentration and emotional control seems strong enough to give it a go. I suspect I would have dismissed the idea a few years ago (maybe even a year ago), and pending changes in the evidence in favour and my own experience, I am prepared to dismiss it again in the future.\nA benchmark I’d like to be able to compare meditation to is focused reading. If I shifted the meditation time to reading, that’s 15 to 20 additional books a year. What is the balance of costs and benefits?\nI use three apps to meditate: Insight Timer, Headspace and 10% Happier. I find 10% Happier most useful as a teacher. Headspace is convenient and easy to use, but I don’t like the gamification element to it, and the packages seem relatively shallow and repetitive (although the repetitive nature is not necessarily a bad thing). At the end of the year when it is time to re-subscribe, I suspect I will drop Headspace and stick with 10% Happier if I am still learning something from it. Insight Timer will otherwise give me what I need.\nI will post more on my thoughts on meditation in the near future - likely through a review of Sam Harris’s Waking Up in the first instance, as that was the book that pushed me across the line.\nI give myself a 60% chance of still being meditating when I write my next post of what I do to focus (planning to do this roughly annually). My lapsing could be due to either changing my mind or failing to sustain the habit.\nDiet: I see diet as closely linked to the ability to focus and be productive. I eat well. My diet might best be described as three parts Paleo, one part early agriculturalist, and 5% rubbish. My diet is mainly fruit (lots), vegetables, tubers, nuts, eggs (a dozen a week), meat, legumes and dairy (a lot of yogurt). I eat grains occasionally, largely in the form of rice (a few times of week) and porridge (once or twice a week). I’ll eat bread maybe once or twice a month (I love hamburgers and eggs on toast). A heuristic I often fall back onto is no processed grains, industrial seed oils or added sugar. There’s some arbitrariness to it, but it works. Stephan Guyenet is my most trusted source on diet.\nIt’s easy to stick to this diet because this is what is in my house. There are no cookies, ice cream or sugar based snacks. I don’t have to go down the aisles of the supermarket when shopping (although my groceries are normally home delivered). If I want to binge, rice crackers and toast are as exciting as I can find in the cupboard.\nExercise: As for diet, part of the productivity package. My major filter for choosing exercise is the desire to still be able to surf and get off the toilet when I’m 80. I surf a couple of times a week. Living within five minutes walk of a beach with good surf is a basic lifestyle criteria.\nI did Crossfit for a few years, but don’t live near a Crossfit gym at the moment. However, I don’t think Crossfit is a sustainable long-term approach - at least if I trained as regularly as expected in the gyms I have been to. The intensity would have me falling apart in old age.\nThat said, I still keep Crossfit elements to my exercise - heavy compound lifts once or twice a week, and a short high intensity burst around once a week (so I’m in the gym once to twice a week). I also walk a lot, including trying to get out of the office for a decent walk at lunch each day. While walking, I consume a lot of audiobooks and podcasts. I stretch for 10 to 15 minutes most days."
  },
  {
    "objectID": "posts/humans-1-chimps-0-correcting-the-record.html",
    "href": "posts/humans-1-chimps-0-correcting-the-record.html",
    "title": "Humans 1, Chimps 0: Correcting the Record",
    "section": "",
    "text": "In 2012, I wrote a post titled Chimps 1, Humans 0 after seeing videos of a chimp named Ayumu. Ayumu could recall the location of numbers, in order, flashed briefly on a screen. Ayumu’s performance far exceeded my feeble attempts. See the below videos to get a sense of the task.\nThe human\n\nAyumu\n\nThis performance, documented by Inoue and Matsuzawa (2007) in Current Biology, was used to assert that chimps have superior working memory to humans. The claim has spread widely, as a brief search on Twitter and Google shows.\nWhen I wrote that post, I didn’t know that this conclusion was already without basis. Here’s Peter Cook and Margaret Wilson (2010a) in Science:\n\nAyumu received extensive practice on the task; the humans to whom he was compared received none. At least one subsequent study (2) shows that, with even very moderate practice, humans can match Ayumu’s performance.\nIn spite of this basic methodological error, the claim of superior spatial working memory in chimpanzees has been widely and uncritically repeated in the popular and scientific media. Propagation of this incorrect idea distracts from more fruitful explorations of chimpanzee memory and undermines ongoing research into human and primate evolution.\n\nThe paper referenced at (2) was by Silberberg and Kearns (2009), who found that trained people could match Ayumu’s performance. Cook and Wilson (2010b) subsequently trained two university students to a level superior to the chimpanzee. A more recent literature review by Read et al. (2022) suggested that chimp working memory matches that of 4 to 5-year-old humans. However, at around two digits (plus or minus one), chimp working memory falls short of adult human performance. (Read et al. also question whether this task is even a test of working memory.)\nPutting it together, the initial claim of superior working memory doesn’t hold up. Ayumu’s performance is impressive at first sight, but that’s about it.\n*Someone brought this to my attention back in November (that’s the date on my note to blog this), but I can’t remember who!\n\n\n\n\nReferences\n\nCook, P., and Wilson, M. (2010a). In practice, chimp memory study flawed. Science, 328(5983), 1228–1228. https://doi.org/10.1126/science.328.5983.1228-c\n\n\nCook, P., and Wilson, M. (2010b). Do young chimpanzees have extraordinary working memory? Psychonomic Bulletin & Review, 17(4), 599–600. https://doi.org/10.3758/pbr.17.4.599\n\n\nInoue, S., and Matsuzawa, T. (2007). Working memory of numerals in chimpanzees. Current Biology, 17(23), R1004–R1005. https://doi.org/10.1016/j.cub.2007.10.027\n\n\nRead, D. W., Manrique, H. M., and Walker, M. J. (2022). On the working memory of humans and great apes: Strikingly similar or remarkably different? Neuroscience & Biobehavioral Reviews, 134, 104496. https://doi.org/10.1016/j.neubiorev.2021.12.019\n\n\nSilberberg, A., and Kearns, D. (2009). Memory for the order of briefly presented numerals in humans as a function of practice. Animal Cognition, 12(2), 405–407. https://doi.org/10.1007/s10071-008-0206-8"
  },
  {
    "objectID": "posts/is-the-marshmallow-test-just-a-measure-of-affluence.html",
    "href": "posts/is-the-marshmallow-test-just-a-measure-of-affluence.html",
    "title": "Is the marshmallow test just a measure of affluence?",
    "section": "",
    "text": "I argued in a recent post that the conceptual replication of the marshmallow test was largely successful. A single data point - whether someone can wait for a larger reward - predicts future achievement.\nThat replication has generated a lot of commentary. Most concerns the extension to the original study, an examination of whether the marshmallow test retained its predictive power if they accounted for factors such as the parent and child’s background (including socioeconomic status), home environment, and measures of the child’s behavioural and cognitive development.\nThe result was that these “controls” eliminated the predictive power of the marshmallow test. If you know those other variables, the marshmallow test does not give you any further information.\nAs I said before, this is hardly surprising. They used around 30 controls - 14 for child and parent background, 9 for the quality of the home environment, 5 for childhood achievement and 2 for behavioural characteristics. It is likely that many of them capture the features that give the marshmallow test its predictive power.\nSo can we draw any conclusions from the inclusion of those particular controls? One of the most circulated interpretations is by Jessica Calarco in the Atlantic, titled Why Rich Kids Are So Good at the Marshmallow Test. The subtitle is “Affluence—not willpower—seems to be what’s behind some kids’ capacity to delay gratification”. Calarco writes:\n\nUltimately, the new study finds limited support for the idea that being able to delay gratification leads to better outcomes. Instead, it suggests that the capacity to hold out for a second marshmallow is shaped in large part by a child’s social and economic background—and, in turn, that that background, not the ability to delay gratification, is what’s behind kids’ long-term success.\n\nThis conclusion is a step too far. For a start, controlling for child background and home environment (slightly more than) halved the predictive power of the marshmallow test. It did not eliminate it. It was only on including additional behavioural and cognitive controls - characteristics of the child themselves - that the predictive power of the marshmallow test was eliminated\nBut the more interesting question in one of causation. Are the social and economic characteristics themselves the cause of later achievement?\nOne story we could tell is that the social and economic characteristics are simply proxies for parental characteristics, which are genetically transmitted to the children. Heritability of traits such as IQ tend to increase with age, so parental characteristics would likely have predictive power in addition to that of the four-year old’s cognitive and behavioural skills.\nOn the flipside, maybe the behavioural and cognitive characteristics of the child are simply reflections of the development environment that the child has been exposed to date. This is effectively Calarco’s interpretation.\nWhich is the right interpretation? This study doesn’t help answer this question. It was never designed to. As lead study author Tyler Watts tweeted in response to the Atlantic article:\nhttps://twitter.com/tw_watts/status/1002992853470064640?s=20\nIf you want to know whether social and economic background causes future success, you should look elsewhere. (I’d start with twin and adoption studies.)\nThat said, there were a couple of interesting elements to this new study. While the marshmallow test was predictive of future achievement at age 15, there was no association between the marshmallow test and two composite measure of behaviours at 15. The composite behaviour measures were for internalising behaviours (such as depression) and externalising behaviours (such as anti-social behaviours). This inability to predict future behavioural problems hints that the marshmallow test may obtain its predictive power through the cognitive rather than the behavioural channel.\nThis possibility is also suggested by the correlation between the marshmallow test and the Applied Problems test, which requires the children to count and solve simple addition problems.\n\n[T]he marshmallow test had the strongest correlation with the Applied Problems subtest of the WJ-R, r(916) = .37, p &lt; .001; and correlations with measures of attention, impulsivity, and self-control were lower in magnitude (rs = .22–.30, p &lt; .001). Although these correlational results were far from conclusive, they suggest that the marshmallow test should not be thought of as a mere behavioral proxy for self-control, as the measure clearly relates strongly to basic measures of cognitive capacity.\n\nNot conclusive, but it points to some areas worth further exploring.\nPS: After writing this post (I usually post on delay of between a week and three months), Robert VerBruggen posted a piece at the Institute for Family Studies, making many of the same points. I would have skipped writing the new content - and simply quoted VerBruggen - if I’d seen it earlier. Inside Higher Ed also has a good write-up by Greg Toppo, including this quote from Walter Mischel:\n\n[A] child’s ability to wait in the ‘marshmallow test’ situation reflects that child’s ability to engage various cognitive and emotion-regulation strategies and skills that make the waiting situation less frustrating. Therefore, it is expected and predictable, as the Watts paper shows, that once these cognitive and emotion-regulation skills, which are the skills that are essential for waiting, are statistically ‘controlled out,’ the correlation is indeed diminished.\n\nAlso from Mischel:\n\nUnfortunately, our 1990 paper’s own cautions to resist sweeping over-generalizations, and the volume of research exploring the conditions and skills underlying the ability to wait, have been put aside for more exciting but very misleading headline stories over many years.\n\nPPS: In another thread to her article, Calarco draws on the concept of scarcity:\n\nThere’s plenty of other research that sheds further light on the class dimension of the marshmallow test. The Harvard economist Sendhil Mullainathan and the Princeton behavioral scientist Eldar Shafir wrote a book in 2013, Scarcity: Why Having Too Little Means So Much, that detailed how poverty can lead people to opt for short-term rather than long-term rewards; the state of not having enough can change the way people think about what’s available now. In other words, a second marshmallow seems irrelevant when a child has reason to believe that the first one might vanish.\n\nI’ve written about scarcity previously in my review of Mullainathan and Shafir’s book. I’m not sure the work on scarcity sheds light on the marshmallow test results. The concept behind scarcity is that poverty-related concerns consume mental bandwidth that isn’t then available for other tasks. A typical experiment to demonstrate scarcity involves priming the experimental subjects with a problem before testing their IQ. When the problem has a large financial cost (e.g. expensive car repairs), the performance of low-income people plunges. Focusing their attention on their lack of resources consumes mental bandwidth. On applying this to the marshmallow test, I haven’t seen much evidence four-year olds are struggling with this problem.\n(As an aside, scarcity seems to be the catchall response to discussions of IQ and achievement, a bit like epigenetics is the response to any discussion of genetics.)\nGiven Calarco’s willingness to bundle the marshmallow test replication into the replication crisis (calling it a “failed replication”), its worth also thinking about scarcity in that light. If I had to predict which results would not survive a pre-registered replication, the experiments in the original scarcity paper are right up there. They involve priming, the poster-child for failed replications. The size of the effect, 13 IQ points from a simple prime, fails the “effect is too large” heuristic.\nThen there is a study that looked at low-income households before and after payday, which found no change in cognitive function either side of that day (you could consider this a “conceptual replication”). In addition, for a while now I have been hearing rumours of file drawers containing failed attempts to elicit the scarcity mindset. I was able to find one pre-registered direct replication, but it doesn’t seem the result has been published. (Sitting in a file drawer somewhere?)\nThere was even debate around whether the original scarcity paper (pdf) showed the claimed result. Reanalysis of the data without dichotomising income (splitting it into two bands rather than treating it as a continuous variable) eliminated the effect. The original authors managed to then resurrect the effect (pdf) by combining the data from three experiments, but once you are at this point, you have well and truly entered the garden of forking paths."
  },
  {
    "objectID": "posts/megastudy-scepticism.html",
    "href": "posts/megastudy-scepticism.html",
    "title": "Megastudy scepticism",
    "section": "",
    "text": "In December last year Katherine Milkman and friends published a “megastudy” testing 54 interventions to increase the gym visits of 61,000 experimental participants. But more than just testing these interventions, the long list of authors stated:\n\nPolicy-makers are increasingly turning to behavioural science for insights about how to improve citizens’ decisions and outcomes. Typically, different scientists test different intervention ideas in different samples using different outcomes over different time intervals. The lack of comparability of such individual investigations limits their potential to inform policy. Here, to address this limitation and accelerate the pace of discovery, we introduce the megastudy - a massive field experiment in which the effects of many different interventions are compared in the same population on the same objectively measured outcome for the same duration.\n\nIn this post I am going to pull apart this experiment and the concept of the megastudy. At its heart, the problem the megastudy seeks to address also constrains its value. It’s also not big enough! There are some great things about this study - I’ll discuss them too - but I’ll describe the experiment and focus on the criticism first."
  },
  {
    "objectID": "posts/megastudy-scepticism.html#introduction",
    "href": "posts/megastudy-scepticism.html#introduction",
    "title": "Megastudy scepticism",
    "section": "",
    "text": "In December last year Katherine Milkman and friends published a “megastudy” testing 54 interventions to increase the gym visits of 61,000 experimental participants. But more than just testing these interventions, the long list of authors stated:\n\nPolicy-makers are increasingly turning to behavioural science for insights about how to improve citizens’ decisions and outcomes. Typically, different scientists test different intervention ideas in different samples using different outcomes over different time intervals. The lack of comparability of such individual investigations limits their potential to inform policy. Here, to address this limitation and accelerate the pace of discovery, we introduce the megastudy - a massive field experiment in which the effects of many different interventions are compared in the same population on the same objectively measured outcome for the same duration.\n\nIn this post I am going to pull apart this experiment and the concept of the megastudy. At its heart, the problem the megastudy seeks to address also constrains its value. It’s also not big enough! There are some great things about this study - I’ll discuss them too - but I’ll describe the experiment and focus on the criticism first."
  },
  {
    "objectID": "posts/megastudy-scepticism.html#the-experiment",
    "href": "posts/megastudy-scepticism.html#the-experiment",
    "title": "Megastudy scepticism",
    "section": "1. The experiment",
    "text": "1. The experiment\nMembers of a national gym chain were asked if they wished to enrol in a “habit-building science-based workout program”. Those who signed up formed the subject pool and were randomly assigned to the experimental conditions, including a control under which they received no further contact.\nOver the following 28-days participants were subject to interventions involving varying mixes of incentives and messages. For example, those in the “3. Social norm (high and increasing)” treatment group received six text message reminders, with content such as:\n\n“Trivia time! What percent of Americans exercised at least 3 times per week in 2016? Reply 1 for 61%, 2 for 64%, 3 for 70% or 4 for 73%”.\n\nIf they respond 1, 2 or 3, they receive a message back stating:\n\nIt’s actually 73%. And this is up from 71% in 2015.\n\nThey also receive emails with similar facts.\nThose in the “47. Social norm (low and decreasing)” group received messages with a less rosy situation and trend:\n\nTrivia time! What percent of Americans exercised at least 3 times per week in 2016? Reply 1 for 35%, 2 for 38%, 3 for 41% or 4 for 44%\n\nAs an aside, there don’t seem be any qualms about using deception here.\nSome interventions involved incentives. For example, the “14. Rigidity Rewarded” intervention paid 500 Amazon points worth $1.79 each time they attended a planned gym visit, and 250 Amazon points worth $0.90 if they attended the gym at another time.\nThe headline results of all the interventions tested in the megastudy are in Figure 1 below. The intervention with the largest effect size involved incentives for returning to the gym after a missed workout.\n\n\nFigure 1: Measured versus predicted changes in weekly gym visits induced by interventions\n\n\n\nTwenty-four of the 53 interventions were found to have a statistically significant effect over the control, increasing visits by between 9% and 27%. That equates to 0.14 to 0.40 extra weekly gym visits over the control average of 1.48 visits per week.\nThis figure also contains predictions made by behavioural practitioners, public health academics and lay people. More on those predictions below."
  },
  {
    "objectID": "posts/megastudy-scepticism.html#comparability",
    "href": "posts/megastudy-scepticism.html#comparability",
    "title": "Megastudy scepticism",
    "section": "2. Comparability",
    "text": "2. Comparability\nAs stated in the abstract, the fundamental problem that the megastudy is designed to address is the lack of comparability of interventions across experiments. The context of two different experiments may be sufficiently different that it is not reasonable to ask which intervention is more effective.\nOn one level, the megastudy solves this problem. The interventions are in the same context. Comparison is easy.\nBut this belief that we cannot easily compare across experiments in different contexts implies a limit to what you learn from the megastudy. What confidence can you have that the ordering or magnitude of intervention effect sizes in the megastudy will be reflected in a different context?\nWe are in a Catch-22 situation. The bigger the comparability problem that the megastudy is seeking to solve, the less useful the megastudy results are for application in other contexts.\nUltimately, this is why good policy or business advice should typically be to run an experiment. A megastudy could provide guidance as to what interventions might be more successful. But I am not convinced a megastudy provides more insight than an equivalent set of experiments across a few different contexts (although it might provide different insight, as will be noted below). If there is large variation in contexts, seeing a few effective interventions tested across different domains might give more confidence in the robustness of the phenomena than one megastudy.\nSo what should we take from this particular megastudy?\nThe context is gym attendance. But more than that, it is gym attendance among a group of gym members at a particular gym who self-selected into a digital program to build better gym habits. Incentives weren’t simply dollars. They were Amazon points. And those Amazon points had different values depending on the intervention.\nTo what other contexts might you be willing to take the results? Further interventions at this same gym? What of another similar gym? A gym with less similar demographics, business model or mode of operation? Gym members who haven’t signed up to a habit building program? A program with physical sign up? Cash incentives instead of Amazon points? People who don’t yet exercise or have gym memberships? Exercise more generally? Other positive habits?\nBeyond similar gyms I wouldn’t take those results too far."
  },
  {
    "objectID": "posts/megastudy-scepticism.html#power",
    "href": "posts/megastudy-scepticism.html#power",
    "title": "Megastudy scepticism",
    "section": "3. Power",
    "text": "3. Power\nWe then come to the question of what we can actually learn.\nOn its face, the megastudy has the benefit of getting a large sample. A total of 61,293 participants sounds solid.\nBut it doesn’t take much thought to realise that across 54 interventions (including the control) there is an average of not much more than 1000 participants per intervention, with most interventions having less than a thousand participants in execution. This number of participants enables the detection of a mean difference in effect size of around 0.32 gym visits with 90% power.\nThe largest effect of any intervention was an increase of 0.4 gym visits. The sample size enables differentiation from the control for 24 of the interventions, but when you have 53 interventions jammed into that range of 0.4 you aren’t going to be able to distinguish many of them from each other. As a result, only 13 of the 53 interventions were statistically distinguished from any interventions beyond the control. Even the best performing intervention could be distinguished from less than half of the other interventions.\nYou can see one cost of this low power when you examine the pre-registered analysis plans. One plan considered the effect of social norms. The communicated norms including variations in the level of exercise and the trend. I gave a couple of examples of the social norm interventions in the description of the experiment above. Table 1 shows the four social norm interventions and the average effect size (in additional gym visits) of each.\nTable 1: Exercise social norms (included 6 text message reminders)\n\n\n\nIntervention\nEffect size\n\n\n\n\n3. High and increasing\n0.345\n\n\n20. Low\n0.193\n\n\n47. Low but increasing\n0.052\n\n\n53. High\n-0.030\n\n\n\nThe “3. High and increasing” intervention was significantly different from the “53. High” intervention, but otherwise the four intervention effect sizes could not be statistically distinguished. And how confident would you be that this isn’t an outlier result, given the “53. High” intervention involved a social norm which typically (if you believe the literature) has a positive effect? It was the only intervention with a negative effect size relative to the control (although not a statistically significant difference). And if you tried to draw a conclusion about whether the trend in exercise is what is important, we have statistically insignificant opposite effects depending on whether the trend is paired with a high or low baseline.\nAbsent the requisite power, to make sense of these results you find yourself referring to other literature. But this was part of what the megastudy is designed to enable you to avoid.\nThe power issue isn’t restricted to this particular megastudy. A megastudy on vaccinations published in May 2021 by most of the same authors has the same problem. The 19 interventions boosted vaccinations by an average of 2.1 percentage points, but the authors noted that “we cannot reject the null hypothesis that all 19 effects have the same true value”. They use some post-hoc analysis to propose some statistically significant differences, but pulling those differences out after the fact is less than convincing.\nA third megastudy (also by many of the same authors) released earlier this year gets closer to the mark. All 22 messages to increase vaccination had an effect size that was a statistically significant difference from the control. (Reminders work!) They were also able to reject the hypothesis that all effects have the same true value."
  },
  {
    "objectID": "posts/megastudy-scepticism.html#the-predictions",
    "href": "posts/megastudy-scepticism.html#the-predictions",
    "title": "Megastudy scepticism",
    "section": "4. The predictions",
    "text": "4. The predictions\nOne interesting concept in this megastudy is that the researchers obtained predictions of the effect sizes of each intervention from third party observers: applied behavioural practitioners, public health academics and lay people.\nThe authors draw two conclusions from the predictions they received.\nThe first is that all three types of third-party observers overestimate the effect size of the interventions by around an order of magnitude. That is a fair conclusion for this particular study, although there is ambiguous evidence as to whether this is a more general trend. For example, Stephano DellaVigna and Elizabeth Linos found that academics overestimated effect sizes in applied trials, whereas applied practitioners did not. Earlier work by DellaVigna and Devin Pope found roughly accurate calibration for “nudges” but underestimation for incentives. In the third megastudy noted above lay people had the effect size roughly right while the behavioural scientists underestimated it.\nI can only assume that the authors of the megastudy themselves also underestimated the expected effect sizes, given the lack of power to distinguish most of the hypotheses proposed. If we turn to the pre-analysis plan I linked above relating to social norms, they stated that they needed a sample size of 3,000 people per condition to detect a change of around ~15-20%. That’s within the realm of the effect size required to differentiate from the controls, but not likely to be enough to separate four separate interventions from each other, which of course was the point of the experiment. (And to top it off they didn’t get the number of participants they sought.)\nThe second conclusion the authors make about the predictions is that the observers were not able to predict the ordering of the effect sizes of the interventions.\nI am going to argue that, at least for this experiment, it is premature to draw this conclusion.\nOne reason is that the prediction exercise lacked power (this is in addition to the power issue discussed above). You can see this in the massive error bars around the predictions in Figure 1. These error bars are, of course, a function of both the range and number of predictions. But each participant was asked to estimate the effect size of only 3 interventions. Each intervention has only 17, 9 and 5 predictions from the lay people, public health experts and behavioural practitioners respectively. None of the predicted effect sizes are statistically significant from any other other.\nI don’t have the statistical nous to work out what number of predictions would be appropriate, but I can see that we have a small number of noisy predictions of some underpowered noisy experimental results. I wouldn’t expect anything to be found.\nBut outside of power, a larger challenge with the predictions is that the experimental design did not enable useful comparisons by observers making predictions. The result is many that predictions are nonsense.\nTo tease this out, let’s look at a couple of sets of predictions. Table 2 lists the social norm interventions described above with additional columns for the predictions by practitioners, professors, lay people and the pooled group.\nTable 2: Exercise social norm effect sizes and predictions\n\n\n\n\n\n\n\n\n\n\n\nIntervention\nEffect size\nPractitioner\nProfessor\nLay people\nAverage Prediction\n\n\n\n\n3. High and increasing\n0.345\n2.18\n3.10\n3.94\n3.48\n\n\n20. Low\n0.193\n2.68\n3.29\n3.72\n3.42\n\n\n47. Low but increasing\n0.052\n2.72\n2.72\n3.27\n2.99\n\n\n53. High\n-0.030\n2.34\n3.15\n3.41\n3.19\n\n\n\nThere is a reasonably-sized literature that social norms can affect behaviour. People tend to follow the masses. If you used this as a basis for your predictions, you would predict the high social norm would be more effective than the low social norm. (The effect of trends is less well established so I’ll ignore them for the moment.)\nBut if we look at Table 2, the low social norm is consistently predicted to have larger effect. On what basis would you have expected them to make this prediction? (I realise there is a certain irony in asking this question given the high social norm actually ended up as the worst performing….)\nThe reason is that none of the participants were asked to compare intervention “20. Low” with intervention “53. High”. No participant was asked to compare more than two of the four social norm interventions (a total of 9 observers had that opportunity). The consequence is that there was little opportunity for observers to calibrate their responses based on their knowledge of empirical phenomena.\nWe see a similar pattern in collection of five interventions in Table 3 below in which incentives were offered (all interventions named “Rigidity Rewarded” in Figure 1). In these interventions, incentives of Amazon points were paid for either attending a planned or unplanned gym session. The interventions are in order of effect size from largest to smallest (the numbers being where they rank in the 54 interventions).\nTable 3: Rigidity rewarded incentives\n\n\n\n\n\n\n\n\n\n\n\n\nRanking\nPoints planned\n$ planned\nPoints other\n$ other\nText messages\nPredicted\n\n\n\n\n14.\n500\n$1.79\n250\n$0.90\n9\n3.70\n\n\n24.\n300\n$0.22\n150\n$0.11\n2\n3.08\n\n\n28.\n725\n$0.52\n250\n$0.18\n8\n3.54\n\n\n48.\n425\n$0.31\n150\n$0.11\n8\n2.77\n\n\n51.\n300\n$1.08\n150\n$0.54\n9\n3.04\n\n\n\nThe experimental results kinda make sense. The largest incentive had the largest effect. Intervention 14 can be statistically differentiated from 51. But that’s it. The others are not significantly different from each other.\nThen we look at the prediction column. The largest incentive was predicted to have the largest effect. All fine there. But intervention 28 with incentives of $0.52 and $0.18 intervention was predicted to have greater effect than the $1.08 and $0.54 associated with intervention 51. What’s a plausible basis for that prediction ordering?\nI can see one argument that people were ranking according to the Amazon points available rather than the value of those points. But accepting that argument further confuses the comparability of these interventions with each other and limits their generalisability if we want to take them to other domains.\nI think the claim that the observers are poor predictors is likely right. Practitioners, professors and the average punter don’t have a great feel for the relative effectiveness of a bunch of weak interventions. But putting the critiques of these predictions together, I’d like to see a replication with more power on both sides: more power to differentiate interventions and a larger number of predictions in which the third-party observers get to see the full set of interventions and make direct comparisons between them.\nIn the newest megastudy on vaccinations, those criteria were largely met. Another prediction competition was run, with lay people and the scientists who designed the interventions among the “competitors”. They were asked to predict the vaccination rates for the people in all 22 treatment groups and the control\nHere we have a more promising result, for the lay people at least. The predictions of the scientists who contributed an intervention were did not correlate with observed observation rates. Conversely, lay people did well. The correlation between observed and predicted vaccination rates was 0.6. And beyond that, their estimate of the average lift in vaccinations was close to that observed. (The scientists underestimated the increase by around 25%.)\nDespite that success by the lay people, it was not without flaws. They did not anticipate the top-performing intervention. If you’re in the mindset that you simply want to maximise vaccinations, that’s an important miss. That, of course, leads us to the usual conclusion:\n\nRegardless, the inability of either scientists or laypeople to anticipate the top-performing intervention underscores the value of empirical testing when seeking the best policy.\n\nFinally, the success of the laypeople relative to the scientists hints at some of the following points I plan to make. Why are we just using behavioural scientists to design these interventions? Do they have any special skill in developing interventions? Why not give marketers a go at designing them! And if only we had some theory to guide our understanding. More on those points below."
  },
  {
    "objectID": "posts/megastudy-scepticism.html#intervention-design",
    "href": "posts/megastudy-scepticism.html#intervention-design",
    "title": "Megastudy scepticism",
    "section": "5. Intervention design",
    "text": "5. Intervention design\nGoing from concept to intervention design is tough. I’ve been there.\nBut almost every time I see an applied behavioural experiment, this challenge strikes me anew. You see all the degrees of freedom in developing the intervention, such as the design (why do academics pretend to have design skills?), precise copy (please, start using decent copywriters!), the choice of medium (e.g. text message or email) and the method of execution (e.g. when is it sent).\nThe result is that it is typically not hard to come up with reasons why (or why not) the intervention will be effective for reasons that don’t relate to the empirical phenomena that is being explicitly tested. “The copy doesn’t convey the concept.” “The wording is confusing.” And so on.\nSome of these problems appear magnified in this megastudy via the involvement of 30 people from 15 universities working in independent teams. Many interventions are only weakly comparable.\nHere’s two examples from this megastudy.\nFirst, some interventions have 2 text messages sent to participants, others nine. Is the difference in effect size due to the number of text messages or the stated form of the intervention?\nSecond, the social norm experiments described above rely on people responding to a quiz to get the full force of the intervention. Are we testing the effect of quizzes or norms?\nAnd here’s an example from the first megastudy on vaccinations.\nThe worst performing message related to health, with the line “It’s flu season & getting a flu shot at your appt is an easy thing you can do to be healthy!” Who are their copywriters?! Would anyone normally spruik the health benefits of a flu shot in this way? I would argue that the poor performance of this message gives little information about the effectiveness of health messaging.\nThese implementation problems again point to that number one piece of advice: test in your own domain. It’s hard to pick up a concept and simply transplant to your own space. Wording and delivery almost always have to change. You need to test which degrees of freedom matter."
  },
  {
    "objectID": "posts/megastudy-scepticism.html#theory",
    "href": "posts/megastudy-scepticism.html#theory",
    "title": "Megastudy scepticism",
    "section": "6. Theory",
    "text": "6. Theory\nMy final gripe isn’t completely fair, but it’s my hobby-horse, so I’ll voice it anyway.\nAt first glance, the list of 54 interventions suggests the megastudy has an underlying philosophy of “throw enough things at a wall and surely something will stick”. If you can’t predict which interventions are most effective, maybe that is what you have to do:\n\nOne could argue that the harder it is to predict the results of experiments, the more valuable the megastudy approach. The more difficult it is to forecast ex ante which interventions will work, the harder it is to decide in advance which interventions to prioritize for testing, and the more useful it is to instead test a large number of treatment approaches.\n\nFair enough. But this concession implicitly means the authors have given up on developing an understanding of human decision making that might allow us to make predictions. Each hypothesis or set of hypotheses they tested concern discrete empirical regularities. They are not derived from or designed to test a core model of human decision making. We have behavioural scientists working as technicians, seeking to optimise a particular objective with the tools at hand.\nIt’s the way the wind is blowing. Instead of taking on the challenge of giving the mass of empirical evidence some theoretical backbone, the best minds have turned to taking an empirical body of work and applying it to real-world problems. As a policy maker or business owner, you might reap the benefits. The academics taking this path are reaping the publication benefits. (This megastudy landed in Nature of all places.) But progress as a science? It feels stagnant.\nI admit I’m asking too much of this study. I want to see behavioural science build theoretical understanding as to what is going on. This study is designed to test how to increase gym attendance. This big, theory free study isn’t going to provide the answer to my question, but nor is it designed to.\nBut this lack of theory is not without costs. For instance, as already noted, when asked to predict the ordering of effect sizes, the practitioners had no idea. In the newer vaccination study they were outperformed by laypeople. We don’t have a theoretical framework that can outperform common sense (and by making us focus on a menu of effects and biases, possibly have a body of knowledge that leads us to underperform common sense.)\nIf we’re simply technicians working on public policy issues, this further builds the case to open the door to other technicians. Why are we just asking behavioural scientists? What if we got some marketers to develop interventions? Here’s one megastudy I would like to see. Get half the interventions from behavioural scientists. Get the other half from random non-academic marketers (plus Rory Sutherland). See which are more successful. I don’t have much confidence that the behavioural scientists would outperform.\nOn that point, the first vaccination megastudy contains the following line:\n\n[O]ur findings show nudges sent via text messages to patients prior to a primary care visit and developed by behavioral scientists to encourage vaccine adoption can substantially boost vaccination rates at close to zero marginal cost.\n\nIt’s true in a sense, but whether you actually need the “behavioral scientists” to develop the interventions hasn’t been tested yet. And again, given their poor performance at predicting what would be successful relative to lay people in the second vaccination megastudy, I’m not convinced."
  },
  {
    "objectID": "posts/megastudy-scepticism.html#the-false-discovery-rate",
    "href": "posts/megastudy-scepticism.html#the-false-discovery-rate",
    "title": "Megastudy scepticism",
    "section": "7. The false discovery rate",
    "text": "7. The false discovery rate\nNow to a couple of things I like about this study. The first is that the study enables calculation of a false discovery rate.\nWhen you run multiple comparisons in an experiment, you will typically adjust the threshold for significance to reduce the number of false positives. If you set a statistical significance threshold (\\alpha) of 0.05, you would expect 1 in 20 interventions to reach this threshold by chance even if they have no effect. Run 54 interventions and you would expect a couple.\nTo eliminate those false positives, there are corrections you can make such as a Bonferroni correction, by which you divide \\alpha by the number of interventions you are testing. These types of adjustments, however, are conservative. Make a Bonferroni adjustment to the megastudy results and (from eyeballing the p-values) you are left with around half a dozen statistically significant effect sizes.\nTo deal with this problem John Storey and Robert Tibshirani developed a test to determine the false discovery rate. The false discovery rate is the rate that interventions with statistically significant effect sizes are truly null. This contrasts with the false positive rate, which is the rate that null interventions are called significant.\nStorey and Tibshirani’s test capitalises on the fact that with multiple comparisons you have multiple p-values that themselves have a distribution. This distribution of p-values can be used to calculate the number of false positives you would expect across the multiple tests that you have constructed.\nUsing this test, the authors of the megastudy argue that there is only a 5.07% chance that any of the statistically significant interventions are false positives. I’m willing to believe that. The experimental participants have signed up to a program to go to the gym more. They are then delivered a range of reminders, messages and incentives to stick to it. I’d be surprised if any of the incentives truly had zero effect. They are just small effects with a lot of noise, so you need a decent sample to detect them.\nBut what does this mean for interpreting the megastudy results? It tells us that this basic concept works. Yes you can get people going to the gym more. But does the test help us differentiate these interventions? No.\nAs a final note for myself, I’m interested in whether the Storey and Tibshirani test is robust with the small number of interventions in the megastudy. Storey and Tibshirani developed the test for genomic analysis, where you might be conducting millions of comparisons, so would have a distribution of millions of p-values. The distribution of p-values in the megastudy comes from far fewer observations. I’ll come back to this another time."
  },
  {
    "objectID": "posts/megastudy-scepticism.html#return-on-investment",
    "href": "posts/megastudy-scepticism.html#return-on-investment",
    "title": "Megastudy scepticism",
    "section": "8. Return on investment",
    "text": "8. Return on investment\nAnother strong feature of this study is that (buried in the supplementary material) the authors examine the fiscal cost and benefits of these interventions. A common refrain for much applied behavioural science is that the effects are small but the costs are also small. Thus, the return on investment pays off.\nOn face value, this argument holds for the megastudy. Drawing on a couple of other studies, the authors estimated the benefit of a gym visit to society due to reduced health care expenditures as being between $1.66 and $3.71 per visit. Given the size of these benefits and the low cost of many of the megastudy interventions, they argue that four of the top five interventions have a positive cost-benefit and would be worth scaling.\nThe gap in this analysis is that it is better described as a fiscal analysis rather than a true cost-benefit analysis. It’s ignoring many of the costs and benefits that a good public policy cost-benefit analysis would typically include.\nOn the cost ledger, the largest missing cost is the time-cost to participants. The benefits side would consider health benefits to the individual themselves: probably some change in quality-adjusted life-years. I suspect this would be substantial: I don’t think there’s much better life advice than to go to the gym.\nAlternatively, you could attempt to capture those personal cost and benefits through some measure of customer willingness to pay.\nAn interesting additional question would be whether the gym chain gets a return that makes it worthwhile from a business perspective. Do more people retain their gym membership? To the extent they do, this builds the business case but also creates another cost to individuals we should consider in our cost-benefit analysis."
  },
  {
    "objectID": "posts/megastudy-scepticism.html#the-common-task-approach",
    "href": "posts/megastudy-scepticism.html#the-common-task-approach",
    "title": "Megastudy scepticism",
    "section": "9. The common task approach",
    "text": "9. The common task approach\nOne interesting suggestion in the paper is that the megastudy drew inspiration from the common task framework.\n\nWe propose an experimental paradigm for evaluating many behavioural interventions at once: the megastudy is a massive field experiment in which many different treatments are tested synchronously in one large sample using a common, objectively measured outcome. This approach takes inspiration from the common task framework, which has substantially accelerated progress in the field of machine learning. In a common task framework, researchers compete to solve the same problem (such as image recognition), subject to the same constraints (for example, the same validation method) and using the same dataset, with complete transparency in terms of hypotheses tested and results.\n\nI like that point of inspiration. Common task datasets have enabled new machine learning approaches to establish their credentials, often resulting in a new paradigm sweeping the field (such as the sudden improvements in performance from a convolutional neural network on Imagenet in 2012).\nBut it is not hard to see differences between the megastudy and the common task framework. For a start, common task tournaments typically create an open playing field by making the dataset generally available. Anyone can enter. Contrast that with the submissions from a relatively narrow set of behavioural science teams.\nMost common task frameworks also allow iterative exploration and progress. Teams can access the data outside of tournaments (although sometimes there is a holdout dataset for testing). Each year people get to return to the problem. Here we’ve got a one-shot tournament.\nMaybe the megastudy study teams should take the inspiration from the common task framework more seriously.\n\nWhy don’t they run this experiment with the gym chain every year? Solicit open entries, with a process to whittle them down to the required number of interventions. Include a range of the best interventions from previous years. (And do it with more power.)\nIntroduce a version out-of-sample testing through replication. Take the top interventions from an initial run of four weeks, and run them with a new group of gym members over another four weeks. This will allow testing of the robustness of the ordering and another check on the magnitude of the winner effect.\nHere’s one half-baked idea: treat the prediction process as a tournament. Maybe one element could involve people submitting models (likely with some combination of Natural Language Processing and an underlying model of the decision maker) to predict which interventions will outperform. Use that to get an insight into theory.\n\nIf we took these extra steps, however, a limitation to the common task framework will become of increasing importance to the megastudy: overfitting. If you run a contest to increase gym attendance again and again there’s a higher probability that one of the interventions will capitalise on an idiosyncratic feature of that gym and its members. Great results, poor generalisation. Maybe we need a different gym each year, or another parallel context, but then you get into questions of comparability."
  },
  {
    "objectID": "posts/megastudy-scepticism.html#other-things-i-like",
    "href": "posts/megastudy-scepticism.html#other-things-i-like",
    "title": "Megastudy scepticism",
    "section": "10. Other things I like",
    "text": "10. Other things I like\nHere’s a list of other things I like about the megastudy but don’t plan to discuss in depth:\n\nI appreciate the economies of scale that can come with the megastudy approach. The more studies that seek to expand interventions and sample size the better. While I’ve implied above a megastudy may not be better than multiple small studies as multiple studies could give us more generalisable insight, multiple megastudies would provide even greater benefit as comparison both within and across domains is possible.\nThe megastudy builds in the publication of null findings. I don’t find the null findings of this gym megastudy interesting as I expect nearly every intervention has a positive effect. But if this megastudy approach is used in other domains, those null findings will be published and start to pile up. And if the power was increased, the constant underperformance of some interventions (even if superior to a null) will become more apparent. (The constant underperformer from my practical experience: loss framing.)\nThe authors note that the intervention with the highest effect size is likely to be suffering from a winner effect. As a result, we have an overestimate of the effect size. Thus, the authors note that “Replicating the effects of outlier interventions identified in megastudies will therefore be important for establishing their true impact.” In the two flu megastudies, the winner effect is explicitly examined and adjusted for in reporting the expected effect size of the most effective intervention."
  },
  {
    "objectID": "posts/megastudy-scepticism.html#conclusion",
    "href": "posts/megastudy-scepticism.html#conclusion",
    "title": "Megastudy scepticism",
    "section": "11. Conclusion",
    "text": "11. Conclusion\nI’ve dedicated most of this post to examining limitations to the megastudy. I don’t want to appear harsh - there are good features - but I feel this study highlights a limitation of behavioural science as it is practised. Even in a world of megastudies, we are still in a world of poor theory, questionable generalisability and inadequate statistical power. I am sure there is a souped-up version of the mega-study that could cover the latter of these, but the other two require something else."
  },
  {
    "objectID": "posts/megastudy-scepticism.html#random-thoughts-that-didnt-fit-above",
    "href": "posts/megastudy-scepticism.html#random-thoughts-that-didnt-fit-above",
    "title": "Megastudy scepticism",
    "section": "12. Random thoughts that didn’t fit above",
    "text": "12. Random thoughts that didn’t fit above\n\nI suppose the “introduction” in the Nature paper is for those who missed the May 2021 paper by most of the same authors, A megastudy of text-based nudges encouraging patients to get vaccinated at an upcoming doctor’s appointment … although to be fair, the exercise megastudy was submitted to a journal first.\nThe “winning” message for that first vaccination study appears, to me at least, a bit deceptive: “…this is a reminder that a flu vaccine has been reserved for your appt…”\nWhat is the threshold is for a megastudy? The vaccination megastudy involved 19 nudges. Does the Behavioural Insights team trial of 7 different messages on organ donation make the cut? It had 1,085,322 participants. And it enabled the testing of multiple hypotheses in a single context while maintaining adequate power. I’ve seen industry trials in the digital realm with more treatments. Is the concept of the megastudy that new? What seems new to me in the emergence of star academic researchers with access to enough resources to run these projects."
  },
  {
    "objectID": "posts/risk-and-loss-aversion-in-ergodicity-economics.html",
    "href": "posts/risk-and-loss-aversion-in-ergodicity-economics.html",
    "title": "Risk and loss aversion in ergodicity economics",
    "section": "",
    "text": "In a previous post I posed the following bet:\n\nSuppose you have $100 and are offered a gamble involving a series of coin flips. For each flip, heads will increase your wealth by 50%. Tails will decrease it by 40%. Flip 100 times.\n\nThe changes in wealth under a sequence of flips of this nature is “non-ergodic”, as the expected value of the bet does not converge with its time-average growth rate. The bet has a positive expected value, 5% of the bettor’s wealth per flip, and the ensemble average across a large enough population will approximate this expected value in growth in overall wealth. But, the time-average growth rate for an individual is approximately a loss of 5% of their wealth with each flip. Most individuals will experience a loss, and in the long-run everyone will. (To understand why this is so, see my primer post on ergodicity economics.)\nThat many people decline bets of this nature suggests that there may be some wisdom in our decision making process. But what is that process?\nAre we risk averse?\nAs I noted in that previous post, economists have a readily available explanation for the rejection of this bet. People are risk averse expected utility maximisers. As I wrote there:\n\nA risk averse person will value the expected outcome of a gamble lower than the same sum with certainty.\nRisk aversion can be represented through the concept of utility, where each level of wealth gives subjective value (utility) for the gambler. If people maximise utility instead of the value of a gamble, it is possible that a person would reject the bet.\nFor example, one common utility function to represent a risk averse individual is the logarithm of their wealth. If we apply the log utility function to the gamble above, the gambler will reject the offer of the coin flip. [The maths here is simply that the expected utility of the gamble is 0.5\\times \\ln(150) + 0.5\\times \\ln(60)=4.55, which is less than the utility of the sure $100, ln(100)=4.61.]\n\nThe concept of a risk averse expected utility maximiser with a utility function such as the logarithmic has been a staple explanation for many decisions. The St Petersberg Paradox is one such problem, with that series of bets rarely valued above $10 despite the infinite expected value of the bet. (It is another non-ergodic system.)\nBut do we need an expected utility function to provide us with such risk aversion? Would a more parsimonious explanation for the rejection of the bet simply be that the person is seeking to maximise the growth rate of their wealth. With that objective and a time-average growth rate of minus 5%, rejection is the obvious thing to do. There is no need for an expected utility function. Rather, the person simply needs a way of deciding whether accepting the bet will maximise the growth-rate of their wealth.\nAn interesting alignment of economic history and ergodicity economics occurs here. One of the most commonly used expected utility functions is the logarithm (as noted above). People maximise utility by maximising the expected logarithm of their wealth.\nYet, the way to maximise the geometric growth rate of your wealth when facing a multiplicative bet is also to maximise the logarithm of your wealth. The calculations of the expected utility maximiser with a logarithmic utility function and of the time-average growth-rate maximiser are the same.\nAs Ole Peters and Alexander Adamou write in their ergodicity economics lecture notes:\n\n[E]xpected utility theory as we have presented it above is consistent with growth rate optimisation, provided a suitable pair of dynamic and utility function is used. For multiplicative dynamics, the necessary utility function is the logarithm. That this is the most widely used utility function in both theory and practice is a psychological fluke in the classic mindset; from our perspective it indicates that our brains have evolved to produce growth-optimal decisions in a world governed by multiplicative dynamics, i.e. where entities produce more of themselves.\n\nThis parallel means that many of the “puzzles” that expected utility maximisation has been used to solve can also be “solved” by growth-rate optimisation. For instance, insurance or the St Petersberg puzzle provide a challenge for expected wealth optimisation, but are equivalently solved by assuming an expected log utility maximiser or a growth-rate optimiser.\nThat these two concepts overlap raises a conundrum. An expected log utility maximiser looks much like a growth-rate maximiser in their behaviour (noting that log utility is only one of many functional forms an expected utility maximiser could theoretically have). If we would expect to see the same decision under both expected log utility and growth-rate maximisation in multiplicative dynamics, how can we differentiate the two?\nAdditive dynamics\nBefore I answer that question, I am going to detour into the world of additive dynamics. What if I offered you the following bet?\n\nSuppose you have $100 and are offered a gamble involving a series of coin flips. For each flip, heads will increase your wealth by $50. Tails will decrease it by $40. Flip 100 times.\n\nYou can see the tweak from the original bet, with dollar sums rather than percentages. The first flip is effectively identical, but future bets will be additive on that result and always involve the same shift of $50 up or $40 down. In contrast, the earlier bet was multiplicative, in that the bettor’s wealth was multiplied by a common factor. As a result, the multiplicative bet scales up and down with wealth.\nAn important feature of this second series of flips is that the system is ergodic. The expected value of each flip is $5 (0.5\\times \\$50-0.5\\times \\$40=\\$5). The time-average growth rate is also $5.\nLet’s simulate as we did for multiplicative bets in the ergodicity economics primer post, with 10,000 people starting with $100 and flipping the coin 100 times. The below plot shows the average wealth of the population, together with the paths of the first 20 of the 10,000 people (in red).\n\n\nCode\n# Load the required packages\n\nlibrary(ggplot2)\nlibrary(scales) #use the percent scale later\n\n# Create a function for running of the bets.\n\nbet &lt;- function(p, n, t, start=100, gain, loss, ergodic=FALSE, absorbing=FALSE){\n\n  #p is probability of a gain\n  #n is how many people in the simulation\n  #t is the number of coin flips simulated for each person\n  #start is the number of dollars each person starts with\n  #if ergodic=FALSE, gain and loss are the multipliers\n  #if ergodic=TRUE, gain and loss are the dollar amounts\n  #if absorbing=TRUE, zero wealth ends the series of flips for that person\n\n  params &lt;- as.data.frame(c(p, n, t, start, gain, loss, ergodic, absorbing))\n  rownames(params) &lt;- c(\"p\", \"n\", \"t\", \"start\", \"gain\", \"loss\", \"ergodic\", \"absorbing\")\n  colnames(params) &lt;- \"value\"\n\n  sim &lt;- matrix(data = NA, nrow = t, ncol = n)\n\n  if(ergodic==FALSE){\n    for (j in 1:n) {\n      x &lt;- start\n      for (i in 1:t) {\n      outcome &lt;- rbinom(n=1, size=1, prob=p)\n      ifelse(outcome==0, x &lt;- x*loss, x &lt;- x*gain)\n      sim[i,j] &lt;- x\n      }\n    }\n  }\n\n if(ergodic==TRUE){\n    for (j in 1:n) {\n      x &lt;- start \n      for (i in 1:t) {\n      outcome &lt;- rbinom(n=1, size=1, prob=p)\n      ifelse(outcome==0, x &lt;- x-loss, x &lt;- x+gain)\n      sim[i,j] &lt;- x\n      if(absorbing==TRUE){\n        if(x&lt;0){\n          sim[i:t,j] &lt;- 0\n            break\n        }\n        }\n      }\n    }\n  }\n\n  sim &lt;- rbind(rep(start,n), sim) #placing the starting sum in the first row\n  sim &lt;- cbind(seq(0,t), sim) #number each period\n  sim &lt;- data.frame(sim)\n  colnames(sim) &lt;- c(\"period\", paste0(\"p\", 1:n))\n  sim &lt;- list(params=params, sim=sim)\n  sim\n}\n\n# Simulate 10,000 people who accept a series of 1000 50:50 bets to win \\$50 or lose \\$40 from a starting wealth of \\$100.\n\nset.seed(20200203)\nergodic &lt;- bet(p=0.5, n=10000, t=1000, gain=50, loss=40, ergodic=TRUE, absorbing=FALSE)\n\n# Create a function for plotting the path of individuals in the population over a set number of flips.\n\nindividualPlot &lt;- function(sim, t, people){\n\n  basePlot &lt;- ggplot(sim$sim[c(1:(t+1)),], aes(x=period)) +\n    labs(y = \"Wealth ($)\")\n\n  for (i in 1:people) {\n    basePlot &lt;- basePlot +\n      geom_line(aes(y = !!sim$sim[c(1:(t+1)),(i+1)]), color = 2)\n  }\n\nbasePlot\n\n}\n\n# Plot both the average outcome and first twenty people on the same plot.\n\njointPlot &lt;- function(sim, t, subset) {\n  individualPlot(sim, t, subset) +\n    geom_line(aes(y = rowMeans(sim$sim[c(1:(t+1)),2:(sim$params[2,]+1)])), color = 1, linewidth=1)\n}\n\nergodicPlot &lt;- jointPlot(sim=ergodic, t=100, subset=20)\nergodicPlot\n\n\n\n\n\nFigure 1: Average wealth of population and path of first 20 people\n\n\n\n\n\n\n\n\n\n\nCode\n# Function to generate summary statistics\n\nsummaryStats &lt;- function(sim, t){\n\n  meanW &lt;- mean(as.matrix(sim$sim[(t+1),2:(sim$params[2,]+1)])) # mean wealth\n  medianW &lt;- median(as.matrix(sim$sim[(t+1),2:(sim$params[2,]+1)])) # median wealth\n  num99 &lt;- sum(sim$sim[(t+1),2:(sim$params[2,]+1)]&lt;(sim$params[4,]/100)) #number who lost more than 99% of their wealth\n  numGain &lt;- sum(sim$sim[(t+1),2:(sim$params[2,]+1)]&gt;sim$params[4,]) #number who gain\n  num100 &lt;- sum(sim$sim[(t+1),2:(sim$params[2,]+1)]&gt;(sim$params[4,]*100)) #number who increase their wealth more than 100-fold\n  winner &lt;- max(sim$sim[(t+1),2:(sim$params[2,]+1)]) #wealth of wealthiest person\n  winnerShare &lt;- winner / sum(sim$sim[(t+1),2:(sim$params[2,]+1)])*100 #percentage wealth share of wealthiest person\n\n  # print(paste0(\"mean: $\", round(meanW, 0)))\n  # print(paste0(\"median: $\", round(medianW, 0)))\n  # print(paste0(\"number who lost more than 99% of their wealth: \", num99))\n  # print(paste0(\"number who gained: \", numGain))\n  # print(paste0(\"number who increase their wealth more than 100-fold: \", num100))\n  # print(paste0(\"wealth of wealthiest person: $\", round(winner)))\n  # print(paste0(\"wealth share of wealthiest person: \", winnerShare, \"%\"))\n\nsummary &lt;-(data.frame(meanW, medianW, num99, numGain, num100, winner, winnerShare))\n\n}\n\n# Generate summary statistics for the population and wealthiest person after 100 and 1000 flips.\n\nsummaryErgodic100 &lt;- summaryStats(sim=ergodic, t=100)\nsummaryErgodic1000 &lt;- summaryStats(sim=ergodic, t=1000)\n\n\nThe individual growth paths cluster on either side of the population average. After 100 flips, the mean wealth is $609 and the median $600. 87% of the population has gained in wealth. The wealthiest person has $2310, or 0.04% of the total wealth of the population. After 1000 rounds (not plotted here), the mean wealth is $5117 and the median $5100. All 10,000 have gained. The wealthiest person has $10320, or 0.02% of the total wealth of the population. This alignment between the mean and median wealth, and the relatively equal distribution of wealth, are characteristic of an ergodic system.\n\n\nCode\n# Determine how many people (in the first people out of n) experienced zero wealth or less during the simulation.\n\nnumZero &lt;- function(sim, t, subset=0){\n\n  #subset\n  data &lt;- if(subset==0 | subset&gt;sim$params[2,]){\n    sim$sim[1:t,2:(sim$params[2,]+1)]\n  } else {\n    sim$sim[1:t,2:(subset+1)]\n  }\n  \n  # number of people who experienced zero wealth or less\n  numZero &lt;- length(data) - sum(sapply(data, function(x) all(x&gt;0)))\n  numZero\n  \n}\n\nnumZeroTwenty100 &lt;- numZero(sim=ergodic, t=100, subset=20)\nnumZeroTwenty1000 &lt;- numZero(sim=ergodic, t=1000, subset=20)\nnumZero100 &lt;- numZero(sim=ergodic, t=100)\nnumZero1000 &lt;- numZero(sim=ergodic, t=1000)\n\n\nNow for a wrinkle, which we can see in the plotted figure. Of those first 20 people plotted on the chart, 12(!) had their wealth go into the negative over those 100 periods. Two more of those first 20 go into the negative over the subsequent 900 periods. We see the same phenomenon across the broader population, with 5439 dropping below zero in those first 100 periods. 5684 drop below zero across the full 1000.\nTo the extent zero wealth is ruinous when it occurs (e.g., death, you cannot continue to play), that event is severe. If the player only incurs the consequences of their final position, the bet is unlikely to result in ruin but still presents a non-zero threat of catastrophe.\nWhat would an expected utility maximiser do here? For a person with log utility, any probability of ruin during the flips would lead them to reject the gambles. The log of zero is negative infinite, which outweighs all other possible outcomes, whatever their magnitude or probability.\nThe growth-rate maximiser would accept the bet if they didn’t fear ruin. The time-average growth of $5 per flip would pull them in. If ruin was feared and consequential, then they might also reject.\nRisk and loss aversion in the two different worlds\nTo the title of my post, what light does this shed on risk or loss aversion?\nLet us suppose humans are growth-rate maximisers. In a multiplicative world, people would exhibit what is by definition risk-averse behaviour - they prefer a certain sum to a gamble with the same expected value. This is a consequence of maximising the growth rate by maximising the expected logarithm of their wealth. This, however, has a different underlying rationale to explanations of log utility based on either psychology or the diminishing utility of wealth.\nWhat of loss aversion, the concept that losses loom larger than gains? Risk aversion results in a phenomena that looks like loss aversion, in that losses are weighted more heavily due to the diminishing utility of additional wealth. However, loss aversion is a dislike of losses over and above that. It involves a “kink” in the utility curve, so should be observed for small amounts and result in a greater aversion to bets than risk aversion alone would predict.\nThe growth-rate maximisation model would not lead us to predict loss aversion. Whatever their wealth, growth-rate maximisation does not produce a marked difference between gains and losses beyond that induced by risk aversion. There is no “kink” at the reference point at which losses hurt more than gains are enjoyed. Are there any phenomena described as loss aversion which this theory would suggest are actually growth-rate maximising behaviour? Not that I can think of.\nIn the additive world, things are more interesting. Growth-rate maximisation is equivalent to wealth maximisation. People aren’t risk averse. (In fact, assuming only growth rate maximisation in an additive environment leaves much about their risk tolerance unspecified.) They simply take the positive value bets.\nHere, the broader evidence across experimental economics and psychology places a question mark over the claim (experiment described below excepting). People regularly reject positive value additive bets. There are ways to attempt to reconcile growth-rate maximisation with these rejections. For instance, we could argue that these people are in a multiplicative world, of which the bet is only a small part. Therefore, the bet described as additive is actually part of a multiplicative dynamic. We know little about their broader circumstances. But even then, the rejected additive bets are often so favourable that even a growth-maximiser in a multiplicative dynamic would generally accept them.\nLoss aversion is also not a prediction of growth-rate maximising behaviour in the additive world. There is no “kink” at the reference point. Losses and gains have the same weight, no matter their scale.\nWe could add loss aversion to the growth-rate maximiser in the additive environment by introducing an absorbing state at zero. The path to ruin can be quicker in an additive world than in a multiplicative as the bet sizes don’t scale down with diminished wealth, plus there is the possibility of losing absolutely everything. But what is the agent’s response to this potential for ruin? We would need to add some assumptions additional to that provided by a simple growth-rate maximisation approach.\nErgodicity and behavioural economics\nIn the twitter-sphere, ergodicity economics has been noted as the “behavioural economics killer”. I’ve already noted loss aversion, but I will state here that many behavioural phenomena remain to be explained even if we accept the foundational ergodicity concepts.\nA core group of these behavioural phenomena involve framing, whereby presentation of effectively the same choice can result in different decisions. Status quo bias, the reflection effect, default effects, and the like, remain. So while ergodicity economics gives a new light to shine on decision making under uncertainty, it hasn’t suddenly solved the raft of behavioural puzzles that have emerged over the last seventy years.\nPart of that is unsurprising. Much of the behavioural critique of expected utility theory is that our decisions don’t look like expected log utility maximisation decision making (or other similar functions). Those puzzles remain for a growth-rate maximiser that maximises their expected log wealth in a multiplicative environment.\nDistinguishing expected utility from growth-rate maximisation: an experiment\nNow, to return to an earlier question. If we expect to see the same decision under both expected utility and growth-rate maximisation in multiplicative dynamics, how can we differentiate the two?\nA group led by Oliver Hulme ran an experiment that sheds some interesting light on this question (branded the “Copenhagen experiment” in the twitter-sphere). The pre-print reporting the experimental results is available on arXiv, with supporting materials and data on GitHub. Despite some of my questions below, this is a innovative and well thought-out experiment.\nThe concept behind the experiment was to differentiate between three possible models of human decision making:\n\nProspect theory, which includes features such as different risk aversion parameters in the gain and loss domains, and loss aversion.\nIsoelastic utility, a classic model of expected utility, of which log utility is a special case\nTime optimal utility, where changes in utility are determined by linear utility under additive dynamics and by logarithmic utility under multiplicative dynamics.\n\nThe third could be differentiated from the other two if the utility function effectively changes when the environment changes between additive and multiplicative dynamics.\nTo test this, the experimental procedure ran as follows.\nEighteen experimental subjects (actually 20, but two were excluded from analysis) participated in a series of gambles over two days. One day, they were exposed to a series of additive bets. The other day involved multiplicative bets. The order of the days was switched for some subjects. They were not directly informed of the nature of each day.\nEach day consisted of a passive session, followed by an active session.\nAt the beginning of the passive session, each experimental subject was endowed with 1000 Danish Krone (approx $150USD). They then watched a series of fractal images, each of which had an effect on their wealth. Their task through the passive session was to learn the effect of each of these fractals on their wealth. Some examples of the images are below:\nExample fractal images \nThere were nine fractals on each day, each with a unique effect on wealth. For the multiplicative day, the fractals changed wealth by the following factors: 0.447, 0.546. 0.668. 0.818, 1, 1.223, 1.496, 1.830, 2.239. On the additive day, the changes were: -428kr, -321kr, -214kr, -107kr, 0, 107kr, 214kr, 321kr, 428kr. The subjects observed a sequence of 333 fractals in the passive sequence (each fractal would appear 37 times), giving them a good feel for the effect of each fractal on their wealth.\nAs the outcome from an equal distribution of these fractals is no growth, each participant ended the 333rd passive fractal with their original wealth of 1000kr. They were then given one additional random fractal to set the level of wealth that they would commence the active session with.\nDuring the active session, they were shown two sets of two fractal images, as in Figure 3 below. The left and the right side represent two possible bets, each of which would involve a 50:50 probability of getting one of the two fractals and the effect of that fractal on their wealth. They were asked to choose one of the bets. For example, choosing the left two images would give them a 50:50 chance for each of those two images on the left.\nExample choice \nHulme and friends then analysed these experimental results to determine which model best described the observed decisions.\nThe headline finding is illustrated in the following chart, which provides the posterior probability of the parameter of risk aversion for each experimental subject. The blue distribution is the additive day, and the red the multiplicative day.\nPosterior probability of the parameter of risk aversion \nA risk aversion parameter of 0 gives us linear utility. A parameter of 1 is logarithmic utility. On that basis, the time optimal utility of ergodicity economics comes out looking strong. There is a change in risk aversion across most participants as they changed between the ergodic and non-ergodic environments.\nHulme and friends also calculated the posterior probability of each model for each participant, with time optimal (the growth rate maximiser) generally having the stronger probability.\nPosterior model probabilities \nThe authors write:\n\n[T]o approximate time optimal behavior, different dynamics require different ergodicity mappings. Thus, when an agent faces a different dynamic, this should evoke the observation of a different utility function. This was observed, in that all subjects showed substantial changes in their estimated utility functions … Second, in shifting from additive to multiplicative dynamics, agents should become more risk averse. This was also observed in all subjects. Third, the predicted increase in risk aversion should be, in the dimensionless units of relative risk aversion, a step change of +1. The mean step change observed across the group was +1.001 (BCI95%[0.829,1.172]). Third, to a first approximation, most (not all) participants modulated their utility functions from ~linear utility under additive dynamics, to ~logarithmic utility under multiplicative dynamics (Fig. 3d). Each of these utility functions are provably optimal for growing wealth under the dynamical setting they adapted to, and in this sense they are reflective of an approximation to time optimality. Finally, Bayesian model comparison revealed strong evidence for the time optimal model compared to both prospect theory and isoelastic utility models, respectively. The latter two models provide no explanation or prediction for how risk preferences should change when gamble dynamics change, and even formally preclude the possibility of maximising the time average growth rate when gamble dynamics do change. Congruent with this explanatory gap, both prospect theory and isoelastic utility models were relatively inadequate in predicting the choices of most participants.\n\nMy major question about the experiment concerns the localised nature of the growth-rate maximisation. These people have lives outside of the experiment and existing wealth (unknown). Yet the behaviour we observed in the multiplicative world was maximisation of the growth rate within the experiment. They effectively maximised the log utility of the in-experiment wealth.\nIf any of these subjects had any material wealth outside of the experiment and were general growth-rate maximisers, their utility function within this experiment should be closer to linear, despite the multiplicative dynamics. The log function has material curvature for small wealth changes near zero. Once you are further up the logarithmic function (higher wealth), a short section of the function is approximately linear. Even though the stakes of this experiment are described as large (~$150USD with potential to win up to ~$600USD), they are likely not large within the context of the subjects’ broader wealth.\nThis point forms one of the central planks of the criticism of expected utility theory emerging from behavioural economics. People reject bets that, if they had any outside wealth, would be “no-brainers” for someone with log utility. Most of this evidence is gathered from experiments with additive dynamics, but there is also little evidence of linear utility in such circumstances.\nWhy did the Copenhagen experiment subjects adopt this narrow frame? It’s unclear, but the explanation must call on psychology or an understanding of the experimental subjects’ broader circumstances.\nAnother line of critique comes from Adam Goldstein, who argues that “the dynamic version of EUT, multi-period EUT, predicts the same change in risk aversion that EE predicts in a simplified model of CE [the Copenhagen experiment].”\nGoldstein is right that EUT predicts a reduction in measured risk aversion in an additive environment. But Goldstein’s analysis depends on people being able to observe each flip and their change in wealth, and then changing their behaviour accordingly. If they could take the bets flip by flip, the first bet on its own is unattractive for a risk averse utility maximiser. But it is possible (indeed likely) for them to reach a level of wealth where a single bet is attractive (in this case, above a wealth of $200), in which case they can continue to accept. Conversely, if they head toward ruin, they can start to reject bets.\nThe possibility of getting up to a level of wealth where the bet becomes attractive can lead an expected logarithmic utility maximiser to accept the first bet due to the potential utility from later bets. The way to determine whether they will do this uses a technique called dynamic programming, which involves working from the last bet backward to work out the expected utility of each single bet.\nHowever, I am unconvinced this critique applies to the experiment by Hulme and friends. The experimental subjects never got to observe the changes in their wealth during the active session (although they might weakly infer the likely direction based on the favourability of the bets they were exposed to). As a result, I’m not convinced we would see the change in risk aversion observed in the experiment under an expected utility framework.\nThat inability to observe outcomes also makes the experiment a weaker examination of dynamics over time than it might otherwise be. In some ways, it is a single-period game where all outcomes are realised simultaneously, multiplying or adding at that point. The absence of seeing how subjects act given a change in wealth removes the ability to see some of the distinguishing phenomena. For instance, the time optimal utility maximiser would not increase risk aversion after losing in the additive environment, whereas in that same environment the traditional utility maximiser would be more likely to reject when the bets become a larger proportion of their wealth. The prospect theory decision maker may become risk seeking if they perceived themselves to be in the domain of losses. The authors note that the lack of update is because they want to avoid mental accounting, but that is, of course, a feature of prospect theory (if I understand their use of the term mental accounting). (I should also say that I understand the lack of updating given the multiple purposes of the experiment, but it would be great to see that relaxed in future iterations.)\nGoldstein also raised a second possible driver of the reduced risk aversion in the additive scenario. Experimental subjects were paid on the based on a random draw of 10 of their active gambles. If the final wealth for an experimental subject from those 10 gambles was negative, they would be given a new draw of 10 gambles. In effect, they were protected from the most severe risks in the additive case, which would reduce risk aversion.\nOne possible mitigant of this effect is that the experimental subjects were not explicitly told they could not lose (although they would likely have inferred they could not suffer loss). I am also not convinced that the strength of this effect would be enough to result in purely linear utility as was observed, but it should be accounted for.\n\n\nCode\n# Simulate the payments to participants based on their actual choices.\n\nlibrary(\"dplyr\", quietly = TRUE, warn.conflicts = FALSE)\n\nset.seed(20200321)\n\n#Import the data on the choices made\nfor (i in 1:19){\n  importData &lt;- read.csv(paste0(\"https://raw.githubusercontent.com/ollie-hulme/ergodicity-breaking-choice-experiment/master/data/TxtFiles_additive/\", i, \"_2.txt\"), sep=\"\")[1:312,] #limit to 312 entries as subject 3 has 314\n  importData &lt;- select(importData, earnings, KP_Final, Gam1_1, Gam1_2, Gam2_1, Gam2_2)\n\n  assign(paste0(\"subject_data_\", i), importData)\n}\n\npayment &lt;- data.frame(matrix(NA, nrow=10, ncol=18))\n\n#Simulate 1000 payments for each participant\nfor (i in c(1:19)){ \n  for (j in 1:1000){\n    subject_data &lt;- get(paste0(\"subject_data_\", i))\n    \n    subject_data &lt;- subject_data %&gt;%\n      mutate(Gam1 = case_when(KP_Final==9 ~ Gam1_1,\n                              KP_Final==8 ~ Gam2_1)) %&gt;%\n      mutate(Gam2 = case_when(KP_Final==9 ~ Gam1_2,\n                              KP_Final==8 ~ Gam2_2)) %&gt;%\n      mutate(result = mapply(function(x,y){sample(c(x,y),1)}, x=Gam1, y=Gam2))\n    \n    #Payment is the initial endowment from the passive phase plus a draw of 10 gambles\n    payment[j,i] &lt;- subject_data$earnings[1] + sum(sample(subject_data$result, 10)) #starting money plus random draw of 10\n  }\n}\n\ncolnames(payment) &lt;- c(1:19)\n\n#remove subject 5 from analysis as excluded in paper\npayment &lt;- payment %&gt;%\n  select(-5)\n\n#Determine how many participants made a loss\nnumGain &lt;- sum(payment&gt;0, na.rm=TRUE)\nnumLoss &lt;- sum(payment&lt;0, na.rm=TRUE)\npercentLoss &lt;- numLoss/(numGain+numLoss)*100\n\n\nI simulated the potential payments of the participants based on the choices they actually made. Only 4% of the potential payments involved a loss, which would have triggered the redraw. That affirms my view that, while it should be accounted for, it is unlikely to explain the experimental result.\nA related point is that the paths involving negative wealth were removed from the passive session on the additive day. This means the subjects were not conditioned to see these negative potential consequences.\n\n\nCode\n# Simulate the passive paths for the Copenhagen experiment.\n\npassiveSim &lt;- function(type=\"additive\", people=10000, start=1000){\n    \n    #parameters used in Copenhagen experiment\n    add &lt;- c(-428, -321, -214, -107, 0, 107, 214, 321, 428)\n    mult &lt;- c(0.447, 0.546, 0.668, 0.818, 1, 1.223, 1.496, 1.830, 2.239)\n\n    add333 &lt;- rep(add,37)\n    mult333 &lt;- rep(mult, 37)\n\n    gamblePath &lt;- cbind(rep(start, people), matrix(data = NA, nrow = people, ncol = 333))\n\n    if(type==\"additive\"){\n        for (i in 1:people){\n        gamble &lt;- sample(add333, size=333, replace=FALSE)\n            for (j in 1:333){\n                gamblePath[i, j+1] &lt;- gamblePath[i, j]+gamble[j]\n            }\n        }\n    }\n\n    if(type==\"multiplicative\"){\n        for (i in 1:people){\n        gamble &lt;- sample(mult333, size=333, replace=FALSE)\n            for (j in 1:333){\n                gamblePath[i, j+1] &lt;- gamblePath[i, j]*gamble[j]\n            }\n        }\n    }\n\n    gamblePath\n}\n\naddSim &lt;- passiveSim(type=\"additive\")\nmultiSim &lt;- passiveSim(type=\"multiplicative\")\n\n# Examine how many simulated paths conform to the required range.\n\n#function to output number below lower limit, above upper limit, and within the range of the two\nnumRange &lt;- function(sim, lower=0, upper=5000, people=10000){\n    \n    low &lt;- people - sum(apply(sim, 1, function(x) all(x&gt;lower)))\n    up &lt;- people - sum(apply(sim, 1, function(x) all(x&lt;upper)))\n    range &lt;- sum(apply(sim, 1, function(x) all(x&gt;lower & x&lt;upper)))\n    df &lt;- data.frame(\n      low = low/people*100,\n      up = up/people*100,\n      range = range/people*100\n    )\n    df\n}\n\naddSimRange &lt;- numRange(addSim)\nmultiSimRange &lt;- numRange(multiSim)\nmultiSimRange1 &lt;- numRange(multiSim, lower=1)\nmultiSimRange10 &lt;- numRange(multiSim, lower=10)\n\n\nIn the simulations I conducted of the additive passive day, 90% of the simulations breach that zero lower bound. 26% breach the 5000kr upper bound (some of the same paths that went below zero), leaving only 2% of the trials that could be provided to subjects on the passive day. In contrast, passive multiplicative paths could not be excluded for going below zero, despite providing a hair-raising ride. 31% of the passive multiplicative paths that I simulated involve wealth dropping to less than 1kr (a 99.9% loss). 60% of the passive multiplicative paths involve wealth dropping to less than 10kr (a 99% loss). Then, at the top end, 92% of the passive multiplicative paths went above $5000kr, leading to their exclusion.\nThe result is that the subjects were conditioned on a limited subset of additive paths that excluded the most negative moments (although also the 26% highest), and on multiplicative paths that excluded the most positive moments. This is a large asymmetry. Obviously, each person saw only one path, and they might have ended up with that combination anyhow, but the systematic conditioning of subjects with benign passive paths and harrowing multiplicative paths should be considered a potential factor in the response of subjects to those fractals.\nIt could be argued that despite the difference in paths, people are simply learning the effect of the fractals that they bet on. However, I am not convinced that experimental subjects would be unaffected by seeing the potential cumulative effect of these bets.\nAs a result, my preliminary view on this experiment is that it provides potential evidence that the dynamics of the environment can influence our model of decision making. However, the experimental results involve behaviour that doesn’t seem to be accounted for by any of the models, and it involves a conditioning process that I’m not completely sold on.\nSome of my other observations on the experiment include:\n\nThe experiment involved several “discrepant trials” where linear utility should have generated one choice and log utility another. These trials generated moderate evidence against the hypothesis of linear utility under additive dynamics. You can also see in Figure 4 above (and in other parts of the paper) that the experimental subjects had mild risk aversion in the additive environment. Similarly, the coefficient of risk aversion in the multiplicative environment seems slightly greater than one - indicating more risk aversion than log utility. (Saying this, I wouldn’t read too much into these particular numbers.)\nAlthough there is a consistent shift for most subjects between the two environments, there is a lot of variation in their degree of risk aversion. This could be due to outside factors, such as total wealth, but raises the question of how much idiosyncracy there is between people in their approaches to growth-rate maximisation (or whatever else it is they are maximising).\nI’m not convinced that the experiment had a design with the strength necessary to elicit a loss aversion parameter of prospect theory (assuming it exists). Every bet involved a choice between a two gambles involving a gain and a loss, rather than having a mix of gain-gain and gain-loss options that might highlight loss aversion. Shifting between those frames would also provide more power to tease out the risk aversion coefficients in the loss and gain domains. (I should note that I’m not confident that the experiment doesn’t have the necessary strength - I use the words “I’m not convinced” deliberately.)\nThere was a required choice between the gambles, which eliminates status quo effects, an arguable driver of many behavioural dynamics (as argued by David Gal).\nThe elicitation of preferences where people need to learn the probabilities through experience is one of the experimental circumstances where loss aversion has generally not been shown to occur (see this literature review by Yechiam and Hochman (pdf)). This provides another reason we might not not expect to elicit loss aversion in this experiment.\nThe set up is complicated. The subjects need to learn fractal relationships. Their payout is based on a random selection of 10 of their bets. The multiplicative environment harder to learn. Does uncertainty drive some of the increase in risk aversion?\n\nSummary\nWhere does this leave us? I take the following lessons from ergodicity economics and the experimental evidence to date:\n\nThe concept that simple growth-rate maximisation results in the same observed behaviour as expected logarithmic utility maximisation in a multiplicative environment (possibly the world we live in), yet possibly provides a more parsimonious explanation, is important. This deserves much more research, including the question of whether this is a model on which we could build the broader decision making architecture. Would prospect theory look different if built on this foundation?\nWe don’t need to throw everything out of the window. Maximising expected utility through using the logarithm of wealth is equivalent to maximising the growth rate in a multiplicative environment. We can continue to use this functional form in much economics work, but should consider a different interpretation on its use.\nFor decision-making under uncertainty, there is a case for placing greater weight on the logarithmic “utility function” over other more highly-specified utility models that do not maximise the growth rate. On this point, Paul Samuelson led a somewhat acrimonious debate about whether an investment strategy using the Kelly criterion - which maximises the geometric growth rate (discussed in my ergodicity economics primer post) - was an appropriate investment strategy. I’ll cover that debate in more detail in a future post, but one of Samuelson’s central points was that Kelly criterion investments are only optimal for an expected log utility maximiser, not for people with other utility functions. The ergodicity economics approach attempts to circumvent this debate by suggesting that our utility function is growth rate maximisation.\nA behavioural response to possible absorbing states (i.e. ruin, death) would seem to require an addition to the growth-rate maximisation model, rather than being directly derived from it. The growth-rate maximisation model also says little about risk-return trade-offs, particularly in an additive environment. (This was also a point raised by Samuelson in the debate about the Kelly criterion, as growth-rate maximisation over finite time horizons can result in catastrophic loss.)\nThere are a lot of decision-making phenomena that would require substantial additions to the ergodicity economics framework if they were to be incorporated. Examples include status quo bias, framing effects, non-linear probability weighting, and rejection of many bets that would seem to maximise a person’s time average growth rate if accepted (or that require an inordinate amount of storytelling to justify it). (Peters and friends have some papers on the application of ergodicity economics to discounting that I’ll deal with in another post.)\n\nOn that final point, Peters often mentions that expected utility theory was an attempt to rescue the failure of expected wealth maximisation to capture decision dynamics. One of the benefits of his model is that the need for psychological explanations is removed.\nHowever, an attempt to remove psychology from decision models will leave a lot of behaviour unexplained. There is a fair question about “what psychology?” is required, and whether this is the psychology of behavioural economics, ecological decision making, resource rationality or something else (see my critical behavioural economics and behavioural science reading list for a flavour of this). But in many situations people do not appear to maximise the growth rate of wealth.\nMy other posts on loss aversion can be found here:\n\nKahneman and Tversky’s debatable loss aversion assumption\nWhat can we infer about someone who rejects a 50:50 bet to win $110 or lose $100? The Rabin paradox explored\nThe case against loss aversion\nErgodicity economics - a primer\nErgodicity economics - Do we need risk or loss aversion to explain our failure to accept some gambles? (this post)"
  },
  {
    "objectID": "posts/the-illusion-of-evidence-based-nudges.html",
    "href": "posts/the-illusion-of-evidence-based-nudges.html",
    "title": "The illusion of evidence-based nudges",
    "section": "",
    "text": "From a recent Journal of Political Economy paper by Stefano DellaVigna, Woojin Kim and Elizabeth Linos (2024):\n\nWe study 30 US cities that ran 73 RCTs with a national nudge unit. Cities adopt a nudge treatment into their communications in 27% of the cases. We find that the strength of the evidence and key city features do not strongly predict adoption; instead, the largest predictor is whether the RCT was implemented using preexisting communication, as opposed to new communication.\n\nA nudge with a negative result is almost as likely to be implemented as a positive result.\n\nThere is no difference in adoption for results with negative point estimates (25% adoption), results with positive but not statistically significant estimates (25%), and estimates that are positive and statistically significant (30%). The likelihood of adoption increases with effect size (measured in percentage points), from 17% in the bottom third to 38% in the top third, though this difference is not statistically significant at conventional levels.\n\nMy cynical take is that running trials with nudge units is cool. Despite more than a decade of nudge unit stories, behavioural insights is still a “shiny new thing” and are a way to say “we’re doing science” The hard work of implementing or scaling an intervention simply isn’t as sexy.\n\n\n\n\n\nReferences\n\nDellaVigna, S., Kim, W., and Linos, E. (2024). Bottlenecks for evidence adoption. Journal of Political Economy, 000–000. https://doi.org/10.1086/729447"
  },
  {
    "objectID": "posts/the-psychological-and-genes-eye-view-of-ergodicity-economics.html",
    "href": "posts/the-psychological-and-genes-eye-view-of-ergodicity-economics.html",
    "title": "The psychological and genes’ eye view of ergodicity economics",
    "section": "",
    "text": "This post was my plan for a presentation at the Foundation of Utility and Risk Conference. I drew on my previous posts laying out the foundations of ergodicity economics and examining what ergodicity economics states about risk preferences. This varied somewhat from delivery (I’m easily waylaid and skipped a couple of sections). Given it’s to a technical audience, there are a few moments that might lose the lay reader.\n–"
  },
  {
    "objectID": "posts/the-psychological-and-genes-eye-view-of-ergodicity-economics.html#introduction",
    "href": "posts/the-psychological-and-genes-eye-view-of-ergodicity-economics.html#introduction",
    "title": "The psychological and genes’ eye view of ergodicity economics",
    "section": "Introduction",
    "text": "Introduction\nThis presentation started with a blog post. Around five years ago when I was ensconced in the corporate world, I wrote a couple of posts on an idea called ergodicity economics. A random physicist, Ole Peters, was riling people up on twitter about how economists were doing it wrong, how expected utility theory was fatally flawed, how you don’t need to introduce psychology to explain human decisions under risk, and how all the anomalies in behavioural economics could be reconciled with his new theory.\nThere were plenty of people countering the stronger statements, but I thought that by writing a post or two I could understand the idea better myself. So I ignored the hyperbole and tried to give a fair hearing to the underlying idea. Since I had some background in evolutionary biology, I also tried to view it from an evolutionary lens.\nIn the spirit of those original posts, I am going to avoid today’s presentation from becoming an exercise in attacking the most outlandish statements. There are a couple of published critiques of ergodicity economics, one by Jason Doctor et al (2020) in Nature Physics and one more recent by Matthew Ford and John Kay (2023) in Econ Journal Watch that do a good job of addressing the claims about economics and expected utility theory. Instead, I’m going to give a flat description of ergodicity economics, before laying out some psychological and evolutionary observations.\n\nThe bet\nSo let me start with the classic example used to illustrate what ergodicity economics is about. You may have seen this before.\n\nSuppose you are offered a series of 100 bets on the flip of a coin. You win 50% of your wealth on heads. You lose 40% of your wealth on tails. Do you take the bet?\n\nThe expected value of the bet is 5% of your wealth each flip. Continue playing for many rounds and your expected wealth is very large.\nHowever, what is the most probable outcome over many repeats of this bet?\nThis plot is the result of a simulation of 10,000 people, each starting with $100, experiencing the 100 flips. The black line is the average wealth of the population. The red lines are paths of the first 20 people in the simulation.\n\n\nSetup code\n# Load the required packages\n\nlibrary(ggplot2)\nlibrary(scales) #use the percent scale later\nlibrary(dplyr) #use the filter function later\n\n\n\n\nCode for bet function\n# Create a function for running of the bets.\n\nbet &lt;- function(p, n, t, start=100, gain, loss, ergodic=FALSE, absorbing=FALSE){\n\n  #p is probability of a gain\n  #n is how many people in the simulation\n  #t is the number of coin flips simulated for each person\n  #start is the number of dollars each person starts with\n  #if ergodic=FALSE, gain and loss are the multipliers\n  #if ergodic=TRUE, gain and loss are the dollar amounts\n  #if absorbing=TRUE, zero wealth ends the series of flips for that person\n\n  params &lt;- as.data.frame(c(p, n, t, start, gain, loss, ergodic, absorbing))\n  rownames(params) &lt;- c(\"p\", \"n\", \"t\", \"start\", \"gain\", \"loss\", \"ergodic\", \"absorbing\")\n  colnames(params) &lt;- \"value\"\n\n  sim &lt;- matrix(data = NA, nrow = t, ncol = n)\n\n  if(ergodic==FALSE){\n    for (j in 1:n) {\n      x &lt;- start\n      for (i in 1:t) {\n      outcome &lt;- rbinom(n=1, size=1, prob=p)\n      ifelse(outcome==0, x &lt;- x*loss, x &lt;- x*gain)\n      sim[i,j] &lt;- x\n      }\n    }\n  }\n\n if(ergodic==TRUE){\n    for (j in 1:n) {\n      x &lt;- start \n      for (i in 1:t) {\n      outcome &lt;- rbinom(n=1, size=1, prob=p)\n      ifelse(outcome==0, x &lt;- x-loss, x &lt;- x+gain)\n      sim[i,j] &lt;- x\n      if(absorbing==TRUE){\n        if(x&lt;0){\n          sim[i:t,j] &lt;- 0\n            break\n        }\n        }\n      }\n    }\n  }\n\n  sim &lt;- rbind(rep(start,n), sim) #placing the starting sum in the first row\n  sim &lt;- cbind(seq(0,t), sim) #number each period\n  sim &lt;- data.frame(sim)\n  colnames(sim) &lt;- c(\"period\", paste0(\"p\", 1:n))\n  sim &lt;- list(params=params, sim=sim)\n  sim\n}\n\n\n\n\nCode to run simulation\n# Simulate 10,000 people who accept a series of 1000 50:50 bets to win \\$50 or lose \\$40 from a starting wealth of \\$100.\n\nset.seed(20240705)\nnonErgodic &lt;- bet(p=0.5, n=10000, t=1000, gain=1.5, loss=0.6, ergodic=FALSE)\n\n\n\n\nCode for plot\n# Function to plot individual paths and average wealth over a set number of periods.\nplotWealth &lt;- function(sim, t = 100, people = NULL) {\n  basePlot &lt;- ggplot(sim$sim[1:(t+1),], aes(x = period)) +\n    labs(y = \"Wealth ($)\")\n\n  # Add lines for individual paths if specified\n  if (!is.null(people)) {\n    for (i in 1:people) {\n      basePlot &lt;- basePlot +\n        geom_line(aes(y = !!sim$sim[c(1:(t+1)), i+1]), color = \"red\")\n    }\n  }\n\n  # Add line for average wealth\n  basePlot &lt;- basePlot +\n    geom_line(aes(y = rowMeans(sim$sim[1:(t+1), 2:(sim$params[2,]+1)])), color = \"black\", linewidth = 1)\n  \n  basePlot\n}\n\n# Generate the plot with individual paths and average wealth\nnonErgodicPlot &lt;- plotWealth(sim = nonErgodic, t = 100, people = 20)\nnonErgodicPlot\n\n\n\n\n\nFigure 1: Plot of first 20 people against average wealth\n\n\n\n\n\n\n\n\nThe sudden drop in mean wealth toward the end of the sequence is an interesting feature that I will ignore for the moment. But look at the red lines. At the end of 100 periods, all are below the mean wealth, and only one of the 20 agents has enough wealth at the end that you can discern the line from the x-axis.\nLet us now use a log scale to enable us to see the pattern more clearly.\n\n\nCode for log plot\n# Plot both the average outcome and first twenty people on the same plot.\n\nlogNonErgodicPlot &lt;- plotWealth(sim=nonErgodic, t=100, people=20)+\n    scale_y_log10(breaks = c(0.0001, 0.1, 100, 100000), labels = c(\"0.0001\", \"0.1\", \"100\", \"100000\"))\nlogNonErgodicPlot\n\n\n\n\n\nFigure 2: Plot of first 20 people against average wealth (log scale)\n\n\n\n\n\n\n\n\nAll 20 of these people are below the mean wealth. Only one is ahead of where they started.\n\n\nCode to create function to generate summary statistics\n# Create a function to generate summary statistics.\nsummaryStats &lt;- function(sim, t = 100) {\n  \n  # Extract the wealth data for the specified time\n  wealth_data &lt;- as.matrix(sim$sim[(t + 1), 2:(sim$params[2, ] + 1)])\n  \n  # Calculate mean wealth\n  mean_wealth &lt;- mean(wealth_data)\n  \n  # Calculate median wealth\n  median_wealth &lt;- median(wealth_data)\n  \n  # Number and percentage who lost more than 99% of their wealth\n  num_lost_99 &lt;- sum(wealth_data &lt; (sim$params[4, ] / 100))\n  perc_lost_99 &lt;- (num_lost_99 / sim$params[2, ]) * 100\n  \n  # Number and percentage who gained wealth\n  num_gain &lt;- sum(wealth_data &gt; sim$params[4, ])\n  perc_gain &lt;- (num_gain / sim$params[2, ]) * 100\n  \n  # Number who increased their wealth more than 100-fold\n  num_increased_100 &lt;- sum(wealth_data &gt; (sim$params[4, ] * 100))\n  \n  # Wealth and wealth share of the wealthiest person\n  max_wealth &lt;- max(wealth_data)\n  perc_max_wealth_share &lt;- max_wealth / sum(wealth_data) * 100\n  \n  # Combine all statistics into a data frame\n  stats &lt;- data.frame(\n    mean_wealth = mean_wealth,\n    median_wealth = median_wealth,\n    num_lost_99 = num_lost_99,\n    perc_lost_99 = perc_lost_99,\n    num_gain = num_gain,\n    perc_gain = perc_gain,\n    num_increased_100 = num_increased_100,\n    max_wealth = max_wealth,\n    perc_max_wealth_share = perc_max_wealth_share\n  )\n  \n  return(stats)\n}\n\n\nnonErgodicStats &lt;- summaryStats(nonErgodic, 100)\n\n\nThese 20 people are representative of the broader population. Across the 10,000 agents in this simulation, 86 per cent lost money. The mean wealth was $22303, but the median wealth was $0.52.\nWhat is the intuition behind this?\nThe black line reflects the expected gain of 5% per flip.\nBut for the red lines, over the long term, an individual will tend to get around half heads and half tails. As the number of flips goes to infinite, the proportion of heads or tails “almost surely” converges to 0.5. This means that each person will tend to get a 50% increase half the time (or 1.5 times the initial wealth), and a 40% decrease half the time (60% of the initial wealth). The time average growth in wealth for an individual is (1.5\\times 0.6)^{0.5} \\sim 0.95, or approximately a 5% decline in wealth each period. Every individual’s wealth will tend to decay at that rate. The black line is held up by a very lucky few.\nA system where the time average converges to the ensemble average (our population mean) is known as an ergodic system. The sequence of gambles I have just shown you is non-ergodic as the time average and the ensemble average diverge. (I’ll ignore the finer debates about whether this problem even involves ergodicity to the side.)\nThis leads to the following claim: as we cannot individually experience the ensemble average, the ensemble average is not what humans consider in their decision making. Instead, people maximise the time average growth rate of wealth. For this bet, as the time average growth rate is negative, an ergodicity economics agent would reject the bet.\nContrast this with the expected utility approach, where the utility of each outcome is weighted by its probability and summed to give the expected utility. Expected utility theory would be consistent with both accepting and rejecting the bet depending on the particular utility function.\nThere is, however, an incidental alignment between ergodicity economics and expected utility theory. If a person has log utility - that is, they maximise the probability-weighted logarithm of the possible outcomes - they will maximise the time average growth rate.\n\n\nAdditive versus multiplicative\nOne important feature of the bet I have just shown is that the outcomes are multiplicative. A win on one flip leads to a larger stake flip on the next bet. The size of the bet scales up or down with wealth.\nWhat if I offered you the following bet instead?\n\nYou have $100 and are offered a gamble involving a series of 100 coin flips. For each flip, heads will increase your wealth by $50. Tails will decrease it by $40. Do you take the bet?\n\nYou can see the tweak from the original bet, with dollar sums rather than percentages. For someone with $100 in wealth, the first flip is effectively identical, but future bets will be additive on that result and always involve the same shift of $50 up or $40 down.\nThis second series of flips is ergodic. The expected value of each flip is $5 (0.5\\times \\$50-0.5\\times \\$40=\\$5). The time-average growth rate is also $5.\nLet’s simulate as we did for multiplicative bets, with 10,000 people starting with $100 and flipping the coin 100 times. This plot shows the average wealth of the population, together with the paths of the first 20 of the 10,000 people (in red).\n\n\nCode to simulate ergodic environment\n# Simulate 10,000 people who accept a series of 1000 50:50 bets to win \\$50 or lose \\$40 from a starting wealth of \\$100.\n\nset.seed(20240705)\nergodic &lt;- bet(p=0.5, n=10000, t=100, gain=50, loss=40, ergodic=TRUE, absorbing=FALSE)\n\n\n\n\nCode to plot ergodic simulation\n# Plot both the average outcome and first twenty people on the same plot.\n\nergodicPlot &lt;- plotWealth(sim=ergodic, t=100, people=20)\nergodicPlot\n\n\n\n\n\nFigure 3: Average wealth of population and path of first 20 people\n\n\n\n\n\n\n\n\n\n\nCode to generate ergodic summary statistics\n# Generate summary statistics for the population and wealthiest person after 100 and 1000 flips}\n\nsummaryErgodic100 &lt;- summaryStats(sim=ergodic, t=100)\n\n\nThe individual growth paths cluster on either side of the population average. After 100 flips, the mean wealth is $602 and the median $600. 87% of the population has gained in wealth. This alignment between the mean and median wealth, and the relatively equal distribution of wealth, are characteristic of an ergodic system.\n\n\nCode to calculate number with zero wealth\n# Determine how many people (in the first people out of n) experienced zero wealth or less during the simulation.\n\nnumZero &lt;- function(sim, t, subset=0){\n\n  #subset\n  data &lt;- if(subset==0 | subset&gt;sim$params[2,]){\n    sim$sim[1:t,2:(sim$params[2,]+1)]\n  } else {\n    sim$sim[1:t,2:(subset+1)]\n  }\n  \n  # number of people who experienced zero wealth or less\n  numZero &lt;- length(data) - sum(sapply(data, function(x) all(x&gt;0)))\n  numZero\n  \n}\n\nnumZeroTwenty100 &lt;- numZero(sim=ergodic, t=100, subset=20)\nnumZeroTwenty1000 &lt;- numZero(sim=ergodic, t=1000, subset=20)\nnumZero100 &lt;- numZero(sim=ergodic, t=100)\nnumZero1000 &lt;- numZero(sim=ergodic, t=1000)\n\n\nNow for a wrinkle, which we can see in the plotted figure. Of those first 20 people plotted on the chart, 11(!) had their wealth go into the negative over those 100 periods. We see the same phenomenon across the broader population, with 5433 dropping below zero in those first 100 periods.\nTo the extent zero wealth is ruinous when it occurs, that event is severe. If the player only incurs the consequences of their final position, the bet is unlikely to result in ruin but still presents a non-zero threat of catastrophe.\nWhat would an expected utility maximiser do here? For a person with log utility, any probability of ruin during the flips would lead them to reject the gamble. The log of zero is negative infinite, which outweighs all other possible outcomes, whatever their magnitude or probability.\nThe growth-rate maximiser would accept the bet if they didn’t fear ruin. The time-average growth of $5 per flip would pull them in. If ruin was feared and consequential, then they might also reject.\n\n\nThe ergodicity economics hypothesis\nThis brings us to the core hypotheses in ergodicity economics.\nFirst, people act to maximise the time-average growth rate of their wealth.\nSecond, the optimal action to maximise the time-average growth rate varies between multiplicative and additive environments. In multiplicative environments, people have log utility. In additive environments, they are risk neutral.\n\n\nWhat are the implications of this finding?\nThe advocates of ergodicity economics have applied this claim to a range of economics problems.\nPeters (2023) argues that we can think of insurance as a tool to make wealth grow faster rather than as protection for the risk averse. This plot shows the outcomes for agents facing a 5% probability of a loss of 95% of their wealth each period. Uninsured agents experience ruinous path over time, losing far more than would be implied by the expected value. Those who insure at a cost experience a slower decline.\nAs an aside, I do find these numbers quite comical: 2500 periods with a 5% probability of loss, leading to an expected value of around 10^{-70} times the initial wealth even under the insured scenario. The expected-value optimisers, even if they stated with every atom in the universe, would be left with a fraction of an atom by the end.\n\nPeters and Adamou (2022) similarly propose a desire to maximise the growth rate as the origin of cooperative behaviour. Here, the green trajectories are the selfish agents, the blue line the cooperators. In some ways, this is just the insurance problem reframed.\n\nOne of the more interesting examples concerns time preference. Adamou et al. (2021) use growth rate maximisation as an explanation for exponential discounting, hyperbolic discounting and preference reversals, depending on the particular wealth dynamic.\nI won’t cover all the scenarios, but this image captures a situation where an agent in an additive world has a choice between a smaller sooner and a larger later pay-off. This agent seeks to maximise the growth rate of their wealth. From the perspective of t_0 the larger later pay-off provides a higher growth rate, which is equal to the slope of the line extended to the top of that pay-off. By the third frame, as the agent moves closer to the smaller, sooner pay-off, the higher growth rate now comes from that early pay-off. They have reversed their preference.\n\nIt is when seeing ideas such as this that the suggestion that ergodicity economics is a “psychology free” seem slightly ridiculous. The agent must be myopic to be unable to see their upcoming preference reversal, or to realise the foregone larger opportunity on the other side of that smaller sooner payment."
  },
  {
    "objectID": "posts/the-psychological-and-genes-eye-view-of-ergodicity-economics.html#the-experiment",
    "href": "posts/the-psychological-and-genes-eye-view-of-ergodicity-economics.html#the-experiment",
    "title": "The psychological and genes’ eye view of ergodicity economics",
    "section": "The experiment",
    "text": "The experiment\nBut rather than going down that rabbit-hole, I want to present an experimental result. In 2019, a working paper was released examining whether shifting between an additive and multiplicative environment would change risk preferences. One of my blog posts was on that working paper. The paper was ultimately published in 2021 in PLOS Computational Biology (Meder et al., 2021).\nThat first experiment was subject to many criticisms, including by me, so Skjold et al. (2024) designed a new experiment. A pre-print describing the results from this experiment was released at the end of May. Some of the criticisms have been addressed in this new experiment, and the result is interesting.\nExperimental participants were randomised to an additive or multiplicative environment where they were asked to make a series of bets. After making the bets in one environment, they were crossed over to the other.\nAt the beginning of the additive and multiplicative session, participants were trained on a set of fractal images, each of which had a specific effect on their endowment: multiplication by some fraction in the multiplicative session, or addition or subtraction of some sum in the additive session. Participants were then tested on whether they had learnt the ordering of the fractals, with the subset of participants with low learning excluded from the analysis. This image shows the training procedure, which essentially comprises exposure to the fractal and then an update to their wealth.\n\nThis use of fractals instead of numbers is one of the confounding factors that could influence the results: we have introduced ambiguity into the subsequent decisions. It was one of my critiques of the original experiment, but I clearly wasn’t very persuasive. Anyhow, let’s ignore that for now.\nParticipants then proceeded to their decision task, where they had to choose between gambles, each of which is represented by two of the fractal images. Over a sequence of 160 decisions, they would be shown the gambles, make their choice, be shown the outcome of the gamble, then see the effect of the outcome on their wealth.\n\nYou can watch a video of the training and decision task below. (I didn’t show this video in the presentation.)\n\nParticipants were incentivised by being paid their relative proportion of points compared to a rolling window of 10 participants. Another exhibit of “why do we make experimental incentives so complicated” and another confound to the experimental result. By rewarding on proportional access to a pool, they have introduced diminishing gains and a cap on winnings. But again, I’ll ignore that for today.\nSo, to the result. The research team modelled participants as having an isoelastic utility function (a reasonably strong assumption), with the parameter \\eta calculated by Bayesian cognitive modelling.\n\nf_{\\eta}(x)=\\begin{cases}\n\\frac{x^{1-\\eta} - 1}{1-\\eta} & \\text{if } \\eta \\neq 1 \\\\[6pt]\n\\ln(x) & \\text{if } \\eta = 1\n\\end{cases}\n\nWhat would we predict the value of \\eta to be in this experiment? Expected utility theory is quiet on the precise value. The ergodicity economics approach, however, gives us a prediction. First, \\eta will be one in the multiplicative condition, as log utility maximises the growth rate. Second, \\eta will be zero in the additive condition. The growth rate is maximised in an additive environment by risk neutral behaviour.\nThis chart shows the result. Thin lines represent individual participants. The thick lines represent the aggregate. For the additive scenario, participants were close to risk neutral: the aggregate estimate of \\eta was 0.1. For the multiplicative condition, although there was a wider distribution of values, the central estimate of \\eta was one.\n\nDespite the elements of this experiment that I would do differently - I have only hinted at a couple - this is a strong result.\nIf the ergodocity economics hypotheis is correct, it is worth thinking about what this means psychologically.\nWhen modelling the utility function, the researchers took the value of x to be the participant’s experimental wealth at the time the participant makes their decision. It is the initial endowment, plus or minus the results of the previous bets. x does not include outside wealth.\nBut if this use of x is an accurate characterisation of the decision making process of the agents, it suggests a form of narrow bracketing or a degree of myopia. Agents are maximising the growth rate within the experiment, not more generally. We need to introduce some psychology to explain this. (This phenomenon is common across lab experiments involving risky decisions.)\nSimilarly, this experiment is part of a broader environment with either multiplicative or additive characteristics. Experimental participants can take their payment from the experiment and invest it. Maximising the growth rate in the additive condition by maximising expected value may not maximise the total growth rate if the world outside the experiment is multiplicative."
  },
  {
    "objectID": "posts/the-psychological-and-genes-eye-view-of-ergodicity-economics.html#an-evolutionary-analysis",
    "href": "posts/the-psychological-and-genes-eye-view-of-ergodicity-economics.html#an-evolutionary-analysis",
    "title": "The psychological and genes’ eye view of ergodicity economics",
    "section": "An evolutionary analysis",
    "text": "An evolutionary analysis\nNow, via a rather long and winding path, I want to turn to a couple of evolutionary observations about ergodicity economics. There is a large literature in the evolution of preferences, not to mention in the evolutionary biology literature itself, that is relevant to an analysis of growth rate maximisation. Since the concepts are already there, I’m going to lean on them and turn them to my own purpose.\nThe first evolutionary angle concerns what happens when we take a gene’s eye view. And to assist me in making this point, let me show a quote from a Nature Physics paper in which Ole Peters (2019) summarises his work:\n\n[I]n maximizing the expectation value - an ensemble average over all possible outcomes of the gamble - expected utility theory implicitly assumes that individuals can interact with copies of themselves, effectively in parallel universes (the other members of the ensemble). An expectation value of a non-ergodic observable physically corresponds to pooling and sharing among many entities. That may reflect what happens in a specially designed large collective, but it doesn’t reflect the situation of an individual decision-maker.\n\nIgnoring the fact that Peters mis-characterises expected utility theory, this idea of interacting with copies of themselves is what happens at the level of genes. By the presence of multiple copies of a gene across individuals, the gene can experience the ensemble average. The following toy model and simulation illustrates.\n\nA toy model\nSuppose two types of agents lived in a non-ergodic world.\nOne type of agent seeks to maximise the time-average growth rate of its number of descendants. This desire to maximise the time-average growth rate is a function of its genotype, and is transmitted to its children.\nThe other type of agent seeks to maximise the expected number of offspring. Similarly, this agent’s preferences are set genetically.\nIn the environment in which these agents live, they have a choice of strategy. One strategy is to have a single offspring asexually with certainty before they die. The other strategy is a 50:50 bet of having either 0.6 or 1.5 offspring. Part offspring sounds weird, but with a large population of agents, you can think of this as the average number of offspring. The simulation works out largely the same if I make the number of children probabilistic in accord with those numbers. You can see I have effectively mimicked the classic ergodicity economics bet.\nOne final feature in this environment will be that each individual experiences its own flip. You might think of this environment as involving idiosyncratic risk. This is an important assumption that I will return to.\nGiven this setup, the type that maximises the expected number of offspring always accepts the bet. The time-average growth maximiser always rejects it. Which would come to dominate the population?\nThe time-average growth rate maximiser population stays constant. One offspring to each asexual parent.\nThis is a chart of the population of the accepting type for a simulation of 100 generations, starting with a population of 10,000.\n\n\nCode for evolutionary simulation\nset.seed(20240705)\n\n# create function to round probabilistically - important when small numbers involved\nprobabilistic_round &lt;- function(x) {\n  if (runif(1) &lt; x - floor(x)) {\n    ceiling(x)\n  } else {\n    floor(x)\n  }\n}\n\n# Function to simulate evolution betting\nevolutionBet &lt;- function(p, n, t, gain, loss) {\n  \n  #p is probability of a gain\n  #region  is how many people in the simulation\n  #t is the number of generations simulated\n  \n  params &lt;- data.frame(value = c(p, n, t, gain, loss))\n  rownames(params) &lt;- c(\"p\", \"n\", \"t\", \"gain\", \"loss\")\n\n  sim &lt;- numeric(t + 1)\n  sim[1] &lt;- n # Start population\n\n  for (i in 1:t) {\n    for (j in 1:round(n)) {\n      outcome &lt;- rbinom(1, 1, p)\n      n &lt;- n + (ifelse(outcome == 1, gain, loss) - 1)\n    }\n    n &lt;- probabilistic_round(n)\n    sim[i + 1] &lt;- n\n    if (n == 0) break\n  }\n\n  sim &lt;- data.frame(period = 0:t, n = sim)\n  list(params = params, sim = sim)\n}\n\nevolution &lt;- evolutionBet(p=0.5, n=10000, t=100, gain=1.5, loss=0.6) #more than 100 periods can take a very long time, simulation slows markedly as population grows\n\n\n\n\nCode for evolutionary plot\n# Plot the population growth for the evolutionary scenario (Figure 8).\n\nbasePlotEvo &lt;- ggplot(evolution$sim[c(1:101),], aes(x=period))\n\nexpectationPlotEvo &lt;- basePlotEvo +\n  geom_line(aes(y=n), color = 1, linewidth=1) +\n  labs(y = \"Population\")\n\nexpectationPlotEvo\n\n\n\n\n\nFigure 4: Population of accepting types\n\n\n\n\n\n\n\n\nYou can see that they have a population growth rate of close to 5%.\nWhy don’t they experience the decline we saw in earlier simulations of this bet? Because their copies experience the full ensemble of outcomes.\nSo here we have a toy model that shows that time-average growth rate maximisation may not be the optimal strategy in a multiplicative environment. The constant population of time-average growth rate maximisers is swamped by the spreading population of the expected value maximisers.\nThis behaviour could also emerge where it did not previously exist. The simulation I just showed you had 10,000 agents with expected value maximising behaviour to start. What if we had a population of time-average growth rate optimisers, and a single expected value maximiser emerged?\nI simulated 10,000 instances of a single individual developing the mutation. This plot shows the population of the first ten mutants. For five of them, the mutation appears and disappears. For the other five, however, they grow into a substantial population.It only takes a few agents with this mutation to effectively diversify the results.\n\n\nCode for evolutionary simulation\n# run 9 simulations with 1 accepting type to start\n\nset.seed(20240705)\n\nmutations &lt;- 10000\n\nfor (i in 1:mutations) {\n  assign(paste0(\"evolution_\", i), evolutionBet(p=0.5, n=1, t=100, gain=1.5, loss=0.6))\n}\n\n# Create a data frame for each simulation and store in a list\ndata_frames &lt;- list()\n\nfor (i in 1:mutations) {\n  # Dynamically retrieve the variable and select the first 101 rows of $sim\n  current_data &lt;- get(paste0(\"evolution_\", i))$sim[1:101, ]\n  \n  # Add a column for 'i' to use as color in plotting\n  current_data$i &lt;- i\n  \n  # Append to the list\n  data_frames[[i]] &lt;- current_data\n}\n\n# Combine all data frames into one\ncombined_data &lt;- do.call(rbind, data_frames)\n\n\n\n\nCode to count mutation spread\n# Count the number of simulations where the mutation spread (where n &gt; 1 when i = 100)\nmutation_count &lt;- sum(combined_data[combined_data$period == 100,]$n &gt; 1, na.rm = TRUE)\n\n\n\n\nCode for evolutionary plot with mutation\n# Plot first 10 lines\nmutationPlotEvo &lt;- ggplot(combined_data[combined_data$i &gt;= 1 & combined_data$i &lt;= 10,], aes(x=period, y=n, color=factor(i))) +\n  geom_line() +\n  labs(y = \"Population\") +\n  #remove legend\n  theme(legend.position = \"none\")\n\n\nmutationPlotEvo\n\n\n\n\n\nFigure 5: Population of accepting types with mutation\n\n\n\n\n\n\n\n\nIf we zoom into the first 35 periods, you can see the dynamic for those where the mutation did not spread.\n\n\nCode for evolutionary plot with mutation (first 30 periods)\n# Plot first 35 periods only\nmutationPlotEvo30 &lt;- ggplot(combined_data[combined_data$i &gt;= 1 & combined_data$i &lt;= 10,], aes(x=period, y=n, color=factor(i))) +\n  geom_line() +\n  labs(y = \"Population\") +\n  xlim(0, 30) +\n  ylim(0, 35) +\n  #remove legend\n  theme(legend.position = \"none\")\n\nmutationPlotEvo30\n\n\n\n\n\nFigure 6: Population of accepting types with mutation (first 30 periods for first 10 agents)\n\n\n\n\n\n\n\n\nThis is a slightly rosier picture than occurs across the full 10,000 simulations, where the mutation did not spread 70% of the time.\nThese illustrations suggest that time-average growth optimisation may not be the optimal strategy, but it hinges on one critical assumption: that risk is idiosyncratic. Diversification enables the gene to experience the ensemble average. What if such diversification is not possible?\nIn that case, we are effectively back to the world that I showed you at the beginning. You could consider each line to represent an individual and their children, with their children all bound to the same bet. In almost all cases, this leads to a decline in frequency. An extraordinarily lucky few might boom through many rolls of the dice, but that is a rare chance.\nFurther, recall decline toward the end of the 100 periods. This occurred as a small number of genetic lines comprise most of the population - in fact, one single line of the original 10,000 comprises 53% of the population at the end of 100 periods - and they cannot diversify their risk. A few unlucky coin flips and they are gone.\nThe result is that, over a long enough time horizon, everyone is wiped out. There is no longer a lucky few. Here’s the same simulation plotted through 1000 iterations, over evolutionary time, in both linear and log scales.\n\n\nCode for evolutionary plot over 1000 periods\nnonErgodicPlot &lt;- plotWealth(sim=nonErgodic, t=1000, people=20)\nnonErgodicPlot\n\n\n\n\n\nFigure 7: Population of accepting types over 1000 periods\n\n\n\n\n\n\n\n\n\n\nCode for evolutionary plot over 1000 periods (log scale)\nlogNonErgodicPlot &lt;- nonErgodicPlot+\n    scale_y_log10(breaks = c(0.0001, 0.1, 100, 100000), labels = c(\"0.0001\", \"0.1\", \"100\", \"100000\"))\nlogNonErgodicPlot\n\n\n\n\n\nFigure 8: Population of accepting types over 1000 periods (log scale)\n\n\n\n\n\n\n\n\nThe result is that with aggregate risk, time-average growth rate maximisation can be the optimal strategy.\n\n\nProbability matching\nTo explore this in more detail, I’m going to turn to another example that has a rich history in the biology literature.\nThe particular example I am going to pick is a bit cartoonish, and comes from Andrew Lo’s Adaptive Markets. This in turn draws on a more technical paper by Thomas Brennan and Lo (2011) published in The Quarterly Journal of Finance to explain the evolution of probability matching. You can find other earlier examples in the literature, such as by Cooper and Kaplan (1982) in The Journal of Theoretical Biology.\nThe example concerns an animal called the tribble. For amusement, I chucked an earlier draft of this presentation into ChatGPT and asked it to illustrate it. Most of what it produced was unusable, but I’ve kept a couple of images for which I thought it did a good job.\nTribbles live in a region comprising valleys and plateaus. Tribbles reproduce once in their life (producing three offspring asexually) and must choose whether to reproduce in the valley or on the plateau. This is a risky decision, however, as the valleys are affected by floods and the plateaus by drought.\n\nEach generation there is a 75 per cent probability of sun. In such a case, the tribbles born on the plateau perish. The other 25 per cent of the time, it rains, leading to flood in the valleys and the death of those tribbles breeding there.\n\n\n\n\n\n\n\n\n\n\nWhat, then, is the growth maximising breeding strategy?\nLet’s set q as the proportion of tribbles that breed in the valley. If we maximise the expected number of offspring, tribbles would breed in the valley 100 per cent of the time. That is, q=1, leading to an an expected 0.75\\times 3=2.25 offspring.\nHowever, in the long run, if all tribbles make this choice, the tribbles will be wiped out. Over the long term, the tribbles will experience droughts 75 per cent of the time and floods 25 per cent of the time. Putting this into the language of ergodicity economics, the time average growth rate is zero.\nTo find the q that maximises the growth rate, we take the log of G and calculate the derivative. The solution is to breed in the valley 75 per cent of the time and on the plateau 25 per cent of the time.\n\\begin{align*}\nG&=3\\times q^{0.75}(1-q)^{0.25}\\\\[6pt]\n\\\\\n\\log(G)&=\\log(3)+0.75\\log(q)+0.25\\log(1-q)\\\\[6pt]\n\\\\\n\\frac{d}{dq}\\log(G)&=\\frac{0.75}{q}-\\frac{0.25}{1-q}=0\\\\[6pt]\n\\\\\nq&=0.75\n\\end{align*}\nProbability matching in this world maximises the time-average growth rate.\nThis result is the same if we approach this from the perspective of log utility. If we maximise log utility as a function of number of offspring, we get the same value of q.\n\\begin{align*}\nU(n)&=0.75\\times \\log(3q)+0.25\\times log(3-3q))\\\\[6pt]\n\\\\\n\\frac{d}{dq}U(n)&=0.75\\times \\frac{3}{3q}-0.25\\times \\frac{3}{3-3q}=0\\\\[6pt]\n\\\\\nq&=0.75\n\\end{align*}\nHere we have a world where maximising the time-average growth rate is the optimal solution. In this case, it is achieved via probability matching.\nThere is one feature of this scenario that might seem off. Probability matching maximises the growth rate for the species. But which individuals are heading off to the plateau where 75% of the time they are going to be fried? If this an altruistic act for the benefit of the species? Why not head down to the cool valley where they, as an individual, is more likely to survive?\nThe answer was provided by Grafen (1999). Suppose we have a large population of size N. If an agent heads to the valley, the expected fraction of the population comprising their offspring (\\pi) will be:\n\n\\pi(\\text{valley})\\approx\\frac{0.75}{qN}=\\frac{1}{N}\n\nIf they stay to fry on the plateau, their offspring will comprise a similar fraction of the smaller remaining population:\n\n\\pi(\\text{plateau})\\approx\\frac{0.25}{(1-q)N}=\\frac{1}{N}\n\nFitness is a relative measure. At equilibrium q, each option delivers the same fitness.\n\n\nIdiosyncratic versus aggregate risk\nI have created two hypothetical worlds, one making the case for maximising expected value despite being in a multiplicative world, and another where maximising the time-average growth rate is the optimal strategy. In both cases, the optimal strategy relies on the gene’s eye view. The variation comes from the probability structure of the environment.\nI am hardly the first to propose this. I’m drawing on a rich history of research into the evolution of geometric growth rate maximisation in the biological literature. This in turn has been used by economic researchers such as Robson and Samuelson (2011) in the analysis of the evolution of preferences.\nBut, I like to think that this ergodicity economics exercise provides a nice opportunity to tie multiple threads together to provide a different perspective.\nWhat’s next? I’ve been building a set of simulations examining mixed environments. What if the probabilistic structure of the environment is a mix of multiplicative and additive bets? You could think of the additive phase of the experiment I described earlier as comprising an few additive gambles in a multiplicative world. What if the environment switches between additive and multiplicative?\nThere isn’t a simple closed form solution to this problem. If you have a mix of additive and multiplicative bets, or even a non-constant multiplicative growth rate, an “ergodic observable” cannot easily be created. At present, it is best examined via simulation."
  },
  {
    "objectID": "posts/using-large-language-models-as-academic.html",
    "href": "posts/using-large-language-models-as-academic.html",
    "title": "Using large language models as an academic",
    "section": "",
    "text": "This post started as a draft email to my colleagues about how I was using large language models in my work to achieve some fairly large efficiency gains. I realised that this was easier to send as a blog post. And why not share more broadly?\nI am a long way from the frontier in how I am using these tools. If you want to see the frontier, go read Ethan Mollick among others. But I would estimate I’m using these tools more than 95% of academics, so hopefully there is something in there for many of you."
  },
  {
    "objectID": "posts/using-large-language-models-as-academic.html#my-toolkit",
    "href": "posts/using-large-language-models-as-academic.html#my-toolkit",
    "title": "Using large language models as an academic",
    "section": "My toolkit",
    "text": "My toolkit\nAs an academic, I gain free access to Github Copilot. (It’s only $10 a month otherwise.) CoPilot is badged as “your coding companion”, providing code suggestions in coding environments. But while badged this way, CoPilot also provides text suggestions more broadly, and that is how I largely use it.\nCoPilot gives me access to whatever language models Microsoft is running behind it. I’m not sure if GPT-4 is available through CoPilot yet, but I would say performance on CoPilot has improved over the six months I have been using it.\nTo access CoPilot, I write markdown documents in Visual Studio Code. Since I started blogging (12 years now!) I have always written in markdown (and at the moment a particular flavour of markdown, Quarto). Before the emergence of CoPilot, I also used RStudio and other text editors that support markdown. But with CoPilot not available in RStudio, I’m almost exclusively in Visual Studio Code now. For those who want more specific guidance on how to set this up, I’ve added that at the bottom of this post.\nI also work with ChatGPT open in a browser. If CoPilot isn’t giving me what I want, ChatGPT often will. If I can frame my requirement as a standalone question, ChatGPT tends to do a better job. But when working on a document, CoPilot’s suggestions are normally better (unless I paste the whole document into ChatGPT).\nI’ve tried Bing, but find it harder work to get what I want unless I have a question for which I want references."
  },
  {
    "objectID": "posts/using-large-language-models-as-academic.html#writing-lecture-notes-efficiency-gain-10-to-30",
    "href": "posts/using-large-language-models-as-academic.html#writing-lecture-notes-efficiency-gain-10-to-30",
    "title": "Using large language models as an academic",
    "section": "Writing lecture notes (efficiency gain: 10 to 30%)",
    "text": "Writing lecture notes (efficiency gain: 10 to 30%)\nI don’t find CoPilot particularly useful if I’m writing something original such as a blog post. In writing this post I wouldn’t say CoPilot has increased my efficiency at all (outside of helping me create the examples). I sometimes accepted suggestions from CoPilot, but the need to filter through the various suggestions resulted in negligible efficiency gain.\nBut for lecture notes, which are more mechanical, there are some clear gains. The notes for my undergraduate behavioural economics subject (which you can access here) were produced with CoPilot.\nHere are a few instances where CoPilot has been useful.\nOften the notes include a statement of a concept followed by a step-by-step example or illustration. The gain comes when you’re writing those steps.\nFor example, in a section about decision making under risk, I might write:\n\nBill is a risk-averse agent with utility function u(x)=\\text{ln}(x) and wealth of $100. He is offered a 50:50 bet to win $100, lose $50.\nWill Bill accept the bet?\n\nCoPilot will sometimes write something like:\n\nCan someone please help me answer the following question: “Will Bill accept the bet?”\n\nThere must be a lot of study cheat sites forming part of the training data.\nBut if CoPilot does that, simply write the first couple of words of the answer “First, we calculate…” and then CoPilot is away.\n\nFirst, we calculate the expected utility of each option. The expected utility of the bet is:\n\n```{=tex}\n\\begin{aligned}\nEU(\\text{bet}) &= 0.5 \\times u(100) + 0.5 \\times u(-50) \\\\\n&= 0.5 \\times \\text{ln}(100) + 0.5 \\times \\text{ln}(50) \\\\\n&= 4.259\n\\end{aligned}\n```\n\nThe expected utility of not taking the bet is:\n\n\n```{=tex}\n\\begin{aligned}\nEU(\\text{no bet}) &= U(100) \\\\\n&= \\text{ln}(100) \\\\\n&= 4.605\n\\end{aligned}\n```\n\nAs the expected utility of the bet is less than the expected utility of not taking the bet, Bill will not accept the bet.\n\nNot bad for a few seconds of work. For those unfamiliar with the mathematical notation, this is \\LaTeX, which renders into nice equations like this.\n\\begin{aligned}\nEU(\\text{bet}) &= 0.5 \\times u(100) + 0.5 \\times u(-50) \\\\\n&= 0.5 \\times \\text{ln}(100) + 0.5 \\times \\text{ln}(50) \\\\\n&= 4.259\n\\end{aligned}\nI might beef this up with a better explanation, or paste the question into ChatGPT direct, where I tend to get more detailed answers. GPT-4 also tends to be better at math. (CoPilot has done OK here - the logs are correct! Maybe more evidence of improvement in CoPilot, although the answer still needs tweaking.)\nCoPilot is also great when I’m doing repetitive tasks such as describing the elements of an equation or diagram. Start describing the first element and it might give you the rest. And if you write a point followed by “Conversely, …”, CoPilot is often on the money."
  },
  {
    "objectID": "posts/using-large-language-models-as-academic.html#coding-efficiency-gain-10x",
    "href": "posts/using-large-language-models-as-academic.html#coding-efficiency-gain-10x",
    "title": "Using large language models as an academic",
    "section": "Coding (efficiency gain: 10x)",
    "text": "Coding (efficiency gain: 10x)\nDespite spending a lot of time in R, I am a crap coder. The simplest errors have me crawling Stack Overflow for hours. I struggle to build a structure when looking at a blank screen.\nMy typical approach now is to write a comment, let CoPilot do the first cut and then tweak the code until it is in shape.\nThat tweaking isn’t manual tweaking either.\nIf there is an error or problem that I can’t resolve, I’ll simply write:\n\nThe above code generates the error “PASTE ERROR”. I fix it by …\n\nAnd a solution appears. Sometimes I’ll also paste the code and the error into ChatGPT and ask for code that does not have that error.\nIf the code looks overly complicated, a request to simplify often yields good results. (I’ve been asking ChatGPT to review some of my old code recently. I’m a bit embarrassed at how much more efficiently I could have written it.)\nHere are two recent coding applications that I was particularly excited about.\n\nCreating diagrams\nIn my lecture notes, I use ggplot to produce nice-looking graphs. (I’m slowly replacing old PowerPoint diagrams that still litter the notes).\nRecently I wanted to generate a chart showing a probability weighting function from prospect theory.\nI created an R code block and typed a comment:\n\n#Plot of probability weighting function using ggplot2\n\nHere’s the result with the diagram rendered below:\n\n#Plot of probability weighting function using ggplot2\nlibrary(ggplot2)\n\n#Define probability weighting function\nprob_weight &lt;- function(p, alpha){\n  exp(-(-log(p))^alpha)\n}\n\n#Create data frame of probabilities and weights\nprob &lt;- seq(0, 1, 0.001)\nprob_df &lt;- data.frame(prob = prob, weight = prob_weight(prob, 0.6))\n\n#Plot\nggplot(prob_df, aes(x = prob, y = weight)) +\n  geom_line() +\n\n  #Add labels\n  labs(x = \"Probability\", y = \"Weight\")\n\n\n\n\n\n\n\n\nExactly what I wanted. When I implemented this in the notes, all I did was tweaked the style and added a 45-degree line.\nCoPilot’s offering uses Prelec’s (1998) probability weighting function. Whether it picked that up from earlier text or just gave the most common function for probability weighting, I don’t know, but it’s what I would have used if doing it manually.\nCoPilot didn’t offer this full chunk of code at once. Each comment was offered, then the piece of code after it, one after the other. But the only work I did was writing the first comment and pressing tab several times as each succeeding comment or chunk of code was suggested. (CoPilot has certainly increased the level of comments in my code too.)\nOne thing I have noted is that CoPilot uses all of the text in the document in giving it’s suggestions. In writing this post I wasn’t able to replicate what I did when writing the lecture notes. I got better results in the lecture notes themselves. A few paragraphs about probability weighting before I ask for the code generates much better results than asking for the chart straight up.\n\n\nConverting code across languages\nMy second example relates to a proposed reproduction of the results in Berkeley Dietvorst’s Algorithm aversion: People erroneously avoid algorithms after seeing them err. Data and the code for analysis was available on ResearchBox.\nThe problem was that the code was in Stata, I don’t have Stata, and I have no desire to learn Stata. So, I pasted the code into ChatGPT and asked for a conversion to R. What I received was 90% of the way there. Initially there was an error, but ChatGPT solved the error first shot. I then had to tweak a couple of the variable manipulations and specify that some of the t-tests were paired t-tests, and that was it. In less than 10 minutes I had code that was reproducing exactly the results from Study 1."
  },
  {
    "objectID": "posts/using-large-language-models-as-academic.html#generating-quiz-questions-efficiency-gain-3x",
    "href": "posts/using-large-language-models-as-academic.html#generating-quiz-questions-efficiency-gain-3x",
    "title": "Using large language models as an academic",
    "section": "Generating quiz questions (efficiency gain: 3x)",
    "text": "Generating quiz questions (efficiency gain: 3x)\nOne of the best ways to learn is to be tested. As a result, I offer students in my undergraduate subject a series of practice quizzes that they can work through. In the first class I tell the students about the effectiveness of spaced repetition as a learning technique. They can then use the practice quiz questions on an intermittent basis to test whether they have learnt core concepts. (When I find the bandwidth, I plan to implement those quiz questions in Orbit or something similar, and place them through the online notes.)\nSome questions are easy to generate: definitions and the like. But it’s hard work to generate questions in bulk and I struggle to generate plausible-sounding but incorrect answers to multiple-choice questions.\nI now ask ChatGPT to generate them. In the prompts, I vary in the specificity of the questions. “Give me 20 multiple choice questions testing the concept of loss aversion.” “Give me 20 multiple choice questions testing prospect theory.” And so on. Out of each batch of 20, only a few will be suitable. There might be no correct answer or two correct answers, or ChatGPT might confuse concepts. But by tweaking my instruction by, say, describing the level of the students (undergraduate) or more explicitly defining the concept, it doesn’t take long to get 10 or so good questions.\nI haven’t used ChatGPT to generate assessable quiz questions yet, but I’m planning to use it for the upcoming final exam. That exam is a closed-book AI-invigilated exam, so is less vulnerable to someone simply feeding the questions back to ChatGPT. One idea I’m tempted to try is to feed it some previous year’s exams and ask for new exams on the same concepts."
  },
  {
    "objectID": "posts/using-large-language-models-as-academic.html#writing-organisational-fluff-efficiency-gain-2x",
    "href": "posts/using-large-language-models-as-academic.html#writing-organisational-fluff-efficiency-gain-2x",
    "title": "Using large language models as an academic",
    "section": "Writing organisational fluff (efficiency gain: 2x)",
    "text": "Writing organisational fluff (efficiency gain: 2x)\nWork in any decent-sized modern organisation and you will have to write some level of fluff to satisfy the higher-ups, clients, government requirements and the like.\nI am hopeless at those tasks. I have to invest heavily to make fluff sound decent.\nMy approach to these exercises depends on the degree of pointlessness.\nIf the task relates to a process that will have zero impact on what anyone will do, I simply give the task to ChatGPT, let it do the first cut and then tweak as required. One or two sentences of guidance often gets you 80% of the way there.\nIf I think there is a positive benefit to the task, or it’s for public consumption, I’ll be more proactive first up. I will write a rough draft first, not caring much about the writing but making sure the concepts I want to include are there. It might be in dot points. Then I’ll ask ChatGPT for a version that is “clearer”, “simpler” or “better written” or “for a ten-year-old”. I often find this process works best in two stages. The “clearer” version often uses the same words as me, but structures them better. Then I ask ChatGPT to write it again, but instructing it to “forget about the original text.” I’ve been pleasantly surprised at how good some of those second versions are."
  },
  {
    "objectID": "posts/using-large-language-models-as-academic.html#slide-filler-efficiency-gain-2x",
    "href": "posts/using-large-language-models-as-academic.html#slide-filler-efficiency-gain-2x",
    "title": "Using large language models as an academic",
    "section": "Slide filler (efficiency gain 2x)",
    "text": "Slide filler (efficiency gain 2x)\nI’m not a big fan of pointless eye candy in slide decks. But if you’re doing pre-recorded material (see below) and you don’t want your mug on the screen, you sometimes need some filler. In that case, I head straight to DALL-E. I’ve adopted a theme for my lecture slides - black and white line drawings - so I ask for a black and white line drawing of something related to what I am talking about.\nThis is much quicker than hunting for images with open usage rights.\nCompared to ChatGPT, DALL-E seems pretty lame. It still struggles with concepts such as “on” or “ten” or “without writing”. Sometimes I’ll give up on a more complex concept (a deck of cards on a table) and go for something simpler (a deck of cards). I’m looking forward to GPT-4 sitting behind an image generator.\nI’ve also tried Stable Diffusion but found it was much harder work to get something I can use."
  },
  {
    "objectID": "posts/using-large-language-models-as-academic.html#whats-next",
    "href": "posts/using-large-language-models-as-academic.html#whats-next",
    "title": "Using large language models as an academic",
    "section": "What’s next?",
    "text": "What’s next?\n\nAI-voiced lectures\nMy next frontier is artificial voice generation for my pre-recorded lectures.\nLast year - my first year taking the undergraduate behavioural economics subject - the subject was structured as two hours of live (but online) lectures and one hour of tutorials (either online or in-person). Both lectures and the online tutorial were recorded, so students could watch at another time. The net result was limited attendance - and most students didn’t watch the videos either.\nThis year I thought I’d move the subject out of the 1950s and make it a combination of some short pre-recorded videos, interactive online seminars and tutorials. This approach has backfired - the students don’t watch the videos and don’t want to interact - but that’s a story for another post, perhaps in combination with my review of Bryan Caplan’s The Case Against Education.\nTo produce the videos I’ve written scripts, which also form the subject notes. I read that accompanied by slides. I’m a hopeless off-the-cuff speaker, so for anything that’s going to be recorded and treated as a long-term asset, I’m scripting.\nThere are three pain points in this exercise. First, I speak too fast (even when I think I’m speaking slow) and I enunciate many words poorly. Second, recording takes a lot of time. I rarely get a two to five-minute video in a single take. Third, if I want to tweak the recording, I need to either re-record solid chunks of text or fiddle around with video edits.\nSo, why not get an artificial voice to do the speaking?\nI’ve been experimenting with Murf.ai, Speechify, play.ht and Eleven. Eleven has comfortably the most realistic voices but doesn’t yet appear to have an easy way to create video using slides. If they introduce that, I’m fully on board. Murf.ai allows me to pair slides and text to create videos, but the voices still sound a bit robotic. But with the ability to pace their speech, Murf.ai is already a better option than me. And they’ll only get better.\nThe production process is then easy. Upload the text (although I do have to write out the equations in the course notes in text). Upload the slides and match to the text. Click play and the new video is generated.\nWith a setup such as this, if I want to tweak parts of the videos, I can simply edit the written text and generate a new version. Adding or editing slides is also simple as they are uploaded and matched to the new text. I’m also able to run much sharper slide changes as I can time exactly the transitions.\nAn interesting question about the artificial voice-over is how the university (and students) will take it. Do they see it as a lazy option? (When I taught at the University of Sydney in 2020 during the early stages of the pandemic, you weren’t allowed to use pre-recorded videos from one semester in a later semester. Someone hated the idea that the academics might not be working hard enough …) If I were able to clone my voice, this could become a mute issue. play.ht and Eleven have the ability to “clone” your voice, but neither can handle non-American accents. The rhythm of my cloned voice (maybe my natural rhythm) also seemed a bit clunky.\nI think the artificial voice-over is a great option. Academics should be investing in developing high-quality assets that can be re-used rather than bumbling along with low-quality off-the-cuff lectures. They can then invest their time in the other parts of teaching: the in-person seminars and tutorials where students can get the personalised bit of their education.\nThis approach does point to a challenge for higher education providers. Once you’ve got high-quality recorded material for basic courses, why would you want them generated by each lecturer in each university? Someone could produce an amazing Behavioural Economics 101 (I want the Dave Malan CS50 version of this), other lecturers can subscribe to the service and they can now focus on those human bits (not that most students want that…).\nI’ve also started looking at options to create an AI avatar - either based on videos/photos of me or a random AI avatar - for some parts of the videos. I’m leaving that one for the moment but can see myself revisiting in the next year.\n\n\nChat integrated with CoPilot (and other Microsoft tools)\nAt the moment I have multiple streams of access to these tools. I’m looking forward to chat appearing in Copilot - I’m on the waiting list to access - which will remove the need for the separate browser with ChatGPT.\nGPT-4 is also coming to other Microsoft tools soon. If I don’t have to exit PowerPoint to get my images, that’s an extra efficiency.\nI’m largely in the Mac ecosystem, but Apple seems absolutely crap in the world of assistants/chat. (“Hey Siri, tell me this most basic fact about the world.” “I’ve sent some irrelevant web results to your iPhone…..”) If at some point these tools become tied to Microsoft hardware or Windows, I’m moving.\n(I exited the Google ecosystem when I had trouble sharing a file from Google Drive because it breached the terms of service. It was a document that included the words “vaccination” and “scepticism”, although if you see the resulting post, you can see the Google algorithm was pretty crude. At that point I opted for what I hope is a bit more privacy and jettisoned Google…)"
  },
  {
    "objectID": "posts/using-large-language-models-as-academic.html#getting-going-with-github-copilot",
    "href": "posts/using-large-language-models-as-academic.html#getting-going-with-github-copilot",
    "title": "Using large language models as an academic",
    "section": "Getting going with Github Copilot",
    "text": "Getting going with Github Copilot\nThe below gives the basic steps to achieve my setup.\nIf you are an academic, you can sign up for (free) academic access to CoPilot at this link. If you’re not an academic or student, sign up for $10 a month as described here. You’ll need a Github account to do this. I think the $10 a month is worth it, but the free ChatGPT will give you most of these gains with a bit more work.\nVisual Studio Code is available here. (I use the Mac version.) Download and install.\nInstall the Github CoPilot extension into Visual Studio Code (or the Github CoPilot Nightly extension if you want access to features earlier). Instructions on installing the extension are here. On installation you’ll be prompted to login to GitHub to gain access to authorise CoPilot.\nOnce you have done that, using CoPilot is easy. You simply type. As you type, you will be given suggestions.\nI write in markdown (or as noted above, a flavour of markdown called Quarto). It allows me to include \\LaTeX math and R computations within any document (as I have in this post). If you’re an academic stuck in the \\LaTeX ecosystem, you can also write in Visual Studio Code wholly in \\LaTeX. (Although I say jettison that dinosaur and use \\LaTeX math in a markdown document - that ability to add computations is worth it, and the readability and experience so much easier.) You can also just wait until GPT-4 appears in Word. I expect that’s not too far away."
  },
  {
    "objectID": "posts/why-i-dont-believe-that-signs-with-fatality-numbers-cause-more-crashes.html",
    "href": "posts/why-i-dont-believe-that-signs-with-fatality-numbers-cause-more-crashes.html",
    "title": "Why I don’t believe that signs with fatality numbers cause more crashes",
    "section": "",
    "text": "In a paper in Science, Jonathan Hall and Joshua Madsen proposed that dynamic signs that reported Texas road fatalities - “1669 deaths this year on Texas roads” - caused more accidents and fatalities.\nRecently, a friend mentioned the paper to me. I replied that I had seen the paper but didn’t believe the result. I wasn’t going to ignore the finding, but I would update my beliefs in only the smallest of ways.\nHe asked how I’d come to that conclusion. In answering I stated that I had spent half an hour with the paper and supplementary materials over breakfast, clicking through to some of the paper’s references. He then questioned whether I had been too hasty.\nI come to conclusions like this all the time. I am not going to read in detail 95% of the papers that might be of interest to me. I can’t. But I’m of the view that a quick peruse - and sometimes even a one sentence description of the paper - are a basis to start considering whether there it is a result that should lead me to update my world view.\nSo, I decided to write down the thought process that led to my conclusion. Years ago when I first started consuming scientific literature I was too gullible, often accepting results as presented. Now I worry that I have become the cynic who shouts “replication crisis” or “garden of forking paths” every time I see a new paper, once again failing to distinguish the good from the bad. Writing this post served as a chance to reflect on whether I had gone too far.\nBelow are my observations based on the initial half-hour peruse of the paper, the supplementary materials and some of the references, and another hour or so of contemplation of the results while I walked. I wrote the first draft of this post before going back to the paper. That took a couple of hours. I then revisited the paper and references to confirm my memory (no errors, but that’s not always my experience in cases like this), grab some specific details, and pull links, quotes and images for this post.\nOne day I might read the paper and references carefully to test whether I should update any of the below, although that is unlikely to change my conclusion materially as my arguments are not that there is anything wrong with the paper. Rather, my observations are based on the framework in which this paper was completed, the proposed mechanism and the broader literature.\nOn that point, this post should not be read as a critique of the paper. I haven’t put in the work required to do that properly. And if it was me developing this research, I would have done a worse job.\nBelow I briefly present the paper’s results. My following points then have two themes: the first being about how much faith we should put in empirical work of this nature, the second relating to the specifics of the paper."
  },
  {
    "objectID": "posts/why-i-dont-believe-that-signs-with-fatality-numbers-cause-more-crashes.html#paper-results",
    "href": "posts/why-i-dont-believe-that-signs-with-fatality-numbers-cause-more-crashes.html#paper-results",
    "title": "Why I don’t believe that signs with fatality numbers cause more crashes",
    "section": "Paper results",
    "text": "Paper results\nThe paper concerns dynamic road safety signs on Texas roads. The signs can be programmed to show different messages, be they messages about the condition of the road ahead (e.g. the presence of an accident) or generic safety messages. Sometimes those messages would be about fatality counts. The presence of safety messages with fatality counts at some times but not others is what enables the experiment.\nThe abstract reads as follows:\n\nAlthough behavioral interventions are designed to seize attention, little consideration has been given to the costs of doing so. We estimated these costs in the context of a safety campaign that, to encourage safe driving, displays traffic fatality counts on highway dynamic message signs for 1 week each month. We found that crashes increase statewide during campaign weeks, which is inconsistent with any benefits. Furthermore, these effects do not persist beyond campaign weeks. Our results show that behavioral interventions, particularly negatively framed ones, can be too salient, crowding out more important considerations and causing interventions to backfire — with costly consequences.\n\nThe paper contains several charts capturing the effect over the year, across years and by distance. The following shows the accident rate over the following 10 kilometres.\n\nThe Research Summary summarised the results as follows, including the following infographic:\n\nWe used instrumental variables to recover the effect of displaying a fatality message and document a significant 4.5% increase in the number of crashes over 10 km. The effect of displaying fatality messages is comparable to raising the speed limit by 3 to 5 miles per hour or reducing the number of highway troopers by 6 to 14%. … The social costs of these fatality messages are large: Back-of-the-envelope calculations suggest that this campaign causes an additional 2600 crashes and 16 fatalities per year in Texas alone, with a social cost of $377 million per year."
  },
  {
    "objectID": "posts/why-i-dont-believe-that-signs-with-fatality-numbers-cause-more-crashes.html#the-meta-view",
    "href": "posts/why-i-dont-believe-that-signs-with-fatality-numbers-cause-more-crashes.html#the-meta-view",
    "title": "Why I don’t believe that signs with fatality numbers cause more crashes",
    "section": "The meta-view",
    "text": "The meta-view\nMy starting point is my perspective on research of this kind. There are tens of thousands of academics (maybe an order of magnitude more?) coming up with ideas for analysis of observational data. The key to getting published in a “good journal” is to find some clever natural experiment or instrument by which to identify an interesting causal effect.\nTo get a sense of the scale of this endeavour, you only need look at what happens when a particularly prospective situation for a natural experiment occurs. Heath and friends found papers testing 114 outcomes using variation in business combination laws, by which different U.S. states have sought to constrain takeover activity. Similarly, they found over 80 papers testing 434 outcomes using the Regulation SHO pilot, a randomised controlled trial conducting by the U.S. Securities and Exchange Commission.\nThis is only the tip of the iceberg. Beyond the 80 public papers involving the Regulation SHO pilot, how many other academics have access the data, had a play or run some analysis? What is the total universe of analyses that has been run? I expect it an order of magnitude more than the 80 public papers.\nAnd that hints at the problem. The statistical framework that typically underpins the reporting of results loses its validity if we don’t see all the analyses. Researchers look for results that pass the test for “significance. Results are declared”significant” if a statistical test shows that data as extreme as that observed has only a 5% probability or less of occurring if the hypotheses that there is no effect is true. If we see only a subset of the results, then we cannot say that we were unlikely to see those results if the hypothesis of no effect were true.\nThis isn’t just a problem when people all look at the same data as for the Regulation SHO pilot. If many people run many studies on many different datasets and we only see a subset of the results, we still have a publication bias problem. And this is ultimately reflected in the evidence of publication bias in empirical work of this nature (for example).\nThis paper on fatality messages causing road accidents was published in Science. I doubt it would have been published in Science if the authors hadn’t found an effect. I would give over 95% probability that I would not have heard of this paper if the result was simply no effect. There are other papers on these safety messages, so it may have been published somewhere, but not somewhere of the profile of Science.\nThis point alone is responsible for a large discount in how much I should believe the result in the paper is a true effect. The same problem contributed to the replication crisis across the social sciences. We don’t see a replication crisis with this empirical work on observational data as it’s not easy - and often impossible - to replicate it. But that doesn’t make the problem any smaller.\nUltimately, it’s hard to absolve an individual piece of research from this problem. It doesn’t matter how many robustness tests are conducted. Rather, you need to move from a perspective of looking at papers to looking at literatures, and then looking at those literatures with a an understanding that the literature is not representative of the full body of research (see Slate Star Codex for a great example of a literature contaminated with dodgy results involving 5-HTTLPR). And as I will discuss further below, I don’t yet see that supporting literature.\nOne potential mitigation arises before the study. If you have an idea and a possible dataset to test it, pre-register it. Then we start to get a sense on the universe of analyses that are out there but we never see. But pre-registration is tough with observational data and rarely done. (One recent review found only one example in economics.) You don’t know what form those data are going to be in and what problems you will find. (There will be problems.) Unless you are a domain expert with previous experience with data of that kind, you will likely have to iterate your hypotheses and analysis. But that doesn’t mean a first cut plan is of no use.\nAlternatively, for this particular empirical phenomena, you could run a randomised controlled trial whereby the fatality count messages are shown at random times across random signs - if you could get a traffic authority to agree to the trial. That should also be pre-registered.\nI find discounting such a broad swathe of empirical results in this way somewhat depressing. But I’m as comfortable doing that as I am discounting experimental behavioural science papers built on weak theory, small samples and no pre-registration."
  },
  {
    "objectID": "posts/why-i-dont-believe-that-signs-with-fatality-numbers-cause-more-crashes.html#the-garden-of-forking-paths",
    "href": "posts/why-i-dont-believe-that-signs-with-fatality-numbers-cause-more-crashes.html#the-garden-of-forking-paths",
    "title": "Why I don’t believe that signs with fatality numbers cause more crashes",
    "section": "The garden of forking paths",
    "text": "The garden of forking paths\nThat previous point applies across studies. But within studies, the many degrees of freedom that typically exist within the analysis can also undermine the validity of the statistical approach.\nIf the authors try many different analytical approaches and we don’t see them all, this introduces a problem similar to the publication bias highlighted above. Then researchers might also try only one analytical approach, but to the extent this is chosen with an eye to getting the desired result (consciously or not), we again have a problem.\nThis point isn’t about deliberate misconduct. Researchers might see their work as iterating toward an analytical approach or choosing a path of analysis that makes sense. But the fact the chosen methodology is not random or representative of the full universe of possible analyses, the assumptions underpinning the statistical analysis no longer hold.\nIt’s not hard to see the many degrees of freedom in this particular paper. The analysis comparing crashes in weeks where the message was displayed to weeks when it wasn’t looked at accidents within the following buckets: 0km-1km, 1km-4km, 4km-7km and 7km-10km. Why were these buckets chosen? How to they compare to a series of 2km wide buckets? If there was nothing in the 7km-10km bucket, would it have been included in the paper? Were accidents beyond 10km analysed and we’re not seeing them because there is nothing there? Would the 0km-1km bucket have been combined with the next if it showed no effect? Similarly, for the analysis that introduced a series of controls, why were 3km, 5km and 10km chosen?\nThen there is the analytical strategy. There are many possibilities here. The main paper contains both a strategy comparing across weeks and one with many controls. The comparison across weeks has a version in the supplementary material with more ambiguous results. Which would have been used in the main paper if the latter showed a larger effect? (More on that analysis in the supplementary material below.)\nThere are plenty of other points in the analysis where you can see the forking paths. We are only seeing a small subset of the analytical approaches that are possible. Many tests for robustness are shown, but this is also only a subset - and again, why that subset? What other analyses could have been run?\nOne point that makes this argument salient for me is that the p-values for the major analyses, although mostly below 0.05, are above 0.01. It’s tended to be experiments with p-values in this sort of range that have fared particularly poorly in the replication crisis across psychology and other social sciences.\nAs for publication bias, one before the fact solution to this garden of forking paths is a pre-analysis plan. But there is also a chance here to get a sense of the garden of forking paths after the fact. The data are publicly available. Over time, other researchers might dig into these data and try other specifications. A multiverse analysis might be one approach. Maybe we’ll then get a sense of the effect of all those forking paths. (My instinct is that even with a multiverse analysis something would still be there.)\n\nAlternative analyses\nHere’s one additional point on the multiverse of possible analyses.\nA challenge with this paper was finding stretches of road with which to compare those stretches downstream of the signs.\nOne way to do this, in what the authors call the univariate analysis, is to compare the same stretch of road at times with or without the safety message. However, this leads to a potential confound as there may be variation between those times for reasons other than the message on the sign.\nAn alternative methodology would be to compare stretches of road immediately before and after the sign. The authors do this and report the result as follows:\n\nAll but one of the downstream estimates is &gt;0, with a 1.7% increase over the (7–10] km downstream of DMSs (P = 0.018).\n\nThat “but one” with a decrease is the 0km-1km range, which should be range in which we are most likely to see a positive effect. This sentence describing the result is storytelling about noise in a way to imply it supports the hypothesis. This particular analysis is admittedly underpowered, but would the authors have included it in the main paper if it had made it across the significance threshold?"
  },
  {
    "objectID": "posts/why-i-dont-believe-that-signs-with-fatality-numbers-cause-more-crashes.html#the-effect-size",
    "href": "posts/why-i-dont-believe-that-signs-with-fatality-numbers-cause-more-crashes.html#the-effect-size",
    "title": "Why I don’t believe that signs with fatality numbers cause more crashes",
    "section": "The effect size",
    "text": "The effect size\nOne effective filter for junk results is the “effect size is too large” heuristic. Is the effect size reasonable given the intervention?\nAlthough I spent only half an hour with the paper, I spent much more time with this question of effect size stirring in my mind.\nThe headline number presented in the Research Summary is a 4.5% increase in the number of crashes over the 10km after a fatality message. My first instinct is to judge this effect size too large for such a small change in environment. The authors note this is equivalent to “raising the speed limit by 3 to 5 miles per hour or reducing the number of highway troopers by 6 to 14%”. In the Research Summary, they also write: “Back-of-the-envelope calculations suggest that this campaign causes an additional 2600 crashes and 16 fatalities per year in Texas alone, with a social cost of $377 million per year.”\nThis isn’t simply a claim that roadside messages are distracting. The argument is that this particular message about fatalities is more distracting or attention grabbing than the other messages on those signs. If those other messages also distract drivers and cause accidents, then the required effect of the fatality messages increases accordingly.\nYou could think of several ways the proposed effect works. All drivers see the number of fatalities and are distracted. This general distraction results a 4.5% higher crash probability across all drivers. Or half see the fatality message and there is a 9% increase for that cohort. Or a particularly high risk cohort are most likely to see the sign and be distracted. And so on. How many drivers see or read the sign? For those that read it, it must be truly distracting (and far more distracting than other safety messages).\nThere is also a general tendency for estimated effect sizes to be biased upward. If the effect size is small and the data noisy, any statistically significant result will exaggerate the effect size. If the discovered effect size wasn’t that big, the analysis wouldn’t have found it. Gelman and Carlin call this a”Type M error”. Even if there is a real negative effect of these fatality messages, I expect the point estimate is substantially biased upward.\nUltimately, I wouldn’t rule this effect size out in the way I do with the “hungry judges”. But I still think that, to the extent the effect exists, it is much smaller than the the estimated effect size."
  },
  {
    "objectID": "posts/why-i-dont-believe-that-signs-with-fatality-numbers-cause-more-crashes.html#the-mechanism",
    "href": "posts/why-i-dont-believe-that-signs-with-fatality-numbers-cause-more-crashes.html#the-mechanism",
    "title": "Why I don’t believe that signs with fatality numbers cause more crashes",
    "section": "The mechanism",
    "text": "The mechanism\nAs noted above, the claim in the paper is not that billboards, signs or safety messages are distracting. There is plenty of evidence that they can be. The claim is that safety messages with the number of deaths are more distracting than the other messages and that this causes an increase in accidents over the following several kilometres. The authors write:\n\nA large body of research documents that attention and working memory are scarce resources, and that distractions create extraneous cognitive load that hampers individuals’ ability to process new information. Examples include longer response times, more mistakes, and failure to process available information. Fatality messages plausibly add greater cognitive load than a typical DMS message because they are designed to be more salient than the typical message and (intentionally) communicate that driving can be deadly (i.e., a negatively framed message). Relative to other messages, fatality messages may thus add more to drivers’ cognitive loads by inducing anxiety about death. Psychologists have documented that high levels of anxiety or arousal can worsen performance on a variety of tasks by causing individuals to focus on the risk rather than on the task and causing some to overthink their actions, overriding faster automatic responses.\n\nLet me break down the argument as follows:\n\nDistractions utilise a scarce resource\nThe fatality messages distract drivers\nThe distraction caused by the fatality messages creates more cognitive load than other messages, leading to anxiety and arousal\nAnxiety and arousal can hamper performance\nThe anxiety and arousal arising from the fatality messages affects driving performance over the next several kilometres\n\nThere is good evidence to support points 1 and 4. These general claims are fair and well supported by the references. I’m also willing to to take 2 as true. To the extent someone reads the message, that is a distraction.\nBut when we go from general claims about distraction and arousal to the nature of the effect of the fatality messages themselves - points 3 and 5 - the evidence becomes weaker. For example, there is only one paper in the references of that section quoted above that directly related to driving. That paper by Strayer and friends involved concurrent distractions (e.g. radio on, talking on phone). This is in contrast to the effect of the fatality messages, which is not argued to be due to immediate distraction, but rather via continued distraction in the minds of drivers further down the road.\nOne kilometre is 36 seconds at 100km per hour or a minute at 60km per hour. Five kilometres takes between 3 and 5 minutes, and possibly longer if traffic is slow. That is a markedly different phenomenon from a concurrent distraction. If I had to estimate how long this message would stick in most people’s minds, I’d say a few seconds, if it registers at all.\nLater in the paper there are other references on distractions and driving. (The following summary is based on a quick glance, probably less than one minute with each paper. I might have missed something. If I revisit this paper, this is the first place I would dig deeper.) Four references (1, 2, 3, 4) relating to signs and billboards were about visual distraction, not ongoing cognitive load. Another referenced paper examines how long it takes to resume a task. The following figure gives a sense of the scale of the delay; the time on the vertical axis is in milliseconds.\n\nThat leaves one paper to support the argument of an effect of any substantial duration. A study in Finland found that a sign warning of slippery road conditions reduced the mean speed by 1–2 km/h over the following 360-1100m. A sign recommending an 80m distance between cars reduced the number of cars with a short “headway” between them. This Finnish study doesn’t have a great experimental design for ruling out confounds, as it’s a simple before and after study at three sites. But I’m also not convinced that we can extend a finding about behaviour in response to a warning of a slippery road to a claim about the duration of distracting cognitive load coming from a fatality count sign. The warning of a slippery road has a more obvious response.\nUltimately, there is little in the referenced material that supports the argument that these fatality messages are particularly distracting with an effect that lasts some minutes, except for the effect reported in this paper itself.\nThe authors point to a few specifics of the empirical evidence that might support their argument. One is that more accidents occur later in the year when the number of deaths for the year to date is larger. A bigger number leads to a bigger effect. But I’m not convinced that people have sufficient sense of scale for the effect to work like this. Are people so scale sensitive that a message about 1000 people in the second quarter of the year has no effect but 2000 in in the third quarter does? If I were to dig through the behavioural literature on how people respond to scale, most would suggest that we are often scale insensitive.\nAnother piece of evidence they cite is that there are more multi-vehicle accidents, with no change in single-vehicle accidents. This feels like after the fact story telling. I could imagine that if the effect was the other way around, with single vehicle accidents more common, they could equally have crafted a story that supported the hypothesis. Absent a pre-analysis plan with this hypothesis, it does not add any weight for me.\nWhen trying to come up with reasons why the effect could be as large as reported, I thought of a few, but nothing that the authors had not already considered. In the supplementary materials, the authors tackle a range of alternative hypotheses as to what might be causing the accidents. Most seem feasible, but the evidence presented against each alternative points to the authors’ preferred hypothesis. For example, the first I thought of was that the messages with the number of fatalities were displacing more important messages, such as a message that there is an accident or hazard on the road ahead. But this was not the case, as traffic authority staff are instructed to prioritise critical messages. It does leave open the possibility that there is an effect due to displacing useful generic safety messages - the paper cited above on the slippery road warning is one example - but that supposes a much stronger effect for those than the evidence seems to support.\nAs a final note, there is one alternative hypothesis that the authors don’t mention: that the effect is just chance."
  },
  {
    "objectID": "posts/why-i-dont-believe-that-signs-with-fatality-numbers-cause-more-crashes.html#updating-my-beliefs",
    "href": "posts/why-i-dont-believe-that-signs-with-fatality-numbers-cause-more-crashes.html#updating-my-beliefs",
    "title": "Why I don’t believe that signs with fatality numbers cause more crashes",
    "section": "Updating my beliefs",
    "text": "Updating my beliefs\nOne unfortunate fact about reading a paper with a claim you have not thought about before is that you can’t unread it. You can no longer determine what your prior belief was as to the probability of the claim being true. What was my belief before reading the paper as to the probability that signs with fatality counts increased crashes? I don’t know.\nIf I had to estimate the probability that a randomised controlled trial involving the use of fatality counts would find that they increased crashes by 4.5% or more, I’d put it at around 3%. If I had to guess my prior belief before reading this paper, I’d say around 1% - but I am of course completely making that up. For an effect size of greater than 1%, I’d put my current belief at around 10%.\nSo, the result of this paper could be seen as a large update - I’ve tripled the probability that an effect of the size found could exist - or a small update of only a few percentage points. That feels about right to me for a single paper in the context I described above. If a consistent literature started to accumulate - particularly via a pre-registered randomised controlled trial - that belief would change fairly quickly."
  },
  {
    "objectID": "posts/why-i-dont-believe-that-signs-with-fatality-numbers-cause-more-crashes.html#some-concluding-thoughts",
    "href": "posts/why-i-dont-believe-that-signs-with-fatality-numbers-cause-more-crashes.html#some-concluding-thoughts",
    "title": "Why I don’t believe that signs with fatality numbers cause more crashes",
    "section": "Some concluding thoughts",
    "text": "Some concluding thoughts\nHaving stated above that you should assess literatures, not individual papers, I also feel that authors should write their conclusions in the context of those literatures rather than taking their paper as providing the sole conclusion.\nTake this dot point from the Research Summary:\n\nDisplaying death counts causes crashes.\n\nWhat they mean is “we found some evidence in one dataset that displaying crash counts may cause crashes.”\nSimilarly, due to the effect persisting across years, they write:\n\nPeople don’t habituate to nudges.\n\nI feel that’s a bit like stating that the average effect size of a nudge is 0.43. Across the huge body of interventions that might be called nudges and the growing literature on habituation to nudges, this blanket statement based on a single study seems to be gross overreach.\nWhat would be a strong claim that could be made from this paper? Given other research on safety messages, I would feel comfortable stating “there is no evidence for a short-term beneficial effect of fatality count messages”. If I was writing in a non-academic publication, I’d even feel comfortable writing “Fatality count messages don’t save lives.” But kill people? That’s a step further than the evidence to date justifies.\nOne final observation is that I dislike the reporting of large effects as point estimates rather than confidence intervals. If you look in the supplementary materials, you can find the information required to calculate that the headline 4.5% increases in crashes has a 95% confidence interval of [0.8%, 8.2%]. That’s more informative that the point estimate. In particular, it gives a sense of the uncertainty under the chosen modelling assumptions."
  },
  {
    "objectID": "reviews.html",
    "href": "reviews.html",
    "title": "Book reviews",
    "section": "",
    "text": "Book reviews\nBelow is a collection of book reviews that I have posted - click the links to go to the full review.\n\nAdam Alter’s Irresistible: Why We Can’t Stop Checking, Scrolling, Clicking and Watching: a well-written and entertaining book about a subject I don’t know much about (addiction), with one glaring slip.\nDan Ariely’s The Honest Truth About Dishonesty: How We Lie to Everyone – Especially Ourselves: Interesting substance as draws on Ariely’s own work, but sometimes meanders. Will the results stand the test of time? [That question has since been answered - not well at all. For example, see here and here.]\nDan Ariely’s Payoff: The Hidden Logic That Shapes Our Motivations: Enjoyable, but not particularly deep read, with most of the results covered in The Upside of Irrationality.\nDan Ariely’s Predictably Irrational: Not a bad book for a sample of behavioural economics if you are new to the area.\nDan Ariely’s The Upside of Irrationality: Some interesting ideas (particularly early in the book) but seems light compared to Predictably Irrational.\nRoy Baumeister and John Tierney’s Willpower: Rediscovery Our Greatest Strength: The book tended into the pop science/self-help genre and there was rarely enough depth to add anything to the current debates.\nEric Beinhocker’s Origin of Wealth: Evolution, Complexity, and the Radical Remaking of Economics: The best readable discussion of the field of “evolutionary economics” and the application of complexity theory to economics.\nShlomo Benartzi (and Jonah Lehrer’s) The Smarter Screen: Surprising Ways to Influence and Improve Online Behaviour: Makes some interesting points and directed me to plenty of interesting material elsewhere. Just don’t bet your house on the parade of results being replicable.\nSam Bowles and Herb Gintis’s A Cooperative Species: Human Reciprocity and Its Evolution: An interesting but unsatisfactory argument for the role of group selection in the emergence of cooperation.\nRobert Boyd and Peter Richerson’s The Origin and Evolution of Cultures: A seminal work on cultural evolution.\nRob Brooks’s Sex, Genes & Rock ‘n’ Roll: How evolution has shaped the modern world: Obesity, population control, infanticide and rock ‘n’ roll from an evolutionary perspective.\nJanet Browne’s Charles Darwin: Voyaging: The first part to the best Charles Darwin biography.\nSusan Cain’s Quiet: The Power of Introverts in a World That Can’t Stop Talking: An important point that many of our environments, social structures and workplaces are unsuited to “introverts”, but falls into a degree of cheer-leading and evidence-free story-telling.\nBryan Caplan’s Selfish Reasons to Have More Kids: Parents can relax as there is not much they can do to change their children. And since they’re easier than you think, why don’t you have more?\nNick Chater’s The Mind is Flat: The Illusion of Mental Depth and the Improvised Mind: A great book arguing that our minds have no hidden depth, with which I have one major fundamental disagreement.\nBrian Christian and Tom Griffiths’s Algorithms to Live By: The Computer Science of Human Decisions: A nice contrast to the competition between perfectly rational decision makers and biased humans. Christian and Griffiths’s decision-making benchmarks are the algorithms developed by mathematicians and computer scientists, with decision making under uncertainty involving trade-offs between efficiency, accuracy and the types of errors you are willing to accept.\nGregory Clark’s The Son Also Rises: Surnames and the History of Social Mobility: Social mobility is low across countries and time because there is a genetic component to social status.\nJohn Coates’s The Hour Between Dog and Wolf: Risk Taking, Gut Feelings and the Biology of Boom and Bust: How our hormones affect decision making in finance - the idea that traders are rational calculating machines driven by their brains is torn apart.\nDavid Colander and Roland Kupers’s Complexity and the Art of Public Policy: An important way to think about policy, even though I’m not convinced by many of their proposed applications.\nPedro Domingos’s The Master Algorithm: How the Quest for the Ultimate Learning Machine Will Remake Our World: Parts verge on techno-Panglossianism. The chapters on the various “tribes” of machine learning and learning without supervision, are excellent. And I simply don’t have the knowledge to judge the value of Domingos’s reports on his progress to the master algorithm.\nBenoit Dubreuil’s Human Evolution and the Origins of Hierarchies: The emergence of modern hierarchies was driven by the cognitive changes associated with the behavioural modernisation of human\nAngela Duckworth’s Grit: The Power of Passion and Perseverance: Conscientiousness rebadged? Interesting but ignores biology and the stories suggest survivorship bias.\nCarol Dweck’s Mindset: Changing the Way You Think to Fulfil Your Potential\nNiall Ferguson’s Civilization: The West and the Rest: Entertaining and some interesting ideas. Those looking for a grand history of everything will be disappointed.\nJames Flynn’s Are We Getting Smarter?: A comprehensive update on the latest in IQ testing from around the globe, plus a few interesting arguments.\nRobert Franks’s Passions Within Reason: The Strategic Role of the Emotions: A fantastic game theoretic approach to the role of the emotions.\nRobert Frank’s Luxury Fever: Weighing the Cost of Excess: People over-consume goods where there is competition for relative rank.\nRobert Frank’s The Darwin Economy: Liberty, Competition, and the Common Good: Many aspects of the economy are Darwinian, not Smithian.\nPaul Frijters with Gigi Foster’s An Economic Theory of Greed, Love, Groups and Networks: An attempt to supplement mainstream economic arguments with a perspective on love, groups and networks. I don’t buy into many of the arguments, but one of the most interesting books I have read.\nFrancis Fukuyama’s The Origins of Political Order: From Prehuman Times to the French Revolution: A grand history with self-interest as a motivating factor of the individual actor.\nArthur Gandolfi, Anna Sachko Gandolfi and David P. Barash’s Economics as an Evolutionary Science: From Utility to Fitness: The title captures the book’s core focus on translating the economic concept of utility into the biological concept of fitness.\nOded Galor’s Unified Growth Theory: A fine summary of the most serious attempt to accommodate human evolution into theories of economic growth. Highly technical and not an easy read.\nSheldon Garon’s Beyond Our Means: Why America Spends While the World Saves: Plenty of interesting historical fodder on savings but feels incomplete.\nGerd Gigerenzer’s Gut Feelings: Short Cuts to Better Decision Making: Perhaps the best balance between nuance and accessibility of Gigerenzer’s popular books. While it still leaves an impression about the accuracy of our instincts that I’m not completely in agreement with, it provides a good overview of how our gut feelings can lead togood decisions.\nGerd Gigerenzer’s Rationality for Mortals: How People Cope with Uncertainty: Great. Good coverage of ecological rationality, heuristics that make us smart, and understanding risk.\nGerd Gigerenzer’s Risk Savvy: Gigerenzer’s least satisfactory but still interesting book.\nGerd Gigerenzer, Peter Todd and the ABC Research Group’s Simple Heuristics That Make Us Smart: Excellent. A nice contrast to the increasing use of complex machine learning algorithms for decision making, although it is that same increasing use that makes some parts of the book are seem a touch dated.\nMalcolm Gladwell’s Outliers: The Story of Success: Like all of Gladwell’s book - flawed but interesting.\nJonathan Gottschall’s The Storytelling Animal: How Stories Make Us Human: Gottschall argues that storytelling is often deeply moral, normally deals with problems of great (evolutionary) relevance to us and is a major cohering force in society. I tend to agree.\nJonathan Haidt’s The Righteous Mind: Why Good People Are Divided by Politics and Religion: A brilliant analysis of why people are divided by politics and religion. Just don’t buy his arguments on group selection.\nDan Hamermesh’s Beauty Pays: Why Attractive People Are More Successful: Beauty has its benefits, but are the beautiful more productive?\nTim Harford’s Adapt: Why Success Always Starts with Failure: Harford applies evolutionary thinking to business, war, accidents and other human pursuits. Excellent.\nJoe Henrich’s The Secret of Our Success: How Culture Is Driving Human Evolution, Domesticating Our Species, and Making Us Smarter: A lot of interesting ideas, but left me with a lot of questions.\nVictor Hwang and Greg Horowitt’s The Rainforest: The Secret to Building the Next Silicon Valley: Entrepreneurship though a biological lens.\nGreg Ip’s Foolproof: Why Safety Can Be Dangerous and How Danger Makes Us Safe: Engineers seek to use the sum of our human knowledge to make us safer. Ecologists recognise that many of our solutions will have unintended consequences that can be worse than the problems we are trying to solve. Much of Ip’s book is a catalogue of the failures of engineering.\nGarett Jones’s Hive Mind: How Your Nation’s IQ Matters So Much More Than Your Own: A fantastic exposition of some important but neglected features of the world.\nDaniel Kahneman’s Thinking, Fast and Slow: Flawed but possibly the best overview of behavioural economics there is. However, it is not standing the test of time particularly well.\nGarry Kasparov’s Deep Thinking: Where Machine Intelligence Ends and Human Creativity Begins: More a history of man versus machine in chess than a deep analysis of human or machine intelligence.\nEric Kaufmann’s Shall the Religious Inherit the Earth?: Demography and Politics in the Twenty-First Century: Even if you assign a low probability to Kaufmann’s projections, it provides another strand to the case that low fertility in the secular West is not without costs.\nJohn Kay’s Other People’s Money: A generally excellent book arguing that the growth in the size of the financial system hasn’t been matched by improvements in the allocation of capital.\nKevin Kelly’s What Technology Wants: Ignoring his near-religious fervour, Kelly provides a strong argument that the growth in technology is primarily beneficial.\nDoug Kenrick and Vlad Griskevicius’s The Rational Animal: How Evolution Made Us Smarter Than We Think: A good introduction to the idea that evolutionary psychology could add a lot of value to behavioural economics, but has the occasional straw man discussion of economics and a heavy reliance on priming research.\nGary Klein’s Sources of Power: How People Make Decisions: An important book describing how many experts make decisions, but with a lingering question mark about how good these decisions actually are.\nJonathan Last’s What to Expect When No One’s Expecting: America’s Coming Demographic Disaster: Much to disagree or argue with, but entertaining and a lot to like.\nDavid Levine’s Is Behavioural Economics Doomed?: A good but slightly frustrating read. Littered with straw man arguments.\nMichael Lewis’s The Undoing Project: A Friendship That Changed The World: A layperson would struggle to find a more accessible and interesting introduction to behavioural science and behavioural economics.\nBenoit Mandelbrot and Richard Hudson’s The (mis)Behaviour of Markets: A Fractal View of Risk, Ruin, and Reward: A clear critique of the underpinnings of modern financial theory.\nJim Manzi’s Uncontrolled: The Surprising Payoff of Trial-and-Error for Business, Politics, and Society: Excellent. The high “causal density” in social science settings nearly always results in the possibility that there is an important factor you have missed or do not understand. No matter what our worldview, we should be prepared to allow experimentation with alternatives, as we may well be wrong.\nJoanna Masel’s Bypass Wall Street: A Biologist’s Guide to the Rat Race: Interesting, although would have benefited from more of a biological lens.\nMichael Mauboussin’s More Than You Know: Finding Financial Wisdom in Unconventional Places: We need an interdisciplinary toolkit to give us the diversity to make good decisions.\nMichael Mauboussin’s Think Twice: Harnessing the Power of Counterintuition: A multi-disciplinary book on how to improve your decision making. Not great depth, but interesting pointers to new ideas.\nGeoffrey Miller’s Spent: Sex, Evolution, and Consumer Behavior: Evolution shaped our consumer preferences but they do not always work perfectly in a modern environment.\nIan Morris’s Why the West Rules - for Now: History is made by lazy, greedy, frightened people (who rarely know what they’re doing) looking for easier, more profitable, and safer ways to do things.\nSendhil Mullainathan and Eldar Shafir’s Scarcity: Why Having Too Little Means So Much: A novel way of looking at scarcity that extends beyond the typical analysis in economics, but I’m not convinced I have been presented with a coherent new perspective on how the world works.\nRichard Nelson and Sidney Winter’s An Evolutionary Theory of Economic Change: The seminal book that established evolutionary economics as a serious field.\nCal Newport’s So Good They Can’t Ignore You: Why Skills Trump Passion in the Quest for Work You Love: Basic (albeit good) economic advice dressed up in self-help style.\nCathy O’Neil’s Weapons of Math Destruction: How Big Data Increases Inequality and Threatens Democracy: O’Neil makes a strong and persuasive case that many algorithms could be developed or used better. But it is often unclear what exactly the problem is or what potential solutions could (should) be.\nPaul Ormerod’s Why Most Things Fail: Failure is all around us. But after building a great picture of the complexity of the world, the applications seem half-baked.\nCharles Perrow’s Normal Accidents: Living with High-Risk Technologies: We should stop trying to fix systems prone to normal accidents in ways that only make them riskier. We should focus instead on reducing the potential for catastrophe when there is failure.\nPaul Rubin’s Darwinian Politics: The Evolutionary Origin of Freedom: Humans naturally seek political freedom and Modern Western societies do the best job of meeting these needs.\nPhil Rosenzweig’s Left Brain, Right Stuff: How Leaders Make Winning Decisions: An entertaining examination of how behavioural economics findings hold up for real world decision making.\nPhil Rosenzweig’s The Halo Effect … and the Eight Other Business Delusions That Deceive Managers: Largely an exercise of shooting fish in a barrel, but is an entertaining read regardless.\nGad Saad’s The Evolutionary Bases of Consumption: This book has more material taking on the Standard Social Science Model approach to consumption than is fun to wade through, but Saad’s book is still the go to source for material on the evolutionary psychology approach to consumption.\nGilles Saint-Paul’s The Tyranny of Utility: Behavioral Social Science and the Rise of Paternalism: Sometimes hard to share Saint-Paul’s anger, but some important underlying points.\nRobert Sapolsky’s Why Zebra’s Don’t Get Ulcers: A wonderful book on the psychology and physiology of stress.\nBarry Schwartz’s The Paradox of Choice: Why More Is Less: An excellent diagnosis with a less than convincing proposal for treatment.\nPaul Seabright’s The War of the Sexes: How Conflict and Cooperation Have Shaped Men and Women from Prehistory to the Present: A decent primer on sexual selection and some interesting applications, but does not always engage with the cutting edge of the debate.\nNate Silver’s The Signal and the Noise: Why So Many Predictions Fail— but Some Don’t: Not a “how to” book, although there are plenty of principles (and suggestions to be humble) worth following.\nRobert Sugden’s The Community of Advantage: A Behavioural Economist’s Defence of the Market: A well-balanced critique on how the behavioural research has been interpreted and used as part of the “nudge” movement to develop recommendations for the “planner”, “benevolent autocrat” or “choice architect”.\nCass Sunstein and Reid Hastie’s Wiser: Getting Beyond Groupthink to Make Groups Smarter: Not an exciting read, but a good catalogue of group decision-making research.\nPhilip Tetlock’s Expert Political Judgment: How Good Is It? How Can We Know?: One of the grander undertakings in social science.\nPhilip Tetlock and Dan Gardner’s Superforecasting: The Art and Science of Prediction: Doesn’t quite measure up to Tetlock’s superb Expert Political Judgment (read EPJ first), but it contains more than enough interesting material to make it worth the read.\nRichard Thaler and Cass Sunstein’s Nudge: If you’re familiar with much of the behavioural science literature, the book covers a lot of stuff you already know. But there are a lot of bureaucrats and politicians who, among others, should read the whole book.\nPeter Thiel’s Zero to One: Notes on Startups, or How to Build the Future: Many interesting and contrary arguments, but this is not a book where they are buttressed with evidence to convince you they are true.\nRobert Trivers’s The Folly of Fools: The Logic of Deceit and Self-Deception in Human Life: Some gems surrounded by political rants.\nThorstein Veblen‘s The Theory of the Leisure Class: The classic on status and conspicuous consumption.\nChris Voss’s (with Tahl Raz) Never Split the Difference: Negotiating as if your life depended on it: Interesting ideas on how to approach negotiation, but I don’t know how much weight to give them. How much expertise could be developed in hostage negotiations? Can that expertise be distilled into principles, or is much of it tacit knowledge?\nE.O. Wilson’s The Social Conquest of Earth: A confusing take on kin and group selection.\nPhilip Zimbardo’s The Lucifer Effect: Understanding How Good People Turn Evil: The situation is more important than a person’s disposition."
  },
  {
    "objectID": "teaching.html",
    "href": "teaching.html",
    "title": "Teaching",
    "section": "",
    "text": "Below are links to course notes for subjects I have taught over the last couple of years.\nAll are works in progress and regularly revised during teaching periods.\nAll are CC-BY, so feel free to take and use with attribution as you see fit.\n\nBehavioural economics: A traditional course on the foundations of behavioural economics. Supporting videos can be found here.\nBehavioural finance\nConsumer financial decision making\nCorporate decision making\nTrial design"
  },
  {
    "objectID": "posts/deception.html",
    "href": "posts/deception.html",
    "title": "The AI that wasn’t: Deception in human-AI experiments",
    "section": "",
    "text": "Experimental economists have a strong norm against deception in experiments. Here’s Andreas Ortmann (2019):\nFor some experimental economics journals, deception is grounds for desk rejection.\nThere are a few reasons for this norm. An important one is that if participants are suspicious that are being deceived in an experiment or that the information they have been provided is not correct, that will alter their behavioural responses, undermining the experimental results. An example of this is the famous Solomon Asch conformity experiments, in which those who believed they were being deceived were less likely to be ().\nThere is also an externality from deception to other experiments. After being deceived in one experiment - as they often discover when promises aren’t delivered or in experimental debriefs - they will be less trusting in future experiments, again changing their behaviour. Today where many experiments are run with public subject pools of people who are completing many experiments and surveys, regular deception could be expected to shift their expectations.\nThere is no such norm in psychology. Deception is common and (argued to be) required for many experiments, although this exposes to them to critiques about whether the observed behaviour can be generalised outside the deceptive experiment.\nThere is also no such norm in human-computer interaction research, as I have discovered in the last couple of years. Across the swathe of human-computer experimental literature I have traversed, deception is often used. And by deception, I am not talking about deception by omission: not telling people everything about the experiment. At some level, that is required in an experiment where people are exposed to different conditions (albeit allowing deception of this nature can become a slippery slope). Rather, the deception involves acts of commission such as telling people things that are not true.\nOne common form of deception concerns the AI the experimental subject is interacting with. Often, there is no actual AI. Rather, the participants are shown a set of hand-crafted responses. I have seen this called “synthetic AI”. Often the human comparator does not exist either.\nBeyond synthetic AI, it also seems common to tell the occasional lie to generate the desired response. Below are examples of each and a few questions in response."
  },
  {
    "objectID": "posts/deception.html#synthetic-ai",
    "href": "posts/deception.html#synthetic-ai",
    "title": "The AI that wasn’t: Deception in human-AI experiments",
    "section": "Synthetic-AI",
    "text": "Synthetic-AI\nTo run an experiment on human responses to an AI, on face value you want an AI. But that’s a challenge. For one, you need to procure and AI, or develop or train an AI on the task or data. Depending on the nature of the AI, you might also be designing an interface. And apart from being a lot of work, there’s no guarantee that your AI will have the features you desire, such as a certain error rate or error boundary.\nIn economics, this challenge has typically led to the use of statistical prediction tasks. Given a dataset about, say, student achievement, you train an AI to predict achievement based on other features of the student. Apart from the easy availability of datasets to create these tasks (such as those available on Kaggle or in repositories such as the UC Irvine Machine Learning Repository), developing statistical models is a bread and butter task for an economist. You can often train a model on these relatively well-structured datasets in a few lines of code.\nThese statistical model outputs are then framed as coming from an AI, an algorithm or whatever other term you might want to give it. As indicated by the mountains of evidence that statistical models outperform human judgments, these “AIs” are typically superior to humans at the task. If you want a weaker model you might withhold data that you give to the human\nUltimately, however, statistical prediction is not the richest environment for studying human-AI interaction. Statistical prediction tasks are only a small subset of the space in which you might want AI support. That moves us into more complex domains where it’s not so easy to develop a model.\nIn experiments coming from the human-computer interaction field, you do see statistical models, but often the experiments requite that richer environment. And that’s where they turn to deception via synthetic AI.\nOne example of synthetic AI is in work by Buçinca et al. (2021) on whether triggering slow thinking can increase engagement with explanations of AI recommendations. They created a task where participants were shown a picture of a meal and information on the main ingredients, and asked what ingredients to substitute to make it low-carb while maintaining the original flavour.\n\nAnd how was this AI developed? They write:\n\nWe designed a simulated AI for the experiment, which had 75% accuracy of correctly recognizing the ingredient with the highest carbohydrate impact in the image of the meal. Note that we did not train an actual machine learning model for this task because we wanted to have control over the type and prevalence of error the AI would make.\n\nThis is one of the more direct statements that they made the AI outputs up, but even where less explicitly stated, it is common practice.’\nThe question then becomes, what do participants believe of this scenario. Do they believe that an AI is actually delivering these suggestions and associated explanations?\n(This paper is illustrative of my complaint of a few months ago about the lack of openness and data sharing in human computer interaction research.)\nSome people do go this extra yard - and full praise to them. As an example, Lai et al. (2022) developed neural architecture to classify Wikipedia comments and Reddit hate speech. The particular architecture they used allowed identification of the rationale, communicated through highlights of the relevant words in the text. It looks like a lot of effort, but they’re not deceiving participants, plus have a more ecologically valid experimental setup. That would boost my confidence in the results."
  },
  {
    "objectID": "posts/deception.html#straight-out-porkie-pies",
    "href": "posts/deception.html#straight-out-porkie-pies",
    "title": "The AI that wasn’t: Deception in human-AI experiments",
    "section": "Straight-out porkie pies",
    "text": "Straight-out porkie pies\nI sympathise with the synthetic AI approach, albeit not being completely on board with it. Coming up with experimental tasks where the AI really is an AI performing the task you say it is is tough. In many cases this requires detailed model builds and software interfaces - and even then, one element is often not quite as stated.\nBut the deception tends not to be limited to synthetic AI. The AI is often completely misrepresented.\nHere’s an example from Rastogi et al (2022), who examined whether they could de-anchor experimental participants from AI advice. I reviewed this paper and the question of whether this even involves anchoring in a post last month.\nThe authors write:\n\nTo induce anchoring bias, the participant was informed at the start of the training section that the AI model was 85% accurate (we carefully chose the training trials to ensure that the AI was indeed 85% accurate over these trials), while the model’s actual accuracy is 70.8% over the entire training set and 66.5% over the test set. Since our goal is to induce anchoring bias and the training time is short, we stated a high AI accuracy.\n\nThe training set they reference here is the data they trained the model on. The training trials are a set of 15\nEffectively, they told people the AI had an accuracy that it didn’t.\nThis disparity between stated accuracy (85%) and true accuracy (70.8%) is realistic if there is a distribution shift between the training and the test\nHowever, let’s consider what they can infer from 15 trials. Suppose the AI actually had 85% accuracy. You are going to see, on average, 12.75 correct trials. A 70.8% correct AI will give 10.575 trials on average.\nSo let’s suppose you see 12 correct trials. What should you infer about the AI accuracy? The 95% confidence interval is approximately [70%, 90%]. You can’t tell the difference between a 70% and 85% accurate algorithm. And that’s assuming they’re counting accurately the number of correct and incorrect answers.\nThey seek to defend the deception as follows:\n\nthis disparity between stated accuracy (85%) and true accuracy (70.8%) is realistic if there is a distribution shift between the training and the test set, which would imply that the humans’ trust in AI is misplaced."
  },
  {
    "objectID": "posts/deception.html#do-participants-believe-the-ai-is-ai",
    "href": "posts/deception.html#do-participants-believe-the-ai-is-ai",
    "title": "The AI that wasn’t: Deception in human-AI experiments",
    "section": "Do participant’s believe the AI is AI?",
    "text": "Do participant’s believe the AI is AI?\nWith generative AI, this is coming closer to being feasible. But looking at many experiments conducted through the last decade, I suspect many participants would not believe an AI"
  },
  {
    "objectID": "posts/deception.html#do-participants-believe-the-human-is-human",
    "href": "posts/deception.html#do-participants-believe-the-human-is-human",
    "title": "The AI that wasn’t: Deception in human-AI experiments",
    "section": "Do participants believe the human is human?",
    "text": "Do participants believe the human is human?\nEven more so"
  },
  {
    "objectID": "posts/deception.html#thoughts",
    "href": "posts/deception.html#thoughts",
    "title": "The AI that wasn’t: Deception in human-AI experiments",
    "section": "Thoughts",
    "text": "Thoughts\nI sense one reason people love experiments with LLMs is that it involves an actual AI! Although being a stochastic AI, one that is rather hard to control, that comes with a bunch of different costs."
  },
  {
    "objectID": "posts/medical-choice-overload.html",
    "href": "posts/medical-choice-overload.html",
    "title": "Does adding additional medical treatment options increase status quo bias?",
    "section": "",
    "text": "From Donald Redelmeier and Eldar Shafir’s (1995) classic paper in JAMA, the Journal of the American Medical Association:\n\nIntervention: The basic version of each scenario presented a choice between two options. The expanded version presented three options: the original two plus a third. The two versions otherwise contained identical information and were randomly assigned. …\nResults: In one scenario involving a patient with osteoarthritis, family physicians were less likely to prescribe a medication when deciding between two medications than when deciding about only one medication (53% vs 72%; P &lt; .005). Apparently, the difficulty in deciding between the two medications led some physicians to recommend not starting either. Similar discrepancies were found in decisions made by neurologists and neurosurgeons concerning carotid artery surgery and by legislators concerning hospital closures.\nConclusions: The introduction of additional options can increase decision difficulty and, hence, the tendency to choose a distinctive option or maintain the status quo. Awareness of this cognitive bias may lead to improved decision making in complex medical situations.\n\nThe paper has been cited 622 times as of today and\nFrom a new paper by Gemma Altinger et al. (2025) in JAMA Network Open (I’m one of the et al.):\n\nQuestion Does offering multiple appropriate treatment alternatives affect the odds of primary care physicians choosing an alternative over the current care plan?\nFindings In this randomized clinical trial of 402 primary care physicians, offering 2 or more appropriate alternatives significantly increased the odds that physicians would choose an alternative (62%) compared with those offered only 1 alternative (44%).\nMeaning Contrary to prior studies suggesting status-quo bias, in this trial, presenting multiple appropriate alternatives in decision support alerts increased the odds that physicians would choose an alternative; indicating that presenting multiple alternatives may improve clinical decision-making and reduce unwarranted variation in health care.\n\n\n\n\n\nReferences\n\nAltinger, G., Maher, C. G., Jones, C. M. P., Collins, J., Linder, J. A., … Traeger, A. C. (2025). Multiple suggested care alternatives and decision-making of primary care physicians: A randomized clinical trial. JAMA Network Open, 8(11), e2542949. https://doi.org/10.1001/jamanetworkopen.2025.42949\n\n\nRedelmeier, D. A., and Shafir, E. (1995). Medical decision making in situations that offer multiple alternatives. JAMA, 273(4), 302–305. https://doi.org/10.1001/jama.1995.03520280048038"
  },
  {
    "objectID": "posts/more-options-more-action-contradicting-a-classic-finding.html",
    "href": "posts/more-options-more-action-contradicting-a-classic-finding.html",
    "title": "More options, more action: contradicting a classic finding",
    "section": "",
    "text": "From Donald Redelmeier and Eldar Shafir’s (1995) classic paper in JAMA, the Journal of the American Medical Association:\n\nIntervention: The basic version of each scenario presented a choice between two options. The expanded version presented three options: the original two plus a third. …\nResults: In one scenario involving a patient with osteoarthritis, family physicians were less likely to prescribe a medication when deciding between two medications than when deciding about only one medication (53% vs 72%; P &lt; .005). Apparently, the difficulty in deciding between the two medications led some physicians to recommend not starting either. Similar discrepancies were found in decisions made by neurologists and neurosurgeons concerning carotid artery surgery and by legislators concerning hospital closures.\nConclusions: The introduction of additional options can increase decision difficulty and, hence, the tendency to choose a distinctive option or maintain the status quo.\n\nThe paper has been cited 622 times as of today.\nFrom a new paper by Gemma Altinger et al. (2025) in JAMA Network Open (I’m one of the et al.):\n\nQuestion Does offering multiple appropriate treatment alternatives affect the odds of primary care physicians choosing an alternative over the current care plan?\nFindings In this randomized clinical trial of 402 primary care physicians, offering 2 or more appropriate alternatives significantly increased the odds that physicians would choose an alternative (62%) compared with those offered only 1 alternative (44%).\nMeaning Contrary to prior studies suggesting status-quo bias, in this trial, presenting multiple appropriate alternatives in decision support alerts increased the odds that physicians would choose an alternative; indicating that presenting multiple alternatives may improve clinical decision-making and reduce unwarranted variation in health care.\n\n\n\n\n\n\nReferences\n\nAltinger, G., Maher, C. G., Jones, C. M. P., Collins, J., Linder, J. A., … Traeger, A. C. (2025). Multiple suggested care alternatives and decision-making of primary care physicians: A randomized clinical trial. JAMA Network Open, 8(11), e2542949. https://doi.org/10.1001/jamanetworkopen.2025.42949\n\n\nRedelmeier, D. A., and Shafir, E. (1995). Medical decision making in situations that offer multiple alternatives. JAMA, 273(4), 302–305. https://doi.org/10.1001/jama.1995.03520280048038"
  },
  {
    "objectID": "posts/the-study-that-just-wont-die-disfluency-edition.html",
    "href": "posts/the-study-that-just-wont-die-disfluency-edition.html",
    "title": "The study that just won’t die: disfluency edition",
    "section": "",
    "text": "This is a story about a study that refuses to die, despite overwhelming evidence it should.\nFrom a new article in HBR on using behavioural science when rolling out AI tools:\nThat claim traces back to a classic paper by Adam Alter and friends (2007).1 In Experiment 1, Alter and friends gave students the cognitive reflection test. Here’s one question:\nThe questions typically have an intuitive, wrong answer and a correct answer that requires a little more thought. In this case, the intuitive (wrong) answer is 10 cents. The correct answer is 5 cents.\nAlter and friends split the experimental participants into two conditions. Some participants were given the questions in an easy-to-read font, others in a hard-to-read font. And as suggested in the HBR article, those who were given the questions in the disfluent font answered more questions correctly. Sixty-five percent of participants in the disfluent group got all questions correct, compared to only 10 percent in the fluent condition.\nThe paper has a series of other similar experiments in which they show that disfluency can trigger more analytical forms of reasoning, sometimes correcting the errors of intuition.\nThis all sounds good until we examine the follow-up studies that have come in since.\nThompson et al. (2013b) ran a series of experiments on disfluency, including three involving the cognitive reflection test, and found no effect on accuracy. They did suggest there might be an effect among the highest-IQ participants, but nothing in aggregate.2\nThen Meyer et al. (2015) ran replications testing disfluent fonts and the cognitive reflection test across 13 labs. Combining those 13 replications with the three by Thompson and friends and the original study by Alter and friends brought the total sample size up to 7,365 people (from the original 40). The headline finding was there was no effect of disfluent fonts. Further, there was also no effect when the analysis was restricted to only the higher-IQ participants.\nThis diagram highlights the stark contrast between the original study and later attempts to replicate. The pooled result (top dot) shows no effect, with the dot centred on zero with a very narrow confidence interval. The 16 individual replications below it tell the same story. Only the original study, isolated at the bottom right, shows an effect.\nFinally, the Many Labs II project (Klein et al., 2018) also included Experiment 4 from Alter and friends’ paper in its replications. That experiment involved syllogistic reasoning problems. Alter and friends found their 41 participants did better with disfluent fonts. The Many Labs II replication found nothing. This wasn’t unexpected: a prediction exercise in advance of the replication gave it a 42% or 32% probability (using survey or market estimations) of replicating (Forsell et al., 2019).\nBack when Meyer et al. (2015) was first published, Terry Burnham suggested we should measure the rate of learning via citations. In April 2015, Terry Burnham suggested we measure learning via citations. The original Alter paper had 344 citations then, while Thompson’s replication had 38.\nTo November 2025, the trend hasn’t been great. Alter’s paper now has 1,442 citations (up 1,000+), while the replications have grown to 349 and 139. The original study is still being cited far more frequently than the evidence it doesn’t replicate.\nWhen people ask me why behavioural economics and behavioural science are still fringe disciplines in applied settings, I have many suggestions. One is this absence of quality filter. The shiny stories get told again and again no matter how much evidence accumulates that they are rubbish. That lack of filter eventually gets found out.\nI’ve been beating this drum for almost 10 years now. Sadly, it still needs beating."
  },
  {
    "objectID": "posts/the-study-that-just-wont-die-disfluency-edition.html#footnotes",
    "href": "posts/the-study-that-just-wont-die-disfluency-edition.html#footnotes",
    "title": "The study that just won’t die: disfluency edition",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis wasn’t the paper linked from the HBR article. The link is to a review on disfluency by Alter and Daniel M. Oppenheimer (2009). Within that review, the claim about hard to read fonts comes from the Alter et al. (2007) paper.↩︎\nThere was some back and forth between Alter et al. (2013) and Thompson et al. (2013a) about how to interpret the replication, but the headline of no improvement in accuracy remains.↩︎"
  }
]