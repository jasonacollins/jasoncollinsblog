---
title: 'Against centaur evaluations'
author: "Jason Collins"
date: 2025-11-03 09:00:00+11:00
draft: true
images: [""]
---

One reliable path to business-school fame is minting a phrase that sticks.


Let's suppose a centaur evaluation was created. A Kaggle style leaderboard as they propose. How long until the top entrants are pure AI? Not long at all. It will be an endless game.

So what of more messy environments? But this is . There is a massive human-computer interaction literature, with many methods, examining how people interact with AI to get the best results, etc. They dismiss the few examples they give (and give no indication they are aware of the broader breadth)

Clean pdf of paper: https://digitaleconomy.stanford.edu/wp-content/uploads/2025/06/CentaurEvaluations.pdf

Open review: https://openreview.net/forum?id=LkdH35003E

ICML paper: https://icml.cc/virtual/2025/poster/40148

Article on Turing trap: https://digitaleconomy.stanford.edu/news/the-turing-trap-the-promise-peril-of-human-like-artificial-intelligence/

We know that model leaderboards are dynamic. From week to week the latest best model according to XYZ shifts.

What of a model that we put in thew hands of exprts and get to the top of the leaderboard. Then we put the AI in the hands of monkeys.



Centaurs aren't just task dependent. They're human dependent. They're interface development. They're what you tell the human before they use them dependent. They're explanation dependent (and not in the way that you think).

