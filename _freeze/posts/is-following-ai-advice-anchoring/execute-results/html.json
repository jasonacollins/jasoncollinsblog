{
  "hash": "7d3334153123db186c45aee503573e1e",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Is following artificial intelligence advice \\\"anchoring bias\\\"?\"\nauthor: \"Jason Collins\"\ndate: 2025-10-24 09:00:00+10:00\ndraft: true\nbibliography: references.bib\n---\n\nWant to demonstrate human \"irrationality\"? Ask half of your audience the following question:\n\n> Was Elvis older or younger than age 45 when he died?\n\nAsk the other half:\n\n> Was Elvis older or younger than age 75 when he died?\n\nThen ask both groups:\n\n> How old was Elvis when he died?\n\nThose asked if Elvis was older or younger than age 45 tend to estimate a lower age than those asked about age 75.\n\nThis is an illustration of the \"anchoring and adjustment\" heuristic. When people estimate a quantity, they often start from an initial value (the anchor) and adjust from that anchor to get a final answer. The heuristic can be biased where we use weak anchors or insufficiently adjust from the anchor.\n\nOne famous experiment on anchors was by @tversky1974. They spun a wheel with numbers between 0 and 100 on it, but rigged to stop at either 10 or 65. They then asked people whether the proportion of African countries who were members of the United Nations was above or below that number (this experiment was in the 1970s). The participants were also asked for a numerical estimate. Those who saw 10 on the wheel estimated 25 percent. Those who saw 65 estimated 45 percent. The (assumed) randomness of the anchor was a nice experimental feature. In my Elvis example above, the respondent might assume I chose the number (45 or 75) for a reason. There might be information in that number. There is no such information in the spinning of the wheel.\n\nInsufficient adjustment from the anchor can be thought of as the complement of using an irrelevant anchor. If there is any trace of an irrelevant anchor in your answer, you haven't adjusted enough. However, many anchors we use are sensible - we just don't adjust enough for our particular case.\n\nAlthough anchoring is often called a \"bias\", it is better to think of anchoring as a heuristic that can backfire in some task environments. If you're considering making an offer for a four-bedroom house, it's not a bad strategy to start with the sale price of the neighbouring three-bedroom house. Adjust for the extra bedroom and any other differences between the two. @griffiths2015 argued that anchoring and adjustment can be \"resource-rational\", as accounting for the computational cost makes anchoring and adjustment an optimal estimation strategy.\n\nWhich brings me to the question of anchoring to artificial intelligence (AI) advice.\n\nA challenge in human-AI interaction is calibrating the user's trust in the AI system. When should a user trust the AI? When should they deviate from the AI recommendation? Most experimental evidence suggests we don't calibrate well. Complementary performance - the human-AI combination outperforming the human or AI alone - is hard to achieve. The most common result is the AI boosting human performance, but not to the level of the AI alone. Deviations from the AI advice are more likely to degrade than improve performance.\n\n@rastogi2022 proposed that one obstacle to complementary performance is \"anchoring bias\". People do not explore the alternative hypotheses once the AI decision has provided an anchor.\n\nTo examine this question, @rastogi2022 conducted two experiments, which I walk through below.\n\n## Experiment 1\n\nThe authors asked a group of [Amazon Mechanical Turk](https://www.mturk.com/) workers to use student data to predict whether the student would pass or fail a class. The data, drawn from a [student performance dataset](https://archive.ics.uci.edu/dataset/320/student+performance), included student characteristics, past performance and demographics. The workers were also provided with an AI model recommendation (logistic regression) based on the 10 most important student features (e.g. mother’s and father’s education, hours spent studying weekly).\n\nAt the start of the session, the authors asked participants to give estimates for 15 training examples. Their training instructions were described by the authors as follows:\n\n> To induce anchoring bias, the participant was informed at the start of the training section that the AI model was 85% accurate (we carefully chose the training trials to ensure that the AI was indeed 85% accurate over these trials), while the model’s actual accuracy is 70.8% over the entire training set and 66.5% over the test set. Since our goal is to induce anchoring bias and the training time is short, we stated a high AI accuracy.\n\nAfter each training estimate, the workers were shown the correct answer and the estimate made by the AI.\n\nEffectively, participants are told the AI is 85% accurate and then see performance through the training set that aligns with that accuracy. Though you could frame this as \"not a lie\", the participants are deliberately deceived about the capability of the AI (we're not provided with the exact wording to make a finer judgment). I have another post in the pipeline on deception in human-AI experiments (surprisingly common) and a little more to say below. But to foreshadow some of my argument, I would not call someone biased if they used information designed to deceive them.\n\nAfter training, the test section involved 36 trials. The AI accuracy in these trials was even lower than the 66% measured in testing as for eight trials the AI prediction was \"flipped\", giving an AI prediction task accuracy of 58%. If the workers \"anchored\" to those eight flipped trials, their performance would be even worse than if given the actual AI predictions.\n\nThe primary hypothesis the authors tested in Experiment 1 is whether anchoring is reduced if the workers were given more time to make their prediction. Workers were given 10, 15, 20 or 25 seconds to make their prediction. If they were less likely to follow the erroneous prediction in those flipped trials, that would be evidence of reduced anchoring.\n\nThe results at first glance, as presented in the following figure, supports the hypothesis. Workers with 25 seconds were more likely to deviate from the incorrect recommendation. This might be thought of as being in line with @griffiths2015, in that more resources enable more calculation and adjustment.\n\n![](img/is-following-ai-advice-anchoring/rastogi-et-al-2022-fig-2a.png)\n\n(I also suspect there is an error in this chart. The average disagreement for the non-probe trials is unbelievably low.)\n\nThere is one reported statistical test stating that time alleviates anchoring bias for those trials. There is no p-value reported, but based on the reported confidence intervals, it only just scrapes into statistical significance.\n\n(To get on my high horse about pre-registration here, I can think of a lot of ways to test the effect of time on anchoring. A linear regression on a bootstrapped 5000 re-samples isn't the first one that would come to mind. Why this choice? What of other options? )\n\nThe authors do not provide broader accuracy data or tests of anchoring beyond the Figure. We can see that more transparently in the data from Experiment 2, and it suggests that reducing \"anchoring bias\" on incorrect recommendations may not be without trade-offs.\n\n(This incomplete reporting - like the lack of pre-registration - is typical of many experiments I have read in the human-computer interaction literature. The authors report limited results and the absence of publicly available data means that the best you can do is speculate on the robustness of the tests. Given we don't even get accuracy data, we don't get much chance to check other angles either.)\n\nMeasuring \"adjustment\" is also quite difficult here, as there is only one type of adjustment - disagreeing with the AI recommendation. Are participants using as anchoring and adjustment heuristic - or is it assume the AI is right unless there is clear evidence to the contrary?\n\n## Experiment 2\n\nIn experiment 2, the authors test a broader set of interventions to reduce \"anchoring bias\". They also report richer data (in the form of a more detailed chart), so we're able to see a little more of the dynamic.\n\nThis experiment used the same task as Experiment 1, except the AI model was trained excluding the second, third and fourth most relevant features for the prediction (hours spent studying weekly, hours spent going out with friends weekly, enrolment in extra educational support). This gave the workers three features that the AI did not have access to, creating potential for complementary performance via the information asymmetry.\n\nThe authors don't state the accuracy of this degraded AI, but the workers are again told at the start of their training that the AI has 85% accuracy. However, the accuracy of the degraded AI on the sample for the participants' main task was around 60%.\n\nWorkers were then places in the following conditions.\n\n-   **Human only**: Workers provide their prediction without an AI prediction. They are given 25 seconds for each prediction\n-   **Constant time**: Workers provide their prediction with the help of the AI prediction, with 18 second per prediction.\\\n-   **Random time**: AI assistance with the time allocation randomly set to either 10 seconds or 25 seconds each trial.\\\n-   **Confidence-based time**: AI assistance with 10 or 25 seconds depending of AI confidence (low confidence = more time).\\\n-   **Confidence-based time with explanation**: As for confidence-based time but the AI confidence is explicitly provided (“low” or “high”).\n\nEach of these conditions aligns with a hypothesis that they might reduce anchoring.\n\n-   H2: Anchoring bias has a negative effect on human-AI collaborative decision-making accuracy when AI is incorrect.\n-   H3: If the human decision-maker has complementary knowledge then allocating more time can help them sufficiently adjust away from the AI prediction.\n-   H4: Confidence-based time allocation yields better performance than Human alone and AI alone.\n-   H5: Confidence-based time allocation yields better human-AI team performance than constant time and random time allocations.\n-   H6: Confidence-based time allocation with explanation yields better human-AI team performance than the other conditions.\n\nThe results are summarised in these two figures (it takes a while to get your head around what they're saying, I'll highlight the most important bits below).\n\n![](img/is-following-ai-advice-anchoring/rastogi-et-al-2022-fig-4.png)\n\nThe most interesting interpretation relates to the central hypothesis, H2, that anchoring bias has a negative effect on human-AI collaborative decision-making accuracy when AI is incorrect. The human only group has higher accuracy than all of the AI groups on instances where the AI is incorrect. The authors take this as evidence in support of H2.\n\nBut is \"anchoring bias\" leading to this negative effect? To not have lower accuracy when the AI is incorrect than the human only condition, workers in the AI conditions would need to completely ignore the AI for those incorrect recommendations. The anchor is completely removed. But is that reasonable when the authors have informed the workers that the AI is 85% accurate - not to mention that is confirmed by their training experience? Labelling the giving of any weight to a piece of information - information that should have some weight - \"anchoring bias\" seems a stretch.\n\nI am not sure \"anchoring\" in the best word to use in this situation, and I would not append bias to the label. Is any use of information for a particular decision, when that information degrades the decision \"anchoring bias\"? It is extending the label anchoring to mean accounting for any information. To call is a bias, I would examine a model of P(accuracy less than 85% \\| experimenter told us accuracy 85% + D)that updates over time. This includes the information given to them by the experimenters! They call it an anchor. I call it a prior. If I was in an economics experiment, the norm against deception would have my prior that the accuracy is actually 85% quite high.\n\nIf one of my kids tells me they've got a normal coin and flip two heads, I'm updating only marginally from my strong prior that the coin is fair. What is the level of trust of experimental participants in experimenters? What is the appropriate level of trust? (One reason deception is frowned upon in economics is because it degrades trust in experimenters, leading to an additional variable you need to account for in your analysis.)\n\nTurning back to the hypothesis, the results also point to a complementary hypothesis:\n\n> Anchoring bias has a positive effect on human-AI collaborative decision-making accuracy when AI is correct.\n\nAgain, for this not to be true, we would need the person to completely ignore the AI. And unsurprisingly, eyeballing the figure, my hypothesis is true. The AI conditions all markedly outperform the human-only group on estimates where the AI recommendation is correct.\n\nThere's actually an interesting pattern across all the conditions. Whenever you decrease the probability of following an incorrect AI recommendation, you also decrease the probability of following a correct recommendation. The result is that performance across all conditions is largely the same. Because the AI in this experiment is degraded to a quality similar to that of humans alone, reliance on the AI is largely a wash.\n\nFinally, the authors provide another interesting interpretation when they state that they achieve complementary knowledge. This is an interesting thread in this experiment. Due to the three excluded variables, the human has more information that the AI. There are a lot of real-world application where the human will have information the AI doesn't - an AI-financial adviser for example - so finding ways to enhance the use of unshared information is vital to achieving complementary performance. In this case, we get complementary knowledge in that the humans do as well as the AI alone despite the fact they don't always follow the AI. Unforinately, we don't get complemtnary performance - that is, outperformance by the human-AI team of both the human and AI alone. But that seems a prospective path.\n\n## The problem is more often insufficient anchoring\n\nThis experiment involved an AI with capability on par with unassisted humans. But in most statistical tasks such as the one in this experiment, the AI outperforms the human. Further, human-AI teams tend to underperform the AI alone.\n\nIn other words, experiments typically find that people adjust too much! Here's some data from @dietvorst2018. People were more likely to use an algorithm if they could adjust it. That led to higher performance, largely because of this greater likelihood of selecting the model.\n\n![@dietvorst2018 Figure 2b](img/is-following-ai-advice-anchoring/dietvorst-et-al-2018-fig-2b.png){width=\"400\"}\n\nBut how does the performance of those who selected the model compare to the model itself. Here is a chart I have generated from the experimental data provided in the [supplementary materials](https://pubsonline.informs.org/doi/suppl/10.1287/mnsc.2016.2643/suppl_file/mnsc.2016.2643-sm-data.zip). (And again noting that business schools are [far ahead of the human-computer interaction crowd](why-i-dont-trust-most-human-ai-interaction-experimental-research.qmd) when it comes to best practice data sharing.) In the \"Adjust by 10\" group, the participant;s adjustments were a wash, with their performance on par with the unadjusted model. But for those in the \"Change 10\" group, their changes increased the error. The AI alone was a stronger performer.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(scales)\n\n# Read the data - specify na values to handle periods as missing\ndata <- read_csv(\"data/Overcoming_Algorithm_Aversion_Data-study-1-data.csv\",\n                 na = c(\"\", \"NA\", \".\"))\n\n# Convert necessary columns to numeric (many are stored as strings due to periods)\ndata <- data %>%\n  mutate(across(c(ModelBonus, ModelAAEEstimate, HumanAAEEstimate,\n                  ModelConfidence, HumanConfidence, Age, Gender, Education,\n                  AAE, ModelAAE, HumanAAE, Bonus, BonusFromModel, BonusFromHuman,\n                  CorrelationWithModel, AvDiffFromModel, AvAdjustmentSize,\n                  AdjustmentDividedByPotential, HumModelCorrelation, HumModelAvDiff,\n                  AvgLargest10Changes, AnyBonus), \n                as.numeric))\n\n# Define condition labels\ncondition_labels <- c(\"1\" = \"Can't change\", \n                     \"2\" = \"Adjust by 10\", \n                     \"3\" = \"Change 10\", \n                     \"4\" = \"Use freely\")\n\n# Filter for participants who chose the model (ModelBonus == 1)\nmodel_choosers <- data %>%\n  filter(ModelBonus == 1)\n\n# Calculate summary statistics by condition\nsummary_stats <- model_choosers %>%\n  group_by(Condition) %>%\n  summarise(\n    n = n(),\n    ModelAAE_mean = mean(ModelAAE, na.rm = TRUE),\n    AAE_mean = mean(AAE, na.rm = TRUE),\n    ModelAAE_sd = sd(ModelAAE, na.rm = TRUE),\n    AAE_sd = sd(AAE, na.rm = TRUE),\n    Difference = AAE_mean - ModelAAE_mean,\n    PercentChange = (Difference / ModelAAE_mean) * 100,\n    AvgAdjustmentSize = mean(AvAdjustmentSize, na.rm = TRUE)\n  ) %>%\n  mutate(ConditionLabel = condition_labels[as.character(Condition)])\n\n# Add info about total participants per condition\ntotal_by_condition <- data %>%\n  group_by(Condition) %>%\n  summarise(total_n = n())\n\nsummary_stats <- summary_stats %>%\n  left_join(total_by_condition, by = \"Condition\") %>%\n  mutate(PropChoseModel = n / total_n)\n\n# Prepare data for plotting\nplot_data <- summary_stats %>%\n  filter(!is.na(AAE_mean) & Condition != 1) %>%  # Remove can't change condition and any conditions with no data\n  select(Condition, ConditionLabel, ModelAAE_mean, AAE_mean) %>%\n  pivot_longer(cols = c(ModelAAE_mean, AAE_mean),\n               names_to = \"ErrorType\",\n               values_to = \"AAE\") %>%\n  mutate(\n    ErrorType = case_when(\n      ErrorType == \"ModelAAE_mean\" ~ \"Model (no adjustment)\",\n      ErrorType == \"AAE_mean\" ~ \"Actual (with adjustment)\"\n    ),\n    ErrorType = factor(ErrorType, levels = c(\"Model (no adjustment)\", \"Actual (with adjustment)\"))\n  )\n\n# Create the plot\np <- ggplot(plot_data, aes(x = ConditionLabel, y = AAE, fill = ErrorType)) +\n  geom_bar(stat = \"identity\", position = \"dodge\", width = 0.7) +\n  scale_fill_manual(values = c(\"Model (no adjustment)\" = \"#3498db\", \n                              \"Actual (with adjustment)\" = \"#e74c3c\")) +\n  labs(\n    title = \"Error Rates for Model Choosers by Adjustment Condition\",\n    subtitle = \"Comparing model performance with and without adjustments\",\n    x = \"Condition\",\n    y = \"Average Absolute Error (AAE)\",\n    fill = \"Error Type\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 16, face = \"bold\", hjust = 0.5),\n    plot.subtitle = element_text(size = 12, hjust = 0.5),\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    legend.position = \"top\",\n    legend.title = element_text(face = \"bold\"),\n    panel.grid.minor = element_blank()\n  ) +\n  scale_y_continuous(limits = c(0, max(plot_data$AAE, na.rm = TRUE) * 1.1), \n                     breaks = pretty_breaks(n = 5)) +\n  # Add percentage change labels on top of bars\n  geom_text(data = summary_stats %>% filter(!is.na(AAE_mean) & Condition != 1),\n            aes(x = ConditionLabel, y = AAE_mean + 0.5, \n                label = paste0(ifelse(Difference >= 0, \"+\", \"\"), \n                               round(PercentChange, 1), \"%\")),\n            inherit.aes = FALSE,\n            size = 3.5,\n            fontface = \"bold\")\n\n# Display the plot\nprint(p)\n```\n\n::: {.cell-output-display}\n![](is-following-ai-advice-anchoring_files/figure-html/dietvorst-error-1.png){width=672}\n:::\n:::\n\n\nThis is a typical finding. Here's a plot of a meta-analysis by @vaccaro2024. Across 370 effect sizes from 106 studies, human-AI combinations more typically underperformed the best of the human or AI than outperformed. (Those positive human-AI teams typically came from creative rather than decision tasks.)\n\n![](img/is-following-ai-advice-anchoring/vaccaro-et-al-2024-fig-1a.png){width=\"600\"}\n\nBringing this back to the anchoring experiment, if the AI actually had accuracy of 85% as the experimental participants were informed, I suspect we would be talking about a different problem: insufficient anchoring. Anchoring and overadjustment. That's a very different problem to solve.",
    "supporting": [
      "is-following-ai-advice-anchoring_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}