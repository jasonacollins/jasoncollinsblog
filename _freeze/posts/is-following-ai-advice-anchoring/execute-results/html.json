{
  "hash": "5163f246fff3e0b445b46b757bf788c2",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Is following artificial intelligence advice \\\"anchoring bias\\\"?\"\nauthor: \"Jason Collins\"\ndate: 2025-10-24 09:00:00+10:00\ndraft: true\nbibliography: references.bib\n---\n\nWant to demonstrate human \"irrationality\"? Ask half of your audience the following question:\n\n> Was Elvis older or younger than age 45 when he died?\n\nAsk the other half:\n\n> Was Elvis older or younger than age 75 when he died?\n\nThen ask both groups:\n\n> How old was Elvis when he died?\n\nThose asked if Elvis was older or younger than age 45 tend to estimate a lower age of death than those asked about age 75.\n\nThis is an illustration of the \"anchoring and adjustment\" heuristic. When people estimate a quantity, they often start from an initial value (the anchor) and adjust from that anchor to get a final answer. The heuristic can be biased where we use weak anchors or insufficiently adjust from the anchor.\n\nOne famous experiment on anchors was by @tversky1974. They spun a wheel with numbers between 0 and 100 on it, but it was rigged to stop at either 10 or 65. They then asked experimental participants who had seen the spin whether the proportion of African countries who were members of the United Nations was above or below that number (this experiment was in the 1970s). The participants were also asked for a numerical estimate. Those who saw 10 on the wheel estimated 25 percent. Those who saw 65 estimated 45 percent. The (assumed) randomness of the anchor was a nice experimental feature. In my Elvis example above, the respondent might assume I chose the number (45 or 75) for a reason. There might be information in that number. There is no such information in the spinning of the wheel.\n\nInsufficient adjustment from the anchor is the flipside of using an irrelevant anchor. If there is any trace of an irrelevant anchor in your answer, you haven't adjusted enough. However, many anchors we use are sensible starting points - we just don't adjust enough for our particular case.\n\nAlthough anchoring is often called a \"bias\", it is better to think of anchoring as a heuristic that can backfire in some task environments. If you're considering making an offer for a four-bedroom house, it's not a bad strategy to start with the sale price of the neighbouring three-bedroom house. Adjust for the extra bedroom and any other differences between the two. @griffiths2015 argued that anchoring and adjustment can be \"resource-rational\", as accounting for the computational cost makes anchoring and adjustment an optimal estimation strategy.\n\nWhich brings me to the question of anchoring to artificial intelligence (AI) advice.\n\nA challenge in human-AI interaction is calibrating the user's trust in the AI system. When should a user trust the AI? When should they deviate from the AI recommendation? Most experimental evidence suggests we don't calibrate well. Achieving complementary performance—where the human-AI team outperforms either alone—is difficult. Typically, AI assistance improves human performance but the combination still underperforms the AI working alone.\n\n@rastogi2022 proposed that one obstacle to complementary performance is \"anchoring bias\". People do not explore the alternative hypotheses once the AI decision has provided an anchor.\n\nTo examine this question, @rastogi2022 conducted two experiments, which I walk through below.\n\n## Experiment 1\n\nThe authors asked a group of [Amazon Mechanical Turk](https://www.mturk.com/) workers to use student data to predict whether a student would pass or fail a class. The data, drawn from a [student performance dataset](https://archive.ics.uci.edu/dataset/320/student+performance), included student characteristics, past performance and demographics. The workers were also provided with an AI model recommendation (logistic regression) based on the 10 most important student features (e.g. mother’s and father’s education, hours spent studying weekly).\n\nParticipants completed 15 training examples. The authors deliberately misled them about AI performance:\n\n> To induce anchoring bias, the participant was informed at the start of the training section that the AI model was 85% accurate (we carefully chose the training trials to ensure that the AI was indeed 85% accurate over these trials), while the model’s actual accuracy is 70.8% over the entire training set and 66.5% over the test set. Since our goal is to induce anchoring bias and the training time is short, we stated a high AI accuracy.\n\nAfter each training estimate, the workers were shown the correct answer and the estimate made by the AI.\n\nEffectively, participants are told the AI is 85% accurate and then see performance through the training set that aligns with that accuracy. This is deliberate deception about the AI's capability, even if technically \"not a lie\". The exact wording isn't provided, but the intent is clear. I'm reluctant to call behaviour \"biased\" when it's based on information designed to deceive. (Deception is surprisingly common in human-AI experiments—I have another post on this coming.)\n\nThe test section had 36 trials. The AI accuracy was even lower than 66% because the experimenters deliberately \"flipped\" eight AI predictions, reducing task accuracy to 58%. Workers who followed these flipped predictions would perform worse than those receiving unmanipulated AI advice.\n\nThe primary hypothesis the authors tested in Experiment 1 is whether anchoring is reduced if the workers were given more time to make their prediction. Workers were given 10, 15, 20 or 25 seconds to make their prediction. If they were less likely to follow the erroneous prediction in those flipped trials, that would be evidence of reduced anchoring.\n\nThe results at first glance, as presented in the following figure, support the hypothesis. Workers with 25 seconds were more likely to deviate from the incorrect recommendation. This might be thought of as being in line with @griffiths2015, in that more resources enable more calculation and adjustment.\n\n![](img/is-following-ai-advice-anchoring/rastogi-et-al-2022-fig-2a.png)\n\n(I also suspect there is an error in this chart. The average disagreement for the non-probe trials is unbelievably low.)\n\nThere is one reported statistical test supporting what appears in the visual. There is no p-value reported, but based on the reported confidence intervals, I expect it only just scrapes into statistical significance.\n\n(The lack of pre-registration is concerning. I can think of a lot of ways to test the effect of time on anchoring. The authors' choice of a linear regression on a bootstrapped 5000 re-samples isn't the first one that would come to mind. Why this choice? What of other options? )\n\nThe authors don't report broader accuracy data or additional anchoring tests. However, experiment 2 provides more detail and suggests that reducing \"anchoring bias\" on incorrect recommendations involves trade-offs.\n\n(The incomplete reporting is typical of human-computer interaction literature. Limited results and no public data make it impossible to test robustness or explore alternative analyses.)\n\nMeasuring \"adjustment\" is difficult here. The only observable adjustment is disagreeing with the AI. This raises a question: are participants anchoring and adjusting, or simply trusting the AI unless they see contrary evidence? These are different cognitive processes.\n\n## Experiment 2\n\nIn experiment 2, the authors test a broader set of interventions to reduce \"anchoring bias\". They also report richer data (in the form of a more detailed chart), so we're able to see a little more of the dynamic.\n\nExperiment 2 used the same task but the experimenters degraded the AI by training it without three important features: study hours, social hours, and educational support enrolment. Workers could see these features, creating an information asymmetry where human input could potentially improve AI predictions.\n\nThe authors don't state the degraded AI's accuracy. Workers were again told it had 85% accuracy during training, but its actual accuracy on the main task was around 60%.\n\nWorkers were then placed in the following conditions.\n\n-   **Human only**: Workers provide their prediction without an AI prediction. They are given 25 seconds for each prediction\n-   **Constant time**: Workers provide their prediction with the help of the AI prediction, with 18 second per prediction.\\\n-   **Random time**: AI assistance with the time allocation randomly set to either 10 seconds or 25 seconds each trial.\\\n-   **Confidence-based time**: AI assistance with 10 or 25 seconds depending of AI confidence (low confidence = more time).\\\n-   **Confidence-based time with explanation**: As for confidence-based time but the AI confidence is explicitly provided (“low” or “high”).\n\nEach of these conditions aligns with a hypothesis that they might reduce anchoring. The one I will focus on is H2: Anchoring bias has a negative effect on human-AI collaborative decision-making accuracy when AI is incorrect.\n\nThe results are summarised in the following figures.\n\n![](img/is-following-ai-advice-anchoring/rastogi-et-al-2022-fig-4.png)\n\nThe human only group has higher accuracy than all of the AI groups on instances where the AI is incorrect. The authors take this as evidence in support of H2.\n\nBut is this really \"anchoring bias\"? To avoid lower accuracy when the AI errs, workers would need to ignore the AI completely on those trials, removing the anchor entirely. But that's unreasonable. The workers were told the AI has 85% accuracy, and their training experience confirmed it.\n\n\"Anchoring\" is the wrong label here and \"anchoring bias\" even more so. If using information degrades a decision, that doesn't make the use of that information a bias. The term \"anchoring\" is being stretched to cover any use of information whatsoever.\n\nTo determine whether behaviour is biased, we need a model that accounts for the information the experimenters provided. The authors call the AI estimate an anchor. It's better understood as a prior, shaped by the experimenters' claim of 85% accuracy. In experimental economics, where deception is forbidden, participants would rationally place high confidence in such a claim. Even in this psychology experiment, treating the stated accuracy as reliable information (or at least having some signal) is reasonable behaviour, not bias.\n\nIf one of my kids tells me they've got a normal coin and flip two heads, I'm updating only marginally from my strong prior that the coin is fair. What is the level of trust of experimental participants in experimenters? What is the appropriate level of trust? (One reason deception is frowned upon in economics is because it degrades trust in experimenters, leading to an additional variable you need to account for in your analysis.)\n\nTurning back to the hypothesis, the results also point to a complementary hypothesis not identified by the authors:\n\n> Anchoring bias has a positive effect on human-AI collaborative decision-making accuracy when AI is correct.\n\nAgain, for this not to be true, we would need the person to completely ignore the AI. And unsurprisingly, eyeballing the figure, my hypothesis is true. The AI conditions all markedly outperform the human-only group on estimates where the AI recommendation is correct.\n\nThere's an interesting pattern across all the conditions. Whenever you decrease the probability of following an incorrect AI recommendation, you also decrease the probability of following a correct recommendation. The result is that performance across all conditions is largely the same. Because the AI in this experiment is degraded to a quality similar to that of humans alone, reliance on the AI is largely a wash.\n\nFinally, the authors provide another interesting interpretation when they state that they achieve complementary knowledge. Due to the three excluded variables, the human has more information that the AI. There are a lot of real-world application where the human will have information the AI doesn't - an AI-financial adviser for example - so finding ways to enhance the use of unshared information is vital to achieving complementary performance. In this case, we get complementary knowledge in that the humans do as well as the AI alone despite the fact they don't always follow the AI. Unfortunately, we don't get complementary performance - that is, outperformance by the human-AI team of both the human and AI alone. But that seems a prospective path.\n\n## The problem is more often insufficient anchoring\n\nThis experiment involved an AI with capability on par with unassisted humans. But in most statistical tasks such as the one in this experiment, the AI outperforms the human. Further, human-AI teams tend to underperform the AI alone.\n\nIn other words, experiments typically find that people adjust too much! Here's some data from @dietvorst2018. People were more likely to use an algorithm if they could adjust it. That led to higher performance, largely because of this greater likelihood of selecting the model.\n\n![@dietvorst2018 Figure 2b](img/is-following-ai-advice-anchoring/dietvorst-et-al-2018-fig-2b.png){width=\"400\"}\n\nBut how does the performance of those who selected the model compare to the model itself. Here is a chart I have generated from the experimental data provided in the [supplementary materials](https://pubsonline.informs.org/doi/suppl/10.1287/mnsc.2016.2643/suppl_file/mnsc.2016.2643-sm-data.zip). (And again noting that business schools are [far ahead of the human-computer interaction crowd](why-i-dont-trust-most-human-ai-interaction-experimental-research.qmd) when it comes to best practice data sharing.) In the \"Adjust by 10\" group, the participant's adjustments were a wash, with their performance on par with the unadjusted model. But for those in the \"Change 10\" group, their changes increased the error. The AI alone was a stronger performer.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(scales)\n\n# Read the data - specify na values to handle periods as missing\ndata <- read_csv(\"data/Overcoming_Algorithm_Aversion_Data-study-1-data.csv\",\n                 na = c(\"\", \"NA\", \".\"))\n\n# Convert necessary columns to numeric (many are stored as strings due to periods)\ndata <- data %>%\n  mutate(across(c(ModelBonus, ModelAAEEstimate, HumanAAEEstimate,\n                  ModelConfidence, HumanConfidence, Age, Gender, Education,\n                  AAE, ModelAAE, HumanAAE, Bonus, BonusFromModel, BonusFromHuman,\n                  CorrelationWithModel, AvDiffFromModel, AvAdjustmentSize,\n                  AdjustmentDividedByPotential, HumModelCorrelation, HumModelAvDiff,\n                  AvgLargest10Changes, AnyBonus), \n                as.numeric))\n\n# Define condition labels\ncondition_labels <- c(\"1\" = \"Can't change\", \n                     \"2\" = \"Adjust by 10\", \n                     \"3\" = \"Change 10\", \n                     \"4\" = \"Use freely\")\n\n# Filter for participants who chose the model (ModelBonus == 1)\nmodel_choosers <- data %>%\n  filter(ModelBonus == 1)\n\n# Calculate summary statistics by condition\nsummary_stats <- model_choosers %>%\n  group_by(Condition) %>%\n  summarise(\n    n = n(),\n    ModelAAE_mean = mean(ModelAAE, na.rm = TRUE),\n    AAE_mean = mean(AAE, na.rm = TRUE),\n    ModelAAE_sd = sd(ModelAAE, na.rm = TRUE),\n    AAE_sd = sd(AAE, na.rm = TRUE),\n    Difference = AAE_mean - ModelAAE_mean,\n    PercentChange = (Difference / ModelAAE_mean) * 100,\n    AvgAdjustmentSize = mean(AvAdjustmentSize, na.rm = TRUE)\n  ) %>%\n  mutate(ConditionLabel = condition_labels[as.character(Condition)])\n\n# Add info about total participants per condition\ntotal_by_condition <- data %>%\n  group_by(Condition) %>%\n  summarise(total_n = n())\n\nsummary_stats <- summary_stats %>%\n  left_join(total_by_condition, by = \"Condition\") %>%\n  mutate(PropChoseModel = n / total_n)\n\n# Prepare data for plotting\nplot_data <- summary_stats %>%\n  filter(!is.na(AAE_mean) & Condition != 1) %>%  # Remove can't change condition and any conditions with no data\n  select(Condition, ConditionLabel, ModelAAE_mean, AAE_mean) %>%\n  pivot_longer(cols = c(ModelAAE_mean, AAE_mean),\n               names_to = \"ErrorType\",\n               values_to = \"AAE\") %>%\n  mutate(\n    ErrorType = case_when(\n      ErrorType == \"ModelAAE_mean\" ~ \"Model (no adjustment)\",\n      ErrorType == \"AAE_mean\" ~ \"Actual (with adjustment)\"\n    ),\n    ErrorType = factor(ErrorType, levels = c(\"Model (no adjustment)\", \"Actual (with adjustment)\"))\n  )\n\n# Create the plot\np <- ggplot(plot_data, aes(x = ConditionLabel, y = AAE, fill = ErrorType)) +\n  geom_bar(stat = \"identity\", position = \"dodge\", width = 0.7) +\n  scale_fill_manual(values = c(\"Model (no adjustment)\" = \"#3498db\", \n                              \"Actual (with adjustment)\" = \"#e74c3c\")) +\n  labs(\n    title = \"Error Rates for Model Choosers by Adjustment Condition\",\n    subtitle = \"Comparing model performance with and without adjustments\",\n    x = \"Condition\",\n    y = \"Average Absolute Error (AAE)\",\n    fill = \"Error Type\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 16, face = \"bold\", hjust = 0.5),\n    plot.subtitle = element_text(size = 12, hjust = 0.5),\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    legend.position = \"top\",\n    legend.title = element_text(face = \"bold\"),\n    panel.grid.minor = element_blank()\n  ) +\n  scale_y_continuous(limits = c(0, max(plot_data$AAE, na.rm = TRUE) * 1.1), \n                     breaks = pretty_breaks(n = 5)) +\n  # Add percentage change labels on top of bars\n  geom_text(data = summary_stats %>% filter(!is.na(AAE_mean) & Condition != 1),\n            aes(x = ConditionLabel, y = AAE_mean + 0.5, \n                label = paste0(ifelse(Difference >= 0, \"+\", \"\"), \n                               round(PercentChange, 1), \"%\")),\n            inherit.aes = FALSE,\n            size = 3.5,\n            fontface = \"bold\")\n\n# Display the plot\nprint(p)\n```\n\n::: {.cell-output-display}\n![](is-following-ai-advice-anchoring_files/figure-html/dietvorst-error-1.png){width=672}\n:::\n:::\n\n\nThis pattern is typical. A meta-analysis by @vaccaro2024 examined 370 effect sizes from 106 studies. Human-AI combinations usually underperformed the better of the two working alone, with the exception of creative tasks where teams more often outperformed individuals.\n\n![](img/is-following-ai-advice-anchoring/vaccaro-et-al-2024-fig-1a.png){width=\"600\"}\n\nBringing this back to the anchoring experiment, if the AI actually had accuracy of 85% as the experimental participants were informed, I suspect we would be talking about a different problem: insufficient anchoring. Anchoring and overadjustment. That's a very different problem to solve.",
    "supporting": [
      "is-following-ai-advice-anchoring_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}