{
  "hash": "609ec5ce7fc003c95cdac30f6f49178a",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Is following artificial intelligence advice \\\"anchoring bias\\\"?\"\nauthor: \"Jason Collins\"\ndate: 2025-07-03 09:00:00+10:00\ndraft: true\nbibliography: references.bib\n---\n\nIf you want a reliable gimmick to demonstrate human \"irrationality\", ask half of your audience the following question:\n\n> Was Elvis older or younger than age 45 when he died?\n\nAsk the other half:\n\n> Was Elvis older or younger than age 85 when he died?\n\nThen ask both groups:\n\n> How old was Elvis when he died?\n\nThose asked if Elvis was older or younger than age 45 tend to estimate a lower age than those asked if he was older or younger than age 85.\n\nThis is an illustration of the \"anchoring and adjustment\" heuristic. When people are estimating a quantity, they often start from an initial value (the anchor) and adjust from that anchor to yield a final answer. The heuristic can lead to bias where we use weak anchors or insufficiently adjust from the anchor.\n\n@tversky1974 illustrated weak anchors in an experiment where they spun a wheel with numbers on it before asking people whether the proportion of African countries who were members of the United Nations was above or below that number (this experiment was in the 1970s). They were then asked for a numerical estimate. Those who had a higher number on the wheel estimated a higher proportion. The randomness of the anchor was a nice experimental feature. In my Elvis example above, the respondent might assume I have chosen the number (45 or 85) for a reason. There might be information in that number. There is no such information in the spinning of the wheel.\n\nInsufficient adjustment from the anchor can be thought of as the other side of the coin to using an irrelevant anchor. If there is any trace of an irrelevant anchor in your answer, you haven't adjusted enough. However, many anchors we use are sensible - we just don't adjust enough for our particular case.\n\nAlthough anchoring is often called a \"bias\", it is better to think of it as a heuristic that can backfire depending on the task environment. If you're considering making an offer for a four-bedroom house, it's not a bad strategy to start with the sale price of the neighbouring 3-bedroom house then adjust for the extra bedroom and any other differences between the two. @griffiths2015 argued that anchoring and adjustment can be \"resource-rational\", where taking into account the computational cost makes anchoring and adjustment an optimal estimation strategy.\n\nWhich brings me to the question of anchoring to artificial intelligence (AI) advice.\n\nA central challenge in human-AI interaction is calibrating the user's trust in the AI system. When should a user trust the AI? When should they deviate from the AI recommendation? The typical finding is that complementary performance - the human-AI combination outperforming the human or AI alone - is hard to achieve. The most common result is the AI boosting human performance, but not to the level of the AI alone. Deviations from the AI advice are more likely to degrade than improve performance.\n\n@rastogi2022 proposed that one obstacle to complementary performance is \"anchoring bias\". People do not explore the alternative hypotheses once the AI decision has provided an anchor.\n\nBefore exploring whether this is \"anchoring bias\", let me describe the experiments.\n\n## Experiment 1\n\nA group of Amazon Mechanical Turk workers were asked to predict whether a student, given their characteristics, past performance and demographics, would pass or fail a class. In making their prediction, participants were provided with an AI model recommendation. The authors trained the AI - a logistic regression model - on a \\[student performance dataset\\](<https://archive.ics.uci.edu/dataset/320/student+performance>).\n\nBefore\n\nThis is where it gets interesting. The authors write:\n\n> To induce anchoring bias, the participant was informed at the start of the training section that the AI model was 85% accurate (we carefully chose the training trials to ensure that the AI was indeed 85% accurate over these trials), while the modelâ€™s actual accuracy is 70.8% over the entire training set and 66.5% over the test set. Since our goal is to induce anchoring bias and the training time is short, we stated a high AI accuracy.\n\nThis is typical of many experiments I have read in the human-computer interaction literature. The authors are selective in the results they report and the absence of publicly available data means that the best you can do is speculate on the underlying phenomena.\n\nThe authors state that they achieve complementary performance, but I don't see it.\n\nMeasuring \"adjustment\" is also quite difficult here, as there is only one type of adjustment - disagreeing with the AI recommendation. Is this anchoring and adjustment - or is it assume the AI is right unless there is clear evidence to the contrary?\n\n## Is this anchoring?\n\nFor it to be anchoring, we need some model of P(accuracy less than 85% \\| experimenter told us accuracy 85% + D) that updates over time. They call it an anchor. I call it another prior. If I was in an economics experiment, my prior that the accuracy is actually 85% would be quite high.\n\nIf one of my kids tells me they've got a normal coin and flip two heads, I'm updating only marginally from my very strong prior. What is the level of trust of experimental participants in experimenters? (One reason deception is frowned upon in economics is because it degrades trust in experimenters, leading to an additional variable you need to account for in your analysis.)\n\n## The problem is more often insufficient anchoring\n\nBy deliberately giving\n\nAlmost every experiment finds that people adjust too much! Here's some data from @dietvorst2018. People were more likely to use an algorithm if they could adjust it. That led to higher performance, largely because of this greater likelihood of selecting the model.\n\n![@dietvorst2018 Figure 2b](img/dietvorst-et-al-2018-fig-2b.png){width=\"400\"}\n\nBut how does the performance of those who selected the model compare to the model itself. Here is a chart I have generated from the experimental data provided in the [supplementary materials](https://pubsonline.informs.org/doi/suppl/10.1287/mnsc.2016.2643/suppl_file/mnsc.2016.2643-sm-data.zip). (And again noting that business schools are [far ahead of the human-computer interaction crowd](why-i-dont-trust-most-human-ai-interaction-experimental-research.qmd) when it comes to best practice data sharing.)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(scales)\n\n# Read the data - specify na values to handle periods as missing\ndata <- read_csv(\"data/Overcoming_Algorithm_Aversion_Data-study-1-data.csv\",\n                 na = c(\"\", \"NA\", \".\"))\n\n# Convert necessary columns to numeric (many are stored as strings due to periods)\ndata <- data %>%\n  mutate(across(c(ModelBonus, ModelAAEEstimate, HumanAAEEstimate,\n                  ModelConfidence, HumanConfidence, Age, Gender, Education,\n                  AAE, ModelAAE, HumanAAE, Bonus, BonusFromModel, BonusFromHuman,\n                  CorrelationWithModel, AvDiffFromModel, AvAdjustmentSize,\n                  AdjustmentDividedByPotential, HumModelCorrelation, HumModelAvDiff,\n                  AvgLargest10Changes, AnyBonus), \n                as.numeric))\n\n# Define condition labels\ncondition_labels <- c(\"1\" = \"Can't change\", \n                     \"2\" = \"Adjust by 10\", \n                     \"3\" = \"Change 10\", \n                     \"4\" = \"Use freely\")\n\n# Filter for participants who chose the model (ModelBonus == 1)\nmodel_choosers <- data %>%\n  filter(ModelBonus == 1)\n\n# Calculate summary statistics by condition\nsummary_stats <- model_choosers %>%\n  group_by(Condition) %>%\n  summarise(\n    n = n(),\n    ModelAAE_mean = mean(ModelAAE, na.rm = TRUE),\n    AAE_mean = mean(AAE, na.rm = TRUE),\n    ModelAAE_sd = sd(ModelAAE, na.rm = TRUE),\n    AAE_sd = sd(AAE, na.rm = TRUE),\n    Difference = AAE_mean - ModelAAE_mean,\n    PercentChange = (Difference / ModelAAE_mean) * 100,\n    AvgAdjustmentSize = mean(AvAdjustmentSize, na.rm = TRUE)\n  ) %>%\n  mutate(ConditionLabel = condition_labels[as.character(Condition)])\n\n# Add info about total participants per condition\ntotal_by_condition <- data %>%\n  group_by(Condition) %>%\n  summarise(total_n = n())\n\nsummary_stats <- summary_stats %>%\n  left_join(total_by_condition, by = \"Condition\") %>%\n  mutate(PropChoseModel = n / total_n)\n\n# Prepare data for plotting\nplot_data <- summary_stats %>%\n  filter(!is.na(AAE_mean) & Condition != 1) %>%  # Remove can't change condition and any conditions with no data\n  select(Condition, ConditionLabel, ModelAAE_mean, AAE_mean) %>%\n  pivot_longer(cols = c(ModelAAE_mean, AAE_mean),\n               names_to = \"ErrorType\",\n               values_to = \"AAE\") %>%\n  mutate(\n    ErrorType = case_when(\n      ErrorType == \"ModelAAE_mean\" ~ \"Model (no adjustment)\",\n      ErrorType == \"AAE_mean\" ~ \"Actual (with adjustment)\"\n    ),\n    ErrorType = factor(ErrorType, levels = c(\"Model (no adjustment)\", \"Actual (with adjustment)\"))\n  )\n\n# Create the plot\np <- ggplot(plot_data, aes(x = ConditionLabel, y = AAE, fill = ErrorType)) +\n  geom_bar(stat = \"identity\", position = \"dodge\", width = 0.7) +\n  scale_fill_manual(values = c(\"Model (no adjustment)\" = \"#3498db\", \n                              \"Actual (with adjustment)\" = \"#e74c3c\")) +\n  labs(\n    title = \"Error Rates for Model Choosers by Adjustment Condition\",\n    subtitle = \"Comparing model performance with and without adjustments\",\n    x = \"Condition\",\n    y = \"Average Absolute Error (AAE)\",\n    fill = \"Error Type\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 16, face = \"bold\", hjust = 0.5),\n    plot.subtitle = element_text(size = 12, hjust = 0.5),\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    legend.position = \"top\",\n    legend.title = element_text(face = \"bold\"),\n    panel.grid.minor = element_blank()\n  ) +\n  scale_y_continuous(limits = c(0, max(plot_data$AAE, na.rm = TRUE) * 1.1), \n                     breaks = pretty_breaks(n = 5)) +\n  # Add percentage change labels on top of bars\n  geom_text(data = summary_stats %>% filter(!is.na(AAE_mean) & Condition != 1),\n            aes(x = ConditionLabel, y = AAE_mean + 0.5, \n                label = paste0(ifelse(Difference >= 0, \"+\", \"\"), \n                               round(PercentChange, 1), \"%\")),\n            inherit.aes = FALSE,\n            size = 3.5,\n            fontface = \"bold\")\n\n# Display the plot\nprint(p)\n```\n\n::: {.cell-output-display}\n![](is-following-ai-advice-anchoring_files/figure-html/dietvorst-error-1.png){width=672}\n:::\n:::\n\n\n\nGRAVEYARD\n\nOne approach to achieve complementary performance is the use of explanations. These explanations might provide information about the AI model (global explanations), about the particular recommendation (local explanations) or some combination of the two. Conceptually, this could help the user build a mental model of the AI and its recommendations, better understanding where it is accurate and where it is more likely to err.\n\nUnfortunately, most experiments find that explanations don't help the user calibrate their use of AI outputs. Explanations tend to increase trust in the AI, leading people to follow the AI recommendation, but don't appear to enable discernment of good and bad AI decisions. Increased trust is a good thing if the person was deviating from the AI too often, but it falls somewhat short of what is required for complementary performance.\n\nIn seeking to understand and ameliorate this problem,",
    "supporting": [
      "is-following-ai-advice-anchoring_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}