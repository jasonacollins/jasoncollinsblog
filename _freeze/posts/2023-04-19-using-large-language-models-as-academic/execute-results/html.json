{
  "hash": "50860be33d0704ad0a9ab34958958377",
  "result": {
    "markdown": "---\ntitle: 'Using large language models as an academic'\nauthor: \"Jason Collins\"\ndate: 2023-04-18 09:00:00+10:00\ndraft: true\naliases:\n  - /using-large-language-models-as-an-academic\n---\n\n\nThis post started as a draft email to my colleagues about how I was using large language models (ChatGPT, Github CoPilot etc.) in my work to achieve some fairly large efficiency gains. I realised that this was easier to send as a blog post. And why not share more broadly?\n\nI am a long way from the frontier in how I am using theses tools. If you want to see the frontier, go read [Ethan Mollick](ADDLINK) among others. But I would estimate I'm using these tools more than 95% of academics, so hopefully there is something in there for many of you.\n\n## My toolkit\n\nAs an academic, I gain free access to [Github Copilot](ADDLINK). (It's only \\$10 a month otherwise.) CoPilot is badged as \"your coding companion\", providing auto-complete suggestions in coding environments. But while badged this way, CoPilot also provides text completion more broadly, and that is how I largely use it.\n\nCoPilot gives me access to whatever language models Microsoft is running behind it. I'm not sure if GPT-4 is available through CoPilot yet, but I would say performance on CoPilot has improved over the six months I have been using it.\n\nTo access CoPilot, I write in [Visual Studio Code](ADDLINK). Since I started blogging (12 years now!) I have always written in markdown (and at the moment a particular flavour of markdown, [Quarto](ADDLINK)). That means most my writing has been in [RStudio](ADDLINK), Visual Studio Code or a text editor that supports markdown and allows me to render to HTML and pdf. With CoPilot not available in RStudio, I'm almost exclusively in Visual Studio Code now. For those who want more specific guidance on how to set this up, I've added that at the bottom of this post.\n\nI also work with [ChatGPT](ADDLINK) open in a browser. If CoPilot isn't giving me what I want, ChatGPT often will. If I can frame my requirement as a standalone question, ChatGPT tends to do the better job. But when working on a document, CoPilot's autocomplete is normally better (unless I paste the whole document into ChatGPT).\n\nI've tried Bing, but find it harder work to get what I want unless I have a question for which I want references.\n\n## Writing lecture notes (efficiency gain: 10 to 30%)\n\nI don't find CoPilot particularly useful if I'm writing something original such as a blog post. In writing this post I wouldn't say CoPilot has increased my efficiency at all (outside of helping me create the examples). I sometimes accepted suggestions from CoPilot, but the need to filter through the various suggestions results in negligible efficiency gain.\n\nBut for lecture notes, which are more mechanical, there are some clear gains. The notes for my undergraduate behavioural economics subject (which you can access [here](https://behaviouraleconomics.jasoncollins.blog)) were produced with CoPilot on.\n\nOften the notes include a statement of a concept followed by a step-by-step example or illustration. The gain comes when you're writing those steps.\n\nFor example, in a section about decision making under risk, I might write:\n\n> Bill is a risk-averse agent with utility function $u(x)=\\text{ln}(x)$ and wealth of \\$100. He is offered a 50:50 bet to win \\$100, lose \\$50.\n> \n> Will Bill accept the bet?\n\nCoPilot will then do one of two things. CoPilot will write something like:\n\n> Can someone please help me answer the following question: \"Will Bill accept the bet?\"\n\nThere must be a lot of study cheat sites forming part of the training data.\n\nBut if CoPilot does that, simply write the first couple of words of the answer \"First, we calculate...\" and then CoPilot is away.\n\n> First, we calculate the expected utility of each option. The expected utility of the bet is:\n\n\n::: {.cell}\n\n```{.tex .cell-code}\n\n\n\\begin{aligned}\nEU(\\text{bet}) &= 0.5 \\times u(100) + 0.5 \\times u(-50) \\\\\n&= 0.5 \\times \\text{ln}(100) + 0.5 \\times \\text{ln}(50) \\\\\n&= 4.259\n\\end{aligned}\n```\n:::\n\n\n> The expected utility of not taking the bet is:\n\n\n```{=tex}\n\\begin{aligned}\nEU(\\text{no bet}) &= U(100) \\\\\n&= \\text{ln}(100) \\\\\n&= 4.605\n\\end{aligned}\n```\n\n\n> As the expected utility of the bet is less than the expected utility of not taking the bet, Bill will not accept the bet.\n\nNot bad for a few seconds work. For those unfamiliar with the mathematical notation, this is $\\LaTeX$, which renders into nice equations like this.\n\n\\begin{aligned}\nEU(\\text{bet}) &= 0.5 \\times u(100) + 0.5 \\times u(-50) \\\\\n&= 0.5 \\times \\text{ln}(100) + 0.5 \\times \\text{ln}(50) \\\\\n&= 4.259\n\\end{aligned}\n\nI might beef this up with a better explanation, or paste the question into ChatGPT direct, where I tend to get more detailed answers. GPT-4 also tends to be better at math (CoPilot has done OK here - maybe more evidence of improvement in CoPilot).\n\nCoPilot is also great when I'm doing repetitive tasks such as describing the elements of an equation or diagram. Start describing the first element and it will give you the rest. And if you write a point followed by \"Conversely, ...\", CoPilot is often on the money.\n\n## Coding (efficiency gain: 10x)\n\nDespite spending a lot of time in R, I am a crap coder. The simplest errors have me crawling Stack Overflow for hours. I struggle to build a structure when looking at a blank screen.\n\nMy typical approach now is to write a clear comment, let CoPilot do the first cut and then tweak the code until it is in shape.\n\nThat tweaking isn't manual tweaking either.\n\nIf there is an error or problem that I can't seem to resolve, I'll simply write the following:\n\n>The above code generates the error \"PASTE ERROR\". I fix it by ...\n\nAnd a solution appears. Sometimes I'll also paste the code and the error into ChatGPT and ask for code that does not have that error. \n\nIf the code looks overly complicated, a request to simplify often yields good results. (I've been asking ChatGPT to review some of my old code recently. I'm a bit embarrassed at how much more efficiently I could have written it.)\n\nHere are two recent coding applications that I was particularly excited about.\n\n### Creating diagrams\n\nIn my lecture notes, I use ggplot to produce nice-looking graphs. (I'm slowly replacing old PowerPoint diagrams that still litter the notes).\n\nRecently I wanted to generate a chart showing a [probability weighting function](ADDLINK) from prospect theory.\n\nI created an R code block and typed a comment:\n\n> \\#Plot of probability weighting function using ggplot2\n\nHere's the result with the diagram rendered below:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Plot of probability weighting function using ggplot2\nlibrary(ggplot2)\n\n#Define probability weighting function\nprob_weight <- function(p, alpha){\n  exp(-(-log(p))^alpha)\n}\n\n#Create data frame of probabilities and weights\nprob <- seq(0, 1, 0.001)\nprob_df <- data.frame(prob = prob, weight = prob_weight(prob, 0.6))\n\n#Plot\nggplot(prob_df, aes(x = prob, y = weight)) +\n  geom_line() +\n\n  #Add labels\n  labs(x = \"Probability\", y = \"Weight\")\n```\n\n::: {.cell-output-display}\n![](2023-04-19-using-large-language-models-as-academic_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\nExactly what I wanted. When I implemented in the notes, all I did was tweaked the formatting and added a 45 degree line.\n\nThis offering includes the functional form of Prelec's (1998 ADD REFERENCE) probability weighting function. Whether it picked that up from my earlier text in the notes or just gave the most common function for probability weighting, I don't know, but it's exactly what I would have used if doing it manually.\n\nCoPilot didn't offer this full chunk of code at once. Each comment was offered then the piece of code after it, one after the other. But the only work I did was writing the first comment and pressing tab several times as each succeeding comment or chunk of code was suggested. (CoPilot has certainly increased the level of comments in my code too.)\n\nOne thing I have noted is that CoPilot uses all of the text in the document in giving it's suggestions. In writing this post I wasn't able to replicate what I did when writing the lecture notes. I got better results in the lecture notes themselves. A few paragraphs about probability weighting before I ask for the code generates much better results than asking for the chart straight up.\n\n### Converting code across languages\n\nMy second example relates to a proposed reproduction of the results in Berkeley Dietvorst's [Algorithm aversion: People erroneously avoid algorithms after seeing them err](https://doi.org/10.1037/xge0000033). Data and the code for analysis was [available on ResearchBox](ADD LINK).\n\nThe problem was that the code was in Stata, I don't have Stata, and I have no desire to learn Stata. So, I pasted the code into ChatGPT and asked for a conversion to R. What I received was 90% of the way there. Initially there was an error, but ChatGPT solved the error first shot. I then had to tweak a couple of the variable manipulations and specify that the t-tests were paired t-tests, and that was it. In less than 10 minutes I had code that was reproducing exactly the results from Study 1.\n\n## Generating quiz questions (efficiency gain: 3x)\n\nOne of the best ways to learn is to be tested. As a result, I offer students in my undergraduate subject a series of practice quizzes that they can work through. In the first class I tell the students about the effectiveness of spaced repetition as a learning technique. They can then use the practice quiz questions on an intermittent basis to test whether they have learnt core concepts. (When I find the bandwidth, I plan to implement those quiz questions in [Orbit](ADDLINK) and place them through the [online notes](https://behaviouraleconomics.jasoncollins.blog).)\n\nSome questions are easy to generate: definitions and the like. But it's hard work to generate questions in bulk and, if I want multiple-choice questions, I sometimes struggle to generate plausible sounding but incorrect answers.\n\nTo generate questions for these quizzes, I now ask ChatGPT to generate them. In the prompts I vary in the specificity of the questions. \"Give me 20 multiple choice questions testing the concept of loss aversion.\" \"Give me 20 multiple choice questions testing prospect theory.\" And so on. Out of each batch of 20 only a few will be suitable. There might be no correct answer, or two correct answers, or ChatGPT might confuse concepts. But by tweaking my instruction - say, describing the level of the students (undergraduate) or more explicitly defining the concept - it doesn't take long to get 10 or so good questions.\n\nI haven't used ChatGPT to generate assessable quiz questions yet, but I'm planning to use it for the upcoming final exam. That exam is a closed-book AI-invigilated exam, so is less vulnerable to someone simply feeding the questions back to ChatGPT. One idea I'm tempted to try is to feed it some previous year's exams and ask for new exams on the same concepts.\n\n## Writing organisational fluff (efficiency gain: 2x)\n\nWork in any decent-sized modern organisation and you will have to write some level of fluff to satisfy the higher-ups, clients, government requirements and the like.\n\nI am hopeless at those tasks. My writing is bland, lacks a depth of vocabulary and is, I like to think, to the point. I have to invest heavily to make fluff sound decent.\n\nMy approach to these exercises depends on the degree of pointlessness.\n\nIf the task relates to a process that will have zero impact on what anyone will do, I simply give the task to ChatGPT, let it do the first cut and then tweak as required. One or two sentences of guidance often gets you 80% of the way there.\n\nIf I think there is a positive benefit to the task, or it's for public consumption, I'll be more proactive first up. I will write a rough draft first, not caring much about the writing but making sure the concepts I want to include are there. It might be in dot points. Then I'll ask ChatGPT for a version that is \"clearer\", \"simpler\" or \"better written\" or \"for a ten-year old\". I often find this process works best in two stages. The \"clearer\" version often uses the same words as me, but structures them better. Then I ask ChatGPT to write it again, but instructing it to \"forget about the original text.\" I've been pleasantly surprised at how good some of those second versions are. \n\n## Slide filler (efficiency gain 2x)\n\nI'm not a big fan of pointless eye candy in slide decks. But if you're doing pre-recorded material and you don't want your mug on the screen, you sometimes need some filler. In that case, I head straight to [DALL-E](https://labs.openai.com GET FULL LINK). I've adopted a theme for my lecture slides - black and white line drawings - so I ask for a black and white line drawing of something related to what I am talking about.\n\nThis is much quicker than hunting for images with open usage rights.\n\nCompared to ChatGPT, DALL-E seems pretty lame. It still struggles with concepts such as \"on\"  or \"ten\" or \"without writing\". Sometimes I'll give up on a more complex concept (a deck of cards on a table) and go for something simpler (a deck of cards). I'm looking forward to GPT-4 sitting behind an image generator.\n\nI've also tried [Stable Diffusion](ADDLINK) but found it was much harder work to get what I want.\n\n## What's next?\n\n### AI-voiced lectures\n\nMy next frontier is artificial voice generation for my pre-recorded lectures.\n\nLast year - my first year taking the undergraduate behavioural economics subject - the subject was structured as two hours of live (but online) lectures and one hour of tutorials (either online or in-person). Both lectures and the online tutorial were recorded, so students could watch at another time. The net result was limited attendance or engagement. (And most students didn't watch the videos either.)\n\nThis year I thought I'd move the subject out of the 1950s and make it a combination of some [short pre-recorded videos](ADD YOUTUBE CHANNEL), interactive online seminars and tutorials. This approach has backfired - the students don't watch the videos and don't want to interact - but that's a story for another post, perhaps in combination with my review of Bryan Caplan's The Case Against Education.\n\nTo produce the videos I've written scripts - which also form the [subject notes](ADDLINK) - and I read that accompanied by slides. I'm a hopeless off-the-cuff speaker, so for anything that's going to be recorded and treated as a long-term asset, I'm scripting.\n\nThere are three pain points in this exercise. First, I speak too fast (even when I think I'm speaking slow) and I enunciating many words poorly. Second, recording takes a lot of time. I rarely get a two to five minute video in a single take. I have to trim the ends. Third, if I want to tweak the recording, I need to either re-record solid chunks of text or fiddle around with video edits.\n\nSo, why not get an artificial voice to do the speaking?\n\nI've been experimenting with [Murf.ai](LINK), [Speechify](LINK) and [play.ht](LINK), and am leaning toward Murf.ai at this point. The available voices still sound a bit robotic, but with the ability to pace their speech, they're already a better option than me. And they'll only get better.\n\nThe production process is then easy. Upload the text (although I do have to write out the equations in the course notes in text). Upload the slides and match to the text. Click WHAT WORD and the new video is available for download.\n\nWith a setup such as this, if I want to tweak parts of the videos, I can simply edit the written text and generate a new version. Adding or editing slides is also simple as they are uploaded and matched to the new text. I'm also able to run much sharper slide changes as I can time exactly the transitions.\n\nAn interesting question about the artificial voice-over is how the university (and students) will take it. Do they see it as a lazy option? (When I taught at the University of Sydney in 2020 during the early stages of the pandemic, you weren't allowed to use pre-recorded videos from one semester in a later semester. Someone hated the idea that the academics might not be working hard enough ...)\n\nI think the artificial voice-over is a great option. Academics should be investing in developing high-quality assets that can be re-used rather than bumbling along with low-quality off-the-cuff lectures. They can then invest their time in the other parts of teaching: the in-person seminars and tutorials where students can get the personalised bit of their education.\n\nThis approach does point to a challenge for higher education providers. Once you've got high-quality lectures for basic courses, why would you want them generated by each lecturer in each university? Someone could produce an amazing Behavioural Economics 101 (I want the Dave Malan [CS50](https://www.edx.org/course/introduction-computer-science-harvardx-cs50x) version of this), other lecturers can subscribe to the service and they can now focus on those human bits - the in-person seminars and tutorials where they can get the tailored bit of their education (not that most students want that...).\n\nI've also started looking at options to create an AI avatar - either based on videos/photos of me or a random AI avatar - for some parts of the videos. I'm leaving that one for the moment but can see myself revisiting in the next year.\n\n### Chat integrated with CoPilot (and other Microsoft tools)\n\nAt the moment I have multiple streams of access to these tools. I'm looking forward to chat appearing in Copilot - I'm on the waiting list to access - which will remove the need for the separate browser with ChatGPT.\n\nGPT-4 is also coming to other Microsoft tools soon. If I don't have to exit PowerPoint to get my images, that's an extra efficiency.\n\nI'm largely in the Mac ecosystem, but Apple seems absolutely crap in the world of assistants/chat. (\"Hey Siri, tell me this most basic fact about the world.\" \"I've sent some irrelevant web results to your iPhone.....\") If at some point these tools become tied to Microsoft hardware, I'm moving.\n\n(I exited the Google ecosystem when I had trouble sharing a file because it breached the terms of service. It was a document that included the words \"vaccination\" and \"scepticism\", although if you see the [resulting post](LINK), the Google algorithm was obviously pretty crude. At that point I opted for what I hope is a bit more privacy and jettisoned Google...)\n\n## Getting going with Github Copilot\n\nThe below gives the basic steps to achieve my setup.\n\nIf you are an academic, you can sign up for (free) academic access to CoPilot at [this link](ADDLINK). If you're not an academic or student, sign up for \\$10 a month [here](ADDLINK). You'll need a Github account to do this. I think the \\$10 a month is worth it, but the free ChatGPT will give you most of these gains with a bit more work.\n\nVisual Studio Code is available [here](). (I use the Mac version.) Download and install.\n\nInstall the Github CoPilot extension into Visual Studio Code (or the Github CoPilot Nightly extension if you want access to features earlier). Instructions on installing the extension are [here]().\n\nOnce installed you will need to login to your CoPilot account. EXPLAIN WHAT HAPPENS\n\nUsing CoPilot is easy. You simply type. As you type, you will be given suggestions.\n\nI write in markdown (or as noted above, a flavour of markdown called [Quarto](ADDLINK)). It allows me to include $\\LaTeX$ math and R computations within any document (as I have in this post).\n\nIf you're an academic stuck in the $\\LaTeX$ ecosystem, you can also write in Visual Studio Code wholly in $\\LaTeX$. (Although I say jettison that dinosaur and use $\\LaTeX$ math in a markdown document - that ability to add computations is worth it, and the readability and experience so much easier.)\n",
    "supporting": [
      "2023-04-19-using-large-language-models-as-academic_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}